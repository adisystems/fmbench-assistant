[
  {
    "filename": "README.md",
    "path": "README.md",
    "directory": "",
    "extension": "md",
    "content": "================================================\n<h1 align=\"center\">\n        <img src=\"https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/img/fmbt-small.png?raw=true\" width=\"25\"></img> FMBench\n</h1>\n\n<p align=\"center\">\n    <p align=\"center\">Benchmark any Foundation Model (FM) on any AWS Generative AI service [Amazon SageMaker, Amazon Bedrock, Amazon EKS, Amazon EC2, or Bring your own endpoint.]\n    <br>\n</p>\n<h4 align=\"center\"><a href=\"\" target=\"_blank\">Amazon Bedrock</a> | <a href=\"\" target=\"_blank\">Amazon SageMaker</a> | <a href=\"\" target=\"_blank\">Amazon EKS</a> | <a href=\"\" target=\"_blank\">Amazon EC2</a></h4>\n<h4 align=\"center\">\n    <a href=\"https://pypi.org/project/fmbench/\" target=\"_blank\">\n        <img src=\"https://img.shields.io/pypi/v/fmbench.svg\" alt=\"PyPI Version\">\n    </a>    \n</h4>\n\n\n\ud83d\udea8 **What's new**: Benchmarks for [Deepseek-R1](https://github.com/deepseek-ai/DeepSeek-R1) models on Amazon EC2 and Amazon SageMaker. Faster setup with `uv` for Python venv and dependency installation.  \ud83d\udea8\n\n`FMBench` is a Python package for running performance benchmarks and accuracy for **any Foundation Model (FM)** deployed on **any AWS Generative AI service**, be it **Amazon SageMaker**, **Amazon Bedrock**, **Amazon EKS**, or **Amazon EC2**. The FMs could be deployed on these platforms either directly through `FMbench`, or, if they are already deployed then also they could be benchmarked through the **Bring your own endpoint** mode supported by `FMBench`. \n\nHere are some salient features of `FMBench`:\n\n1. **Highly flexible**: in that it allows for using any combinations of instance types (`g5`, `p4d`, `p5`, `Inf2`), inference containers (`DeepSpeed`, `TensorRT`, `HuggingFace TGI` and others) and parameters such as tensor parallelism, rolling batch etc. as long as those are supported by the underlying platform. \n\n1. **Benchmark any model**: it can be used to be benchmark _open-source models_, _third party models_, and _proprietary models_ trained by enterprises on their own data. Benchmarking includes both performance benchmaking and model evaluations (accuracy measurement given ground truth). \ud83d\udea8 **NEW**: Model evaluations done by a **Panel of LLM Evaluators** added in release 2.0.0 \ud83d\udea8\n\n1. **Run anywhere**: it can be run on any AWS platform where we can run Python, such as Amazon EC2, Amazon SageMaker, or even the AWS CloudShell. _It is important to run this tool on an AWS platform so that internet round trip time does not get included in the end-to-end response time latency_.\n\n#### Intro Video\n\n[![FMBench Intro](img/fmbench-thumbnail.png)](https://www.youtube.com/watch?v=yvRCyS0J90c)\n\n#### Determine the optimal price|performance serving stack for your generative AI workload\n\nUse `FMBench` to benchmark an LLM on any AWS generative AI service for price and performance (inference latency, transactions/minute). Here is one of the plots generated by `FMBench` to help answer the price performance question for the `Llama2-13b` model when hosted on Amazon SageMaker (_the instance types in the legend have been blurred out on purpose, you can find them in the actual plot generated on running `FMBench`_).\n\n![business question](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/img/business_summary.png?raw=true)\n\n#### Determine the optimal model for your generative AI workload\n\nUse `FMBench` to determine model accuracy using a panel of LLM evaluators (PoLL [[1]](#1)). Here is one of the plots generated by `FMBench` to help answer the accuracy question for various FMs on Amazon Bedrock (the model ids in the charts have been blurred out on purpose, you can find them in the actual plot generated on running FMBench).\n\n![Accuracy trajectory with prompt size](img/accuracy_trajectory_per_payload.png)\n\n![Overall accuracy](img/overall_candidate_model_majority_voting_accuracy.png)\n\n## Models benchmarked\n\nConfiguration files are available in the [configs](./src/fmbench/configs) folder for the following models in this repo.\n\n### Full list of benchmarked models\n\n\n| Model                           | Amazon EC2                     | Amazon SageMaker                           | Amazon Bedrock                     |\n|:--------------------------------|:-------------------------------|:-------------------------------------------|:-----------------------------------|\n| **Deepseek-R1 distilled**        | g6e                           | g6e                                           |                            |\n| **Llama3.3-70b instruct**        |                               |                                           | On-demand                           |\n| **Qwen2.5-72b**                  | g5, g6e                       |                                           |                                    |\n| **Amazon Nova**                  |                               |                                           | On-demand                          |\n| **Anthropic Claude-3 Sonnet**    |                               |                                           | On-demand, provisioned             |\n| **Anthropic Claude-3 Haiku**     |                               |                                           | On-demand                          |\n| **Mistral-7b-instruct**          | inf2, trn1                     | g4dn, g5, p3, p4d, p5                       | On-demand                          |\n| **Mistral-7b-AWQ**               |                               | p5                                        |                                    |\n| **Mixtral-8x7b-instruct**        |                               |                                           | On-demand                          |\n| **Llama3.2-1b instruct**         | g5                            |                                           |                                    |\n| **Llama3.2-3b instruct**         | g5                            |                                           |                                    |\n| **Llama3.1-8b instruct**         | g5, p4d, p4de, p5, p5e, g6e, g6, inf2, trn1        | g4dn, g5, p3, inf2, trn1                     | On-demand                          |\n| **Llama3.1-70b instruct**        | p4d, p4de, p5, p5e, g6e, g5, inf2, trn1            | inf2, trn1                                 | On-demand                          |\n| **Llama3-8b instruct**           | g5, g6e, inf2, trn1, c8g      | g4dn, g5, p3, inf2, trn1, p4d, p5e             | On-demand                          |\n| **Llama3-70b instruct**          | g5                            | g4dn, g5, p3, inf2, trn1, p4d                 | On-demand                          |\n| **Llama2-13b chat**              |                               | g4dn, g5, p3, inf2, trn1, p4d                 | On-demand                          |\n| **Llama2-70b chat**              |                               | g4dn, g5, p3, inf2, trn1, p4d                 | On-demand                          |\n| **NousResearch-Hermes-70b**      |                               | g5, inf2, trn1                            | On-demand                          |\n| **Amazon Titan text lite**       |                               |                                           | On-demand                          |\n| **Amazon Titan text express**    |                               |                                           | On-demand                          |\n| **Cohere Command text**          |                               |                                           | On-demand                          |\n| **Cohere Command light text**    |                               |                                           | On-demand                          |\n| **AI21 J2 Mid**                  |                               |                                           | On-demand                          |\n| **AI21 J2 Ultra**                |                               |                                           | On-demand                          |\n| **Gemma-2b**                     |                               | g4dn, g5, p3                                |                                    |\n| **Phi-3-mini-4k-instruct**       |                               | g4dn, g5, p3                                |                                    |\n| **distilbert-base-uncased**      |                               | g4dn, g5, p3                                |                                    |\n\n## New in this release\n\n## 2.1.4\n\n1. `Llama3.1-8b` config file for `p5en` instance type.\n1. Remove `vllm` from `pyproject.toml`.\n\n## 2.1.3\n\n1. SGLang support.\n\n## 2.1.2\n\n1. Deepseek prompt updates.\n1. Handle case for < 1 txn/minute.\n\n## 2.1.1\n\n1. Optimized prompt templates and config files for DeepSeek-R1 and Amazon Nova for `ConvFinQA` and `LongBench` datasets.\n\n\n\n[Release history](./release_history.md)\n\n## Getting started\n\n`FMBench` is available as a Python package on [PyPi](https://pypi.org/project/fmbench) and is run as a command line tool once it is installed. All data that includes metrics, reports and results are stored in an Amazon S3 bucket. \n\n> [!IMPORTANT]\n> \ud83d\udca1 [All **documentation** for `FMBench` is available on the `FMBench` website](https://aws-samples.github.io/foundation-model-benchmarking-tool/index.html)  \n\nYou can run `FMBench` on either a SageMaker notebook or on an EC2 VM. Both options are described [here](https://aws-samples.github.io/foundation-model-benchmarking-tool/gettingstarted.html) as part of the documentation. You can even run `FMBench` as a [Docker container](https://aws-samples.github.io/foundation-model-benchmarking-tool/run_as_container.html) A Quickstart guide for SageMaker is bring provided below as well.\n\n\ud83d\udc49 The following sections are discussing running `FMBench` the tool, as different from where the FM is actually deployed. For example, we could run `FMBench` on EC2 but the model being deployed is on SageMaker or even Bedrock. \n\n### Quickstart\n\n**_FMBench on a Amazon SageMaker Notebook_**\n\n1. Each `FMBench` run works with a configuration file that contains the information about the model, the deployment steps, and the tests to run. A typical `FMBench` workflow involves either directly using an already provided config file from the [`configs`](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/main/src/fmbench/configs) folder in the `FMBench` GitHub repo or editing an already provided config file as per your own requirements (say you want to try benchmarking on a different instance type, or a different inference container etc.).\n\n    \ud83d\udc49 A simple config file with key parameters annotated is included in this repo, see [`config-llama2-7b-g5-quick.yml`](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/main/src/fmbench/configs/llama2/7b/config-llama2-7b-g5-quick.yml). This file benchmarks performance of Llama2-7b on an `ml.g5.xlarge` instance and an `ml.g5.2xlarge` instance. You can use this config file as it is for this Quickstart.\n\n1. Launch the AWS CloudFormation template included in this repository using one of the buttons from the table below. The CloudFormation template creates the following resources within your AWS account: Amazon S3 buckets, Amazon IAM role and an Amazon SageMaker Notebook with this repository cloned. A read S3 bucket is created which contains all the files (configuration files, datasets) required to run `FMBench` and a write S3 bucket is created which will hold the metrics and reports generated by `FMBench`. The CloudFormation stack takes about 5-minutes to create.\n\n   |AWS Region                |     Link        |\n   |:------------------------:|:-----------:|\n   |us-east-1 (N. Virginia)    | [<img src=\"./img/ML-FMBT-cloudformation-launch-stack.png\">](https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=fmbench&templateURL=https://aws-blogs-artifacts-public.s3.amazonaws.com/artifacts/ML-FMBT/template.yml) |\n   |us-west-2 (Oregon)    | [<img src=\"./img/ML-FMBT-cloudformation-launch-stack.png\">](https://console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/new?stackName=fmbench&templateURL=https://aws-blogs-artifacts-public.s3.amazonaws.com/artifacts/ML-FMBT/template.yml) |\n   |us-gov-west-1 (GovCloud West)    | [<img src=\"./img/ML-FMBT-cloudformation-launch-stack.png\">](https://us-gov-west-1.console.amazonaws-us-gov.com/cloudformation/home?region=us-gov-west-1#/stacks/new?stackName=fmbench&templateURL=https://aws-blogs-artifacts-public.s3.amazonaws.com/artifacts/ML-FMBT/template.yml) |\n\n1. Once the CloudFormation stack is created, navigate to SageMaker Notebooks and open the `fmbench-notebook`.\n\n1. On the `fmbench-notebook` open a Terminal and run the following commands.\n\n    ```{.bash}\n    curl -LsSf https://astral.sh/uv/install.sh | sh\n    export PATH=\"$HOME/.local/bin:$PATH\"\n    uv venv .fmbench_python312 --python 3.12\n    source .fmbench_python312/bin/activate\n    uv pip install -U fmbench\n    ```\n\n1. Now you are ready to `fmbench` with the following command line. We will use a sample config file placed in the S3 bucket by the CloudFormation stack for a quick first run.\n    \n    1. We benchmark performance for the `Llama2-7b` model on a `ml.g5.xlarge` and a `ml.g5.2xlarge` instance type, using the `huggingface-pytorch-tgi-inference` inference container. This test would take about 30 minutes to complete and cost about $0.20.\n    \n    1. It uses a simple relationship of 750 words equals 1000 tokens, to get a more accurate representation of token counts use the `Llama2 tokenizer` (instructions are provided in the next section). ***It is strongly recommended that for more accurate results on token throughput you use a tokenizer specific to the model you are testing rather than the default tokenizer. See instructions provided later in this document on how to use a custom tokenizer***.\n\n        ```{.bash}\n        account=`aws sts get-caller-identity | jq .Account | tr -d '\"'`\n        region=`aws configure get region`\n        fmbench --config-file s3://sagemaker-fmbench-read-${region}-${account}/configs/llama2/7b/config-llama2-7b-g5-quick.yml > fmbench.log 2>&1\n        ```\n\n    1. Open another terminal window and do a `tail -f` on the `fmbench.log` file to see all the traces being generated at runtime.\n    \n        ```{.bash}\n        tail -f fmbench.log\n        ```\n        \n    1. \ud83d\udc49 For streaming support on SageMaker and Bedrock checkout these config files:\n        1. [config-llama3-8b-g5-streaming.yml](src/configs/llama3/8b/config-llama3-8b-g5-streaming.yml)\n        1. [config-bedrock-llama3-streaming.yml](src/configs/bedrock/config-bedrock-llama3-streaming.yml)\n    \n1. The generated reports and metrics are available in the `sagemaker-fmbench-write-<replace_w_your_aws_region>-<replace_w_your_aws_account_id>` bucket. The metrics and report files are also downloaded locally and in the `results` directory (created by `FMBench`) and the benchmarking report is available as a markdown file called `report.md` in the `results` directory. You can view the rendered Markdown report in the SageMaker notebook itself or download the metrics and report files to your machine for offline analysis.\n\n_If you would like to understand what is being done under the hood by the CloudFormation template, see [the DIY version with gory details](./misc/the-diy-version-w-gory-details.md)_\n\n#### `FMBench` on SageMaker in GovCloud\n\nNo special steps are required for running `FMBench` on GovCloud. The CloudFormation link for `us-gov-west-1` has been provided in the section above.\n\n1. Not all models available via Bedrock or other services may be available in GovCloud. The following commands show how to run `FMBench` to benchmark the [Amazon Titan Text Express](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-text-models.html#titantx-express) model in the GovCloud. See the [Amazon Bedrock GovCloud](https://docs.aws.amazon.com/govcloud-us/latest/UserGuide/govcloud-bedrock.html) page for more details.\n\n```{.bash}\naccount=`aws sts get-caller-identity | jq .Account | tr -d '\"'`\nregion=`aws configure get region`\nfmbench --config-file s3://sagemaker-fmbench-read-${region}-${account}/configs/bedrock/config-bedrock-titan-text-express.yml > fmbench.log 2>&1\n```\n\n## Running `FMBench` via the `FMBench-orchestrator`\n\n**_FMBench on Amazon EC2 via the FMBench orchestrator_**\n\nIf you want to benchmark FMs on Amazon EC2 then you can use the [`fmbench-orchestrator`](https://github.com/awslabs/fmbench-orchestrator) as a quick and simple way to get started. The `orchestrator` is a Python program that can be installed on an EC2 machine and it in turn launches other EC2 machines for benchmarking purposes. The orchestrator installs and runs `FMBench` on these EC2 machines, downloads the benchmarking result from these machines and finally terminates these machines once the benchmarking finished.\n\nAs an example, consider a scenario that you want to benchmark say the `Llama3.1-8b` model on a `g5.2xlarge`, `g6.2xlarge`, `p4d.24xlarge`, `p5e.48xlarge` and a `trn1.32xlarge`. Usually this would mean that you have to create these EC2 instances, install the pre-requisites, installed FMBench, run FMBench, download the results and then repeat the process for the next instance. This is tedious work. The orchestrator makes this super convenient by doing all this for you and doing this in parallel. It will spawn all these EC2 VMs and do all the steps mentioned above and at the end of the test you will have results from all the instances downloaded on the orchestrator VM and all the EC2 VMs that were spawned would have automatically been terminated. See the [`orchestrator README`](https://github.com/awslabs/fmbench-orchestrator?tab=readme-ov-file#overview) for more details.\n\n\n## Results\n\nDepending upon the experiments in the config file, the `FMBench` run may take a few minutes to several hours. Once the run completes, you can find the report and metrics in the local `results-*` folder in the directory from where `FMBench` was run. The rpeort and metrics are also written to the write S3 bucket set in the [config file](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/src/fmbench/configs/config-mistral-7b-tgi-g5.yml#L12).\n\nHere is a screenshot of the `report.md` file generated by `FMBench`.\n![Report](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/img/results.gif?raw=true)\n\n\n## Benchmark models deployed on different AWS Generative AI services ([Docs](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking.html))\n\n`FMBench` comes packaged with configuration files for benchmarking models on different AWS Generative AI services i.e. Bedrock, SageMaker, EKS and EC2 or bring your own endpoint even.\n\n# Enhancements\n\nView the [ISSUES](https://github.com/aws-samples/foundation-model-benchmarking-tool/issues) on GitHub and add any you might think be an beneficial iteration to this benchmarking harness.\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis library is licensed under the MIT-0 License. See the [LICENSE](./LICENSE) file.\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=aws-samples/foundation-model-benchmarking-tool&type=Date)](https://star-history.com/#aws-samples/foundation-model-benchmarking-tool&Date)\n\n[![Stargazers repo roster for @aws-samples/foundation-model-benchmarking-tool](https://reporoster.com/stars/aws-samples/foundation-model-benchmarking-tool)](https://github.com/aws-samples/foundation-model-benchmarking-tool/stargazers)\n\n## Support\n\n- Schedule Demo \ud83d\udc4b - send us an email \ud83d\ude42\n- [Community Discord \ud83d\udcad](https://discord.gg/ydXV8mYFtF)\n- Our emails \u2709\ufe0f aroraai@amazon.com / madhurpt@amazon.com\n\n\n## Contributors\n\n<a href=\"https://github.com/aws-samples/foundation-model-benchmarking-tool/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=aws-samples/foundation-model-benchmarking-tool\" />\n</a>\n\n## References\n<a id=\"1\">[1]</a> \n[Pat Verga et al., \"Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models\",    arXiv:2404.18796, 2024.](https://arxiv.org/abs/2404.18796)\n\n\n\n================================================"
  },
  {
    "filename": "check_s3_content.sh",
    "path": "check_s3_content.sh",
    "directory": "",
    "extension": "sh",
    "content": "================================================\n# this scripts creates a local directory for running FMBench\n# without any s3 dependency and copies all relevant files\n# for public FMBench content\n#  RUN THIS SCRIPT AS A SANITY CHECK BEFORE EVERY NEW RELEASE\n\n# sanity check to confirm that all config files and other files listed in manifest.txt are \n# indeed present in the blogs bucket. If a file is not present in the blogs bucket then the\n# CloudFormation template would error out.\nBUCKET=aws-blogs-artifacts-public\nCONFIGS_ONLY=configs\nlist_of_files_to_be_uploaded=()\nfor i in `cat manifest.txt | grep $CONFIGS_ONLY`\ndo\n  wget -q --spider https://${BUCKET}.s3.amazonaws.com/artifacts/ML-FMBT/$i --no-check-certificate\n  result=$?\n  if [ $result -eq 0 ]; then\n    #echo $i exists in the ${BUCKET}\n    : # noop\n    # check if this file is differnet from what we have in the repo\n    # if so then we need to flag for uploading the latest version to\n    # the bucket\n    # compare the contents of the downloaded file with the local file\n    TEMP_FILE=/tmp/tempfile\n    REMOTE_FILE=https://${BUCKET}.s3.amazonaws.com/artifacts/ML-FMBT/$i\n    wget -q $REMOTE_FILE --no-check-certificate -O $TEMP_FILE\n    \n    LOCAL_FILE=fmbench/$i\n    if diff \"$TEMP_FILE\" \"$LOCAL_FILE\" -w> /dev/null; then\n        #echo \"The contents of the remote file and the local file are identical.\"\n        : # noop\n    else\n        echo \"contents of the $REMOTE_FILE and the local $LOCAL_FILE are different, needs to be updated manually\"\n        list_of_files_to_be_uploaded+=($LOCAL_FILE)\n    fi\n    rm -f $TEMP_FILE\n\n  else\n    echo $i does not exist in the ${BUCKET}, copy it there manually\n    list_of_files_to_be_uploaded+=($i)\n  fi\ndone\n\n# print a full list of all files to be uploaded\necho \"here is a list of all the files to be uploaded\"\nfor item in \"${list_of_files_to_be_uploaded[@]}\"; do\n  echo \"$item\"\ndone\n\n\n\n\n================================================"
  },
  {
    "filename": "CITATION.cff",
    "path": "CITATION.cff",
    "directory": "",
    "extension": "cff",
    "content": "================================================\ncff-version: 1.2.0\nmessage: \"If you use this software, please cite it as below.\"\nauthors:\n- family-names: \"Arora\"\n  given-names: \"Amit\"\n  orcid: \"https://orcid.org/0000-0001-6697-2724\"\n- family-names: \"Prashant\"\n  given-names: \"Madhur\"\n  orcid: \"https://orcid.org/0009-0002-4086-2003\"\n\ntitle: \"FMBench: benchmark any foundation model on any AWS GenAI service\"\nversion: 2.0.1\ndoi: 10.5281/zenodo.13324418\ndate-released: 2024-08-14\nurl: \"https://aws-samples.github.io/foundation-model-benchmarking-tool/\"\n\n\n\n================================================"
  },
  {
    "filename": "CODE_OF_CONDUCT.md",
    "path": "CODE_OF_CONDUCT.md",
    "directory": "",
    "extension": "md",
    "content": "================================================\n## Code of Conduct\nThis project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct).\nFor more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact\nopensource-codeofconduct@amazon.com with any additional questions or comments.\n\n\n\n================================================"
  },
  {
    "filename": "CONTRIBUTING.md",
    "path": "CONTRIBUTING.md",
    "directory": "",
    "extension": "md",
    "content": "================================================\n# Contributing Guidelines\n\nThank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional\ndocumentation, we greatly value feedback and contributions from our community.\n\nPlease read through this document before submitting any issues or pull requests to ensure we have all the necessary\ninformation to effectively respond to your bug report or contribution.\n\n\n## Reporting Bugs/Feature Requests\n\nWe welcome you to use the GitHub issue tracker to report bugs or suggest features.\n\nWhen filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already\nreported the issue. Please try to include as much information as you can. Details like these are incredibly useful:\n\n* A reproducible test case or series of steps\n* The version of our code being used\n* Any modifications you've made relevant to the bug\n* Anything unusual about your environment or deployment\n\n\n## Contributing via Pull Requests\nContributions via pull requests are much appreciated. Before sending us a pull request, please ensure that:\n\n1. You are working against the latest source on the *main* branch.\n2. You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already.\n3. You open an issue to discuss any significant work - we would hate for your time to be wasted.\n\nTo send us a pull request, please:\n\n1. Fork the repository.\n2. Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change.\n3. Ensure local tests pass.\n4. Commit to your fork using clear commit messages.\n5. Send us a pull request, answering any default questions in the pull request interface.\n6. Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation.\n\nGitHub provides additional document on [forking a repository](https://help.github.com/articles/fork-a-repo/) and\n[creating a pull request](https://help.github.com/articles/creating-a-pull-request/).\n\n\n## Finding contributions to work on\nLooking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start.\n\n\n## Code of Conduct\nThis project has adopted the [Amazon Open Source Code of Conduct](https://aws.github.io/code-of-conduct).\nFor more information see the [Code of Conduct FAQ](https://aws.github.io/code-of-conduct-faq) or contact\nopensource-codeofconduct@amazon.com with any additional questions or comments.\n\n\n## Security issue notifications\nIf you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our [vulnerability reporting page](http://aws.amazon.com/security/vulnerability-reporting/). Please do **not** create a public github issue.\n\n\n## Licensing\n\nSee the [LICENSE](LICENSE) file for our project's licensing. We will ask you to confirm the licensing of your contribution.\n\n\n\n================================================"
  },
  {
    "filename": "copy_s3_content.sh",
    "path": "copy_s3_content.sh",
    "directory": "",
    "extension": "sh",
    "content": "================================================\n#!/bin/bash\n# this scripts creates a local directory for running FMBench\n# without any s3 dependency and copies all relevant files\n# for public FMBench content\nBASE_DIR=${1:-/tmp}\nFMBENCH_READ_DIR=$BASE_DIR/fmbench-read\nFMBENCH_WRITE_DIR=$BASE_DIR/fmbench-write\nBUCKET=aws-blogs-artifacts-public\n\nmkdir -p $FMBENCH_READ_DIR\nmkdir -p $FMBENCH_READ_DIR/tokenizer\nmkdir -p $FMBENCH_READ_DIR/llama2_tokenizer\nmkdir -p $FMBENCH_READ_DIR/llama3_tokenizer\nmkdir -p $FMBENCH_READ_DIR/llama3_1_tokenizer\nmkdir -p $FMBENCH_READ_DIR/llama3_2_tokenizer\nmkdir -p $FMBENCH_READ_DIR/mistral_tokenizer\nwget https://${BUCKET}.s3.amazonaws.com/artifacts/ML-FMBT/manifest.txt -P ${FMBENCH_READ_DIR}/\n\n# First create all directories from manifest\nfor i in `cat ${FMBENCH_READ_DIR}/manifest.txt`\ndo\n  dir_path=`dirname $i`\n  mkdir -p ${FMBENCH_READ_DIR}/$dir_path\ndone\n\n# Then download all non-.keep files\nfor i in `cat ${FMBENCH_READ_DIR}/manifest.txt`\ndo\n  # Skip if filename contains \".keep\" in it\n  if echo \"$i\" | grep -q \".keep\"; then\n    continue\n  fi\n  dir_path=`dirname $i`\n  wget https://${BUCKET}.s3.amazonaws.com/artifacts/ML-FMBT/$i -P ${FMBENCH_READ_DIR}/$dir_path\ndone\n\n\n\n================================================"
  },
  {
    "filename": "create_manifest.py",
    "path": "create_manifest.py",
    "directory": "",
    "extension": "py",
    "content": "================================================\n\"\"\"\nScript to create a manifest.txt file. The manifest.txt file\nlists which (public) files should be copied from the source bucket\nto the account specific FMBench bucket\n\"\"\"\nimport os\nimport glob\nfrom typing import List\nfrom pathlib import Path\n\nMANIFEST_FILE: str = \"manifest.txt\"\nMANIFEST_MD_FILE: str = \"manifest.md\"\n\nBASE_FILE_LIST: List[str] = [\"prompt_template/.keep\",\n                             \"tokenizer/.keep\",\n                             \"llama2_tokenizer/.keep\",\n                             \"llama3_tokenizer/.keep\",\n                             \"llama3_1_tokenizer/.keep\",\n                             \"llama3_2_tokenizer/.keep\",\n                             \"mistral_tokenizer/.keep\",\n                             \"phi_tokenizer/.keep\",\n                             \"scripts/.keep\",\n                             \"configs/pricing.yml\",\n                             \"configs/pricing_fallback.yml\",\n                             # add new datasets here\n                             \"source_data/2wikimqa_e.jsonl\",\n                             \"source_data/2wikimqa.jsonl\",\n                             \"source_data/hotpotqa_e.jsonl\",\n                             \"source_data/hotpotqa.jsonl\",\n                             \"source_data/narrativeqa.jsonl\",\n                             \"source_data/triviaqa_e.jsonl\",\n                             \"source_data/triviaqa.jsonl\",\n                             \"source_data/just_text.jsonl\",\n                             \"source_data/500_token_prompts_synthetic_data.jsonl\",\n                             \"source_data/synthetic_data_large_prompts.jsonl\",\n                             \"source_data/LICENSE.txt\",\n                             \"source_data/THIRD_PARTY_LICENSES.txt\"]\n\nimport subprocess\nimport re\n\ndef get_tree_output(directory='.'):\n    \"\"\"Get the output of the `tree` command for a given directory.\"\"\"\n    result = subprocess.run(['tree', '-f', directory], capture_output=True, text=True)\n    return result.stdout\n\ndef convert_to_markdown_links(tree_output, directory):\n    \"\"\"Convert tree command output to Markdown hyperlinks.\"\"\"\n    lines = tree_output.splitlines()\n    markdown_links = []\n\n    # Regex to match file paths by excluding tree structure characters\n    path_pattern = re.compile(r'\\s*(?:\u251c\u2500\u2500|\u2514\u2500\u2500|\u2500|\u2502)?\\s*(.*)')\n\n    for line in lines:\n        match = path_pattern.match(line)\n        if match:\n            path = match.group(1).strip()\n            if path:\n                # Format the path as a Markdown hyperlink\n                path_link = path.replace('\u251c\u2500\u2500 ', '')\\\n                    .replace('\u2514\u2500\u2500', '')\\\n                    .replace('\u2502\u00a0\u00a0 ', '')\\\n                    .strip()\n                path_readable = path.replace(f\"{directory}/\", '')\n                # if the path is adirectory then we dont want to put a link for it\n                # because we dont support directory listing, it will just return a broken\n                # page\n                from pathlib import Path\n                if Path(path).suffix == '':\n                    markdown_link = f\"**{path_readable}**  \"\n                else:\n                    markdown_link = f'[{path_readable}]({path_link})  '\n                markdown_links.append(markdown_link)\n\n    # remove the first line, it is just name of the configs directory\n    # remove the last line, it is the number of files and directories\n    # like: [14 directories, 69 files](14 directories, 69 files)\n    return '\\n'.join(markdown_links[1:-1])\n\ndef create_dir_listing_as_markdown(directory):\n    tree_output = get_tree_output(directory)\n    markdown_links = convert_to_markdown_links(tree_output, directory)\n    preamble = \"\"\"Here is a listing of the various configuration files available out-of-the-box with `FMBench`. Click on any link to view a file. You can use these files as-is or use them as templates to create a custom configuration file for your use-case of interest.\\n\\n\"\"\"\n    Path(MANIFEST_MD_FILE).write_text(preamble + markdown_links)\n\ndef create_manifest_file(config_yml_dir):\n    config_yml_files = glob.glob(os.path.join(config_yml_dir, \"**/*\", \"*.yml\"),\n                                recursive=True)\n    config_yml_files = [f.replace(\"fmbench\" + \"/\", \"\") for f in config_yml_files]\n    print(f\"there are {len(config_yml_files)} config yml files\")\n\n    # append them to the base list\n    all_manifest_files = config_yml_files + BASE_FILE_LIST\n\n    # sort so that diff between versions is easier to understand\n    all_manifest_files = sorted(all_manifest_files)\n    # and write to manifest.txt\n    written: int = Path(MANIFEST_FILE).write_text(\"\\n\".join([f for f in all_manifest_files]))\n    print(f\"written {written} bytes to {MANIFEST_FILE}\")\n\n# all .yml files in the config directory need to be appended to the list above\nconfig_yml_dir = os.path.join(\"fmbench\", \"configs\")\ncreate_manifest_file(config_yml_dir)\n\n# create the directory listing to put on the website\nDOCS_DIR: str = \"docs\"\nCONFIG_DIR_FOR_LISTING: str = os.path.join(\"configs\")\ncwd = os.getcwd()\nos.chdir(DOCS_DIR)\ncreate_dir_listing_as_markdown(CONFIG_DIR_FOR_LISTING)\nos.chdir(cwd)\n\n\n\n================================================"
  },
  {
    "filename": "debug.sh",
    "path": "debug.sh",
    "directory": "",
    "extension": "sh",
    "content": "================================================\n# script for a debug/developer workflow\n# 1. Builds and install a local wheel\n# 2. There is no step 2 :)\n\nCONFIG_FILE_PATH=fmbench/configs/deepseek/config-deepseek-r1-sglang.yml\nLOGFILE=fmbench.log\n\nuv build\nuv pip install -U dist/*.whl\n\n# run the newly installed version\necho \"going to run fmbench now\"\nfmbench --config-file $CONFIG_FILE_PATH  --local-mode yes --write-bucket placeholder --tmp-dir /tmp -A model_id=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B -A hf_tokenizer_model_id=deepseek-ai/DeepSeek-R1 -A instance_type=g6e.xlarge -A results_dir=DeepSeek-R1-Distill-Qwen-1.5B -A prompt_template=prompt_template_deepseek_longbench.txt  > $LOGFILE 2>&1\n\n# Use FMBench to benchmark models on hosted on EC2 using the command below. If you want to write the metrics and results to an\n# s3 bucket, replace `placeholder` with the name of that s3 bucket in your AWS account. Optionally, you can send the results to\n# a custom tmp directory by setting the '--tmp-dir' argument followed by the path to that custom tmp directory. If '--tmp-dir' is not\n# provided, the default 'tmp' directory will be used.\n#fmbench --config-file $CONFIG_FILE_PATH --local-mode yes --write-bucket placeholder --tmp-dir /path/to/your_tmp_directory > $LOGFILE 2>&1\necho \"all done\"\n\n\n\n================================================"
  },
  {
    "filename": "Dockerfile",
    "path": "Dockerfile",
    "directory": "",
    "extension": "",
    "content": "================================================\n# Use the Amazon Linux 2023 image\nFROM public.ecr.aws/amazonlinux/amazonlinux:latest\n\nENV PYTHONUNBUFFERED=1\n\n# Install necessary packages\nRUN yum install -y --skip-broken wget jq aws-cli bzip2 ca-certificates curl git gcc gcc-c++ make openssl-devel libffi-devel zlib-devel && \\\n    yum clean all\n    \n# Install Miniconda\nRUN wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O /tmp/miniconda.sh && \\\n    bash /tmp/miniconda.sh -b -p /opt/miniconda && \\\n    rm /tmp/miniconda.sh && \\\n    /opt/miniconda/bin/conda clean --all --yes\n\n# Set conda environment variables\nENV PATH=\"/opt/miniconda/bin:$PATH\"\nENV CONDA_AUTO_UPDATE_CONDA=false\n\n# Create and activate the conda environment\nRUN conda create --name fmbench_python311 -y python=3.11 ipykernel && \\\n    echo \"source activate fmbench_python311\" > ~/.bashrc\n\n# Install fmbench\nRUN /opt/miniconda/bin/conda run -n fmbench_python311 pip install -U fmbench\n\n# Set working directorys\nWORKDIR /app\n\n# Set the entrypoint to run the fmbench command\nENTRYPOINT [\"conda\", \"run\", \"-n\", \"fmbench_python311\", \"sh\", \"-c\"]\nCMD [\"fmbench\", \"--help\"]\n\n\n================================================"
  },
  {
    "filename": "ec2_template.yaml",
    "path": "ec2_template.yaml",
    "directory": "",
    "extension": "yaml",
    "content": "================================================\nAWSTemplateFormatVersion: '2010-09-09'\nDescription: 'CloudFormation template for FMBench set up on an EC2 instance'\n\nParameters:\n     KeyPairName:\n       Type: AWS::EC2::KeyPair::KeyName\n       Description: Name of an existing EC2 KeyPair to enable SSH access to the instance\n\n# The Deep learning pytorch AMI is different for each of the region\nMappings:\n  RegionMap:\n    us-west-2:\n      AMI: ami-05075044f63a733ad\n    us-east-1:\n      AMI: ami-05c3e698bd0cffe7e\n\nResources:\n  FMBenchSecurityGroup:\n   Type: 'AWS::EC2::SecurityGroup'\n   Properties:\n    GroupDescription: Enable SSH access via port 22\n    SecurityGroupIngress:\n      - IpProtocol: tcp\n        FromPort: 22\n        ToPort: 22\n        CidrIp: 0.0.0.0/0\n  FMBenchKeyPair:\n    Type: 'AWS::EC2::KeyPair'\n    Properties:\n      KeyName: fmbench-key-pair\n  FMBenchEC2Role:\n    Type: 'AWS::IAM::Role'\n    Properties:\n      RoleName: !Sub 'FMBenchEC2Role-${AWS::Region}'\n      AssumeRolePolicyDocument:\n        Version: '2012-10-17'\n        Statement:\n          - Effect: Allow\n            Principal:\n              Service: ec2.amazonaws.com\n            Action: 'sts:AssumeRole'\n          - Effect: Allow\n            Principal:\n              Service: sagemaker.amazonaws.com\n            Action: 'sts:AssumeRole'\n          - Effect: Allow\n            Principal:\n              Service: bedrock.amazonaws.com\n            Action: 'sts:AssumeRole'\n      ManagedPolicyArns:\n        - arn:aws:iam::aws:policy/AmazonSageMakerFullAccess\n        - arn:aws:iam::aws:policy/AmazonBedrockFullAccess\n\n  FMBenchInstanceProfile:\n    Type: 'AWS::IAM::InstanceProfile'\n    Properties:\n      Path: \"/\"\n      Roles:\n        - !Ref FMBenchEC2Role\n\n  FMBenchInstance:\n    Type: AWS::EC2::Instance\n    Properties:\n      ImageId: !FindInMap \n        - RegionMap\n        - !Ref 'AWS::Region'\n        - AMI  # AMI ID for \"Deep Learning OSS Nvidia Driver AMI GPU PyTorch\"\n      InstanceType: g5.12xlarge\n      KeyName: !Ref KeyPairName\n      IamInstanceProfile: !Ref FMBenchInstanceProfile\n      SecurityGroupIds:\n        - !Ref FMBenchSecurityGroup\n      Tags: \n        - Key: Name\n          Value: FMBenchInstance\n        - Key: fmbench-version\n          Value: \"1.0\"\n      BlockDeviceMappings:\n        - DeviceName: /dev/xvda\n          Ebs:\n            VolumeSize: 100\n            VolumeType: gp2\n      UserData:\n        Fn::Base64: !Sub |\n          #!/bin/bash\n          apt-get update\n          apt-get install -y wget\n          wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n          bash Miniconda3-latest-Linux-x86_64.sh -b -p /home/ubuntu/miniconda3\n          rm -f Miniconda3-latest-Linux-x86_64.sh\n          echo 'export PATH=\"/home/ubuntu/miniconda3/bin:$PATH\"' >> /home/ubuntu/.bashrc\n          export PATH=\"/home/ubuntu/miniconda3/bin:$PATH\"\n          conda init bash\n\n          apt-get install --reinstall -y docker.io\n          apt-get install -y docker-compose\n          docker compose version\n\n          conda create --name fmbench_python311 -y python=3.11 ipykernel\n          source activate fmbench_python311\n          pip install -U fmbench\n\n          curl -s https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/copy_s3_content.sh | sh -s -- /tmp\n\nOutputs:\n  InstanceId:\n    Description: InstanceId of the newly created EC2 instance\n    Value: !Ref FMBenchInstance\n  PublicDNS:\n    Description: Public DNSName of the newly created EC2 instance\n    Value: !GetAtt FMBenchInstance.PublicDnsName\n  PublicIP:\n    Description: Public IP address of the newly created EC2 instance\n    Value: !GetAtt FMBenchInstance.PublicIp\n  KeyPairName:\n    Description: Name of the created key pair\n    Value: !Ref FMBenchKeyPair\n  KeyPairFingerprint:\n    Description: Fingerprint of the created key pair\n    Value: !GetAtt FMBenchKeyPair.KeyFingerprint\n\n\n================================================"
  },
  {
    "filename": "LICENSE",
    "path": "LICENSE",
    "directory": "",
    "extension": "",
    "content": "================================================\nMIT No Attribution\n\nCopyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the \"Software\"), to deal in\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of\nthe Software, and to permit persons to whom the Software is furnished to do so.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS\nFOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR\nCOPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER\nIN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN\nCONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\n================================================"
  },
  {
    "filename": "MANIFEST.in",
    "path": "MANIFEST.in",
    "directory": "",
    "extension": "in",
    "content": "================================================\nrecursive-include fmbench *\ninclude LICENSE\ninclude README.md\n\n\n================================================"
  },
  {
    "filename": "manifest.txt",
    "path": "manifest.txt",
    "directory": "",
    "extension": "txt",
    "content": "================================================\nconfigs/NousResearchHermes70B/config-Nous-Hermes3-1-70b-g5.48xl-tp-8-mc-max-djl.yml\nconfigs/NousResearchHermes70B/config-Nous-Hermes3-1-70b-inf2.48xl-triton-tp24.yml\nconfigs/NousResearchHermes70B/config-Nous-Hermes3-1-70b-trn1.32xl-deploy-ec2-triton.yml\nconfigs/bedrock/config-bedrock-all-anthropic-models-longbench-data.yml\nconfigs/bedrock/config-bedrock-anthropic-models-OpenOrca.yml\nconfigs/bedrock/config-bedrock-claude.yml\nconfigs/bedrock/config-bedrock-evals-only-conc-1.yml\nconfigs/bedrock/config-bedrock-haiku-sonnet-majority-voting.yml\nconfigs/bedrock/config-bedrock-llama3-1-70b-streaming.yml\nconfigs/bedrock/config-bedrock-llama3-1-8b-streaming.yml\nconfigs/bedrock/config-bedrock-llama3-1-no-streaming.yml\nconfigs/bedrock/config-bedrock-llama3-1.yml\nconfigs/bedrock/config-bedrock-llama3-streaming.yml\nconfigs/bedrock/config-bedrock-models-OpenOrca.yml\nconfigs/bedrock/config-bedrock-titan-text-express.yml\nconfigs/bedrock/config-bedrock.yml\nconfigs/bedrock/config-claude-3-5-sonnet-v2.yml\nconfigs/bedrock/config-claude-dolly-dataset.yml\nconfigs/bedrock/config-llama-3-2-11b-databricks-dolly-15k.yml\nconfigs/bedrock/config-llama-3-2-1b-3b-no-evals.yml\nconfigs/bedrock/config-llama-3-2-1b-3b.yml\nconfigs/bedrock/config-llama-3-2-all-models-longbench-hf-version.yml\nconfigs/bedrock/config-llama-3-2-all-models.yml\nconfigs/bedrock/config-llama-3-3-all-models-open-orca.yml\nconfigs/bedrock/config-llama-3-3-all-models.yml\nconfigs/bedrock/config-nova-all-models-convfinqa.yml\nconfigs/bedrock/config-nova-all-models-dolly-dataset.yml\nconfigs/bedrock/config-nova-all-models-openarca.yml\nconfigs/bedrock/config-nova-all-models.yml\nconfigs/bert/config-distilbert-base-uncased.yml\nconfigs/byoe/config-byo-custom-rest-predictor-tinyllama.yml\nconfigs/byoe/config-byo-custom-rest-predictor.yml\nconfigs/byoe/config-model-byo-sagemaker-endpoint.yml\nconfigs/deepseek/config-deepseek-r1-ollama.yml\nconfigs/deepseek/config-deepseek-r1-quant1.58-longbench-byoe.yml\nconfigs/deepseek/config-deepseek-r1-sglang.yml\nconfigs/deepseek/config-deepseek-r1-vllm-convfinqa.yml\nconfigs/deepseek/config-deepseek-r1-vllm-longbench.yml\nconfigs/deepseek/config-deepseek-r1-vllm-openorca.yml\nconfigs/embeddings/bge-base-en-v1-5-c5-embeddings.yml\nconfigs/embeddings/bge-base-en-v1-5-g5-embeddings.yml\nconfigs/embeddings/bge-base-en-v1-5-g5-g4dn-c7-embeddings.yml\nconfigs/gemma/config-gemma-2b-g5.yml\nconfigs/generic/ec2/Qwen2.5_djl.yml\nconfigs/generic/ec2/djl.yml\nconfigs/generic/ec2/llama3.1_djl.yml\nconfigs/llama2/13b/config-bedrock-sagemaker-llama2.yml\nconfigs/llama2/13b/config-byo-rest-ep-llama2-13b.yml\nconfigs/llama2/13b/config-llama2-13b-inf2-g5-p4d.yml\nconfigs/llama2/13b/config-llama2-13b-inf2-g5.yml\nconfigs/llama2/70b/config-ec2-llama2-70b.yml\nconfigs/llama2/70b/config-llama2-70b-g5-p4d-tgi.yml\nconfigs/llama2/70b/config-llama2-70b-g5-p4d-trt.yml\nconfigs/llama2/70b/config-llama2-70b-inf2-g5.yml\nconfigs/llama2/7b/config-llama2-7b-byo-sagemaker-endpoint.yml\nconfigs/llama2/7b/config-llama2-7b-g4dn-g5-trt.yml\nconfigs/llama2/7b/config-llama2-7b-g5-no-s3-quick.yml\nconfigs/llama2/7b/config-llama2-7b-g5-quick.yml\nconfigs/llama2/7b/config-llama2-7b-inf2-g5.yml\nconfigs/llama3.1/70b/config-ec2-llama3-1-70b-inf2-48xl-deploy-ec2-djl.yml\nconfigs/llama3.1/70b/config-ec2-llama3-1-70b-inf2-deploy-sm.yml\nconfigs/llama3.1/70b/config-ec2-llama3-1-70b-p4de.24xl-deploy-ec2-large-prompts.yml\nconfigs/llama3.1/70b/config-ec2-llama3-1-70b-p4de.24xl-deploy-ec2-longbench.yml\nconfigs/llama3.1/70b/config-ec2-llama3-1-70b-p4de.24xl-deploy-ec2-summarization.yml\nconfigs/llama3.1/70b/config-llama3-1-70b-g5.48xl-tp-8-mc-max-djl.yml\nconfigs/llama3.1/70b/config-llama3-1-70b-g6.48xl-tp-8-mc-max-djl.yml\nconfigs/llama3.1/70b/config-llama3-1-70b-g6e.24xl-tp-4-mc-max-djl.yml\nconfigs/llama3.1/70b/config-llama3-1-70b-g6e.48xl-tp-8-mc-max-djl.yml\nconfigs/llama3.1/70b/config-llama3-1-70b-inf2.48xl-deploy-ec2-triton.yml\nconfigs/llama3.1/70b/config-llama3-1-70b-inf2.48xl-triton-tp24.yml\nconfigs/llama3.1/70b/config-llama3-1-70b-p5-djl-lmi.yml\nconfigs/llama3.1/70b/config-llama3-1-70b-trn1.32xl-deploy-ec2-triton.yml\nconfigs/llama3.1/70b/config-llama3-1-7b-inf2.48xl-triton-ec2.yml\nconfigs/llama3.1/8b/client-config-ec2-llama3-1-8b.yml\nconfigs/llama3.1/8b/config-ec2-llama3-1-8b-g6e-2xlarge-byoe-ollama.yml\nconfigs/llama3.1/8b/config-ec2-llama3-1-8b-inf2-48xl-deploy-ec2-tp24-bs12.yml\nconfigs/llama3.1/8b/config-ec2-llama3-1-8b-inf2-48xl-deploy-ec2.yml\nconfigs/llama3.1/8b/config-ec2-llama3-1-8b-inf2.yml\nconfigs/llama3.1/8b/config-ec2-llama3-1-8b-p4-tp-2-mc-max.yml\nconfigs/llama3.1/8b/config-ec2-llama3-1-8b-p4-tp-4-mc-max.yml\nconfigs/llama3.1/8b/config-ec2-llama3-1-8b-p4-tp-8-mc-max.yml\nconfigs/llama3.1/8b/config-ec2-llama3-1-8b-p5-tp-2-mc-max.yml\nconfigs/llama3.1/8b/config-ec2-llama3-1-8b-tp-8-mc-auto-p5.yml\nconfigs/llama3.1/8b/config-ec2-llama3-1-8b-trn1-32xl-deploy-ec2-tp32-bs8.yml\nconfigs/llama3.1/8b/config-llama3-1-8b-g5.12xlarge-djl-lmi-sm.yml\nconfigs/llama3.1/8b/config-llama3-1-8b-p5en-djl.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-g5-ec2.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-g5.12xl-tp-2-mc-auto-ec2.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-g5.12xl-tp-2-mc-max-ec2.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-g5.12xl-tp-4-mc-max-ec2.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-g5.24xl-tp-2-mc-auto-ec2.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-g5.24xl-tp-2-mc-max-ec2.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-g5.24xl-tp-4-mc-max-ec2.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-g5.2xl-g5.4xl-sm.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-g5.2xl-tp-1-mc-max-ec2-conc-1-2.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-g5.2xl-tp-1-mc-max-ec2.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-g5.48xl-tp-2-mc-auto-ec2.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-g5.48xl-tp-2-mc-max-ec2.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-g5.48xl-tp-4-mc-max-ec2.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-g5.48xl-tp-8-mc-max-ec2.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-g5.xl-tp-1-mc-max-ec2-conc-1-2.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-g5.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-g6e.12xl-tp-2-mc-max-djl.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-g6e.12xl-tp-4-mc-max-djl.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-g6e.24xl-tp-2-mc-max-djl.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-g6e.24xl-tp-4-mc-max-djl.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-g6e.2xl-ollama.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-g6e.2xl-tp-1-mc-max-djl.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-g6e.48xl-tp-2-mc-max-djl.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-g6e.48xl-tp-4-mc-max-djl.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-g6e.48xl-tp-8-mc-max-djl.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-g6e.4xl-tp-1-mc-max-djl.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-inf2-48xl-deploy-tp-24-ec2.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-inf2-48xl-deploy-tp-8-ec2.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-trn1-32xl-deploy-tp-8-ec2.yml\nconfigs/llama3.1/8b/config-llama3.1-8b-trn32xl-triton-vllm.yml\nconfigs/llama3.1/8b/server-config-ec2-llama3-1-8b-inf2-48xl-deploy-ec2.yml\nconfigs/llama3.2/11b/config-llama3.2-11b-g6e-2xl-tp-1-mc-max-djl-vllm-ec2.yml\nconfigs/llama3.2/1b/config-llama3.2-1b-g5.2xl-summarization-500-50.yml\nconfigs/llama3.2/1b/config-llama3.2-1b-g5.2xl-tp-1-mc-max-djl-ec2.yml\nconfigs/llama3.2/1b/config-llama3.2-1b-g5.4xl-tp-1-mc-max-djl-ec2.yml\nconfigs/llama3.2/1b/config-llama3.2-1b-g6e.2xl-tp-1-mc-max-djl.yml\nconfigs/llama3.2/1b/config-llama3.2-1b-m5-16xlarge-ec2.yml\nconfigs/llama3.2/1b/config-llama3.2-1b-m7a-16xlarge-ec2.yml\nconfigs/llama3.2/1b/config-llama3.2-1b-m7a-24xlarge-ec2-summarization.yml\nconfigs/llama3.2/1b/config-llama3.2-1b-m7a-24xlarge-ec2.yml\nconfigs/llama3.2/1b/config-llama3.2-1b-m7i-12xlarge-ec2.yml\nconfigs/llama3.2/3b/config-llama3.2-3b-g5.4xl-tp-1-mc-max-djl-ec2.yml\nconfigs/llama3/70b/config-bedrock.yml\nconfigs/llama3/70b/config-ec2-llama3-70b-instruct.yml\nconfigs/llama3/70b/config-ec2-neuron-llama3-70b-inf2-48xl-deploy-sm.yml\nconfigs/llama3/70b/config-llama3-70b-instruct-g5-48xl.yml\nconfigs/llama3/70b/config-llama3-70b-instruct-g5-p4d.yml\nconfigs/llama3/70b/config-llama3-70b-instruct-p4d.yml\nconfigs/llama3/8b/config-bedrock.yml\nconfigs/llama3/8b/config-ec2-llama3-8b-c5-18xlarge.yml\nconfigs/llama3/8b/config-ec2-llama3-8b-c8g-24xlarge.yml\nconfigs/llama3/8b/config-ec2-llama3-8b-g6e-2xlarge.yml\nconfigs/llama3/8b/config-ec2-llama3-8b-inf2-48xl.yml\nconfigs/llama3/8b/config-ec2-llama3-8b-m5-16xlarge.yml\nconfigs/llama3/8b/config-ec2-llama3-8b-m7a-16xlarge.yml\nconfigs/llama3/8b/config-ec2-llama3-8b-m7a-24xlarge.yml\nconfigs/llama3/8b/config-ec2-llama3-8b-m7i-12xlarge.yml\nconfigs/llama3/8b/config-ec2-llama3-8b-m7i-16xlarge.yml\nconfigs/llama3/8b/config-ec2-llama3-8b-m7i-24xlarge.yml\nconfigs/llama3/8b/config-ec2-llama3-8b-neuron-trn1-32xl-tp16-sm.yml\nconfigs/llama3/8b/config-ec2-llama3-8b-p4d-tp-2-mc-max.yml\nconfigs/llama3/8b/config-ec2-llama3-8b-p4d-tp-4-mc-max.yml\nconfigs/llama3/8b/config-ec2-llama3-8b-p4d-tp-8-mc-max.yml\nconfigs/llama3/8b/config-ec2-llama3-8b-p4de-tp-2-mc-max.yml\nconfigs/llama3/8b/config-ec2-llama3-8b-p4de-tp-4-mc-max.yml\nconfigs/llama3/8b/config-ec2-llama3-8b-p4de-tp-8-mc-max.yml\nconfigs/llama3/8b/config-ec2-llama3-8b-p5-tp-2-mc-max.yml\nconfigs/llama3/8b/config-ec2-llama3-8b-p5-tp-8-mc-auto.yml\nconfigs/llama3/8b/config-ec2-llama3-8b.yml\nconfigs/llama3/8b/config-ec2-neuron-llama3-8b-inf2-24xl-deploy-sm.yml\nconfigs/llama3/8b/config-ec2-neuron-llama3-8b-inf2-48xl-deploy-sm.yml\nconfigs/llama3/8b/config-llama3-8b-eks-inf2.yml\nconfigs/llama3/8b/config-llama3-8b-g5-streaming.yml\nconfigs/llama3/8b/config-llama3-8b-g5.12xl-tp-2-mc-max-djl-ec2.yml\nconfigs/llama3/8b/config-llama3-8b-g5.12xl-tp-2-mc-max-triton-ec2.yml\nconfigs/llama3/8b/config-llama3-8b-g5.12xl-tp-4-mc-max-djl-ec2.yml\nconfigs/llama3/8b/config-llama3-8b-g5.12xl-tp-4-mc-max-triton-ec2.yml\nconfigs/llama3/8b/config-llama3-8b-g5.48xl-tp-8-mc-max-triton-ec2.yml\nconfigs/llama3/8b/config-llama3-8b-g5.4xl-tp-1-mc-max-triton-ec2.yml\nconfigs/llama3/8b/config-llama3-8b-g6e.12xl-tp-2-mc-max-djl-ec2.yml\nconfigs/llama3/8b/config-llama3-8b-g6e.12xl-tp-2-mc-max-triton-ec2.yml\nconfigs/llama3/8b/config-llama3-8b-g6e.12xl-tp-4-mc-max-djl-ec2.yml\nconfigs/llama3/8b/config-llama3-8b-g6e.12xl-tp-4-mc-max-triton-ec2.yml\nconfigs/llama3/8b/config-llama3-8b-g6e.24xl-tp-2-mc-max-djl-ec2.yml\nconfigs/llama3/8b/config-llama3-8b-g6e.24xl-tp-2-mc-max-triton-ec2.yml\nconfigs/llama3/8b/config-llama3-8b-g6e.24xl-tp-4-mc-max-djl-ec2.yml\nconfigs/llama3/8b/config-llama3-8b-g6e.24xl-tp-4-mc-max-triton-ec2.yml\nconfigs/llama3/8b/config-llama3-8b-g6e.2xl-tp-1-mc-max-triton-ec2.yml\nconfigs/llama3/8b/config-llama3-8b-g6e.48xl-tp-2-mc-max-djl-ec2.yml\nconfigs/llama3/8b/config-llama3-8b-g6e.48xl-tp-2-mc-max-triton-ec2.yml\nconfigs/llama3/8b/config-llama3-8b-g6e.48xl-tp-4-mc-max-djl-ec2.yml\nconfigs/llama3/8b/config-llama3-8b-g6e.48xl-tp-4-mc-max-triton-ec2.yml\nconfigs/llama3/8b/config-llama3-8b-g6e.48xl-tp-8-mc-max-djl-ec2.yml\nconfigs/llama3/8b/config-llama3-8b-g6e.48xl-tp-8-mc-max-triton-ec2.yml\nconfigs/llama3/8b/config-llama3-8b-g6e.4xl-tp-1-mc-max-djl-ec2.yml\nconfigs/llama3/8b/config-llama3-8b-g6e.4xl-tp-1-mc-max-triton-ec2.yml\nconfigs/llama3/8b/config-llama3-8b-g6e.xl-tp-1-mc-max-triton-ec2.yml\nconfigs/llama3/8b/config-llama3-8b-inf2-24xl-tp=8-bs=4-byoe.yml\nconfigs/llama3/8b/config-llama3-8b-inf2-48xl-tp=8-bs=4-byoe.yml\nconfigs/llama3/8b/config-llama3-8b-inf2-48xlarge-triton-djl.yml\nconfigs/llama3/8b/config-llama3-8b-inf2-g5-byoe-w-openorca.yml\nconfigs/llama3/8b/config-llama3-8b-inf2-g5.yml\nconfigs/llama3/8b/config-llama3-8b-instruct-all.yml\nconfigs/llama3/8b/config-llama3-8b-instruct-g5-12xl-4-instances.yml\nconfigs/llama3/8b/config-llama3-8b-instruct-g5-12xl.yml\nconfigs/llama3/8b/config-llama3-8b-instruct-g5-24xl.yml\nconfigs/llama3/8b/config-llama3-8b-instruct-g5-2xl.yml\nconfigs/llama3/8b/config-llama3-8b-instruct-g5-48xl.yml\nconfigs/llama3/8b/config-llama3-8b-instruct-g5-p4d.yml\nconfigs/llama3/8b/config-llama3-8b-instruct-g6-12xl.yml\nconfigs/llama3/8b/config-llama3-8b-instruct-g6-24xl.yml\nconfigs/llama3/8b/config-llama3-8b-instruct-g6-48xl.yml\nconfigs/llama3/8b/config-llama3-8b-instruct-p4d-djl-lmi-dist.yml\nconfigs/llama3/8b/config-llama3-8b-instruct-p4d-djl-vllm.yml\nconfigs/llama3/8b/config-llama3-8b-instruct-p5-djl-lmi-dist.yml\nconfigs/llama3/8b/config-llama3-8b-trn1-32xl-tp-16-bs-4-byoe.yml\nconfigs/llama3/8b/config-llama3-8b-trn1-32xl-tp-8-bs-4-byoe.yml\nconfigs/llama3/8b/config-llama3-8b-trn1-32xl-tp16-bs-4-ec2.yml\nconfigs/llama3/8b/config-llama3-8b-trn1-32xlarge-triton-djl.yml\nconfigs/llama3/8b/config-llama3-8b-trn1-32xlarge-triton-vllm.yml\nconfigs/llama3/8b/config-llama3-8b-trn1.yml\nconfigs/llama3/8b/llama3-8b-inf2-24xl-byoe-g5-12xl.yml\nconfigs/llama3/8b/llama3-8b-inf2-48xl-byoe-g5-24xl.yml\nconfigs/llama3/8b/llama3-8b-trn1-32xl-byoe-g5-24xl.yml\nconfigs/mistral/config-mistral-7b-eks-inf2.yml\nconfigs/mistral/config-mistral-7b-tgi-g5.yml\nconfigs/mistral/config-mistral-7b-trn1-32xl-triton.yml\nconfigs/mistral/config-mistral-instruct-AWQ-p4d.yml\nconfigs/mistral/config-mistral-instruct-AWQ-p5-byo-ep.yml\nconfigs/mistral/config-mistral-instruct-AWQ-p5.yml\nconfigs/mistral/config-mistral-instruct-p4d.yml\nconfigs/mistral/config-mistral-instruct-v1-p5-trtllm.yml\nconfigs/mistral/config-mistral-instruct-v2-p4d-lmi-dist.yml\nconfigs/mistral/config-mistral-instruct-v2-p4d-trtllm.yml\nconfigs/mistral/config-mistral-instruct-v2-p5-lmi-dist.yml\nconfigs/mistral/config-mistral-instruct-v2-p5-trtllm.yml\nconfigs/mistral/config-mistral-trn1-32xl-deploy-ec2-tp32.yml\nconfigs/mistral/config-mistral-v3-inf2-48xl-deploy-ec2-tp24.yml\nconfigs/mixtral/config-mixtral-8x7b-g6e.48xl-ec2.yml\nconfigs/multimodal/bedrock/config-claude-scienceqa.yml\nconfigs/multimodal/bedrock/config-llama-3-2-11b-vision-instruct-marqo-GS-10M.yml\nconfigs/multimodal/bedrock/config-llama-3-2-11b-vision-instruct-scienceqa.yml\nconfigs/multimodal/bedrock/config-llama-3-2-claude-models-scienceqa.yml\nconfigs/phi/config-phi-3-g5.yml\nconfigs/pricing.yml\nconfigs/pricing_fallback.yml\nllama2_tokenizer/.keep\nllama3_1_tokenizer/.keep\nllama3_2_tokenizer/.keep\nllama3_tokenizer/.keep\nmistral_tokenizer/.keep\nphi_tokenizer/.keep\nprompt_template/.keep\nscripts/.keep\nsource_data/2wikimqa.jsonl\nsource_data/2wikimqa_e.jsonl\nsource_data/500_token_prompts_synthetic_data.jsonl\nsource_data/LICENSE.txt\nsource_data/THIRD_PARTY_LICENSES.txt\nsource_data/hotpotqa.jsonl\nsource_data/hotpotqa_e.jsonl\nsource_data/just_text.jsonl\nsource_data/narrativeqa.jsonl\nsource_data/synthetic_data_large_prompts.jsonl\nsource_data/triviaqa.jsonl\nsource_data/triviaqa_e.jsonl\ntokenizer/.keep\n\n\n================================================"
  },
  {
    "filename": "mkdocs.yml",
    "path": "mkdocs.yml",
    "directory": "",
    "extension": "yml",
    "content": "================================================\nsite_name: Foundation Model Benchmarking Tool (FMBench)\nrepo_name: aws-samples/foundation-model-benchmarking-tool\nrepo_url: https://github.com/aws-samples/foundation-model-benchmarking-tool\nedit_uri: https://github.com/aws-samples/foundation-model-benchmarking-tool/edit/master/\nsite_url: https://foundation-model-benchmarking-tool.github.io\nuse_directory_urls: false\n \n\nmarkdown_extensions:\n  - toc:\n      permalink: true\n  - pymdownx.highlight:\n      anchor_linenums: true\n      line_spans: __span\n      pygments_lang_class: true\n  - pymdownx.inlinehilite\n  - pymdownx.snippets\n  - pymdownx.superfences\n\ntheme:\n  name: material\n  logo: ./img/fmbt-small.png\n  favicon: website/img/favicon.png\n  features:\n    - navigation\n    - content.code.copy\n  palette:\n      - scheme: default\n        primary: light-blue\n        accent: light-blue\n        toggle:\n            icon: material/weather-sunny\n            name: Switch to dark mode\n      - scheme: slate\n        primary: black\n        accent: deep-orange\n        toggle:\n            icon: material/weather-night\n            name: Switch to light mode\n  edit_uri: ''\n\nextra:\n  analytics:\n    provider: google\n    property: G-MX1DMWF4GF\n  social:\n    - icon: fontawesome/brands/github-alt\n      link: https://github.com/aws-samples/foundation-model-benchmarking-tool\n    - icon: fontawesome/brands/slack\n      link: https://github.com/aws-samples/foundation-model-benchmarking-tool\nplugins:\n  - search\n  - mknotebooks\n\nnav: \n  - Home: \n      - index.md\n      - Announcement: announcement.md\n  - Getting started:\n      - Main: gettingstarted.md\n      - CLI: cli.md\n      - Features: features.md\n      - Workflow: workflow.md\n      - SageMaker: quickstart.md      \n      - EC2: ec2.md\n      - Quarto: quarto.md\n  - Benchmarking:\n      - Main: benchmarking.md\n      - Model evaluations: accuracy.md\n      - Deepseek: deepseek.md\n      - Bedrock: benchmarking_on_bedrock.md\n      - Sagemaker: benchmarking_on_sagemaker.md\n      - EKS: benchmarking_on_eks.md\n      - EC2: benchmarking_on_ec2.md\n      - Neuron: neuron.md\n      - Docker: run_as_container.md\n  \n  - Configs:\n      - Files: manifest.md\n  - Advanced functionality:\n      - Main: advanced.md\n      - Customizations: customize_config_files.md\n      - BYO dataset: byo_dataset.md\n      - Build FMBench: build.md\n      - Analytics: analytics.md\n      - Running multiple model copies: mm_copies.md\n\n  - Results:\n      - Report: results.md\n      - Website: website.md\n\n  - Releases:\n      - Major release: announcement.md\n      - releases.md\n  - Resources:\n      - resources.md\n\n\n\n================================================"
  },
  {
    "filename": "pyproject.toml",
    "path": "pyproject.toml",
    "directory": "",
    "extension": "toml",
    "content": "================================================\n[build-system]\nrequires = [\n    \"setuptools>=61.0.0\",  # This version supports newer metadata\n    \"wheel\",\n]\n\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"fmbench\"\nversion = \"2.1.4\"\ndescription = \"Benchmark performance of **any Foundation Model (FM)** deployed on **any AWS Generative AI service**, be it **Amazon SageMaker**, **Amazon Bedrock**, **Amazon EKS**, or **Amazon EC2**. The FMs could be deployed on these platforms either directly through `FMbench`, or, if they are already deployed then also they could be benchmarked through the **Bring your own endpoint** mode supported by `FMBench`.\"\nauthors = [\n    { name = \"Amit Arora\", email = \"aroraai@amazon.com\" },\n    { name = \"Madhur Prashant\", email = \"Madhurpt@amazon.com\" }\n]\n\nreadme = \"README.md\"\nlicense = { text = \"MIT\" }\n\n\nkeywords = [\"benchmarking\", \"sagemaker\", \"bedrock\", \"bring your own endpoint\", \"generative-ai\", \"foundation-models\", \"llama3\", \"ec2\", \"eks\"]\nrequires-python = \">=3.12,<3.13\"\nclassifiers = [\n    \"Programming Language :: Python :: 3\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Operating System :: OS Independent\",\n]\ndependencies = [\n   \"boto3>=1.36.6\",\n   \"datasets>=3.2.0\",\n   \"ec2-metadata>=2.14.0\",\n   \"ipykernel>=6.29.5\",\n   \"ipywidgets>=8.1.5\",\n   \"jinja2>=3.1.5\",\n   \"jupyter>=1.1.1\",\n   \"jupyter-client>=8.6.3\",\n   \"kaleido==0.2.1\",\n   \"litellm>=1.59.8\",\n   \"mkdocs>=1.6.1\",\n   \"mkdocs-material>=9.5.50\",\n   \"mknotebooks>=0.8.0\",\n   \"nvitop>=1.4.1\",\n   \"pandas>=2.2.3\",\n   \"papermill>=2.6.0\",\n   \"pip>=24.3.1\",\n   \"plotly>=5.24.1\",\n   \"psutil>=6.1.1\",\n   \"pydantic>=2.10.6\",\n   \"pyyaml>=6.0.2\",\n   \"ray>=2.41.0\",\n   \"requests>=2.32.3\",\n   \"sagemaker>=2.237.3\",\n   \"seaborn>=0.13.2\",\n   \"sentence-transformers>=3.4.0\",\n   \"tomark>=0.1.4\",\n   \"torch>=2.5.1\",\n   \"tqdm>=4.67.1\",\n   \"transformers>=4.48.1\",\n   \"twine>=6.1.0\",\n   \"typing-extensions>=4.12.2\",\n   \"zmq>=0.0.0\",\n]\n\n[tool.setuptools]\npackages = [\"fmbench\"]\ninclude-package-data = true\n\n[project.scripts]\nfmbench = \"fmbench.main:main\"\n\n\n\n================================================"
  },
  {
    "filename": "release_history.md",
    "path": "release_history.md",
    "directory": "",
    "extension": "md",
    "content": "================================================\n## 2.1.0\n\n1. Deepseek-R1 distilled model support using [`vllm`](https://github.com/vllm-project/vllm).\n1. Evaluate Deepseek performance with `LongBench`, `OpenOrca`, `Dolly` and [`ConvFinQA`](https://huggingface.co/datasets/AdaptLLM/ConvFinQA) datasets.\n1. Replace `conda` with [`uv`](https://docs.astral.sh/uv/) for faster installs.\n\n## 2.0.27\n\n1. Ollama end to end support\n\n## 2.0.26\n\n1. Bug fix for missing HuggingFace token file.\n1. Config file enhancements\n\n## 2.0.25\n\n1. Fix bug with an alternate VariantName for SageMaker BYOE.\n\n## 2.0.24\n\n1. ARM benchmarking support (AWS Graviton 4).\n1. Relax IAM permission requirements for Amazon SageMaker bring your own endpoint.\n\n## 2.0.23\n\n1. Bug fixes for Amazon SageMaker BYOE.\n1. Additional config files.\n\n## 2.0.22\n1. Benchmarks for the [Amazon Nova](https://docs.aws.amazon.com/nova/latest/userguide/what-is-nova.html) family of models.\n1. Benchmarks for multi-modal models: LLama3.2-11B, Claude 3 Sonnet and Claude 3.5 Sonnet using the [ScienceQA](https://huggingface.co/datasets/derek-thomas/ScienceQA) dataset.\n\n## 2.0.21\n1. Dynamically get EC2 pricing from Boto3 API.\n1. Update pricing information and model id for Amazon Bedrock models.\n\n## 2.0.20\n1. Add `hf_tokenizer_model_id` parameter to automatically download tokenizers from Hugging Face.\n\n## 2.0.19\n1. Config files for `Llama3.1-1b` on AMD/Intel CPU instance types.\n1. Bug fixes for token counting for vLLM.\n\n## 2.0.18\n1. Delete SageMaker endpoint as soon as the run finishes.\n\n## 2.0.17\n1. Add support for embedding models through SageMaker jumpstart\n1. Add support for LLama 3.2 11b Vision Instruct benchmarking through FMBench\n1. Fix DJL Inference while deploying djl on EC2(424 Inference bug)\n\n\n## 2.0.16\n1. Update to torch 2.4 for compatibility with SageMaker Notebooks.\n\n## 2.0.15\n1. Support for [Ollama](https://github.com/ollama/ollama), see more details [here](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#benchmarking-models-on-ollama).\n1. Fix bugs with token counting.\n\n\n## 2.0.14\n\n1. `Llama3.1-70b` config files and more.\n1. Support for [`fmbench-orchestrator`](https://github.com/awslabs/fmbench-orchestrator).\n\n## 2.0.13\n\n1. Update `pricing.yml` additional config files.\n\n\n## 2.0.11\n\n1. `Llama3.2-1b` and `Llama3.2-3b` support on EC2 g5.\n1. `Llama3-8b` on EC2 `g6e` instances.\n\n## 2.0.9\n\n1. Triton-djl support for AWS Chips.\n1. Tokenizer files are now downloaded directly from Hugging Face (unless provided manually as before) \n\n## 2.0.8\n\n1. Support Triton-TensorRT for GPU instances and Triton-vllm for AWS Chips.\n1. Misc. bug fixes.\n\n## 2.0.6\n\n1. Run multiple model copies with the DJL serving container and an Nginx load balancer on Amazon EC2.\n1. Config files for `Llama3.1-8b` on `g5`, `p4de` and `p5` Amazon EC2 instance types.\n1. Better analytics for creating internal leaderboards.\n\n## 2.0.5\n\n1. Support for Intel CPU based instances such as `c5.18xlarge` and `m5.16xlarge`.\n\n## 2.0.4\n\n1. Support for AMD CPU based instances such as `m7a`.\n\n## 2.0.3\n\n1. Support for a EFS directory for benchmarking on EC2.\n\n## 2.0.2\n\n1. Code cleanup, minor bug fixes and report improvements.\n\n## 2.0.0\n\n1. \ud83d\udea8 Model evaluations done by a **Panel of LLM Evaluators[[1]](#1)** \ud83d\udea8\n\n## v1.0.52\n\n1. Compile for AWS Chips (Trainium, Inferentia) and deploy to SageMaker directly through `FMBench`.\n1. `Llama3.1-8b` and `Llama3.1-70b` config files for AWS Chips (Trainium, Inferentia).\n1. Misc. bug fixes.\n\n## v1.0.51\n\n1. `FMBench` has a [website](https://aws-samples.github.io/foundation-model-benchmarking-tool/index.html) now. Rework the README file to make it lightweight.\n1. `Llama3.1` config files for Bedrock.\n\n### v1.0.50\n1. `Llama3-8b` on Amazon EC2 `inf2.48xlarge` config file.\n1. Update to new version of DJL LMI (0.28.0).\n\n### v1.0.49\n1. Streaming support for Amazon SageMaker and Amazon Bedrock.\n1. Per-token latency metrics such as time to first token (TTFT) and mean time per-output token (TPOT).\n1. Misc. bug fixes.\n\n### v1.0.48\n1. Faster result file download at the end of a test run.\n1. `Phi-3-mini-4k-instruct` configuration file.\n1. Tokenizer and misc. bug fixes.\n\n### v1.0.47\n1. Run `FMBench` as a Docker container.\n1. Bug fixes for GovCloud support.\n1. Updated README for EKS cluster creation.\n\n### v1.0.46\n1. Native model deployment support for EC2 and EKS (i.e. you can now deploy and benchmark models on EC2 and EKS).\n1. FMBench is now available in GovCloud.\n1. Update to latest version of several packages.\n\n### v1.0.45\n1. Analytics for results across multiple runs.\n1. `Llama3-70b` config files for `g5.48xlarge` instances.\n\n### v1.0.44\n1. Endpoint metrics (CPU/GPU utilization, memory utiliztion, model latency) and invocation metrics (including errors) for SageMaker Endpoints.\n1. `Llama3-8b` config files for `g6` instances.\n\n### v1.0.42\n1. Config file for running `Llama3-8b` on all instance types except `p5`.\n1. Fix bug with business summary chart.\n1. Fix bug with deploying model using a DJL DeepSpeed container in the no S3 dependency mode.\n\n### v1.0.40\n1. Make it easy to run in the Amazon EC2 without any dependency on Amazon S3 dependency mode.\n\n### v1.0.39\n1. Add an internal `FMBench` website.\n\n### v1.0.38\n1. Support for running `FMBench` on Amazon EC2 without any dependency on Amazon S3.\n1. [`Llama3-8b-Instruct`](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) config file for `ml.p5.48xlarge`.\n\n### v1.0.37\n1. `g5`/`p4d`/`inf2`/`trn1` specific config files for [`Llama3-8b-Instruct`](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct).\n    1. `p4d` config file for both `vllm` and `lmi-dist`.\n\n### v1.0.36\n1. Fix bug at higher concurrency levels (20 and above).\n1. Support for instance count > 1.\n\n\n### v1.0.35\n\n1. Support for [Open-Orca](https://huggingface.co/datasets/Open-Orca/OpenOrca) dataset and corresponding prompts for Llama3, Llama2 and Mistral.\n\n### v1.0.34\n1. Don't delete endpoints for the bring your own endpoint case.\n1. Fix bug with business summary chart.\n\n### v1.0.32\n\n1. Report enhancements: New business summary chart, config file embedded in the report, version numbering and others.\n\n1. Additional config files: Meta Llama3 on Inf2, Mistral instruct with `lmi-dist` on `p4d` and `p5` instances.\n\n## 2.0.8\n\n1. Support Triton-TensorRT for GPU instances and Triton-vllm for AWS Chips.\n1. Misc. bug fixes.\n\n\n\n================================================"
  },
  {
    "filename": "render_fmbench_website.py",
    "path": "render_fmbench_website.py",
    "directory": "",
    "extension": "py",
    "content": "================================================\n\"\"\"\nCreate a _quarto.yml file to render all the available reports as a Quarto website.\nUse this _quarto.yml file to render a website using the Quarto docker container.\n\"\"\"\n\nimport os\nimport json\nimport glob\nimport yaml\nimport docker\nimport logging\nimport subprocess\n\nlogging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nRESULTS_FOLDER_PREFIX: str = \"results-\"\nQUARTO_YML_FNAME: str = '_quarto.yml'\n\n# a canned Quarto website YML file, the code below\n# will populate the contents array with the results-* folder\nQUARTO_YML_CONTENT: str =  {\n        'project': {'type': 'website', \n                    'render': ['results-*/report.md'],\n                    'resources': [f'{RESULTS_FOLDER_PREFIX}*/*.csv'],\n                    'output-dir': 'fmbench-website'\n                   },\n        'website': {\n            'title': 'FMBench - Foundation Model Benchmarking Tool',\n            'sidebar': {\n                'style': 'docked',\n                'contents': []\n            },\n            'search': {\n                'limit': 5,\n                'collapse-after': 1,\n                'show-item-context': False\n            }\n        },\n        'format': {'html': {'self-contained': True, 'toc': True}},\n    }\n\nlogger.info(QUARTO_YML_CONTENT)\n\n# read the results folder available\nresult_folders = sorted(glob.glob(RESULTS_FOLDER_PREFIX + \"*/\"))\nlogger.info(f\"there are {len(result_folders)} folders, result_folders={result_folders}\")\n\n\nyml_content = QUARTO_YML_CONTENT\n# Append result_folders to the contents if they are not already present\nfor folder in result_folders:\n    folder_entry = {'text': folder.replace(RESULTS_FOLDER_PREFIX, \"\"), 'href': f'{folder}/report.md'}\n    folder_entry_splitted = folder.split(\"-\")\n    # the second and third tokens constitute the model id\n    # for example results-llama3-8b-trn1-32xl-tp=8-bs=4-byoe\n    # have llama3-8b as the model\n    model_id = f\"{folder_entry_splitted[1]}-{folder_entry_splitted[2]}\"\n    model_id = model_id.lower()\n    if yml_content['website']['sidebar']['contents'] != []:\n        sections = [e['section'] for e in yml_content['website']['sidebar']['contents'] if e]\n    else:\n        sections = []\n    if model_id not in sections:\n        #print(json.dumps(yml_content, indent=2))\n        logger.info(f\"{model_id} not in {sections}, creating a new section\")\n        yml_content['website']['sidebar']['contents'] = [{'section': model_id,\n                                                         'collapsed': True,\n                                                         'contents': [folder_entry]}]\n    else:\n        logger.info(f\"{model_id} is present in {sections}, going to append folder entry to it\")\n        for e in yml_content['website']['sidebar']['contents']:\n            if e['section'] == model_id:\n                e['contents'].append(folder_entry)\n\n# Save the modified YAML content back to the file\nwith open(QUARTO_YML_FNAME, 'w') as yml_fp:\n    yaml.dump(yml_content, yml_fp, default_flow_style=False)\n\nlogger.info(f\"Result folders have been appended to {json.dumps(yml_content, indent=2)}\")\n\n# _quarto.yml created, going to render the website note\ndocker_running = False\n\ntry:\n    docker_client = docker.DockerClient()\n    docker_running = docker_client.ping()\nexcept Exception as e:\n    logger.error(f\"seems like docker is not installed or not running, exception={e}\")\n\nif docker_running is True:\n    cmd = f\"docker run --rm -v $(pwd):/public -w /public -u $(id -u):$(id -g) registry.gitlab.com/quarto-forge/docker/quarto quarto render\"\n    logger.info(f\"going to create self-conained html wensite with cmd=\\\"{cmd}\\\"\")\n    logger.info(\"**this will take a minute or so**\")\n    process = subprocess.Popen(cmd,\n                               stdout=subprocess.PIPE, \n                               stderr=subprocess.PIPE,\n                               text=True,\n                               shell=True)\n    std_out, std_err = process.communicate()\n    logger.info(std_out.strip())\n    logger.info(std_err)\n\nelse:\n    logger.error(f\"docker is not available, not going to create self-conained html report\")\n\n\n================================================"
  },
  {
    "filename": "template.yml",
    "path": "template.yml",
    "directory": "",
    "extension": "yml",
    "content": "================================================\nAWSTemplateFormatVersion: 2010-09-09\nDescription: \"Foundation model benchmarking tool\"\n\nParameters:\n  S3BucketNameForRead:\n    Default: sagemaker-fmbench-read\n    Type: String\n    Description: Name of the Amazon S3 bucket for holding datasets, scripts and tokenizer files. AWS region and account id would be suffixed automatically for uniqueness.\n    MinLength: 1\n    MaxLength: 63\n    AllowedPattern: (?!(^xn--|.+-s3alias$))^[a-z0-9][a-z0-9-]{1,61}[a-z0-9]$\n  S3BucketNameForWrite:\n    Default: sagemaker-fmbench-write\n    Type: String\n    Description: Name of the Amazon S3 bucket for holding metrics and reports. AWS region and account id would be suffixed automatically for uniqueness.\n    MinLength: 1\n    MaxLength: 63\n    AllowedPattern: (?!(^xn--|.+-s3alias$))^[a-z0-9][a-z0-9-]{1,61}[a-z0-9]$\n\nConditions:\n  IsGovCloudPartition: !Equals\n    - !Select [0, !Split [ \"aws-us-gov\", !Ref \"AWS::Partition\"]]\n    - \"\"\n  IsNotGovCloud: !Not [ !Condition IsGovCloudPartition ]\n\nResources:\n  CodeRepository:\n    Type: AWS::SageMaker::CodeRepository\n    Condition: IsNotGovCloud\n    Properties:\n      GitConfig:\n        RepositoryUrl: https://github.com/aws-samples/foundation-model-benchmarking-tool\n\n  NotebookInstance:\n    Type: AWS::SageMaker::NotebookInstance\n    Properties:\n      NotebookInstanceName: !Sub ${AWS::StackName}-notebook\n      InstanceType: ml.m6i.xlarge\n      RoleArn: !GetAtt NotebookRole.Arn\n      DefaultCodeRepository: !If [ IsNotGovCloud, !GetAtt CodeRepository.CodeRepositoryName, !Ref \"AWS::NoValue\" ]\n      VolumeSizeInGB: 200\n\n  NotebookRole:\n    Type: AWS::IAM::Role\n    Properties:\n      RoleName: !Sub ${AWS::StackName}-${AWS::Region}-role\n      ManagedPolicyArns:\n        - !Sub arn:${AWS::Partition}:iam::aws:policy/AmazonSageMakerFullAccess\n        - !Sub arn:${AWS::Partition}:iam::aws:policy/AmazonS3FullAccess\n        - !Sub arn:${AWS::Partition}:iam::aws:policy/AWSCloudFormationReadOnlyAccess\n        - !Sub arn:${AWS::Partition}:iam::aws:policy/AmazonBedrockFullAccess\n        - !Sub arn:${AWS::Partition}:iam::aws:policy/AWSPriceListServiceFullAccess \n      AssumeRolePolicyDocument:\n        Version: 2012-10-17\n        Statement:\n          - Effect: Allow\n            Principal:\n              Service:\n                - sagemaker.amazonaws.com\n            Action:\n              - \"sts:AssumeRole\"\n          - Effect: Allow\n            Principal:\n              Service:\n                - ec2.amazonaws.com\n            Action:\n              - \"sts:AssumeRole\"\n\n  S3BucketForRead:\n    Type: AWS::S3::Bucket\n    Description: Amazon S3 bucket to hold source data\n    Properties:\n      BucketName: !Join\n        - \"-\"\n        - - !Ref S3BucketNameForRead\n          - !Sub ${AWS::Region}\n          - !Sub ${AWS::AccountId}\n\n  S3BucketForWrite:\n    Type: AWS::S3::Bucket\n    Description: Amazon S3 bucket to hold metrics and reports\n    Properties:\n      BucketName: !Join\n        - \"-\"\n        - - !Ref S3BucketNameForWrite\n          - !Sub ${AWS::Region}\n          - !Sub ${AWS::AccountId}\n\n  cleanupReadBucketOnDelete:\n    Type: Custom::cleanupbucket\n    Properties:\n      ServiceToken: !GetAtt \"DeleteS3Bucket.Arn\"\n      BucketName: !Ref S3BucketForRead\n    DependsOn: S3BucketForRead\n\n  cleanupWriteBucketOnDelete:\n    Type: Custom::cleanupbucket\n    Properties:\n      ServiceToken: !GetAtt \"DeleteS3Bucket.Arn\"\n      BucketName: !Ref S3BucketForWrite\n    DependsOn: S3BucketForWrite\n\n  DeleteS3Bucket:\n    Type: AWS::Lambda::Function\n    Properties:\n      Handler: index.lambda_handler\n      Description: \"Delete all objects in S3 bucket\"\n      Timeout: 300\n      Role: !GetAtt \"LambdaBasicExecutionRole.Arn\"\n      Runtime: python3.9\n      Code:\n        ZipFile: |\n          import json, boto3, logging\n          import cfnresponse\n          logger = logging.getLogger()\n          logger.setLevel(logging.INFO)\n\n          def lambda_handler(event, context):\n              logger.info(\"event: {}\".format(event))\n              try:\n                  bucket = event['ResourceProperties']['BucketName']\n                  logger.info(\"bucket: {}, event['RequestType']: {}\".format(bucket,event['RequestType']))\n                  if event['RequestType'] == 'Delete':\n                      s3 = boto3.resource('s3')\n                      bucket = s3.Bucket(bucket)\n                      for obj in bucket.objects.filter():\n                          logger.info(\"delete obj: {}\".format(obj))\n                          s3.Object(bucket.name, obj.key).delete()\n\n                  sendResponseCfn(event, context, cfnresponse.SUCCESS)\n              except Exception as e:\n                  logger.info(\"Exception: {}\".format(e))\n                  sendResponseCfn(event, context, cfnresponse.FAILED)\n\n          def sendResponseCfn(event, context, responseStatus):\n              responseData = {}\n              responseData['Data'] = {}\n              cfnresponse.send(event, context, responseStatus, responseData, \"CustomResourcePhysicalID\")\n\n  CustomSGResource:\n    Type: AWS::CloudFormation::CustomResource\n    Properties:\n      ServiceToken: !GetAtt \"CustomFunctionCopyContentsToS3Bucket.Arn\"\n\n  LambdaBasicExecutionRole:\n    Type: AWS::IAM::Role\n    Properties:\n      AssumeRolePolicyDocument:\n        Statement:\n          - Effect: Allow\n            Principal:\n              Service: lambda.amazonaws.com\n            Action: sts:AssumeRole\n      Path: /\n      Policies:\n        - PolicyName: S3Access\n          PolicyDocument:\n            Version: \"2012-10-17\"\n            Statement:\n              - Effect: Allow\n                Action:\n                  - logs:CreateLogGroup\n                  - logs:CreateLogStream\n                  - logs:PutLogEvents\n                Resource: !Sub arn:${AWS::Partition}:logs:*:*:*\n              - Effect: Allow\n                Action:\n                  - s3:*\n                Resource: \"*\"\n\n  CustomFunctionCopyContentsToS3Bucket:\n    Type: AWS::Lambda::Function\n    Properties:\n      Handler: index.lambda_handler\n      Description: \"Copies files from the Blog bucket to bucket in this account\"\n      MemorySize: 1024\n      EphemeralStorage:\n        Size: 2048\n      Timeout: 900\n      Role: !GetAtt \"LambdaBasicExecutionRole.Arn\"\n      Runtime: python3.9\n      Environment:\n        Variables:\n          READ_BUCKET: !Ref S3BucketForRead\n          WRITE_BUCKET: !Ref S3BucketForWrite\n          MY_AWS_REGION: !Ref AWS::Region\n          ROLE_ARN: !GetAtt \"NotebookRole.Arn\"\n      Code:\n        ZipFile: |\n          import os\n          import boto3\n          import logging\n          import urllib3\n          from urllib3.exceptions import HTTPError\n          import cfnresponse\n\n          logger = logging.getLogger()\n          logger.setLevel(logging.INFO)\n          BLOGS_BUCKET = \"aws-blogs-artifacts-public\"\n          SRC_PREFIX = \"artifacts/ML-FMBT\"\n          MANIFEST = os.path.join(SRC_PREFIX, \"manifest.txt\")\n\n          MANIFEST_URL = f\"https://{BLOGS_BUCKET}.s3.amazonaws.com/{SRC_PREFIX}/manifest.txt\"\n\n          s3 = boto3.client(\"s3\")\n          http = urllib3.PoolManager()\n\n          # s3://aws-blogs-artifacts-public/artifacts/ML-15729/docs/manifest.txt\n          def lambda_handler(event, context):\n              logger.info(\"got event {}\".format(event))\n              if event[\"RequestType\"] == \"Delete\":\n                  logger.info(\n                      f\"copy files function called at the time of stack deletion, skipping\"\n                  )\n                  response = dict(files_copied=0, error=None)\n                  cfnresponse.send(event, context, cfnresponse.SUCCESS, response)\n                  return\n                  # End DELETE\n\n              try:\n                  read_bucket = os.environ.get(\"READ_BUCKET\")\n                  write_bucket = os.environ.get(\"WRITE_BUCKET\")\n\n                  logger.info(f\"Read Bucket: {read_bucket}\")\n                  logger.info(f\"Write Bucket: {write_bucket}\")\n\n                  # obj = s3.get_object(Bucket=BLOGS_BUCKET, Key=MANIFEST)\n                  manifest_data = []\n                  try:\n                      logger.debug(f\"about to fetch manifest file: {MANIFEST_URL}\")\n                      response = http.request(\"GET\", MANIFEST_URL)\n                      manifest_data = response.data.decode(\"utf-8\").splitlines()\n                      manifest_data.append(\"manifest.txt\")\n                      logger.info(f\"Fetched manifest file: {MANIFEST_URL}\")\n                      logger.debug(manifest_data)\n                  except HTTPError as e:\n                      logger.warning(f\"Error downloading manifest {MANIFEST_URL}: {e}\")\n                      raise e\n                  finally:\n                      response.close()\n\n                  ctr = 0\n                  for fname in manifest_data:\n                      is_config = fname.startswith(\"configs\")\n                      key = os.path.join(SRC_PREFIX, fname)\n                      logger.info(f\"going to read {key} from bucket={BLOGS_BUCKET}\")\n\n                      content = None\n                      try:\n                          file_url = f\"https://{BLOGS_BUCKET}.s3.amazonaws.com/{key}\"\n                          logger.debug(f\"about to fetch: {file_url}\")\n                          response = http.request(\"GET\", file_url)\n                          content = response.data.decode(\"utf-8\")\n                          logger.info(f\"fetched: {file_url}\")\n                      except HTTPError as e:\n                          logger.error(f\"Error downloading {file_url}: {e}\")\n                          raise e\n                      finally:\n                          response.close()\n\n                      # Retrieve the object from S3\n                      if is_config:\n                          logger.debug(f\"found config file... updating content template: {fname}\")\n                          content = content.replace(\"{region}\", os.environ.get(\"MY_AWS_REGION\"))\\\n                                          .replace(\"{role_arn}\", os.environ.get(\"ROLE_ARN\"))\\\n                                          .replace(\"{write_bucket}\", write_bucket)\\\n                                          .replace(\"{read_bucket}\", read_bucket)\\\n                                          .replace(\"{write_tmpdir}\", \"{write_tmpdir}\")\\\n                                          .replace(\"{read_tmpdir}\", \"{read_tmpdir}\")\n                          logger.info(f\"Updating config file: {fname}\")\n                      s3.put_object(Bucket=read_bucket, Key=fname, Body=content)\n                      logger.info(f\"saving file to destination: Bucket:{read_bucket}/{key}\")\n                      ctr += 1\n\n                  logger.info(\"Done!\")\n                  response = dict(files_copied=ctr, error=None)\n                  cfnresponse.send(event, context, cfnresponse.SUCCESS, response)\n              except Exception as e:\n                  logger.error(e)\n                  response = dict(files_copied=0, error=str(e))\n                  cfnresponse.send(event, context, cfnresponse.FAILED, response)\n\n              return\n\nOutputs:\n  S3BucketForRead:\n    Value: !GetAtt S3BucketForRead.Arn\n  S3BucketForWrite:\n    Value: !GetAtt S3BucketForWrite.Arn\n  FilesCopied:\n    Description: Files copied\n    Value: !GetAtt \"CustomSGResource.files_copied\"\n  FileCopyError:\n    Description: Files copy error\n    Value: !GetAtt \"CustomSGResource.error\"\n  Region:\n    Description: Deployed Region\n    Value: !Ref AWS::Region\n\n\n\n================================================"
  },
  {
    "filename": "THIRD_PARTY_LICENSES.txt",
    "path": "THIRD_PARTY_LICENSES.txt",
    "directory": "",
    "extension": "txt",
    "content": "================================================\nThe FMBench (https://github.com/aws-samples/foundation-model-benchmarking-tool) includes the following third-party software/licensing:\n\n** LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding\nhttps://github.com/THUDM/LongBench\n\nMIT License\n\nCopyright (c) 2023 THU-KEG & Zhipu AI\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n\n================================================"
  },
  {
    "filename": ".pre-commit-config.yaml",
    "path": ".pre-commit-config.yaml",
    "directory": "",
    "extension": "yaml",
    "content": "================================================\nrepos:\n  - repo: local\n    hooks:\n      - id: jupyter-nb-clear-output\n        name: jupyter-nb-clear-output\n        files: \\.ipynb$\n        stages: [commit]\n        language: system\n        entry: jupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace\n\n\n\n\n================================================"
  },
  {
    "filename": "analytics.py",
    "path": "analytics/analytics.py",
    "directory": "analytics",
    "extension": "py",
    "content": "================================================\n\"\"\"\nAnalyze data across multiple fmbench runs\n\"\"\"\nimport re\nimport os\nimport sys\nimport math\nimport glob\nimport json\nimport yaml\nimport logging\nimport argparse\nimport pandas as pd\nfrom pathlib import Path\nfrom tomark import Tomark\nfrom sagemaker_cost_rpm_plot import plot_best_cost_instance_heatmap, plot_tps_vs_cost\n\nlogging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nRPM_LIST = [1, 10, 100, 1000, 10000]\n# default values for latency and concurrency thresholds. To configure the summary table\n# based on custom thresholds, add them as command line arguments\nLATENCY_THRESHOLD: int = 2\nCONCURRENCY_THRESHOLD: int = 1\nANALYTICS_RESULTS_DIR: str = os.path.join(\"analytics\", \"results\")\nos.makedirs(ANALYTICS_RESULTS_DIR, exist_ok=True)\nPAYLOAD_FILE_OF_INTEREST: str = \"payload_en_1000-2000.jsonl\"\nPRICING_FILE_PATH: str = os.path.join(\"src\", \"fmbench\", \"configs\",\n                                      \"pricing.yml\")\nDEFAULT_COST_WEIGHT: float = 0.6\n\ndef cost_per_txn(row, pricing):\n    txns_per_hour = row['transactions_per_minute'] * 60\n    if pricing['pricing']['instance_based'].get(row['instance_type']) is not None:\n        instance_cost_per_hour = pricing['pricing']['instance_based'][row['instance_type']]\n        cost_per_txn = round(instance_cost_per_hour / txns_per_hour, 4)\n    else:\n        input_token_cost = pricing['pricing']['token_based'][row['instance_type']]['input-per-1k-tokens']\n        output_token_cost = pricing['pricing']['token_based'][row['instance_type']]['output-per-1k-tokens']\n        cost_per_txn = (row['prompt_token_count_mean']/1000) * input_token_cost + \\\n                       (row['completion_token_count_mean']/1000) * output_token_cost\n        cost_per_txn = round(cost_per_txn, 4)\n    return cost_per_txn\n\n\ndef cost_per_1k_tokens(row, pricing):\n    txns_per_hour = row['transactions_per_minute'] * 60\n    tokens_per_hour = (row['prompt_token_count_mean'] + row['completion_token_count_mean']) * txns_per_hour\n    if pricing['pricing']['instance_based'].get(row['instance_type']) is not None:\n        instance_cost_per_hour = pricing['pricing']['instance_based'][row['instance_type']]\n        cost_per_1k_tokens = round(1000 * (instance_cost_per_hour / tokens_per_hour), 8)\n    else:\n        input_token_cost = pricing['pricing']['token_based'][row['instance_type']]['input-per-1k-tokens']\n        output_token_cost = pricing['pricing']['token_based'][row['instance_type']]['output-per-1k-tokens']\n        total_tokens = row['prompt_token_count_mean'] + row['completion_token_count_mean']\n\n        cost_per_1k_tokens = (row['prompt_token_count_mean'] / total_tokens) * input_token_cost + \\\n                             (row['completion_token_count_mean'] / total_tokens) * output_token_cost\n        cost_per_1k_tokens = round(cost_per_1k_tokens, 8)\n    return cost_per_1k_tokens\n\ndef parse_yaml_config(file_path):\n    \"\"\"\n    This function parses a yaml file in the results folder (that represents the configuration file that was used to benchmark)\n    and extracts the tensor parallel degree, batch size, and the config file name.\n    \"\"\"\n    config_file_properties: Optional[Dict] = None\n    tensor_parallel_degree: Optional[str] = None\n    batch_size: Optional[int] = None\n    serving_properties: Optional[str] = None\n    model_copies: Optional[str] = None\n    image_uri: Optional[str] = None\n    try:\n        with open(file_path, 'r') as file:\n            config = yaml.safe_load(file)\n            logger.info(f\"Loaded the configuration file: {config}\")\n        experiment_config = config.get('experiments', [])\n        if isinstance(experiment_config, list) and len(experiment_config) == 1:\n            serving_properties = experiment_config[0].get('serving.properties', None)\n            image_uri = experiment_config[0].get('image_uri', None)\n            if serving_properties:\n                logger.info(f\"serving_properties: {serving_properties}.\")\n                tp_match = re.search(r'option\\.tensor_parallel_degree=(\\d+)', serving_properties)\n                if tp_match:\n                    tensor_parallel_degree = str(tp_match.group(1))\n\n                model_copies = experiment_config[0]['inference_spec'].get('model_copies', None)\n\n                bs_match = re.search(r'option\\.max_rolling_batch_size=(\\d+)', serving_properties)\n                if bs_match:\n                    batch_size = int(bs_match.group(1))\n            else:\n                logger.error(\"No 'serving.properties' found in the experiment configuration.\")\n        else:\n            logger.error(f\"Experiment configuration is list or the number of experiments is not 1, num experiments={len(experiment_config)}\")\n        config_file_properties = dict(config_file=os.path.basename(file_path),\n                                      image_uri=image_uri,\n                                      tensor_parallel_degree=tensor_parallel_degree,\n                                      batch_size=batch_size,\n                                      model_copies=model_copies)\n        \n    except Exception as e:\n        logger.error(f\"Error parsing the config file {file_path}: {e}\")\n        config_file_properties = None\n    logger.info(f\"config_file_properties={config_file_properties}\")\n    return config_file_properties\n\n# Determine how many instances would be required to run 100 requests/minute,\n# 1000 requests/minute, 10000 requests/minute. The idea being that at the \n# low end of the total number of requests/minute smaller instances which provide\n# good inference latency at low concurrencies would suffice (said another way, \n# the larger more expensive instances are an overkill at this stage) but as \n# the number of requests/minute increase there comes an inflexion point beyond\n# which the number of smaller instances required would be so much that it \n# would be more economical to use fewer instances of the larger more expensive instances.\ndef cost_per_n_rpm(r, rpm, pricing):\n    if pricing['pricing']['instance_based'].get(r['instance_type']):\n        instance_count_needed = math.ceil(rpm / r['transactions_per_minute'])\n        cost = round(instance_count_needed * pricing['pricing']['instance_based'][r['instance_type']], 2)\n    else:\n        input_token_cost = pricing['pricing']['token_based'][r['instance_type']]['input-per-1k-tokens']\n        output_token_cost = pricing['pricing']['token_based'][r['instance_type']]['output-per-1k-tokens']\n        total_tokens = r['prompt_token_count_mean'] + r['completion_token_count_mean']\n\n        cost_per_txn = (r['prompt_token_count_mean']/1000) * input_token_cost + \\\n                             (r['completion_token_count_mean']/1000) * output_token_cost\n        #txn_per_hour = r['transactions_per_minute'] * 60\n        txn_per_hour = rpm * 60\n        cost = round(cost_per_txn * txn_per_hour, 8)\n        instance_count_needed = 1\n\n    return (instance_count_needed, cost)\n\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='Analyze multiple FMBench runs')\n    parser.add_argument('--results-dir',\n                        type=str,\n                        help=f'Root directory containing results-* folders',\n                        required=True)\n\n    parser.add_argument('--exclude-pattern',\n                        type=str,\n                        default=None,\n                        help=f'Exclude result folders matching this pattern, default is None',\n                        required=False)\n                        \n    parser.add_argument('--latency-threshold',\n                        type=int,\n                        default=LATENCY_THRESHOLD,\n                        help=f'Latency threshold, runs with p95 above this are not useful, default={LATENCY_THRESHOLD}',\n                        required=False)\n    parser.add_argument('--concurrency-threshold', \n                        type=int, \n                        default=CONCURRENCY_THRESHOLD, \n                        help=f'Concurrency threshold, runs with the number of concurrent requests handled under this are not useful, default={CONCURRENCY_THRESHOLD}')\n    parser.add_argument('--payload-file',\n                        type=str,\n                        default=PAYLOAD_FILE_OF_INTEREST,\n                        help=f'Payload file representing payload of interest, default={PAYLOAD_FILE_OF_INTEREST}',\n                        required=False)\n    # the model id is a required field. This model_id must match the model_id in your results folders so it is \n    # used during creating the summary table\n    parser.add_argument('--model-id',\n                        type=str,\n                        help=f'Model for which data is being analyzed, this is a required field',\n                        required=True)\n    parser.add_argument('--cost-weight',\n                        type=float,\n                        default=DEFAULT_COST_WEIGHT,\n                        help=f\"Weightage to assign to cost while choosing best instance type, \"\n                             f\"instance count is assigned \\\"1 - cost weightage\\\" automatically, \"\n                             f\"default={DEFAULT_COST_WEIGHT}\",\n                        required=False)\n    \n    args = parser.parse_args()\n    print(f\"main, {args} = args\")\n\n    # load pricing info\n    pricing =  yaml.safe_load(Path(PRICING_FILE_PATH).read_text())\n    logger.info(f\"pricing={json.dumps(pricing, indent=2)}\")\n\n    # all results file to be parsed\n    summary_file_pattern: str = os.path.join(args.results_dir,\n                                             f\"results-{args.model_id}-*\",\n                                             \"all_metrics_summary.csv\")\n    all_metrics_summary_files = glob.glob(summary_file_pattern,\n                                          recursive=True)\n    if args.exclude_pattern is not None:\n        all_metrics_summary_files = [f for f in all_metrics_summary_files if args.exclude_pattern not in f]\n    files_found: int = len(all_metrics_summary_files)\n    logger.info(f\"found {files_found} files \"\n                f\"{all_metrics_summary_files} \")\n    if files_found == 0:\n        logger.error(f\"no file found using the following pattern={summary_file_pattern}, exiting\")\n        sys.exit(1)\n\n    # config file that was used to create the benchmarks\n    # in each results diredctory there is a .yml file which is the config file\n    result_and_configfile_combinations = []\n    for f in all_metrics_summary_files:\n        d = str(Path(f).parents[0].absolute())\n        config_files = glob.glob(os.path.join(d, '*.yml'))\n        if len(config_files) == 0:\n            logger.error(f\"no config file found in {d}\")            \n        else:\n            result_and_configfile_combinations.append((f, config_files[0]))\n    combined_data = []\n\n    logger.info(f\"there are {len(result_and_configfile_combinations)} result and config file combinations\")\n\n    for result_file, config_file in result_and_configfile_combinations: \n        #zip(all_metrics_summary_files, possible_config_files):\n        # Read result and configuration files\n        logger.info(f\"result_file={result_file},\\nconfig_file={config_file}\")\n        result_df = pd.read_csv(result_file)\n        config_info = parse_yaml_config(config_file)\n        if config_info:\n            config_df = pd.DataFrame([config_info])\n            logger.info(f\"config_df: {config_df}\")\n            # match the length of result_df to concat the config file vars to the corresponding \n            # results folder\n            config_df_repeated = pd.concat([config_df] * len(result_df), ignore_index=True)\n            combined_df = pd.concat([result_df, config_df_repeated], axis=1)\n            combined_data.append(combined_df)\n        else:\n            logger.warning(f\"No config data found for {config_file}, using result only.\")\n            combined_data.append(result_df)\n    df = pd.concat(combined_data, ignore_index=True)\n    logger.info(f\"Final dataframe: {df}\")\n\n    # filter to keep only relevant data\n    logger.info(f\"df columns: {df.columns}\")\n    # filter for the p95 latency threshold and the concurrency threshold\n\n    df_selected = df[(df.latency_p95 <= args.latency_threshold) & (df.concurrency >= args.concurrency_threshold) & (df.error_rate == 0)]\n    logger.info(f\"after filtering to keep rows with latency_p95 <= \",\n                f\"{args.latency_threshold}s, concurrency <=\",\n                f\"{args.concurrency_threshold}\",\n                f\"df shape {df_selected.shape}\")\n\n\n    # select row with highest concurrency level\n    grouping_cols = [\"experiment_name\", \"payload_file\", \"instance_type\", \"instance_count\"]\n    # adding selected metrics for when the concurrency is the highest and the completion tokens are given out, indicating valid responses\n    df_selected = df_selected[df_selected.completion_token_count_mean.notna()]\n    logger.info(f\"df_selected: {df_selected.completion_token_count_mean}\")\n    df_summary_all = df_selected.loc[df_selected.groupby(grouping_cols)['concurrency'].transform(max) == df_selected['concurrency']]\n\n    # find price per txn and price per token\n    df_summary_all['cost_per_txn'] = df_summary_all.apply(lambda r: cost_per_txn(r, pricing), axis=1)\n    df_summary_all['cost_per_1k_tokens'] = df_summary_all.apply(lambda r: cost_per_1k_tokens(r, pricing), axis=1)\n\n    # extrapolate to price per n requests per minue\n    for rpm in RPM_LIST:\n        col_name = f\"instance_count_and_cost_{rpm}_rpm\"\n        df_summary_all[col_name] = df_summary_all.apply(lambda r: cost_per_n_rpm(r, rpm, pricing), axis=1)\n\n    df_summary_all = df_summary_all.sort_values(by=\"cost_per_1k_tokens\")\n    summary_file: str = os.path.join(ANALYTICS_RESULTS_DIR,\n                                     f\"{args.model_id}-summary-p95-latency={args.latency_threshold}s.csv\")\n    df_summary_all.to_csv(summary_file, index=False)\n    logger.info(f\"saved df_summary_all dataframe of shape={df_summary_all.shape} in {summary_file}\")\n    \n    summary_file_payload_of_interest: str = os.path.join(ANALYTICS_RESULTS_DIR,\n                                                         f\"{args.model_id}-summary-{Path(args.payload_file).stem}-p95-latency={args.latency_threshold}s.csv\")\n    summary_file_payload_of_interest_raw_metrics: str = os.path.join(ANALYTICS_RESULTS_DIR,\n                                                         f\"{args.model_id}-summary-{Path(args.payload_file).stem}-p95-latency-concurrency={args.latency_threshold}s-raw.csv\")                                       \n    df_summary_payload_of_interest = df_summary_all[df_summary_all.payload_file == args.payload_file]\n    df_summary_payload_of_interest = df_summary_payload_of_interest.sort_values(by=\"cost_per_1k_tokens\")\n    # create a csv file with all the raw metrics\n    df_summary_payload_of_interest.to_csv(summary_file_payload_of_interest_raw_metrics, index=False)\n    cols_to_remove = ['payload_file', 'instance_count', 'error_rate', 'prompt_token_count_mean', 'prompt_token_throughput', 'completion_token_count_mean', 'latency_p50',\n                      'latency_p99', 'completion_token_throughput']\n    # filter out the columns as needed and only give the relevant columns in the analysis markdown table\n    df_summary_payload_of_interest_trimmed = df_summary_payload_of_interest.drop(columns=cols_to_remove)\n    df_summary_payload_of_interest_trimmed_grouped = df_summary_payload_of_interest_trimmed.loc[df_summary_payload_of_interest_trimmed.groupby('instance_type')['concurrency'].idxmax()].reset_index()\n    df_summary_payload_of_interest_trimmed_grouped.to_csv(summary_file_payload_of_interest, index=False)\n    logger.info(\"all done\")\n\n    # cost RPM plot, the function saves the html to a file\n    heatmap_fname: str = os.path.join(ANALYTICS_RESULTS_DIR,\n                                      f\"{args.model_id}-cost-rpm-heatmap-for-\"\n                                      f\"{Path(args.payload_file).stem}-p95-latency={args.latency_threshold}s.html\")\n    df1 = pd.read_csv(summary_file_payload_of_interest)\n    logger.info(f\"df columns: {df1.columns}\")\n    # if an instance type has multiple entries then keep the one with the least cost per token\n    shape_before = df1.shape\n    df1 = df1.loc[df1.groupby('instance_type').cost_per_1k_tokens.idxmin()].reset_index(drop=True)\n    shape_after = df1.shape\n    if shape_before[0] != shape_after[0]:\n        logger.warning(f\"there were multiple entries for some instance types, kept ones with min per token cost, \"\n                       f\"shape_before={shape_before}, shape_after={shape_after}\")\n    prompt_spec: str = args.payload_file.split(\".\")[0]\n    subtitle: str = f\"Prompt: {prompt_spec} tokens, latency p95 threshold: {args.latency_threshold}s\"\n    _ = plot_best_cost_instance_heatmap(df1,\n                                    heatmap_fname,\n                                    args.model_id,\n                                    subtitle,\n                                    args.cost_weight,\n                                    1 - args.cost_weight)\n    # save the line chart\n    # TPS vs Cost line chart\n    tps_vs_cost_fname: str = os.path.join(ANALYTICS_RESULTS_DIR,\n                                        f\"{args.model_id}-tps-vs-cost-for-\"\n                                        f\"{Path(args.payload_file).stem}-p95-latency={args.latency_threshold}s.html\")\n\n    _ = plot_tps_vs_cost(df1,\n                        tps_vs_cost_fname,\n                        args.model_id,\n                        subtitle)\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n\n================================================"
  },
  {
    "filename": "sagemaker_cost_rpm_plot.py",
    "path": "analytics/sagemaker_cost_rpm_plot.py",
    "directory": "analytics",
    "extension": "py",
    "content": "================================================\nimport re\nimport logging\nimport pandas as pd\nimport seaborn as sns\nfrom typing import List\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\n\n# Configure logging\nlogging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\ndef _pre_process_df(summary_payload_df: pd.DataFrame) -> tuple[pd.DataFrame, List]:\n    \"\"\"\n    Pre-processes the summary payload DataFrame to extract relevant data for plotting.\n\n    Args:\n        summary_payload_df (pd.DataFrame): Input DataFrame containing instance data.\n\n    Returns:\n        tuple: A tuple containing the processed DataFrame and a list of RPM values.\n    \"\"\"\n    logger.info(\"======================================\")\n    logger.info(f\"Loaded dataframe, shape is: {summary_payload_df.shape}\")\n    logger.info(\"======================================\")\n\n    rpm_values = []\n    # Dynamically extract RPM values based on DataFrame columns\n    for name in summary_payload_df.columns:\n        print(name)\n        match = re.search(r'(\\d+)_rpm', name)\n        if match:\n            rpm_values.append(int(match.group(1)))\n\n    logger.info(\"======================================\")\n    logger.info(f\"RPM Values extracted: {rpm_values}\")\n    logger.info(\"======================================\")\n\n    rows = []\n    # Extract and reformat data for each RPM value\n    for index, row in summary_payload_df.iterrows():\n        for value in rpm_values:\n            column = f'instance_count_and_cost_{value}_rpm'\n            count, cost = tuple(v.lstrip(\"('\").rstrip(\"')\") for v in row[column].split(\", \"))\n            rows.append({'instance_type': row['instance_type'],\n                         'instance_count': int(count),\n                         'RPM': value,\n                         'cost': float(cost),\n                         'TPM': row['transactions_per_minute'],\n                         'TP_Degree': row['tensor_parallel_degree']})\n\n    plot_df = pd.DataFrame(rows)\n\n    logger.info(\"======================================\")\n    logger.info(f\"Completed pre-processing the data, new dataframe shape: {plot_df.shape}\")\n    logger.info(\"======================================\")\n\n    return plot_df, rpm_values\n\n\ndef plot_best_cost_instance_heatmap(summary_payload_df: pd.DataFrame,\n                                    output_filename: str,\n                                    model_id: str,\n                                    subtitle: str,\n                                    cost_weight: float,\n                                    instance_count_weight: float) -> go.Figure:\n    \"\"\"\n    Creates a heatmap plot to visualize the cost of running different instance types at various RPM values.\n\n    Args:\n        summary_payload_df (pd.DataFrame): Input DataFrame containing instance data.\n\n    Returns:\n        go.Figure: Plotly heatmap figure object.\n    \"\"\"\n\n    logger.info(summary_payload_df.columns)\n    plot_df, rpm_values = _pre_process_df(summary_payload_df)\n\n    logger.info(\"======================================\")\n    logger.info(f\"Processed dataframe, shape is: {plot_df.shape}\")\n    logger.info(f\"RPM Values loaded are: {rpm_values}\")\n    logger.info(\"======================================\")\n\n    heatmap_data = plot_df.pivot(index=\"RPM\", columns=\"instance_type\", values=\"cost\")\n    heatmap_data.index = heatmap_data.index.astype(str)\n\n    # Sorting instance types by cost in ascending order for RPM = 1\n    sorted_columns = heatmap_data.loc['1'].sort_values().index\n    heatmap_data = heatmap_data[sorted_columns]\n\n    heatmap_data_instance_count = plot_df.pivot(index=\"RPM\", \n                                                columns=\"instance_type\",\n                                                values=\"instance_count\")\n    heatmap_data_instance_count = heatmap_data_instance_count[sorted_columns]\n\n    # Create hover text for the heatmap\n    hover_text_combined = []\n    for i in range(heatmap_data.shape[0]):\n        row = []\n        for j in range(heatmap_data.shape[1]):\n            rpm = heatmap_data.index[i]\n            instance_type = heatmap_data.columns[j]\n            cost = heatmap_data.iloc[i, j]\n            instance_count = heatmap_data_instance_count.iloc[i, j]\n            hover_text = (\n                f'Instance Type: {instance_type}<br>'\n                f'Instance Count: {instance_count}<br>'\n                f'Cost: ${cost}<br>'\n                f'RPM: {rpm}'\n            )\n            row.append(hover_text)\n        hover_text_combined.append(row)\n\n    text_arr = heatmap_data.values.copy().astype(str)\n\n    logger.info(\"======================================\")\n    logger.info(\"Adding annotations\")\n    logger.info(\"======================================\")\n\n    # Annotate the heatmap with additional information\n    for i, (cost_row, instance_count_row) in enumerate(zip(heatmap_data.values,\n                                                           heatmap_data_instance_count.values)):\n        min_cost_idx = cost_row.argmin()\n        min_inst_idx = instance_count_row.argmin()\n        normalized_val = (cost_weight * (cost_row / cost_row.max())) +\\\n                         (instance_count_weight * (instance_count_row / instance_count_row.max()))\n        normalized_idx = normalized_val.argmin()\n\n        text_arr[i, min_cost_idx] = f\"<b>{cost_row[min_cost_idx]:.2f}<br>(least cost)\"\n        text_arr[i, min_inst_idx] = f\"<b>{cost_row[min_inst_idx]:.2f}<br>(fewest instances)\"\n        text_arr[i, normalized_idx] = f\"<b>{cost_row[normalized_idx]:.2f}<br>(best choice)\"\n\n    # Create the heatmap\n    fig = go.Figure(data=go.Heatmap(\n        z=heatmap_data.values,\n        x=heatmap_data.columns,\n        y=heatmap_data.index,\n        colorscale='Dense',\n        colorbar=dict(title=\"Cost\"),\n        zmin=heatmap_data.values.min(),\n        zmax=heatmap_data.values.max(),\n        text=text_arr,\n        texttemplate=\"$%{text}\",\n        hovertext=hover_text_combined,\n        hovertemplate='%{hovertext}<extra></extra>'\n    ))\n\n    num_rows, num_cols = heatmap_data.shape\n    dynamic_font_size = _calculate_dynamic_font_size(num_rows, num_cols)\n    fig.update_traces(textfont=dict(size=dynamic_font_size))\n\n    logger.info(\"======================================\")\n    logger.info(\"Updating layout for better visualization\")\n    logger.info(\"======================================\")\n\n    # Update layout for better visualization\n    fig.update_layout(\n        title=f\"Serving costs for \\\"{model_id}\\\" for different requests/minute and instance options<br>{subtitle}<br>Hover your mouse over a cell for additional information.\",\n        xaxis_title=\"\",\n        yaxis_title=\"Requests/minute\",\n        autosize=True,\n        width=1500,\n        height=700, \n        font=dict(size=dynamic_font_size)\n    )\n\n    fig.update_traces(textfont_size=dynamic_font_size)\n    \n    fig.add_annotation(\n       showarrow=False,\n       xanchor='left',\n       xref='paper', \n       x=0, \n       yref='paper',\n       y=-0.15,\n       text=f\"Note: <b><i>best choice</i></b> based on {100*cost_weight}% weightage to cost and {100*instance_count_weight}% to number of instances needed. <b><i>least cost</i></b> and <b><i>fewest instances</i></b> called out only when different from <b><i>best choice</i></b>.\",\n       font=dict(size=max(dynamic_font_size - 2, 8)))\n\n    # Save the figure as an HTML file\n    fig.write_html(output_filename)\n\n    logger.info(\"======================================\")\n    logger.info(f\"Heatmap plotting completed, saved to {output_filename}\")\n    logger.info(\"======================================\")\n\n    return fig\n\ndef plot_tps_vs_cost(df: pd.DataFrame,\n                     output_filename: str,\n                     model_id: str,\n                     subtitle: str) -> go.Figure:\n    \"\"\"\n    Creates an interactive line chart to visualize the cost per second vs transactions per second\n    for different instance types.\n\n    Args:\n        df (pd.DataFrame): Input DataFrame containing instance data.\n        output_filename (str): Name of the file to save the plot.\n        model_id (str): ID of the model being analyzed.\n        subtitle (str): Subtitle for the plot.\n\n    Returns:\n        go.Figure: Plotly figure object.\n    \"\"\"\n    # Calculate transactions per second for each instance type\n    df['transactions_per_second'] = df['transactions_per_minute'] / 60\n    \n    # Calculate cost per second\n    df['cost_per_second'] = df['transactions_per_second'] * df['cost_per_txn']\n\n    # Create the plot\n    fig = go.Figure()\n\n    # Add line traces for each instance type\n    for instance_type, group in df.groupby('instance_type'):\n        # Sort the group by TPS to ensure proper line connection\n        group = group.sort_values('transactions_per_second')\n        \n        fig.add_trace(go.Scatter(\n            x=group['transactions_per_second'],\n            y=group['cost_per_second'],\n            mode='lines+markers',\n            name=instance_type,\n            line=dict(shape='linear', width=2),  # Ensure linear interpolation and set line width\n            marker=dict(size=8),  # Marker size\n            connectgaps=True,\n            hoverinfo='text',\n            hovertext=[f\"Instance Type: {instance_type}<br>\"\n                       f\"TPS: {tps:.2f}<br>\"\n                       f\"Cost per Second: ${cost_per_sec:.4f}<br>\"\n                       f\"Cost per Txn: ${cost_per_txn:.4f}<br>\"\n                       f\"Transactions per Minute: {tpm:.0f}\"\n                       for tps, cost_per_sec, cost_per_txn, tpm in zip(group['transactions_per_second'], \n                                                                       group['cost_per_second'],\n                                                                       group['cost_per_txn'],\n                                                                       group['transactions_per_minute'])]\n        ))\n\n    # Customize the plot\n    fig.update_layout(\n        title=f\"Cost per Second vs TPS for {model_id}<br>{subtitle}\",\n        xaxis_title=\"Transactions Per Second (TPS)\",\n        yaxis_title=\"Cost per Second ($)\",\n        legend_title=\"Instance Type\",\n        font=dict(size=14),\n        hovermode=\"closest\",\n        hoverdistance=10,  # Increase hover \"snapping\" distance\n    )\n\n    # Update axes to show more gridlines\n    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey')\n    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey')\n\n    # Save the plot as an HTML file\n    fig.write_html(output_filename)\n    logger.info(f\"Interactive TPS vs Cost per Second plot saved as {output_filename}\")\n\n    return fig\n\ndef _calculate_dynamic_font_size(num_rows: int, num_cols: int):\n    \"\"\"\n    Adjust the dynamic font size of the text in the heatmap based on the number of rows\n    and columns\n    \"\"\"\n    base_size: int = 14\n    scale_factor = min(1000 / max(num_rows, num_cols), 1)\n    # Ensure minimum font size of 10\n    return max(int(base_size * scale_factor), 10)\n\n\n\n================================================"
  },
  {
    "filename": "sagemaker_metrics_plot.py",
    "path": "analytics/sagemaker_metrics_plot.py",
    "directory": "analytics",
    "extension": "py",
    "content": "================================================\nimport logging\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nlogging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef plot_sm_utilization_metrics(endpoint_metrics_df : pd.DataFrame) -> sns.FacetGrid:\n    '''\n    This function reads a CSV file containing endpoint metrics data, processes the data,\n    and generates a FacetGrid line plot to visualize utilization metrics for different \n    concurrency levels and instance types. The function returns the FacetGrid object.\n\n    Parameters:\n    endpoint_metrics_df (pd.DataFrame): Dataframe containing the endpoint metrics data.\n\n    Returns:\n    FacetGrid: The FacetGrid object containing the plotted data.\n\n    Example Usage:\n    ```\n    sm_utilization_metrics_plot = plot_sm_utilization_metrics('sagemaker_endpoint_metrics.csv')\n    sm_utilization_metrics_plot.savefig('sm_utilization_metrics_plot.png')\n    ```\n    '''\n    # Load the data from CSV file\n\n    logger.info(\"======================================\")\n    logger.info(f\"loaded dataframe, shape is: {endpoint_metrics_df.shape}\")\n    logger.info(\"======================================\")\n    \n\n    # Ensure Timestamp is in datetime format\n    endpoint_metrics_df['Timestamp'] = pd.to_datetime(endpoint_metrics_df['Timestamp'])\n\n    # Melt the DataFrame to long format\n    utilization_metrics_melted_data = endpoint_metrics_df.melt(\n        id_vars=['Timestamp', 'instance_type', 'concurrency'], \n        value_vars=['CPUUtilization', 'DiskUtilization', 'GPUMemoryUtilization', 'GPUUtilization', 'MemoryUtilization'],\n        var_name='Metric', value_name='Value'\n    )\n\n    logger.info(\"======================================\")\n    logger.info(f\"Melted dataframe, new shape is :  {utilization_metrics_melted_data.shape}\")\n    logger.info(\"======================================\")\n\n    # Define markers for the line plots\n    markers = {\"CPUUtilization\": \"o\", \"DiskUtilization\": \"s\", \"GPUMemoryUtilization\": \"X\", \"GPUUtilization\": \"^\", \"MemoryUtilization\": 'v'}\n\n    # Create the FacetGrid for line plot\n    utilization_metrics_plot = sns.FacetGrid(\n        utilization_metrics_melted_data, col='instance_type', row='concurrency', hue='Metric', \n        palette='muted', height=4, aspect=1.25, sharex=False\n    )\n    utilization_metrics_plot.map(sns.lineplot, 'Timestamp', 'Value', dashes=False)\n\n    # Update markers\n    for ax in utilization_metrics_plot.axes.flat:\n        lines = ax.get_lines()\n        for line, (metric, marker) in zip(lines, markers.items()):\n            line.set_marker(marker)\n\n    # Add legend\n    utilization_metrics_plot.add_legend()\n\n    # Create a subtitle\n    with sns.plotting_context('paper', font_scale=1.3):\n        utilization_metrics_plot.figure.suptitle(\"Utilization metrics for different concurrency levels and instance types\")\n        utilization_metrics_plot.set_titles(row_template=\"concurrency={row_name}\", col_template=\"instance={col_name}\", size=8)\n\n    # Bold the concurrency titles\n    for ax in utilization_metrics_plot.axes.flat:\n        title_text = ax.get_title()\n        if \"concurrency\" in title_text:\n            ax.set_title(title_text, fontsize=10, fontweight='bold')\n\n    # Set x and y labels for this chart\n    utilization_metrics_plot.set_ylabels(\"Utilization (%)\")\n    utilization_metrics_plot.set_xlabels(\"Timestamp\")\n    #top below controls the spacing for title of the plot, hspace and wspace control the spacing between the grid\n    utilization_metrics_plot.figure.subplots_adjust(top=.93, hspace=0.5, wspace=0.2)\n\n    # Rotate x-axis labels for better readability\n    for ax in utilization_metrics_plot.axes.flat:\n        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n    logger.info(f'\\nPlotting Complete, returning FacetGrid Object\\n')\n    return utilization_metrics_plot\n\n\n================================================"
  },
  {
    "filename": "scratchpad.ipynb",
    "path": "analytics/scratchpad.ipynb",
    "directory": "analytics",
    "extension": "ipynb",
    "content": "================================================\n# Jupyter notebook converted to Python script.\n\nimport os\nimport json\nimport glob\nimport logging\nimport pandas as pd\nfrom pathlib import Path\n\nlogging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nRESULTS_DIR: str = \"../results-*\"\nFIRST_N: int = 10\n\ndata_path = os.path.join(RESULTS_DIR, \"per_inference\", \"*.json\")\nlogger.info(f\"looking for per inference files in {data_path}\")\nfile_list = glob.glob(data_path)\nlogger.info(f\"found {len(file_list)} files, listing first {FIRST_N}\\n{file_list[:FIRST_N]}\")\n# Output:\n#   [2024-09-23 20:35:14,362] p121634 {1289656933.py:2} INFO - looking for per inference files in ../results-*/per_inference/*.json\n\n#   [2024-09-23 20:35:14,365] p121634 {1289656933.py:4} INFO - found 1037 files, listing first 10\n\n#   ['../results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2/per_inference/1727121590.5035987.json', '../results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2/per_inference/1727121633.7328534.json', '../results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2/per_inference/1727121754.196286.json', '../results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2/per_inference/1727121509.0425491.json', '../results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2/per_inference/1727121685.8263414.json', '../results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2/per_inference/1727121484.6407592.json', '../results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2/per_inference/1727121362.5149496.json', '../results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2/per_inference/1727121833.160436.json', '../results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2/per_inference/1727121777.999922.json', '../results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2/per_inference/1727121572.2959073.json']\n\n\nunique_result_files_list = list(set([Path(f).parent.absolute().parent.name for f in file_list]))\nunique_result_files_list\n# Output:\n#   ['results-llama3-8b-g5.12xl-tp=2-mc=max-triton-ec2',\n\n#    'results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2']\n\n# read the files into a dataframe\ncontent = [json.loads(Path(f).read_text()) | dict(filename=Path(f).parent.absolute().parent.name) \\\n           for f in file_list]\nlogger.info(f\"sample data {json.dumps(content[0], indent=2)}\")\ndf = pd.DataFrame(content)\nlogger.info(f\"shape of data from {df.shape}\")\ndf.head()\n# Output:\n#   [2024-09-23 20:35:14,491] p121634 {2288595646.py:4} INFO - sample data {\n\n#     \"endpoint_name\": \"http://127.0.0.1:8080/invocations\",\n\n#     \"prompt\": \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context in the section demarcated by \\\"```\\\" to answer the question. \\nThe context may contain multiple question answer pairs as an example. Only answer the final question provided in the question section below.\\nIf you dont know the answer just say that you dont know. Use three sentences maximum and keep the answer concise. \\n\\n```\\nPassage 1:\\nEricaceae\\nThe Ericaceae () are a family of flowering plants, commonly known as the heath or heather family, found most commonly in acidic and infertile growing conditions. The family is large, with c. 4250 known species spread across 124 genera, making it the 14th most species-rich family of flowering plants. The many well known and economically important members of the Ericaceae include the cranberry, blueberry, huckleberry, rhododendron (including azaleas), and various common heaths and heathers (Erica, Cassiope, Daboecia, and Calluna for example).\\n\\nDescription\\nThe Ericaceae contain a morphologically diverse range of taxa, including herbs, dwarf shrubs, shrubs, and trees. Their leaves are usually evergreen, alternate or whorled, simple and without stipules. Their flowers are hermaphrodite and show considerable variability. The petals are often fused (sympetalous) with shapes ranging from narrowly tubular to funnelform or widely urn-shaped. The corollas are usually radially symmetrical (actinomorphic) and urn-shaped, but many flowers of the genus Rhododendron are somewhat bilaterally symmetrical (zygomorphic). Anthers open by pores.\\n\\nTaxonomy\\nMichel Adanson used the term Vaccinia to describe a similar family, but Antoine Laurent de Jussieu first used the term Ericaceae. The name comes from the type genus Erica, which appears to be derived from the Greek word ere\\u00edk\\u0113 (\\u1f10\\u03c1\\u03b5\\u03af\\u03ba\\u03b7). The exact meaning is difficult to interpret, but some sources show it as meaning 'heather'. The name may have been used informally to refer to the plants before Linnaean times, and simply been formalised when Linnaeus described Erica in 1753, and then again when Jussieu described the Ericaceae in 1789.Historically, the Ericaceae included both subfamilies and tribes. In 1971, Stevens, who outlined the history from 1876 and in some instances 1839, recognised six subfamilies (Rhododendroideae, Ericoideae, Vaccinioideae, Pyroloideae, Monotropoideae, and Wittsteinioideae), and further subdivided four of the subfamilies into tribes, the Rhododendroideae having seven tribes (Bejarieae, Rhodoreae, Cladothamneae, Epigaeae, Phyllodoceae, and Diplarcheae). Within tribe Rhodoreae, five genera were described, Rhododendron L. (including Azalea L. pro parte), Therorhodion Small, Ledum L., Tsusiophyllum Max., Menziesia J. E. Smith, that were eventually transferred into Rhododendron, along with Diplarche from the monogeneric tribe Diplarcheae.In 2002, systematic research resulted in the inclusion of the formerly recognised families Empetraceae, Epacridaceae, Monotropaceae, Prionotaceae, and Pyrolaceae into the Ericaceae based on a combination of molecular, morphological, anatomical, and embryological data, analysed within a phylogenetic framework. The move significantly increased the morphological and geographical range found within the group. One possible classification of the resulting family includes 9 subfamilies, 126 genera, and about 4000 species:\\nEnkianthoideae Kron, Judd & Anderberg (one genus, 16 species)\\nPyroloideae Kosteltsky (4 genera, 40 species)\\nMonotropoideae Arnott (10 genera, 15 species)\\nArbutoideae Niedenzu (up to six genera, about 80 species)\\nCassiopoideae Kron & Judd (one genus, 12 species)\\nEricoideae Link (19 genera, 1790 species)\\nHarrimanelloideae Kron & Judd (one species)\\nEpacridoideae Arn. (=Styphelioideae Sweet) (35 genera, 545 species)\\nVaccinioideae Arnott (50 genera, 1580 species)\\n\\nGenera\\nSee the full list at List of Ericaceae genera.\\n\\nDistribution and ecology\\nThe Ericaceae have a nearly worldwide distribution. They are absent from continental Antarctica, parts of the high Arctic, central Greenland, northern and central Australia, and much of the lowland tropics and neotropics.The family is largely composed of plants that can tolerate acidic, infertile conditions. Like other stress-tolerant plants, many Ericaceae have mycorrhizal fungi to assist with extracting nutrients from infertile soils, as well as evergreen foliage to conserve absorbed nutrients. This trait is not found in the Clethraceae and Cyrillaceae, the two families most closely related to the Ericaceae. Most Ericaceae (excluding the Monotropoideae, and some Epacridoideae) form a distinctive accumulation of mycorrhizae, in which fungi grow in and around the roots and provide the plant with nutrients. The Pyroloideae are mixotrophic and gain sugars from the mycorrhizae, as well as nutrients.In many parts of the world, a \\\"heath\\\" or \\\"heathland\\\" is an environment characterised by an open dwarf-shrub community found on low-quality acidic soils, generally dominated by plants in the Ericaceae. A common example is Erica tetralix. This plant family is also typical of peat bogs and blanket bogs; examples include Rhododendron groenlandicum and Kalmia polifolia. In eastern North America, members of this family often grow in association with an oak canopy, in a habitat known as an oak-heath forest.In heathland, plants in the family Ericaceae serve as hostplants to the butterfly Plebejus argus.Some evidence suggests eutrophic rainwater can convert ericoid heaths with species such as Erica tetralix to grasslands. Nitrogen is particularly suspect in this regard, and may be causing measurable changes to the distribution and abundance of some ericaceous species.\\nPassage 2:\\nDeutzia silvestrii\\nDeutzia silvestrii is a species of Safflower in the family Hydrangeaceae. It is found in central China (\\u7ea2\\u82b1\\u6eb2\\u758f; h\\u00f3ng hu\\u0101 s\\u014du sh\\u016b).\\nPassage 3:\\nCassiope mertensiana\\nCassiope mertensiana is a species of flowering plant known by the common names western moss heather and white mountain heather.\\nThis heather is native to subalpine areas of western North America, from Alaska to the mountains of California. It is a small, branching shrub which forms patches along the ground and in rocky crevices.\\n\\nDescription\\nCassiope mertensiana has short, erect, snakelike stems that are covered in tiny leathery scalelike leaves only a few millimeters long. From between the layers of scale leaves emerge reddish  pedicels each bearing a petite, hanging, down-facing, bell-shaped flower. The bractlets are red and the contrasting flower is white.\\n\\nAlthough the shrub tends to grow in areas where there is a lot of accumulation of snow, adequate rain precipitation is needed for the continued growth of Cassiope Mertensiana. The shrub must be exposed to enough sunlight and warmer conditions for proper growth during the growing season.\\nPassage 4:\\nCassiope lycopodioides\\nCassiope lycopodioides, Haida Gwaii mountain-heather or clubmoss mountain heather, is a plant species native to North America.\\n\\nDistribution\\nIt is found in southern Alaska, British Columbia, and the US State of Washington.\\nIt is found on rocky slopes in arctic and alpine tundra at elevations up to 2000 m. In Washington, it is reported only from King County. The specific epithet \\\"lycopodioides\\\" refers to the plant's superficial resemblance to some species of clubmoss (Lycopodium sensu lato).\\n\\nSubspecies\\nCassiope lycopodioides subsp. cristapilosa, known only from the Haida Gwaii (formerly called the Queen Charlotte Islands), is recognized as a distinct taxon by some authorities but not others.\\n\\nDescription\\nCassiope lycopodioides is a perennial herb forming mats lying close to the ground. Leaves are narrow, up to 3 mm long, closely pressed against the stem. Flowers are white, bell-shaped, up to 20 mm across.\\nPassage 5:\\nDeutzia\\nDeutzia ( or ) is a genus of about 60 species of flowering plants in the family Hydrangeaceae, native to eastern and central Asia (from the Himalayas east to Japan and the Philippines), and Central America and also Europe. By far the highest species diversity is in China, where 50 species occur.\\nThe species are shrubs ranging from 1\\u20134 m (3 ft 3 in \\u2013 13 ft 1 in) in height. Most are deciduous, but a few subtropical species are evergreen. The leaves are opposite, simple, with a serrated margin. The flowers are produced in panicles or corymbs; they are white in most species, sometimes pink or reddish. The fruit is a dry capsule containing numerous small seeds. Identification of the species is very difficult, requiring often microscopic detail of the leaf hairs and seed capsule structure.\\nDeutzia is named after the 18th century Dutch patron of botany, Johan van der Deutz.\\n\\nSelected species\\n\\nCultivation and uses\\nThe deutzias are fairly new to gardens: the exception, D. scabra, was noticed in Japanese gardens by Engelbert Kaempfer (1712) and Carl Peter Thunberg (1784) but not actually seen in Europe till the 1830s; two-thirds of the species noted in the R.H.S. Dictionary were gathered in from the wild during the 20th century.Deutzias are commonly grown as ornamental plants for their white and pink flowers. Many cultivars and hybrids have been selected for garden use, including selections with double flowers. For example, Deutzia \\u00d7 lemoinei is a hybrid of D. gracilis and D. parviflora. The following cultivars and hybrids have gained the Royal Horticultural Society's Award of Garden Merit:-\\n\\nThe temperate deutzias are mostly hardy shrubs from far eastern regions where winters are dependably frozen; in milder climates, like much of England, the early-flowering species and hybrids are coaxed into premature bloom by mild spells, then spoilt by frost. Alice Coats remarks that deutzias have done better in Edinburgh, on the chilly east coast of Scotland, than in London. A solution in milder climates might be to site deutzia in the garden's most exposed, coldest microclimate, as is often done with early-flowering magnolias.\\nIdentification can be difficult, and in particular, many of the plants in cultivation sold as D. scabra are actually D. crenata (Huxley 1992). The selected hybrid white double \\\"Pride-of-Rochester\\\", already in cultivation in 1881, was originated by the Rochester, New York nurserymen Ellwanger and Barry.Deutzia scabra is used by joiners in Japan to polish wood.\\nPassage 6:\\nCassiope\\nCassiope is a genus of 9-12 small shrubby species in the family Ericaceae. It is the sole genus in the subfamily Cassiopoideae. They are native to the Arctic and north temperate montane regions. The genus is named after Cassiopeia of Greek mythology. Common names, shared with several other similar related genera, include heather and heath. They have scale-like leaves lying against the stems, and produce solitary bell-shaped flowers in late spring. Though hardy, flowers can be damaged by late frosts.\\nThey are cultivated in gardens, suitable sites being rock gardens, peat banks or glades in woodland areas.,\\nPassage 7:\\nHarrimanella\\nHarrimanella is a genus of flowering plant in the heath family Ericaceae, with a single species, Harrimanella hypnoides, also known as moss bell heather or moss heather.   It was originally named Cassiope hypnoides by Linnaeus (1737) in his Flora Lapponica, but Harrimanella hypnoides is now the accepted name at Integrated Taxonomic Information System. The species name hypnoides means 'like Hypnum ', which is a genus mosses.\\nThe plant is a cold hardy dicot perennial found growing on rock crevices in the Canadian arctic, Quebec, the Northeastern United States, Greenland, Iceland, the mountains of Norway, Sweden and Finland, Svalbard and arctic Russia, including the Ural mountains.Harrimanella hypnoides produces moss-like cushions, about 5 cm high, often of prostrate stems with ascending shoot tips. The leaves are scale-like, looking like those of a moss. The flowers are conspicuous, white and bell shaped with five fused petals and five sepals. They are borne singly on short reddish pedicels. The fruit is an erect capsule.\\nPassage 8:\\nDeutzia ningpoensis\\nDeutzia ningpoensis is a shrub in the family Hydrangeaceae.  The species is endemic to China. It grows to between 1 and 2.5 metres high and produces panicles of white flowers from May to July in its native range.\\n```\\n\\nQuestion: Which as more species Cassiope or Deutzia? \\n\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\",\n\n#     \"question\": \"Which as more species Cassiope or Deutzia?\",\n\n#     \"ground_truth\": \"Deutzia\",\n\n#     \"payload_file\": \"payload_en_3000-3840.jsonl\",\n\n#     \"do_sample\": true,\n\n#     \"temperature\": 0.1,\n\n#     \"top_p\": 0.92,\n\n#     \"top_k\": 120,\n\n#     \"max_new_tokens\": 100,\n\n#     \"completion\": \"{\\\"generated_text\\\": \\\"\\\\n\\\\nAccording to the provided context, Deutzia has about 60 species, while Cassiope has 9-12 species. Therefore, Deutzia has more species.\\\"}\",\n\n#     \"prompt_tokens\": 3010,\n\n#     \"completion_tokens\": 44,\n\n#     \"latency\": 1.6929611009982182,\n\n#     \"time_to_first_token\": null,\n\n#     \"time_per_output_token\": null,\n\n#     \"time_to_last_token\": null,\n\n#     \"uuid\": \"f8d4f8a95cf84a6d866513d8802bbbf9\",\n\n#     \"experiment_name\": \"Meta-Llama-3-8B-Instruct\",\n\n#     \"concurrency\": 2,\n\n#     \"filename\": \"results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2\"\n\n#   }\n\n#   [2024-09-23 20:35:14,499] p121634 {2288595646.py:6} INFO - shape of data from (1037, 26)\n\n#                          endpoint_name  \\\n\n#   0  http://127.0.0.1:8080/invocations   \n\n#   1  http://127.0.0.1:8080/invocations   \n\n#   2  http://127.0.0.1:8080/invocations   \n\n#   3  http://127.0.0.1:8080/invocations   \n\n#   4  http://127.0.0.1:8080/invocations   \n\n#   \n\n#                                                 prompt  \\\n\n#   0  <|begin_of_text|><|start_header_id|>user<|end_...   \n\n#   1  <|begin_of_text|><|start_header_id|>user<|end_...   \n\n#   2  <|begin_of_text|><|start_header_id|>user<|end_...   \n\n#   3  <|begin_of_text|><|start_header_id|>user<|end_...   \n\n#   4  <|begin_of_text|><|start_header_id|>user<|end_...   \n\n#   \n\n#                                               question  \\\n\n#   0         Which as more species Cassiope or Deutzia?   \n\n#   1               Both WAGS Atlanta and WAGS are what?   \n\n#   2  Passage:\\nUnits of Measurement - University of...   \n\n#   3  Are Kakwa River and Bighead River located in t...   \n\n#   4  Passage:\\nTubman: Conductor of the Underground...   \n\n#   \n\n#                                           ground_truth  \\\n\n#   0                                            Deutzia   \n\n#   1                 American reality television series   \n\n#   2  English inch,Inch (unit),International inch,De...   \n\n#   3                                                yes   \n\n#   4  National Underground Railroad Network to Freed...   \n\n#   \n\n#                    payload_file  do_sample  temperature  top_p  top_k  \\\n\n#   0  payload_en_3000-3840.jsonl       True          0.1   0.92    120   \n\n#   1   payload_en_500-1000.jsonl       True          0.1   0.92    120   \n\n#   2  payload_en_2000-3000.jsonl       True          0.1   0.92    120   \n\n#   3  payload_en_3000-3840.jsonl       True          0.1   0.92    120   \n\n#   4  payload_en_3000-3840.jsonl       True          0.1   0.92    120   \n\n#   \n\n#      max_new_tokens  ... time_to_last_token                              uuid  \\\n\n#   0           100.0  ...               None  f8d4f8a95cf84a6d866513d8802bbbf9   \n\n#   1           100.0  ...               None  c77f4eeb09304c4fb927b8008be87886   \n\n#   2           100.0  ...               None  4cbfd69056854939a36bb34268f87b85   \n\n#   3           100.0  ...               None  a933e435434e4a90b80575cad2f0a3f1   \n\n#   4           100.0  ...               None  23c57b884e1d45dea30006d3132146fc   \n\n#   \n\n#               experiment_name  concurrency  \\\n\n#   0  Meta-Llama-3-8B-Instruct            2   \n\n#   1  Meta-Llama-3-8B-Instruct            4   \n\n#   2  Meta-Llama-3-8B-Instruct            6   \n\n#   3  Meta-Llama-3-8B-Instruct            1   \n\n#   4  Meta-Llama-3-8B-Instruct            4   \n\n#   \n\n#                                           filename bad_words stop_words pad_id  \\\n\n#   0  results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2       NaN        NaN    NaN   \n\n#   1  results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2       NaN        NaN    NaN   \n\n#   2  results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2       NaN        NaN    NaN   \n\n#   3  results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2       NaN        NaN    NaN   \n\n#   4  results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2       NaN        NaN    NaN   \n\n#   \n\n#     end_id  max_tokens  \n\n#   0    NaN         NaN  \n\n#   1    NaN         NaN  \n\n#   2    NaN         NaN  \n\n#   3    NaN         NaN  \n\n#   4    NaN         NaN  \n\n#   \n\n#   [5 rows x 26 columns]\n\n# analyze payload_en_3000-3840 for a couple of result folders\n\"\"\"\n\"payload_file\": \"payload_en_1-500.jsonl\",\n  \"do_sample\": true,\n  \"temperature\": 0.1,\n  \"top_p\": 0.92,\n  \"top_k\": 120,\n  \"max_new_tokens\": 100,\n  \"completion\": \"{\\\"generated_text\\\": \\\"\\\\n\\\\n```Stauntonia is a genus of flowering plants in the family Lardizabalaceae. It is named after George Staunton, who brought it to Britain from China in the 19th century.\\\\n\\\\nSpecies\\\\nSpecies accepted by the Plants of the World Online as of March 2023:\\\\nSinofranchetia\\\\nSinofranchetia is a genus of flowering plant in the Lardizabalaceae family. It contains a single species, Sinofr\\\"}\",\n  \"prompt_tokens\": 328,\n  \"completion_tokens\": 111,\n  \"latency\": 2.4156091760087293,\n  \"time_to_first_token\": null,\n  \"time_per_output_token\": null,\n  \"time_to_last_token\": null,\n  \"uuid\": \"5aca6360ee5244a68d2a951f0067d68b\",\n  \"experiment_name\": \"Meta-Llama-3-8B-Instruct\",\n  \"concurrency\": 4,\n  \"filename\": \"results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2\"\n\"\"\"\nPAYLOAD_OF_INTEREST: str = \"payload_en_3000-3840.jsonl\" #payload_en_3000-3840.jsonl\"\ndf1 = df[(df.payload_file == PAYLOAD_OF_INTEREST)]\nRESULT_FOLDER1_OF_INTEREST: str = \"results-llama3-8b-g5.12xl-tp=4-mc=max-djl-ec2\"\n# df1 = df[(df.payload_file == PAYLOAD_OF_INTEREST) & \\\n#          (df.filename == RESULT_FOLDER1_OF_INTEREST)]\n# logger.info(f\"shape of dataframe with data for {PAYLOAD_OF_INTEREST} and {RESULT_FOLDER1_OF_INTEREST} is {df1.shape}\")\n# df1.head()\n\ndf2 = df1[['filename', 'completion_tokens', 'latency', 'question', 'ground_truth', 'completion']]\ndf2\n# Output:\n#                                                 filename  completion_tokens  \\\n\n#   0        results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2                 44   \n\n#   3        results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2                 24   \n\n#   4        results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2                117   \n\n#   5        results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2                 19   \n\n#   11       results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2                 14   \n\n#   ...                                                ...                ...   \n\n#   1019  results-llama3-8b-g5.12xl-tp=2-mc=max-triton-ec2                 23   \n\n#   1020  results-llama3-8b-g5.12xl-tp=2-mc=max-triton-ec2                  6   \n\n#   1023  results-llama3-8b-g5.12xl-tp=2-mc=max-triton-ec2                 20   \n\n#   1032  results-llama3-8b-g5.12xl-tp=2-mc=max-triton-ec2                  6   \n\n#   1035  results-llama3-8b-g5.12xl-tp=2-mc=max-triton-ec2                 11   \n\n#   \n\n#          latency                                           question  \\\n\n#   0     1.692961         Which as more species Cassiope or Deutzia?   \n\n#   3     1.271467  Are Kakwa River and Bighead River located in t...   \n\n#   4     4.177421  Passage:\\nTubman: Conductor of the Underground...   \n\n#   5     1.376350  Passage:\\nGummo Marx\\nMilton \"Gummo\" Marx (Oct...   \n\n#   11    1.123095  Hardley Flood is an area of lagoons that suppo...   \n\n#   ...        ...                                                ...   \n\n#   1019  1.322193  Passage:\\nGuillemot\\nGuillemots is the common ...   \n\n#   1020  1.132178  Which film has the director born earlier, Dos ...   \n\n#   1023  1.279956       Are Euptelea and Muehlenbeckia both genuses?   \n\n#   1032  0.987512  Passage:\\nHoy (boat)\\nA hoy was a  small sloop...   \n\n#   1035  1.049198  Passage:\\nDanube Waltz River Cruise Videos - 2...   \n\n#   \n\n#                                              ground_truth  \\\n\n#   0                                               Deutzia   \n\n#   3                                                   yes   \n\n#   4     National Underground Railroad Network to Freed...   \n\n#   5     Karl Marx,Karl Heinrich Marx,K. H. Marx,Marx, ...   \n\n#   11                                                 duck   \n\n#   ...                                                 ...   \n\n#   1019  Sea bird,Marine birds,Sea-bird,Marine bird,Sea...   \n\n#   1020                                        Dos Basuras   \n\n#   1023                                                yes   \n\n#   1032  Blustery,Eolic,Aeolian Action,Wind Cycle,Cyclo...   \n\n#   1035  Budimpe\u0161ta,Budapest,Veres P\u00e9ter Gimn\u00e1zium,Buda...   \n\n#   \n\n#                                                completion  \n\n#   0     {\"generated_text\": \"\\n\\nAccording to the provi...  \n\n#   3     {\"generated_text\": \"\\n\\nYes, Kakwa River and B...  \n\n#   4     {\"generated_text\": \"\\n\\nThe answer to the ques...  \n\n#   5     {\"generated_text\": \"\\n\\nI don't know the answe...  \n\n#   11              {\"generated_text\": \"\\n\\nI don't know.\"}  \n\n#   ...                                                 ...  \n\n#   1019  I don't know the answer to this question. The ...  \n\n#   1020                                      I don't know.  \n\n#   1023  Yes, both Euptelea and Muehlenbeckia are gener...  \n\n#   1032                                      I don't know.  \n\n#   1035          I don't know the answer to this question.  \n\n#   \n\n#   [367 rows x 6 columns]\n\nfrom typing import Dict\ndef extract_answer(x):\n    if \"generated_text\" in x:\n        return json.loads(x).get('generated_text')\n    return x\ndf2['completion'] = df2.completion.map(extract_answer)\n# Output:\n#   /tmp/ipykernel_121634/3548883129.py:6: SettingWithCopyWarning: \n\n#   A value is trying to be set on a copy of a slice from a DataFrame.\n\n#   Try using .loc[row_indexer,col_indexer] = value instead\n\n#   \n\n#   See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n#     df2['completion'] = df2.completion.map(extract_answer)\n\n\ndf2\n# Output:\n#                                                 filename  completion_tokens  \\\n\n#   0        results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2                 44   \n\n#   3        results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2                 24   \n\n#   4        results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2                117   \n\n#   5        results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2                 19   \n\n#   11       results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2                 14   \n\n#   ...                                                ...                ...   \n\n#   1019  results-llama3-8b-g5.12xl-tp=2-mc=max-triton-ec2                 23   \n\n#   1020  results-llama3-8b-g5.12xl-tp=2-mc=max-triton-ec2                  6   \n\n#   1023  results-llama3-8b-g5.12xl-tp=2-mc=max-triton-ec2                 20   \n\n#   1032  results-llama3-8b-g5.12xl-tp=2-mc=max-triton-ec2                  6   \n\n#   1035  results-llama3-8b-g5.12xl-tp=2-mc=max-triton-ec2                 11   \n\n#   \n\n#          latency                                           question  \\\n\n#   0     1.692961         Which as more species Cassiope or Deutzia?   \n\n#   3     1.271467  Are Kakwa River and Bighead River located in t...   \n\n#   4     4.177421  Passage:\\nTubman: Conductor of the Underground...   \n\n#   5     1.376350  Passage:\\nGummo Marx\\nMilton \"Gummo\" Marx (Oct...   \n\n#   11    1.123095  Hardley Flood is an area of lagoons that suppo...   \n\n#   ...        ...                                                ...   \n\n#   1019  1.322193  Passage:\\nGuillemot\\nGuillemots is the common ...   \n\n#   1020  1.132178  Which film has the director born earlier, Dos ...   \n\n#   1023  1.279956       Are Euptelea and Muehlenbeckia both genuses?   \n\n#   1032  0.987512  Passage:\\nHoy (boat)\\nA hoy was a  small sloop...   \n\n#   1035  1.049198  Passage:\\nDanube Waltz River Cruise Videos - 2...   \n\n#   \n\n#                                              ground_truth  \\\n\n#   0                                               Deutzia   \n\n#   3                                                   yes   \n\n#   4     National Underground Railroad Network to Freed...   \n\n#   5     Karl Marx,Karl Heinrich Marx,K. H. Marx,Marx, ...   \n\n#   11                                                 duck   \n\n#   ...                                                 ...   \n\n#   1019  Sea bird,Marine birds,Sea-bird,Marine bird,Sea...   \n\n#   1020                                        Dos Basuras   \n\n#   1023                                                yes   \n\n#   1032  Blustery,Eolic,Aeolian Action,Wind Cycle,Cyclo...   \n\n#   1035  Budimpe\u0161ta,Budapest,Veres P\u00e9ter Gimn\u00e1zium,Buda...   \n\n#   \n\n#                                                completion  \n\n#   0     \\n\\nAccording to the provided context, Deutzia...  \n\n#   3     \\n\\nYes, Kakwa River and Bighead River are bot...  \n\n#   4     \\n\\nThe answer to the question \"Who presents \"...  \n\n#   5         \\n\\nI don't know the answer to this question.  \n\n#   11                                    \\n\\nI don't know.  \n\n#   ...                                                 ...  \n\n#   1019  I don't know the answer to this question. The ...  \n\n#   1020                                      I don't know.  \n\n#   1023  Yes, both Euptelea and Muehlenbeckia are gener...  \n\n#   1032                                      I don't know.  \n\n#   1035          I don't know the answer to this question.  \n\n#   \n\n#   [367 rows x 6 columns]\n\ncomparison = {}\ndont_know_type_responses = {}\nvalid_responses = {}\nfor f in unique_result_files_list:\n    dont_know_type_responses[f] = 0\n    valid_responses[f] = 0\nfor r in df2.iterrows():\n    r = r[1]\n    if r['question'] not in comparison:\n        comparison[r['question']] = []\n    r = dict(r)    \n    comparison[r['question']].append(r)\n    if 'don\\'t know' in r['completion']:\n        dont_know_type_responses[r['filename']] += 1\n    else:\n        valid_responses[r['filename']] += 1\n\n    \n\n\nlogger.info(f\"dont_know_type_responses={dont_know_type_responses}\\nvalid_responses={valid_responses}\")\n# Output:\n#   [2024-09-23 20:35:14,571] p121634 {2041944104.py:1} INFO - dont_know_type_responses={'results-llama3-8b-g5.12xl-tp=2-mc=max-triton-ec2': 48, 'results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2': 233}\n\n#   valid_responses={'results-llama3-8b-g5.12xl-tp=2-mc=max-triton-ec2': 16, 'results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2': 70}\n\n\nPath(\"comparison.json\").write_text(json.dumps(comparison, indent=2))\n# Output:\n#   615563\n\nfor k in comparison.keys():\n    print(json.dumps(comparison[k], indent=2))\n    break\n# Output:\n#   [\n\n#     {\n\n#       \"filename\": \"results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2\",\n\n#       \"completion_tokens\": 44,\n\n#       \"latency\": 1.6929611009982182,\n\n#       \"question\": \"Which as more species Cassiope or Deutzia?\",\n\n#       \"ground_truth\": \"Deutzia\",\n\n#       \"completion\": \"\\n\\nAccording to the provided context, Deutzia has about 60 species, while Cassiope has 9-12 species. Therefore, Deutzia has more species.\"\n\n#     },\n\n#     {\n\n#       \"filename\": \"results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2\",\n\n#       \"completion_tokens\": 48,\n\n#       \"latency\": 4.967612214997644,\n\n#       \"question\": \"Which as more species Cassiope or Deutzia?\",\n\n#       \"ground_truth\": \"Deutzia\",\n\n#       \"completion\": \"\\n\\nAccording to the provided context, Deutzia has about 60 species, while Cassiope has 9-12 species. Therefore, Deutzia has more species than Cassiope.\"\n\n#     },\n\n#     {\n\n#       \"filename\": \"results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2\",\n\n#       \"completion_tokens\": 48,\n\n#       \"latency\": 2.8179733110009693,\n\n#       \"question\": \"Which as more species Cassiope or Deutzia?\",\n\n#       \"ground_truth\": \"Deutzia\",\n\n#       \"completion\": \"\\n\\nAccording to the provided context, Cassiope has 9-12 species, while Deutzia has about 60 species. Therefore, Deutzia has more species than Cassiope.\"\n\n#     },\n\n#     {\n\n#       \"filename\": \"results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2\",\n\n#       \"completion_tokens\": 44,\n\n#       \"latency\": 1.5979820789980295,\n\n#       \"question\": \"Which as more species Cassiope or Deutzia?\",\n\n#       \"ground_truth\": \"Deutzia\",\n\n#       \"completion\": \"\\n\\nAccording to the provided context, Deutzia has about 60 species, while Cassiope has 9-12 species. Therefore, Deutzia has more species.\"\n\n#     },\n\n#     {\n\n#       \"filename\": \"results-llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2\",\n\n#       \"completion_tokens\": 48,\n\n#       \"latency\": 3.5731225259987696,\n\n#       \"question\": \"Which as more species Cassiope or Deutzia?\",\n\n#       \"ground_truth\": \"Deutzia\",\n\n#       \"completion\": \"\\n\\nAccording to the provided context, Cassiope has 9-12 species, while Deutzia has about 60 species. Therefore, Deutzia has more species than Cassiope.\"\n\n#     },\n\n#     {\n\n#       \"filename\": \"results-llama3-8b-g5.12xl-tp=2-mc=max-triton-ec2\",\n\n#       \"completion_tokens\": 40,\n\n#       \"latency\": 1.5886268419999396,\n\n#       \"question\": \"Which as more species Cassiope or Deutzia?\",\n\n#       \"ground_truth\": \"Deutzia\",\n\n#       \"completion\": \"According to the provided context, Cassiope has 9-12 species, while Deutzia has about 60 species. Therefore, Deutzia has more species than Cassiope.\"\n\n#     }\n\n#   ]\n\n\n\n\n================================================"
  },
  {
    "filename": "accuracy.md",
    "path": "docs/accuracy.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# Model evaluations using panel of LLM evaluators\n\n`FMBench` release 2.0.0 adds support for evaluating candidate models using Majority Voting with a Panel of LLM Evaluators (PoLL). It gathers quantitative metrics such as [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) and overall majority voting accuracy metrics to measure the similarity and accuracy of model responses compared to the ground truth. \n\nAccuracy is defined as percentage of responses generated by the LLM that **_match_** the ground truth included in the dataset (as a separate column). In order to determine if an LLM generated response _matches_ the ground truth we ask other LLMs called the evaluator LLMs to compare the LLM output and the ground truth and provide a verdict if the LLM generated ground truth is correct or not given the ground truth. Here is the [link](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/src/fmbench/prompt_template/eval_criteria/claude_eval_prompt_templates/claude_eval_majority_vote.txt) to the Anthropic Claude 3 Sonnet model prompt being used as an evaluator (or a judge model). A combination of the cosine similarity and the LLM evaluator verdict decides if the LLM generated response is correct or incorrect. Finally, one LLM evaluator could be biased, could have inaccuracies so instead of relying on the judgement of a single evaluator, we rely on the majority vote of 3 different LLM evaluators. By default we use the Anthropic Claude 3 Sonnet, Meta Llama3-70b and the Cohere Command R plus model as LLM evaluators. See \n[Pat Verga et al., \"Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models\",    arXiv:2404.18796, 2024.](https://arxiv.org/abs/2404.18796) for more details on using a Panel of LLM Evaluators (PoLL).\n\n\n## Evaluation Flow\n\n1. Provide a dataset that includes ground truth responses for each sample. `FMBench` uses the [LongBench](https://huggingface.co/datasets/THUDM/LongBench) dataset by default. \n\n1. Configure the candidate models to be evaluated in the `FMBench` config file. See [this](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/src/fmbench/configs/bedrock/config-bedrock.yml) config file for an example that runs evaluations for multiple models available via Amazon Bedrock. Running evaluations only requires the following two changes to the config file:\n\n    - Set the `4_get_evaluations.ipynb: yes`, see [this](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/7bb00debea4ffacecf7acaeaf54c9039a42022f7/src/fmbench/configs/bedrock/config-bedrock.yml#L77) line.\n    - Set the `ground_truth_col_key: answers` and `question_col_key: input` parameters, see [this](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/7bb00debea4ffacecf7acaeaf54c9039a42022f7/src/fmbench/configs/bedrock/config-bedrock.yml#L90) line. The value of `ground_truth_col_key` and the `question_col_key` is set to the name of the column in the dataset that contains the ground truth and question respectively.\n\n1. Run FMBench, which will: \n\n- Fetch the inference results containing the model responses \n\n- Calculate quantitative metrics (Cosine Similarity) \n\n- Use a Panel of LLM Evaluators to compare each model response to the ground truth \n\n- Each LLM evaluator will provide a binary verdict (correct/incorrect) and an explanation \n\n- Validate the LLM evaluations using Cosine Similarity thresholds \n\n- Categorize the final evaluation for each response as correctly correct, correctly incorrect, or needs further evaluation \n\n1. Review the `FMBench` report to analyze the evaluation results and compare the performance of the candidate models. The report contains tables and charts that provide insights into model accuracy.\n\nBy leveraging ground truth data and a Panel of LLM Evaluators, FMBench provides a comprehensive and efficient way to assess the quality of generative AI models. The majority voting approach, combined with quantitative metrics, enables a robust evaluation that reduces bias and latency while maintaining consistency across responses.\n\n\n\n\n\n================================================"
  },
  {
    "filename": "advanced.md",
    "path": "docs/advanced.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# Advanced\n\nBeyond running `FMBench` with the configuration files provided, you may want try out bringing your own dataset or endpoint to `FMBench`.\n\n\n\n================================================"
  },
  {
    "filename": "analytics.md",
    "path": "docs/analytics.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# Generate downstream summarized reports for further analysis\n\nYou can use several results from various `FMBench` runs to generate a summarized report of all runs based on your cost, latency, and concurrency budgets. This report helps answer the following question:\n\n_What is the minimum number of instances N, of most cost optimal instance type T, that are needed to serve a real-time workload W while keeping the average transaction latency under L seconds?\u201d_\n\n    W: = {R transactions per-minute, average prompt token length P, average generation token length G}\n    \n\n- With this summarized report, we test the following **hypothesis**: At the low end of the total number of requests/minute smaller instances which provide good inference latency at low concurrencies would suffice (said another way, the larger more expensive instances are an overkill at this stage) but as the number of requests/minute increase there comes an inflection point beyond which the number of smaller instances required would be so much that it would be more economical to use fewer instances of the larger more expensive instances.\n\n### An example report that gets generated is as follows:\n\n#### Summary for payload: payload_en_x-y\n\n- The metrics below in the table are examples and do not represent any specific model or instance type. This table can be used to make analysis on the cost and instance maintenance perspective based on the use case. For example, `instance_type_1` costs 10 dollars and requires 1 instance to host `model_1` until it can handle 100 requests per minute. As the requests scale to a 1,000 requests per minute, 5 instances are required and cost 50 dollars. As the requests scale to 10,000 requests per minute, the number of instances to maintain scale to 30, and the cost becomes 450 dollars. \n\n- On the other hand, `instance_type_2` is more costly, with a price of $499 for 10,000 requests per minute to host the same model, but only requires 22 instances to maintain, which is 8 less than when the model is hosted on `instance_type_1`. \n\n- Based on these summaries, users can make decisions based on their use case priorities. For a real time and latency sensitive application, a user might select `instance_type_2` to host `model_1` since the user would have to maintain 8 lesser instances than hosting the model on `instance_type_1`. Hosting the model on `instance_type_2` would also maintain the `p_95 latency` (0.5s), which is half compared to `instance_type_1` (`p_95 latency`: 1s) even though it costs more than `instance_type_1`. On the other hand, if the application is cost sensitive, and the user is flexible to maintain more instances at a higher latency, they might want to shift gears to using `instance_type_1`.\n\n- Note: _Based on varying needs for prompt size, cost, and latency, the table might change._\n\n| experiment_name | instance_type | concurrency | latency_p95 | transactions_per_minute | instance_count_and_cost_1_rpm | instance_count_and_cost_10_rpm | instance_count_and_cost_100_rpm | instance_count_and_cost_1000_rpm | instance_count_and_cost_10000_rpm |\n|-----------------|---------------|-------------|-------------|--------------------------|-------------------------------|--------------------------------|---------------------------------|----------------------------------|----------------------------------|\n| model_1         | instance_type_1 | 1           | 1.0         | _                        | (1, 10)                       | (1, 10)                        | (1, 10)                         | (5, 50)                          | (30, 450)                        |\n| model_1         | instance_type_2 | 1           | 0.5         | _                        | (1, 10)                       | (1, 20)                        | (1, 20)                         | (6, 47)                          | (22, 499)                        |\n\n### FMBench Heatmap\n\nThis step also generates a heatmap that contains information about each instance, and how much it costs with per `request-per-minute` (`rpm`) breakdown. The default breakdown is [1 `rpm`, 10 `rpm`, 100 `rpm`, 1000 `rpm`, 10000 `rpm`]. View an example of a heatmap below. The model name, instance type, is masked but can be generated for your specific use case/requirements.\n\n![FMBench heatmap](img/heatmap.png)\n\n## Steps to run analytics\n\n1. Clone the `FMBench` repo from GitHub.\n\n1. Place all of the `result-{model-id}-...` folders that are generated from various runs in the top level directory.\n\n1. Run the following command to generate downstream analytics and summarized tables. Replace `x`, `y`, `z` and `model_id` with the latency, concurrency thresholds, payload file of interest (for example `payload_en_1000-2000.jsonl`) and the `model_id` respectively. The `model_id` would have to be appended to the `results-{model-id}` folders so the [analytics.py](analytics/analytics.py) file can generate a report for all of those respective result folders. \n\n    ```{.bash}\n    python analytics/analytics.py --latency-threshold x --concurrency-threshold y  --payload-file z --model-id model_id\n    ```\n\n\n\n\n================================================"
  },
  {
    "filename": "announcement.md",
    "path": "docs/announcement.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# Release 2.1 announcement\n\nWe are excited to announce some major new enhancements for `FMBench`.\n\n**Deepseek-R1 support**: The distilled version of Deepseek-R1 models are now supported for both performance benchmarking and model evaluations \ud83c\udf89. You can use built in support for 4 different datasets: [`LongBench`](https://huggingface.co/datasets/THUDM/LongBench), [`Dolly`](https://huggingface.co/datasets/databricks/databricks-dolly-15k), [`OpenOrca`](https://huggingface.co/datasets/Open-Orca/OpenOrca) and [`ConvFinQA`](https://huggingface.co/datasets/AdaptLLM/finance-tasks/tree/refs%2Fconvert%2Fparquet/ConvFinQA). You can deploy the Deepseek-R1 distilled models on Amazon EC2, Amazon Bedrock or Amazon SageMaker.\n\n**Faster installs with `uv`**: We now use `uv` instead of `conda` for creating a Python environment and installing dependencies for `FMBench`.\n\n\n# Release 2.0 announcement\n\nWe are excited to share news about a major FMBench release, we now have release 2.0 for FMBench that supports model evaluations through a panel of LLM evaluators\ud83c\udf89. With the recent feature additions to FMBench we are already seeing increased interest from customers and hope to reach even more customers and have an even greater impact. Check out all the latest and greatest features from FMBench on the FMBench website.\n\n\n**Support for Model Evaluations**: FMBench now adds support for evaluating candidate models using Majority Voting with a [Panel of LLM Evaluators](https://arxiv.org/abs/2404.18796). Customers can now use FMBench to evaluate model accuracy across open-source and custom datasets, thus FMBench now enables customers to not only measure performance (inference latency, cost, throughput) but also model accuracy.\n\n\n**Native support for LLM compilation and deployment on AWS Silicon**: FMBench now supports end-to-end compilation and model deployment on AWS Silicon. Customers no longer have to wait for models to be available for AWS Chips via SageMaker JumpStart and neither do they have to go through the process of compiling the model to Neuron themselves, FMBench does it all for them. We can simply put the relevant configuration options in the FMBench config file and it will compile and deploy the model on SageMaker ([config](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-inf2.yml)) or EC2 ([config](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-inf2-48xl-deploy-ec2.yml)).\n\n\n**Website for better user experience**: FMBench has a [website](https://aws-samples.github.io/foundation-model-benchmarking-tool/) now along with an [introduction video](https://youtu.be/yvRCyS0J90c). The website is fully searchable to ease common tasks such as installation, finding the right config file, benchmarking on various hosting platforms (EC2, EKS, Bedrock, Neuron, Docker), model evaluation, etc. This website was created based on feedback from several internal teams and external customers.\n\n\n**Native support for all AWS generative AI services**: FMBench now benchmarks and evaluates any Foundation Model (FM) deployed on any AWS Generative AI service, be it Amazon SageMaker, Amazon Bedrock, Amazon EKS, or Amazon EC2. We initially built FMBench for SageMaker, and later extended it to Bedrock and then based on customer requests extended it to support models on EKS and EC2 as well. See [list of config files](https://aws-samples.github.io/foundation-model-benchmarking-tool/manifest.html) supported out of the box, you can use these config files either as is or as templates for creating your own custom config.\n\n\n\n================================================"
  },
  {
    "filename": "benchmarking.md",
    "path": "docs/benchmarking.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# Benchmark models deployed on different AWS Generative AI services\n\n`FMBench` comes packaged with configuration files for benchmarking models on different AWS Generative AI services. \n\n## Full list of benchmarked models\n\n\n| Model                           | Amazon EC2                     | Amazon SageMaker                           | Amazon Bedrock                     |\n|:--------------------------------|:-------------------------------|:-------------------------------------------|:-----------------------------------|\n| **Deepseek-R1 distilled**        | g6e                           | g6e                                           |                            |\n| **Llama3.3-70b instruct**        |                               |                                           | On-demand                           |\n| **Qwen2.5-72b**                  | g5, g6e                       |                                           |                                    |\n| **Amazon Nova**                  |                               |                                           | On-demand                          |\n| **Anthropic Claude-3 Sonnet**    |                               |                                           | On-demand, provisioned             |\n| **Anthropic Claude-3 Haiku**     |                               |                                           | On-demand                          |\n| **Mistral-7b-instruct**          | inf2, trn1                     | g4dn, g5, p3, p4d, p5                       | On-demand                          |\n| **Mistral-7b-AWQ**               |                               | p5                                        |                                    |\n| **Mixtral-8x7b-instruct**        |                               |                                           | On-demand                          |\n| **Llama3.2-1b instruct**         | g5                            |                                           |                                    |\n| **Llama3.2-3b instruct**         | g5                            |                                           |                                    |\n| **Llama3.1-8b instruct**         | g5, p4d, p4de, p5, p5e, g6e, g6, inf2, trn1        | g4dn, g5, p3, inf2, trn1                     | On-demand                          |\n| **Llama3.1-70b instruct**        | p4d, p4de, p5, p5e, g6e, g5, inf2, trn1            | inf2, trn1                                 | On-demand                          |\n| **Llama3-8b instruct**           | g5, g6e, inf2, trn1, c8g      | g4dn, g5, p3, inf2, trn1, p4d, p5e             | On-demand                          |\n| **Llama3-70b instruct**          | g5                            | g4dn, g5, p3, inf2, trn1, p4d                 | On-demand                          |\n| **Llama2-13b chat**              |                               | g4dn, g5, p3, inf2, trn1, p4d                 | On-demand                          |\n| **Llama2-70b chat**              |                               | g4dn, g5, p3, inf2, trn1, p4d                 | On-demand                          |\n| **NousResearch-Hermes-70b**      |                               | g5, inf2, trn1                            | On-demand                          |\n| **Amazon Titan text lite**       |                               |                                           | On-demand                          |\n| **Amazon Titan text express**    |                               |                                           | On-demand                          |\n| **Cohere Command text**          |                               |                                           | On-demand                          |\n| **Cohere Command light text**    |                               |                                           | On-demand                          |\n| **AI21 J2 Mid**                  |                               |                                           | On-demand                          |\n| **AI21 J2 Ultra**                |                               |                                           | On-demand                          |\n| **Gemma-2b**                     |                               | g4dn, g5, p3                                |                                    |\n| **Phi-3-mini-4k-instruct**       |                               | g4dn, g5, p3                                |                                    |\n| **distilbert-base-uncased**      |                               | g4dn, g5, p3                                |                                    |\n\n\n================================================"
  },
  {
    "filename": "benchmarking_multimodal_models_on_bedrock.md",
    "path": "docs/benchmarking_multimodal_models_on_bedrock.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n### Benchmarking Multimodal Models on Amazon Bedrock\n\nFMBench now enables customers to benchmark multimodal models available through Amazon Bedrock. This feature supports both Claude and Llama 3.2 models, allowing you to evaluate their performance on multimodal tasks. Currently, FMBench supports multimodal benchmarking for: `Anthropic Claude` and `Meta Llama 3.2` Vision models.\n\n#### Prerequisites\n\nBefore running multimodal benchmarks, ensure you have:\n\n1. Enabled model access to `meta.llama3-2-11b-instruct-v1:0` in your Amazon Bedrock console.\n\n#### Running Multimodal Benchmarks on FMBench\n\nTo benchmark multimodal models on Amazon Bedrock, use the provided configuration files. Here's an example command:\n\n``` {.bash}\nfmbench --config-file https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/fmbench/configs/bedrock/config-llama-3-2-11b-vision-instruct-scienceqa.yml > fmbench.log 2>&1\n```\n\n##### **This command will**:\n\n1. Load the specified configuration file for Llama 3.2 11B Vision model\n\n1. Run the benchmark using the `derek-thomas/ScienceQA` dataset.\n\n\n================================================"
  },
  {
    "filename": "benchmarking_on_bedrock.md",
    "path": "docs/benchmarking_on_bedrock.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# Benchmark models on Bedrock\n\nChoose any config file from the [`bedrock`](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/main/src/fmbench/configs/bedrock) folder and either run these directly or use them as templates for creating new config files specific to your use-case. Here is an example for benchmarking the `Llama3.1` models on Bedrock.\n\n```{.bash}\nfmbench --config-file https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/fmbench/configs/bedrock/config-bedrock-llama3-1.yml > fmbench.log 2>&1\n```\n\n\n\n================================================"
  },
  {
    "filename": "benchmarking_on_ec2.md",
    "path": "docs/benchmarking_on_ec2.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# Benchmark models on EC2\n\nYou can use `FMBench` to benchmark models on hosted on EC2. This can be done in one of two ways:\n\n- Deploy the model on your EC2 instance independently of `FMBench` and then benchmark it through the [Bring your own endpoint](#bring-your-own-endpoint-aka-support-for-external-endpoints) mode.\n- Deploy the model on your EC2 instance through `FMBench` and then benchmark it.\n \nThe steps for deploying the model on your EC2 instance are described below. \n\n\ud83d\udc49 In this configuration both the model being benchmarked and `FMBench` are deployed on the same EC2 instance.\n\nCreate a new EC2 instance suitable for hosting an LMI as per the steps described [here](misc/ec2_instance_creation_steps.md). _Note that you will need to select the correct AMI based on your instance type, this is called out in the instructions_.\n\nThe steps for benchmarking on different types of EC2 instances (GPU/CPU/Neuron) and different inference containers differ slightly. These are all described below.\n\n## Benchmarking options on EC2\n- [Benchmarking on an instance type with NVIDIA GPUs or AWS Chips](#benchmarking-on-an-instance-type-with-nvidia-gpus-or-aws-chips)\n- [Benchmarking on an instance type with NVIDIA GPU and the Triton inference server](#benchmarking-on-an-instance-type-with-nvidia-gpu-and-the-triton-inference-server)\n- [Benchmarking on an instance type with AWS Chips and the Triton inference server](#benchmarking-on-an-instance-type-with-aws-chips-and-the-triton-inference-server)\n- [Benchmarking on an CPU instance type with AMD processors](#benchmarking-on-an-cpu-instance-type-with-amd-processors)\n- [Benchmarking on an CPU instance type with Intel processors](#benchmarking-on-an-cpu-instance-type-with-intel-processors)\n- [Benchmarking on an CPU instance type with ARM processors (Graviton 4)](#benchmarking-on-an-cpu-instance-type-with-arm-processors)\n\n- [Benchmarking the Triton inference server](#benchmarking-the-triton-inference-server)\n- [Benchmarking models on Ollama](#benchmarking-models-on-ollama)\n\n## Benchmarking on an instance type with NVIDIA GPUs or AWS Chips\n\n1. Connect to your instance using any of the options in EC2 (SSH/EC2 Connect), run the following in the EC2 terminal. This command installs `uv` on the instance which is then used to create a new virtual environment for `FMBench`.\n\n    ```{.bash}\n    curl -LsSf https://astral.sh/uv/install.sh | sh\n    export PATH=\"$HOME/.local/bin:$PATH\"\n    ```\n\n1. Install `docker-compose`.\n\n    ```{.bash}\n    sudo apt-get update\n    sudo apt-get install --reinstall docker.io -y\n    sudo apt-get install -y docker-compose\n    docker compose version \n    ```\n\n1. Setup the `.fmbench_python311` Python environment.\n\n    ```{.bash}\n    uv venv .fmbench_python311 --python 3.11\n    source .fmbench_python311/bin/activate\n    # Add the Python environment activation and directory navigation to .bashrc\n    echo 'source $HOME/.fmbench_python311/bin/activate' >> $HOME/.bashrc\n    uv pip install -U fmbench\n    ```\n\n1. Create local directory structure needed for `FMBench` and copy all publicly available dependencies from the AWS S3 bucket for `FMBench`. This is done by running the `copy_s3_content.sh` script available as part of the `FMBench` repo. **Replace `/tmp` in the command below with a different path if you want to store the config files and the `FMBench` generated data in a different directory**.\n\n    ```{.bash}\n    # Replace \"/tmp\" with \"/path/to/your/custom/tmp\" if you want to use a custom tmp directory\n    TMP_DIR=\"/tmp\"\n    curl -s https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/copy_s3_content.sh | sh -s -- \"$TMP_DIR\"\n    ```\n\n1. To download the model files from HuggingFace, create a `hf_token.txt` file in the `/tmp/fmbench-read/scripts/` directory containing the Hugging Face token you would like to use. In the command below replace the `hf_yourtokenstring` with your Hugging Face token. **Replace `/tmp` in the command below if you are using `/path/to/your/custom/tmp` to store the config files and the `FMBench` generated data**.\n\n    ```{.bash}\n    echo hf_yourtokenstring > $TMP_DIR/fmbench-read/scripts/hf_token.txt\n    ```\n\n1. Run `FMBench` with a packaged or a custom config file. **_This step will also deploy the model on the EC2 instance_**. The `--write-bucket` parameter value is just a placeholder and an actual S3 bucket is not required. **_Skip to the next step if benchmarking for AWS Chips_**. You could set the `--tmp-dir` flag to an EFA path instead of `/tmp` if using a shared path for storing config files and reports.\n\n    ```{.bash}\n    fmbench --config-file $TMP_DIR/fmbench-read/configs/llama3/8b/config-ec2-llama3-8b.yml --local-mode yes --write-bucket placeholder --tmp-dir $TMP_DIR > fmbench.log 2>&1\n    ```\n\n1. For example, to run `FMBench` on a `llama3-8b-Instruct` model on an `inf2.48xlarge` instance, run the command \ncommand below. The config file for this example can be viewed [here](src/fmbench/configs/llama3/8b/config-ec2-llama3-8b-inf2-48xl.yml).\n\n    ```{.bash}\n    fmbench --config-file $TMP_DIR/fmbench-read/configs/llama3/8b/config-ec2-llama3-8b-inf2-48xl.yml --local-mode yes --write-bucket placeholder --tmp-dir $TMP_DIR > fmbench.log 2>&1\n    ```\n\n1. Open a new Terminal and do a `tail` on `fmbench.log` to see a live log of the run.\n\n    ```{.bash}\n    tail -f fmbench.log\n    ```\n\n1. All metrics are stored in the `/tmp/fmbench-write` directory created automatically by the `fmbench` package. Once the run completes all files are copied locally in a `results-*` folder as usual.\n\n## Benchmarking on an instance type with NVIDIA GPU and the Triton inference server\n\n1. Follow steps in the [Benchmarking on an instance type with NVIDIA GPUs or AWS Chips](#benchmarking-on-an-instance-type-with-nvidia-gpus-or-aws-chips) section to install `FMBench` but do not run any benchmarking tests yet.\n\n1. Once `FMBench` is installed then install the following additional dependencies for Triton.\n\n    ```{.bash}\n    cd ~\n    git clone https://github.com/triton-inference-server/tensorrtllm_backend.git  --branch v0.12.0\n    # Update the submodules\n    cd tensorrtllm_backend\n    # Install git-lfs if needed\n    sudo apt --fix-broken install\n    sudo apt-get update && sudo apt-get install git-lfs -y --no-install-recommends\n    git lfs install\n    git submodule update --init --recursive\n    ```\n\n1. Now you are ready to run benchmarking with Triton. For example for benchmarking `Llama3-8b` model on a `g5.12xlarge` use the following command:\n\n    ```{.bash}\n    fmbench --config-file $TMP_DIR/fmbench-read/configs/llama3/8b/config-llama3-8b-g5.12xl-tp-2-mc-max-triton-ec2.yml --local-mode yes --write-bucket placeholder --tmp-dir $TMP_DIR > fmbench.log 2>&1\n    ```\n\n## Benchmarking on an instance type with AWS Chips and the Triton inference server\n\n**_As of 2024-09-26 this has been tested on a `trn1.32xlarge` instance_**\n\n1. Connect to your instance using any of the options in EC2 (SSH/EC2 Connect), run the following in the EC2 terminal. This command installs `uv` on the instance which is then used to create a new Python virtual environment for `FMBench`.(Note: **_Your EC2 instance needs to have at least 200GB of disk space for this test_**)\n\n    ```{.bash}\n    # Install Docker and Git using the YUM package manager\n    sudo yum install docker git -y\n\n    # Start the Docker service\n    sudo systemctl start docker\n\n    curl -LsSf https://astral.sh/uv/install.sh | sh\n    export PATH=\"$HOME/.local/bin:$PATH\"\n    ```\n\n1. Setup the `.fmbench_python311` Python virtual environment.\n\n    ```{.bash}\n    uv venv .fmbench_python311 --python 3.11\n    source .fmbench_python311/bin/activate\n    # Add the Python environment activation and directory navigation to .bashrc\n    echo 'source $HOME/.fmbench_python311/bin/activate' >> $HOME/.bashrc\n    uv pip install -U fmbench\n    ```\n\n1. First we need to build the required docker image for `triton`, and push it locally. To do this, curl the `Triton Dockerfile` and the script to build and push the triton image locally:\n\n    ```{.bash}\n    # curl the docker file for triton\n    curl -o ./Dockerfile_triton https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/fmbench/scripts/triton/Dockerfile_triton\n\n    # curl the script that builds and pushes the triton image locally\n    curl -o build_and_push_triton.sh https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/fmbench/scripts/triton/build_and_push_triton.sh\n\n    # Make the triton build and push script executable, and run it\n    chmod +x build_and_push_triton.sh\n    ./build_and_push_triton.sh\n    ```\n   - Now wait until the docker image is saved locally and then follow the instructions below to start a benchmarking test.\n\n1. Create local directory structure needed for `FMBench` and copy all publicly available dependencies from the AWS S3 bucket for `FMBench`. This is done by running the `copy_s3_content.sh` script available as part of the `FMBench` repo. **Replace `/tmp` in the command below with a different path if you want to store the config files and the `FMBench` generated data in a different directory**.\n\n    ```{.bash}\n    # Replace \"/tmp\" with \"/path/to/your/custom/tmp\" if you want to use a custom tmp directory\n    TMP_DIR=\"/tmp\"\n    curl -s https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/copy_s3_content.sh | sh -s -- \"$TMP_DIR\"\n    ```\n\n1. To download the model files from HuggingFace, create a `hf_token.txt` file in the `/tmp/fmbench-read/scripts/` directory containing the Hugging Face token you would like to use. In the command below replace the `hf_yourtokenstring` with your Hugging Face token. **Replace `/tmp` in the command below if you are using `/path/to/your/custom/tmp` to store the config files and the `FMBench` generated data**.\n\n    ```{.bash}\n    echo hf_yourtokenstring > $TMP_DIR/fmbench-read/scripts/hf_token.txt\n    ```\n\n1. Run `FMBench` with a packaged or a custom config file. **_This step will also deploy the model on the EC2 instance_**. The `--write-bucket` parameter value is just a placeholder and an actual S3 bucket is not required. You could set the `--tmp-dir` flag to an EFA path instead of `/tmp` if using a shared path for storing config files and reports. \n\n    ```{.bash}\n    fmbench --config-file $TMP_DIR/fmbench-read/configs/llama3/8b/config-llama3-8b-trn1-32xlarge-triton-djl.yml --local-mode yes --write-bucket placeholder --tmp-dir $TMP_DIR > fmbench.log 2>&1\n    ```\n\n1. Open a new Terminal and and do a `tail` on `fmbench.log` to see a live log of the run.\n\n    ```{.bash}\n    tail -f fmbench.log\n    ```\n\n1. All metrics are stored in the `/tmp/fmbench-write` directory created automatically by the `fmbench` package. Once the run completes all files are copied locally in a `results-*` folder as usual.\n\n - **Note**: To deploy a model on AWS Chips using Triton with `djl` or `vllm` backend, the configuration file requires the `backend` and `container_params` parameters within the `inference_spec` dictionary. The backend options are `vllm`/`djl` and the `container_params` contains container specific parameters to deploy the model, for example `tensor parallel degree`, `n positions`, etc. Tensor parallel degree is a necessary field to be added. If no other parameters are provided, the container will choose the default parameters during deployment.\n\n    ``` {.python}\n      # Backend options: [djl, vllm]\n      backend: djl\n\n      # Container parameters that are used during model deployment\n      container_params:\n        # tp degree is a mandatory parameter\n        tp_degree: 8\n        amp: \"f16\"\n        attention_layout: 'BSH'\n        collectives_layout: 'BSH'\n        context_length_estimate: 3072, 3584, 4096\n        max_rolling_batch_size: 8\n        model_loader: \"tnx\"\n        model_loading_timeout: 2400\n        n_positions: 4096\n        output_formatter: \"json\"\n        rolling_batch: \"auto\"\n        rolling_batch_strategy: \"continuous_batching\"\n        trust_remote_code: true\n        # modify the serving properties to match your model and requirements\n        serving.properties:\n    ```\n\n## Benchmarking on an CPU instance type with AMD processors\n\n**_As of 2024-08-27 this has been tested on a `m7a.16xlarge` instance_**\n\n1. Connect to your instance using any of the options in EC2 (SSH/EC2 Connect), run the following in the EC2 terminal. This command installs `uv` on the instance which is then used to create a new Python virtual environment for `FMBench`.\n\n    ```{.bash}\n    # Install Docker and Git using the YUM package manager\n    sudo yum install docker git -y\n\n    # Start the Docker service\n    sudo systemctl start docker\n\n    curl -LsSf https://astral.sh/uv/install.sh | sh\n    export PATH=\"$HOME/.local/bin:$PATH\"\n    ```\n\n1. Setup the `.fmbench_python311` Python virtual environment.\n\n    ```{.bash}\n    uv venv .fmbench_python311 --python 3.11\n    source .fmbench_python311/bin/activate\n    # Add the Python environment activation and directory navigation to .bashrc\n    echo 'source $HOME/.fmbench_python311/bin/activate' >> $HOME/.bashrc\n    uv pip install -U fmbench\n    ```\n\n1. Build the `vllm` container for serving the model. \n\n    1. \ud83d\udc49 The `vllm` container we are building locally is going to be references in the `FMBench` config file.\n\n    1. The container being build is for CPU only (GPU support might be added in future).\n\n        ```{.bash}\n        # Clone the vLLM project repository from GitHub\n        git clone https://github.com/vllm-project/vllm.git\n\n        # Change the directory to the cloned vLLM project\n        cd vllm\n\n        # Build a Docker image using the provided Dockerfile for CPU, with a shared memory size of 4GB\n        sudo docker build -f Dockerfile.cpu -t vllm-cpu-env --shm-size=4g .\n        ```\n\n1. Create local directory structure needed for `FMBench` and copy all publicly available dependencies from the AWS S3 bucket for `FMBench`. This is done by running the `copy_s3_content.sh` script available as part of the `FMBench` repo. **Replace `/tmp` in the command below with a different path if you want to store the config files and the `FMBench` generated data in a different directory**.\n\n    ```{.bash}\n    # Replace \"/tmp\" with \"/path/to/your/custom/tmp\" if you want to use a custom tmp directory\n    TMP_DIR=\"/tmp\"\n    curl -s https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/copy_s3_content.sh | sh -s -- \"$TMP_DIR\"\n    ```\n\n1. To download the model files from HuggingFace, create a `hf_token.txt` file in the `/tmp/fmbench-read/scripts/` directory containing the Hugging Face token you would like to use. In the command below replace the `hf_yourtokenstring` with your Hugging Face token. **Replace `/tmp` in the command below if you are using `/path/to/your/custom/tmp` to store the config files and the `FMBench` generated data**.\n\n    ```{.bash}\n    echo hf_yourtokenstring > $TMP_DIR/fmbench-read/scripts/hf_token.txt\n    ```\n\n1. Before running FMBench, add the current user to the docker group. Run the following commands to run Docker without needing to use `sudo` each time.\n\n    ```{.bash}\n    sudo usermod -a -G docker $USER\n    newgrp docker\n    ```\n\n1. Install `docker-compose`.\n\n    ```{.bash}\n    DOCKER_CONFIG=${DOCKER_CONFIG:-$HOME/.docker}\n    mkdir -p $DOCKER_CONFIG/cli-plugins\n    sudo curl -L https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m) -o $DOCKER_CONFIG/cli-plugins/docker-compose\n    sudo chmod +x $DOCKER_CONFIG/cli-plugins/docker-compose\n    docker compose version\n    ```\n\n1. Run `FMBench` with a packaged or a custom config file. **_This step will also deploy the model on the EC2 instance_**. The `--write-bucket` parameter value is just a placeholder and an actual S3 bucket is not required. You could set the `--tmp-dir` flag to an EFA path instead of `/tmp` if using a shared path for storing config files and reports.\n\n    ```{.bash}\n    fmbench --config-file $TMP_DIR/fmbench-read/configs/llama3/8b/config-ec2-llama3-8b-m7a-16xlarge.yml --local-mode yes --write-bucket placeholder --tmp-dir $TMP_DIR > fmbench.log 2>&1\n    ```\n\n1. Open a new Terminal and and do a `tail` on `fmbench.log` to see a live log of the run.\n\n    ```{.bash}\n    tail -f fmbench.log\n    ```\n\n1. All metrics are stored in the `/tmp/fmbench-write` directory created automatically by the `fmbench` package. Once the run completes all files are copied locally in a `results-*` folder as usual.\n\n\n## Benchmarking on an CPU instance type with Intel processors\n\n**_As of 2024-08-27 this has been tested on `c5.18xlarge` and `m5.16xlarge` instances_**\n\n1. Connect to your instance using any of the options in EC2 (SSH/EC2 Connect), run the following in the EC2 terminal. This command installs `uv` on the instance which is then used to create a new Python virtual environment for `FMBench`.\n\n    ```{.bash}\n    # Install Docker and Git using the YUM package manager\n    sudo yum install docker git -y\n\n    # Start the Docker service\n    sudo systemctl start docker\n\n    curl -LsSf https://astral.sh/uv/install.sh | sh\n    export PATH=\"$HOME/.local/bin:$PATH\"\n    ```\n\n1. Setup the `.fmbench_python311` Python virtual environment.\n\n    ```{.bash}\n    uv venv .fmbench_python311 --python 3.11\n    source .fmbench_python311/bin/activate\n    # Add the Python environment activation and directory navigation to .bashrc\n    echo 'source $HOME/.fmbench_python311/bin/activate' >> $HOME/.bashrc\n    uv pip install -U fmbench\n    ```\n\n1. Build the `vllm` container for serving the model. \n\n    1. \ud83d\udc49 The `vllm` container we are building locally is going to be references in the `FMBench` config file.\n\n    1. The container being build is for CPU only (GPU support might be added in future).\n\n        ```{.bash}\n        # Clone the vLLM project repository from GitHub\n        git clone https://github.com/vllm-project/vllm.git\n\n        # Change the directory to the cloned vLLM project\n        cd vllm\n\n        # Build a Docker image using the provided Dockerfile for CPU, with a shared memory size of 12GB\n        sudo docker build -f Dockerfile.cpu -t vllm-cpu-env --shm-size=12g .\n        ```\n\n1. Create local directory structure needed for `FMBench` and copy all publicly available dependencies from the AWS S3 bucket for `FMBench`. This is done by running the `copy_s3_content.sh` script available as part of the `FMBench` repo. **Replace `/tmp` in the command below with a different path if you want to store the config files and the `FMBench` generated data in a different directory**.\n\n    ```{.bash}\n    # Replace \"/tmp\" with \"/path/to/your/custom/tmp\" if you want to use a custom tmp directory\n    TMP_DIR=\"/tmp\"\n    curl -s https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/copy_s3_content.sh | sh -s -- \"$TMP_DIR\"\n    ```\n\n1. To download the model files from HuggingFace, create a `hf_token.txt` file in the `/tmp/fmbench-read/scripts/` directory containing the Hugging Face token you would like to use. In the command below replace the `hf_yourtokenstring` with your Hugging Face token. **Replace `/tmp` in the command below if you are using `/path/to/your/custom/tmp` to store the config files and the `FMBench` generated data**.\n\n    ```{.bash}\n    echo hf_yourtokenstring > $TMP_DIR/fmbench-read/scripts/hf_token.txt\n    ```\n\n1. Before running FMBench, add the current user to the docker group. Run the following commands to run Docker without needing to use `sudo` each time.\n\n    ```{.bash}\n    sudo usermod -a -G docker $USER\n    newgrp docker\n    ```\n\n1. Install `docker-compose`.\n\n    ```{.bash}\n    DOCKER_CONFIG=${DOCKER_CONFIG:-$HOME/.docker}\n    mkdir -p $DOCKER_CONFIG/cli-plugins\n    sudo curl -L https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m) -o $DOCKER_CONFIG/cli-plugins/docker-compose\n    sudo chmod +x $DOCKER_CONFIG/cli-plugins/docker-compose\n    docker compose version\n    ```\n\n1. Run `FMBench` with a packaged or a custom config file. **_This step will also deploy the model on the EC2 instance_**. The `--write-bucket` parameter value is just a placeholder and an actual S3 bucket is not required. You could set the `--tmp-dir` flag to an EFA path instead of `/tmp` if using a shared path for storing config files and reports.\n\n    ```{.bash}\n    fmbench --config-file $TMP_DIR/fmbench-read/configs/llama3/8b/config-ec2-llama3-8b-c5-18xlarge.yml --local-mode yes --write-bucket placeholder --tmp-dir $TMP_DIR > fmbench.log 2>&1\n    ```\n\n1. Open a new Terminal and and do a `tail` on `fmbench.log` to see a live log of the run.\n\n    ```{.bash}\n    tail -f fmbench.log\n    ```\n\n1. All metrics are stored in the `/tmp/fmbench-write` directory created automatically by the `fmbench` package. Once the run completes all files are copied locally in a `results-*` folder as usual.\n\n\n## Benchmarking models on Ollama\n\n**_As of 10/24/2024, this has been tested on `g6e.2xlarge` with `llama 3.1 8b`_**\n\n1. Install Ollama.\n\n    ```{bash}\n\n    curl -fsSL https://ollama.com/install.sh | sh\n\n    ```\n\n1. Pull the model required.\n\n    ```{bash}\n\n    ollama pull llama3.1:8b\n\n    ```\n\n1. Serve the model. This might produce the following error message: `Error: accepts 0 arg(s), received 1` but you can safely ignore this error.\n\n    ```{bash}\n\n    ollama serve llama3.1:8b\n\n    ```\n    \n1. Create local directory structure needed for `FMBench` and copy all publicly available dependencies from the AWS S3 bucket for `FMBench`. This is done by running the `copy_s3_content.sh` script available as part of the `FMBench` repo. **Replace `/tmp` in the command below with a different path if you want to store the config files and the `FMBench` generated data in a different directory**.\n\n    ```{.bash}\n    # Replace \"/tmp\" with \"/path/to/your/custom/tmp\" if you want to use a custom tmp directory\n    TMP_DIR=\"/tmp\"\n    curl -s https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/copy_s3_content.sh | sh -s -- \"$TMP_DIR\"\n    ```\n\n\n1. Run `FMBench` with a packaged or a custom config file. The `--write-bucket` parameter value is just a placeholder and an actual S3 bucket is not required. You could set the `--tmp-dir` flag to an EFA path instead of `/tmp` if using a shared path for storing config files and reports.\n\n    ```{.bash}\n    fmbench --config-file $TMP_DIR/fmbench-read/configs/llama3.1/8b/config-ec2-llama3-1-8b-g6e-2xlarge-byoe-ollama.yml --local-mode yes --write-bucket placeholder --tmp-dir $TMP_DIR > fmbench.log 2>&1\n    ```\n\n\n## Benchmarking on an CPU instance type with ARM processors\n\n**_As of 12/24/2024, this has been tested on `c8g.24xlarge` with `llama 3 8b Instruct` on Ubuntu Server 24.04 LTS (HVM), SSD Volume Type_**\n\n\n1. Connect to your instance using any of the options in EC2 (SSH/EC2 Connect), run the following in the EC2 terminal. This command installs `Docker` and `uv` on the instance which is then used to create a new Python virtual environment for `FMBench`.\n\n    ```{.bash}\n\n    sudo apt-get update -y\n    sudo apt-get install -y docker.io git\n    sudo systemctl start docker\n    sudo systemctl enable docker\n\n    curl -LsSf https://astral.sh/uv/install.sh | sh\n    export PATH=\"$HOME/.local/bin:$PATH\"\n    ```\n\n1. Setup the `.fmbench_python311` Python virtual environment.\n\n    ```{.bash}\n    uv venv .fmbench_python311 --python 3.11\n    source .fmbench_python311/bin/activate\n    # Add the Python environment activation and directory navigation to .bashrc\n    echo 'source $HOME/.fmbench_python311/bin/activate' >> $HOME/.bashrc\n    uv pip install -U fmbench\n    ```\n\n1. Build the `vllm` container for serving the model. \n\n    1. \ud83d\udc49 The `vllm` container we are building locally is going to be referenced in the `FMBench` config file.\n\n    1. The container being built is for ARM CPUs only.\n\n        ```{.bash}\n        # Clone the vLLM project repository from GitHub\n        git clone https://github.com/vllm-project/vllm.git\n\n        # Change the directory to the cloned vLLM project\n        cd vllm\n\n        # Build a Docker image using the provided Dockerfile for CPU, with a shared memory size of 12GB\n        sudo docker build -f Dockerfile.arm -t vllm-cpu-env --shm-size=12g .\n        ```\n\n1. Create local directory structure needed for `FMBench` and copy all publicly available dependencies from the AWS S3 bucket for `FMBench`. This is done by running the `copy_s3_content.sh` script available as part of the `FMBench` repo. **Replace `/tmp` in the command below with a different path if you want to store the config files and the `FMBench` generated data in a different directory**.\n\n    ```{.bash}\n    # Replace \"/tmp\" with \"/path/to/your/custom/tmp\" if you want to use a custom tmp directory\n    TMP_DIR=\"/tmp\"\n    curl -s https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/copy_s3_content.sh | sh -s -- \"$TMP_DIR\"\n    ```\n\n1. To download the model files from HuggingFace, create a `hf_token.txt` file in the `/tmp/fmbench-read/scripts/` directory containing the Hugging Face token you would like to use. In the command below replace the `hf_yourtokenstring` with your Hugging Face token. **Replace `/tmp` in the command below if you are using `/path/to/your/custom/tmp` to store the config files and the `FMBench` generated data**.\n\n    ```{.bash}\n    echo hf_yourtokenstring > $TMP_DIR/fmbench-read/scripts/hf_token.txt\n    ```\n\n1. Before running FMBench, add the current user to the docker group. Run the following commands to run Docker without needing to use `sudo` each time.\n\n    ```{.bash}\n    sudo usermod -a -G docker $USER\n    newgrp docker\n    ```\n\n1. Install `docker-compose`.\n\n    ```{.bash}\n    DOCKER_CONFIG=${DOCKER_CONFIG:-$HOME/.docker}\n    mkdir -p $DOCKER_CONFIG/cli-plugins\n    sudo curl -L https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m) -o $DOCKER_CONFIG/cli-plugins/docker-compose\n    sudo chmod +x $DOCKER_CONFIG/cli-plugins/docker-compose\n    docker compose version\n    ```\n\n1. Run `FMBench` with a packaged or a custom config file. **_This step will also deploy the model on the EC2 instance_**. The `--write-bucket` parameter value is just a placeholder and an actual S3 bucket is not required. You could set the `--tmp-dir` flag to an EFA path instead of `/tmp` if using a shared path for storing config files and reports.\n\n    ```{.bash}\n    fmbench --config-file $TMP_DIR/fmbench-read/configs/llama3/8b/config-ec2-llama3-8b-c8g-24xlarge.yml --local-mode yes --write-bucket placeholder --tmp-dir $TMP_DIR > fmbench.log 2>&1\n    ```\n\n1. Open a new Terminal and and do a `tail` on `fmbench.log` to see a live log of the run.\n\n    ```{.bash}\n    tail -f fmbench.log\n    ```\n\n1. All metrics are stored in the `/tmp/fmbench-write` directory created automatically by the `fmbench` package. Once the run completes all files are copied locally in a `results-*` folder as usual.\n\n\n\n================================================"
  },
  {
    "filename": "benchmarking_on_eks.md",
    "path": "docs/benchmarking_on_eks.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# Benchmark models on EKS\n\nYou can use `FMBench` to benchmark models on hosted on EKS. This can be done in one of two ways:\n\n- Deploy the model on your EKS cluster independantly of `FMBench` and then benchmark it through the [Bring your own endpoint](#bring-your-own-endpoint-aka-support-for-external-endpoints) mode.\n- Deploy the model on your EKS cluster through `FMBench` and then benchmark it.\n \nThe steps for deploying the model on your EKS cluster are described below.\n\n\ud83d\udc49 **_EKS cluster creation itself is not a part of the `FMBench` functionality, the cluster needs to exist before you run the following steps_**. Steps for cluster creation are provided in [this](misc/eks_cluster-creation_steps.md) file but it would be best to consult the [DoEKS](https://github.com/awslabs/data-on-eks) repo on GitHub for comprehensive instructions.\n\n1. Add the following IAM policies to your existing `FMBench` Role:\n\n    1. [AmazonEKSClusterPolicy](https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonEKSClusterPolicy.html): This policy provides Kubernetes the permissions it requires to manage resources on your behalf.\n    \n    1. [AmazonEKS_CNI_Policy](https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonEKS_CNI_Policy.html): This policy provides the Amazon VPC CNI Plugin (amazon-vpc-cni-k8s) the permissions it requires to modify the IP address configuration on your EKS worker nodes. This permission set allows the CNI to list, describe, and modify Elastic Network Interfaces on your behalf.\n    \n    1. [AmazonEKSWorkerNodePolicy](https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonEKSWorkerNodePolicy.html): This policy allows Amazon EKS worker nodes to connect to Amazon EKS Clusters.\n \n1. Once the EKS cluster is available you can use either the following two files or create your own config files using these files as examples for running benchmarking for these models. **_These config files require that the EKS cluster has been created as per the steps in these [instructions](https://awslabs.github.io/data-on-eks/docs/gen-ai/inference/llama3-inf2)_**.\n\n    1. [config-llama3-8b-eks-inf2.yml](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/src/fmbench/configs/llama3/8b/config-llama3-8b-eks-inf2.yml): Deploy Llama3 on Trn1/Inf2 instances.\n    \n    2. [config-mistral-7b-eks-inf2.yml](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/src/fmbench/configs/mistral/config-mistral-7b-eks-inf2.yml): Deploy Mistral 7b on Trn1/Inf2 instances.\n    \n    For more information about the [blueprints](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/main/src/fmbench/configs/eks_manifests) used by FMBench to deploy these models, view: [DoEKS docs gen-ai](https://awslabs.github.io/data-on-eks/docs/gen-ai).\n    \n1. Run the `Llama3-8b` benchmarking using the command below (replace the config file as needed for a different model). This will first deploy the model on your EKS cluster and then run benchmarking on the deployed model.\n\n    ```{.bash}\n    fmbench --config-file https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/fmbench/configs/llama3/8b/config-llama3-8b-eks-inf2.yml > fmbench.log 2>&1\n    ```\n\n1. As the model is getting deployed you might want to run the following `kubectl` commands to monitor the deployment progress. Set the _model_namespace_ to `llama3` or `mistral` or a different model as appropriate.\n\n    1. `kubectl get pods -n <model_namespace> -w`: Watch the pods in the model specific namespace.\n    1. `kubectl -n karpenter get pods`: Get the pods in the karpenter namespace.\n    1. `kubectl describe pod -n <model_namespace> <pod-name>`: Describe a specific pod in the mistral namespace to view the live logs.\n\n\n\n================================================"
  },
  {
    "filename": "benchmarking_on_sagemaker.md",
    "path": "docs/benchmarking_on_sagemaker.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# Benchmark models on SageMaker\n\nChoose any config file from the model specific folders, for example the [`Llama3`](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/main/src/fmbench/configs/llama3) folder for `Llama3` family of models. These configuration files also include instructions for `FMBench` to first deploy the model on SageMaker using your configured instance type and inference parameters of choice and then run the benchmarking. Here is an example for benchmarking `Llama3-8b` model on an `ml.inf2.24xlarge` and `ml.g5.12xlarge` instance. \n\n```{.bash}\nfmbench --config-file https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/fmbench/configs/llama3/8b/config-llama3-8b-inf2-g5.yml > fmbench.log 2>&1\n```\n\n\n\n================================================"
  },
  {
    "filename": "build.md",
    "path": "docs/build.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# Building the `FMBench` Python package\n\nIf you would like to build a dev version of `FMBench` for your own development and testing purposes, the following steps describe how to do that.\n\n1. Clone the `FMBench` repo from GitHub, change directory to `foundation-model-benchmarking-tool`.\n\n    ```{.bashrc}\n    cd foundation-model-benchmarking-tool\n    ```\n\n1. Install [`uv`](https://docs.astral.sh/uv/getting-started/) and create a virtual environment with all the dependencies that `FMBench` needs.\n   \n    ```{.bash}\n    curl -LsSf https://astral.sh/uv/install.sh | sh\n    uv venv .fmbench_python312 && source .fmbench_python312/bin/activate && uv pip install --requirement pyproject.toml\n    export UV_PROJECT_ENVIRONMENT=.fmbench_python312\n    uv add zmq\n    python -m ipykernel install --user --name=.venv --display-name=\"Python (uv fmbench env)\"\n    sudo apt-get install tree\n    ```\n\n1. Make any code changes as needed. If you want edit any notebooks in the `FMBench` then select `Python (uv fmbench env)` conda kernel for your notebook.\n\n1. Build the `FMBench` Python package.\n\n    ```{.bash}\n    uv build\n    ```\n\n1. The `.whl` file is generated in the `dist` folder. Install the `.whl` in your current Python environment.\n\n    ```{.bash}\n    uv pip install -U dist/*.whl\n    ```\n\n1. Run `FMBench` as usual through the `FMBench` CLI command.\n\n1. You may have added new config files as part of your work, to make sure these files are called out in the `manifest.txt` run the following command. This command will overwrite the existing `manifest.txt` and `manifest.md` files. Both these files need to be committed to the repo. Reach out to the maintainers of this repo so that they can add new or modified config files to the blogs bucket (the CloudFormation stack would fail if a new file is added to the manifest but is not available for download through the S3 bucket).\n\n    ```{.bash}\n    python create_manifest.py\n    ```\n\n1. To create updated documentation run the following command. You need to be added as a contributor to the `FMBench` repo to be able to publish to the website, so this command would not work for you if you are not added as a contributor to the repo.\n\n    ```{.bash}\n    mkdocs gh-deploy\n    ```\n\n\n\n\n\n================================================"
  },
  {
    "filename": "byo_dataset.md",
    "path": "docs/byo_dataset.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# Bring Your Own Dataset\n\nBy default `FMBench` uses the [`LongBench dataset`](https://github.com/THUDM/LongBench) dataset for testing the models, but this is not the only dataset you can test with. You may want to test with other datasets available on HuggingFace or use your own datasets for testing. \n\n## Hugging Face Data Preparation is now integrated within FMBench\n\nFMBench supports direct loading of Hugging Face datasets with a simplified prefixing method. To specify a Hugging Face dataset and its split, include `hf:`, followed by the `dataset identifier`, `subset name`, and `split name`. \n\nIf you only provide the `dataset-id` and not the `subset name` and `split name`, the following defaults will be used:\n  - Subset name: `default`\n  - Split name: `train`\n\n**Important**: If your dataset does not have the default `subset name` and `split name` provided above, then provide the dataset information in the config file in the following format: `hf:dataset-id/subset-name/split-name.`\n\n\nExample formats:\n  ```yaml\n  source_data_files:\n  # Full specification\n  - hf:databricks/databricks-dolly-15k/default/train\n\n  # Using defaults (subset: default, split: train)\n  - hf:databricks/databricks-dolly-15k\n  ```\n\nIn your configuration file, add entries to `source_data_files` using the following format:\n\n\n1. In your config file, prefix the dataset name with `hf:` in the `source_data_files` section:\n\n    ```yaml\n    source_data_files:\n    # Format: hf:dataset-id/subset-name/split-name.\n    - hf:THUDM/LongBench/2wikimqa_e/test\n    - hf:THUDM/LongBench/2wikimqa/test\n    - hf:THUDM/LongBench/hotpotqa_e/test\n    - hf:THUDM/LongBench/hotpotqa/test\n    - hf:THUDM/LongBench/narrativeqa/test\n    - hf:THUDM/LongBench/triviaqa_e/test\n    - hf:THUDM/LongBench/triviaqa/test\n    ```\n\nWhen FMBench encounters a dataset prefixed with `hf:`, it will:\n\n- Automatically download the dataset from Hugging Face\n- Convert it to the required JSON Lines format\n- Handle both text and image datasets dynamically\n- Store the processed data in either:\n  - The S3 read bucket for cloud deployments\n  - The `/tmp/fmbench-read/source_data/` directory for local runs\n\n> **Note**: This requires a Hugging Face token to be configured in your environment for private or gated datasets.\n\n## Using Custom Datasets\n\nIf you want to use your own dataset or a pre-processed dataset, you can:\n\n- Provide the dataset path without the `hf:` prefix in the config:\n\n    ```yaml\n    source_data_files:\n    - my-custom-dataset.jsonl\n    ```\n\n- Or, use the `[`bring_your_own_dataset`](./src/fmbench/bring_your_own_dataset.ipynb) notebook` to convert your custom dataset to JSON Lines format and upload it to the appropriate S3 bucket or local directory.\n\nFMBench will use these files directly from the specified location without any preprocessing.\n\n## Support for new Image and Text datasets\n\nWhile you can use any hugging face dataset without pre processing, FMBench provides configuration files for running `llama3-2-11b-instruct`, `claude-3-sonnet`, `claude-3-5-sonnet` on the following image and text datasets:\n\n1. Databricks dolly dataset: [config-llama-3-2-11b-databricks-dolly-15k.yml](https://github.com/aws-samples/foundation-model-benchmarking-tool/src/fmbench/configs/bedrock/config-llama-3-2-11b-databricks-dolly-15k.yml)\n1. Multimodal ScienceQA dataset: [config-llama-3-2-11b-vision-instruct-scienceqa.yml](https://github.com/aws-samples/foundation-model-benchmarking-tool/src/fmbench/configs/multimodal/bedrock/config-llama-3-2-11b-vision-instruct-scienceqa.yml)\n1. Multimodal marqo-GS-10M dataset: [config-llama-3-2-11b-vision-instruct-marqo-GS-10M.yml](https://github.com/aws-samples/foundation-model-benchmarking-tool/src/fmbench/configs/multimodal/bedrock/config-llama-3-2-11b-vision-instruct-marqo-GS-10M.yml)\n\n## Support for Open-Orca dataset\n\nSupport for [Open-Orca](https://huggingface.co/datasets/Open-Orca/OpenOrca) dataset and corresponding prompts for Llama3, Llama2 and Mistral, see:\n\n1. [bring_your_own_dataset.ipynb](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/main/src/fmbench/bring_your_own_dataset.ipynb)\n1. [prompt templates](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/main/src/fmbench/prompt_template)\n1. [Llama3 config file with OpenOrca](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/main/src/fmbench/configs/llama3/8b/config-llama3-8b-inf2-g5-byoe-w-openorca.yml)\n\n\n\n================================================"
  },
  {
    "filename": "byo_rest_predictor.md",
    "path": "docs/byo_rest_predictor.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# Bring your own `REST Predictor` ([`data-on-eks`](https://github.com/awslabs/data-on-eks/tree/7173cd98c9be6f555afc42f8311cc7849f74a038) version)\n\n`FMBench` now provides an example of bringing your own endpoint as a `REST Predictor` for benchmarking. View this [`script`](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/REST-predictor-fmbench/src/fmbench/scripts/rest_predictor.py) as an example. This script is an inference file for the `NousResearch/Llama-2-13b-chat-hf` model deployed on an [Amazon EKS](https://docs.aws.amazon.com/whitepapers/latest/overview-deployment-options/amazon-elastic-kubernetes-service.html) cluster using [Ray Serve](https://docs.ray.io/en/latest/ray-overview/examples.html). The model is deployed via `data-on-eks` which is a comprehensive resource for scaling your data and machine learning workloads on Amazon EKS and unlocking the power of Gen AI. Using `data-on-eks`, you can harness the capabilities of AWS Trainium, AWS Inferentia and NVIDIA GPUs to scale and optimize your Gen AI workloads and benchmark those models on FMBench with ease. \n\n\n\n================================================"
  },
  {
    "filename": "byoe.md",
    "path": "docs/byoe.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# Bring your own endpoint (a.k.a. support for external endpoints)\n\nIf you have an endpoint deployed on say `Amazon EKS` or `Amazon EC2` or have your models hosted on a fully-managed service such as `Amazon Bedrock`, you can still bring your endpoint to `FMBench` and run tests against your endpoint. To do this you need to do the following:\n\n1. Create a derived class from [`FMBenchPredictor`](./src/fmbench/scripts/fmbench_predictor.py) abstract class and provide implementation for the constructor, the `get_predictions` method and the `endpoint_name` property. See [`SageMakerPredictor`](./src/fmbench/scripts/sagemaker_predictor.py) for an example. Save this file locally as say `my_custom_predictor.py`.\n\n1. Upload your new Python file (`my_custom_predictor.py`) for your custom FMBench predictor to your `FMBench` read bucket and the scripts prefix specified in the `s3_read_data` section (`read_bucket` and `scripts_prefix`).\n\n1. Edit the configuration file you are using for your `FMBench` for the following:\n    - Skip the deployment step by setting the `2_deploy_model.ipynb` step under `run_steps` to `no`.\n    - Set the `inference_script` under any experiment in the `experiments` section for which you want to use your new custom inference script to point to your new Python file (`my_custom_predictor.py`) that contains your custom predictor.\n\n\n\n================================================"
  },
  {
    "filename": "cli.md",
    "path": "docs/cli.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# `FMBench` CLI\n\nHere are the command line options currently supported by the `fmbench` CLI.\n\n```{.bashrc}\nusage: fmbench [-h] --config-file CONFIG_FILE [--role-arn ROLE_ARN] [--local-mode {yes,no}] [--tmp-dir TMP_DIR] [--write-bucket WRITE_BUCKET] -A [key=value]\n\nRun FMBench with a specified config file.\n```\n\noptions:  \n  `-h`, `--help`            show this help message and exit  \n\n  `--config-file` CONFIG_FILE\n                        The S3 URI of your Config File  \n\n  `--role-arn` ROLE_ARN   (_Optional_) The ARN of the role to be used for FMBench. If an Amazon SageMaker endpoint is being deployed through FMBench then this role would also be used by that endpoint  \n\n  `--local-mode` {yes,no}  Specify if running in local mode or not. Options: yes, no. Default is no.  \n\n  `--tmp-dir` TMP_DIR    (_Optional_)  An optional tmp directory if fmbench is running in local mode.  \n\n  `--write-bucket` WRITE_BUCKET  Write bucket that is used for sagemaker endpoints in local mode and storing metrics in s3 mode.  \n\n  `-A` key=value        (_Optional_) Specify a key value pair which will be used to replace the `{key}` in the given config file. The key could be anything that you have templatized in the config file as `{key}` and it will be replaced with `value`. This comes in handy when using a generic configuration file and replace keys such `model_id`, `tp_degree` etc. Note that you are not limited to pre-defined set of keys, you can put any key in the config file as `{key}` and it will get replaced with its value. If there are multiple key value pairs, then simply specify the `-A` option multiple times in the command line.  \n\n\n\n\n\n================================================"
  },
  {
    "filename": "customize_config_files.md",
    "path": "docs/customize_config_files.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# Customize config files for specific use cases\n\n## Overview\n\nTo run `FMBench`, you have to provide a configuration file. A configuration file is simple `yml` file that contains the information about the models to benchmark, dataset information, prompt templates, custom thresholds for latency, cost and accuracy and other important metrics. View an annotated config file [here](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/fmbench/configs/llama2/7b/config-llama2-7b-g5-quick.yml). A typical `FMBench` workflow involves either directly using an already provided config file from the `configs` folder provided in the `FMBench` [website](https://aws-samples.github.io/foundation-model-benchmarking-tool/manifest.html) or [Github repo](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/main/src/fmbench/configs) or editing an already provided config file as per your own requirements (for example, benchmarking on a different instance type, or a different inference container, or even with custom datasets and various models).\n\nIn this documentation, we will do a walkthrough of the different sections that you can change within the config file based on your specific use case and requirements. **We will take an example of a user who wants to create a config file for `NousResearch/Hermes-3-Llama-3.1-70B` model on a `trn1.32xlarge` EC2 instance.**\n\n***Note: This lab is not a hands-on lab. It is a walk through of a sample configuration file that `FMBench` uses to benchmark any Foundation Model (FM) on any AWS generative AI service and description of sections that users can tweak for their own use case.***\n\nLet's get started:\n\n### FMBench Configuration Walkthrough\n\nLet's take an example to walk through a sample config file. Say a user is interested in using `llama3-70b` for their `question-answering` and `doc-summarization` use cases. A couple of questions they would ask themselves before beginning the benchmarking process is: ***Which model should I use? Should it be open-source/closed-source or proprietary fine-tuned models?, What instance should I host this model on so I can get my minimum requirements for latency, cost and accuracy satisfied?, Which dataset should I use - is there an open source data that I can use as a representation of my own dataset, or can I benchmark using my custom enterprise data? How do I compute pricing? What are the ways I can evaluate my models on accuracy?*** and so on. \n\nThe `FMBench` configuration file takes away the cognitive burden to figure out the answer to these questions and organizing them into parameters for `model id`, `instance types`, `inference containers`, datasets to use, and various other metrics that play a role in model performance and accuracy. The `FMBench` config file is broadly divided in the following:\n\n### Model Information\n\nTo decide on which model to use, on a given instance type and container, fill out the information in the `experiments` section of the configuration file. This `experiments` section contains configuration about experiments to be run. The `experiments` section is an array so more than one experiments can be added, these could belong to the **same model but different instance types, or different models, or even different hosting options**. Each experiment represents model under test and the specific information associated to that model. View an example below.\n\n  ```{.yaml}\n  experiments:\n  - name: \"Hermes-3-Llama-3.1-70B\"\n      region: {region}\n      model_id: NousResearch/Hermes-3-Llama-3.1-70B #model id, version and image uri not needed for byo endpoint\n      hf_tokenizer_model_id: meta-llama/Llama-3.1-70B\n      model_version:\n      model_name: \"Hermes-3-Llama-3.1-70B\"\n      ep_name: 'http://localhost:8080/v2/models/Hermes-3-Llama-3.1-70B/generate' \n      instance_type: \"trn1.32xlarge\"\n      image_uri: tritonserver-neuronx:fmbench \n      deploy: yes #setting to yes to run deployment script for ec2\n      instance_count: \n      deployment_script: ec2_deploy.py\n      inference_script: ec2_predictor.py\n      # This section defines the settings for Amazon EC2 instances\n      ec2:\n      model_loading_timeout: 10000\n      inference_spec:\n      parameter_set: ec2_djl\n      container_type: triton\n      # For deploying a model using the triton inference container: \n      # mention the backend type, if any. Options for triton are: [djl, vllm or tensorrt]\n      backend: djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured,\n      # max: FMBench figures out the max number of model containers to be run\n      #      based on TP degree configured and number of neuron cores/GPUs available.\n      #      For example, if TP=2, GPUs=8 then FMBench will start 4 containers and 1 load balancer,\n      # auto: only supported if the underlying inference container would automatically \n      #       start multiple copies of the model internally based on TP degree and neuron cores/GPUs\n      #       available. In this case only a single container is created, no load balancer is created.\n      #       The DJL serving containers supports auto.  \n      model_copies: max\n      shm_size: 12g\n      # The model.json parameters are replaced within the model.json file\n      # for the triton on vllm/djl/tensorrt options. The model.json already contains\n      # the tp degree and model id from above in this config file. This is a dictionary\n      # that contains other variables, such as max_num_seqs, dtype, max_model_len, block_size and more\n      # For tensorrt, the tp degree, batch size and other relevant parameters are \n      # extracted directly from the inference spec.\n      container_params:\n          # tp degree is a mandatory parameter\n          tp_degree: 32\n          amp: \"f16\" # and so on\n          # modify the serving properties to match your model and requirements\n          serving.properties:\n\n  ```\n\n  Here are the list of parameters that can be changed based on your use case and models you would like to benchmark:\n\n  - **Model Id (`model_id`)**: This is the `model id` of the model that you would like to benchmark. This can be any open source model on `HuggingFace`, a `SageMaker Jumpstart` model, a `Bedrock` model, or any Foundation Model that you would like to benchmark on **any AWS generative AI service**. For this specific example, the user was interested in benchmarking the fine-tuned version of `Llama-3.1-70B`, so they pointed the `model_id` to the HF model: `NousResearch/Hermes-3-Llama-3.1-70B`. You can change the `name` and the `model_name` parameter to any custom name that you would like to based on the `model_id` that you are using in the config file. \n\n  - **Tokenizer (`hf_tokenizer_model_id`)**: If your model is a Hugging Face model, and if you would like to use that model's tokenizer, then point the `hf_tokenizer_model_id` parameter to the `model_id` on hugging face and that specific model's tokenizer will be used in the benchmarking test.\n\n  - **Instance Type (`instance_type`)**: This is the instance type/hardware on which the model is deployed and hosted. In this case, the user was interested to deploy the model on a `trn1.32xlarge` instance, so they pointed the `instance_type` parameter to `trn1.32xlarge`. You can point this parameter to any `instance_type` that you want to deploy the model on. This can either be a `GPU`/`CPU`/`AWS Silicon (i.e. inf2/trn1/trn2)` instance. View the list of models that have been benchmarked on various instances using `FMBench` [here](https://github.com/aws-samples/foundation-model-benchmarking-tool?tab=readme-ov-file#full-list-of-benchmarked-models)\n\n\n  - **Inference Container (`image_uri`)**: If the user is interested in using a specific container of choice, they can point the `image_uri` parameter to that inference container. `FMBench` supports the `HF TGI`, `Triton`, `Deep Java Library`, `vLLM` and `Ollama` containers. This means that the user would not have to write any custom code to deploy the model or benchmark it using any of these containers that `FMBench` provides built in support for. In this case, the user was interested in benchmarking `NousResearch/Hermes-3-Llama-3.1-70B` on the `triton` inference server, so they pointed the `image_uri` to `tritonserver-neuronx:fmbench`. Users can bring their own containers and point to that within the configuration file (this would require the user to provide a custom deployment and inference script that supports the deployment and prediction format that the specific inference container supports if it is not already supported on `FMBench`). \n\n\n  - **Inference/Deployment Scripts (`deployment_script`, `inference_script`)**: `FMBench` comes packaged with multiple inference and deployment scripts. These scripts will deploy models on `SageMaker`, `Bedrock`, `EC2`, `EKS`, and also support inference on those models based on their respective inference scripts. If users deploy and make inferences from a model using a format that is not already supported on `FMBench`, users can **bring in custom deployment and predictor scripts**. Given above is an example for a model deployed on an Amazon `EC2` instance using the `ec2_deploy.py` deployment script and make inferences on the model using the `ec2_predictor.py inference script.` To view how you can bring your own custom deployment and inference files to `FMBench` to benchmark your custom models, view [here](https://aws-samples.github.io/foundation-model-benchmarking-tool/byo_rest_predictor.html). An example custom inference script [here](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/fmbench/scripts/rest_predictor.py) that is specified in [this](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/82fc6f08f168b9c1c2ff1d72a01b1004525708d6/fmbench/configs/llama2/13b/config-byo-rest-ep-llama2-13b.yml#L184) configuration file.\n\n\n  - **Endpoint Name (`ep_name`)**: This parameter specifies the endpoint URL where the model will be accessible. In the example, it's set to `'http://localhost:8080/v2/models/Hermes-3-Llama-3.1-70B/generate'` since the model is hosted on an EC2 instance. For models deployed on `Bedrock`, the `ep_name` is the `Bedrock model_id` since that is what is used while running inferences against the model. If your model is deployed on `SageMaker`, then the endpoint name is dynamically created based on what you provide as the `ep_name` in the configuration file. If you already have a model deployed and want to use your own endpoint, you can:\n      - Set `deploy: no` in the experiment configuration\n      - Provide your existing `EC2` endpoint URL/SageMaker endpoint in the `ep_name` field\n      - Skip the deployment-specific parameters as they won't be needed\n\n      For more information on bringing your own endpoint, view the documentation on it [here](https://aws-samples.github.io/foundation-model-benchmarking-tool/byoe.html).\n\n\n  - **Container Parameters (`container_params`/`serving.properties`)**: This section allows you to configure model-specific serving parameters such as:\n    - `tp_degree`: Tensor parallelism degree for distributed inference\n    - `amp`: Automatic mixed precision settings (e.g., \"f16\", \"bf16\")\n    - `serving.properties`: Additional serving configuration parameters specific to your inference container such as `max_rolling_batch_size`, `n_positions`, etc.\n    These parameters are not limited and can be changed/extended based on the parameters supported by your inference container.\n\n### Inference Parameters\n\nAfter configuring the model deployment settings, the next step is to specify how you want the model to generate responses. The inference parameters section allows you to customize the generation behavior based on your use case:\n  ```yaml\n  inference_parameters: \n    ec2_djl:\n      top_k: 50  \n      max_new_tokens: 100\n  ```\n  These parameters directly affect the model's output and performance characteristics. For example:\n\n  - For a summarization use case, a user might want to set `max_new_tokens` to a higher value like `512` or `1024` to allow for comprehensive summaries of longer documents.\n  - For a quick Q&A application, you might keep `max_new_tokens` lower at `100-200` to get concise responses.\n  - The `top_k` parameter controls response diversity by limiting the token selection to the k most likely next tokens.\n\n  You can add any parameter that your **inference container supports**. The parameters are organized by deployment type (ec2_djl, SageMaker, bedrock, any custom parameters that you would want to set etc.) to match the `parameter_set` specified in your experiment configuration. For example, if using Bedrock's model, you would specify `bedrock` or any custom parameter set name:\n\n  ```yaml\n  inference_parameters:\n    bedrock:\n      temperature: 0.7\n      max_new_tokens: 512\n      top_p: 0.9\n  ```\n\n  Once you have defined your inference parameters, you can point to that inference parameter spec in the experiment section as given below:\n\n  ```yaml\n  # Model configurations for llama-2 7b for deploying on g5 x and 2x large instances\nexperiments:\n  - name: <model_name>\n    model_id: # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: <your-model-name>\n    ep_name: \"<your-endpoint-name>\"\n    .\n    .\n    .\n    inference_spec:\n      parameter_set: bedrock # you can have a different inference parameter set for each experiment depending on the\n                             # model inference parameters\n  ``` \n\n\n#### Use custom datasets & prompts within `FMBench` \n\n`FMBench` now supports benchmarking models using datasets from Hugging Face with a simplified prefixing method. To specify a Hugging Face dataset and its split, use the `hf:` prefix followed by the `dataset identifier`, `subset name`, and `split name`. If a `subset name` is not provided, it defaults to `default`. If a `split name` is not provided, `FMBench` automatically selects the next available split at runtime.\n\n  - To configure your dataset in `FMBench`, add entries to `source_data_files` in your configuration file:\n    ```yaml\n    source_data_files:\n    # Format: hf:dataset-id/subset-name/split-name\n    # If no subset name is provided, use \"default\".\n    - hf:THUDM/LongBench/2wikimqa_e/test\n    - hf:THUDM/LongBench/2wikimqa/test\n    - hf:THUDM/LongBench/hotpotqa_e/test\n    - hf:THUDM/LongBench/hotpotqa/test\n    - hf:THUDM/LongBench/narrativeqa/test\n    - hf:THUDM/LongBench/triviaqa_e/test\n    - hf:THUDM/LongBench/triviaqa/test\n    ```\nYou can follow this format for any `text` or `image-based` dataset from Hugging Face. Alternatively, you can use custom datasets in `JSONL` format.\n\n- For domain-specific or personalized benchmarking, you can use custom datasets. These datasets can be:\n  - **Synthetic/Open source datasets (available on Hugging Face)**\n  - **Proprietary data (not publicly available)**\n\n- To use custom data, convert it into JSONL format. We provide a sample notebook to help convert Hugging Face or custom datasets into JSONL and upload them to an S3 bucket used by FMBench. Follow the steps in the [bring_your_own_dataset]((https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/src/fmbench/bring_your_own_dataset.ipynb)) notebook to integrate your own dataset into `FMBench`. Place these `JSONL` files in the `source_data` directory within `/tmp/fmbench-read/source_data` in your local instance. .\n\n- **Use specific keys from the dataset in your prompts**: Since `FMBench` uses `LongBench` as the dataset under test by default, it requires specific keys that contain `user queries`, `context`, or other necessary fields. To specify dataset keys, add them under `prompt_template_keys` in the `datasets` section of your configuration file:\n\n  ```yaml\n  datasets:\n      prompt_template_keys:\n        - input\n        - context\n  ```\n\n  These keys correspond to fields in the Hugging Face dataset, as shown in the example below:\n\n  ![hf_ds_keys](img/hf_ds_keys.png)\n\n- **Using a Custom Prompt Template**: The specified dataset keys can be used in a custom prompt template for generating input payloads. Below is an example of a prompt template utilizing these keys:\n\n  ```\n  <think>\n    There can be multiple question answer pairs in the context.\n    As soon as you find the first question in the text below immediately stop reading any further and just answer the question.\n    Always start your response with \"<think>\" at the beginning of every output and think step by step.\n    Keep your thinking process short and your answers concise, do not overthink.\n    Make sure to always provide an answer, if you do not know the answer then say I do not known but never leave the answer field empty in your response.\n    </think>\n\n    <answer>\n    Put your final answer in one line starting with the word Answer:\n    </answer>\n\n    Here is the text for you to work on:\n\n    <text>\n    {input}\n\n    {context}\n    </text>\n  ```\n\n- **Adding the Prompt Template to FMBench**: To use the custom prompt template, place it in the `/tmp/fmbench-read/prompt_templates` directory. `FMBench` will download and apply it during benchmarking.\n\n  ``` yaml\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n  ```\n  By following these steps, you can seamlessly integrate Hugging Face datasets or custom data into FMBench, enabling tailored benchmarking for your use case.\n\n**Filtering Options**: If your dataset contains multiple languages and includes a language field, you can filter it to retain only prompts in a specific language. Additionally, you can filter prompts based on token length, which is determined using the tokenizer specified in the tokenizer_prefix in the S3 bucket. The example below filters for English prompts with a token length between `1,000` and `2,000`, saving the results in a designated payload file that `FMBench` then uses in the benchmarking test. You can filter this based on your custom token length filtering preferences.\n\n  ```yaml\n  datasets:\n    filters:\n      - language: en    \n        min_length_in_tokens: 1000\n        max_length_in_tokens: 2000\n        payload_file: payload_en_1000-2000.jsonl\n  ```\n\n**Metrics Configuration**: Specify `dataset_of_interest` for focused performance analysis. While the tests would run on all the datasets configured in the experiment entries below but the price|performance analysis is only done for 1 dataset which is listed below as the dataset_of_interest. If a user is interested in seeing model benchmarks for prompt sizes `1000-2000` tokens, then set the `dataset_of_interest` to `en_1000-2000`. If it is a `summarization use case` and your dataset is large enough, you can add a filter to use `payload_en_3000-3840.jsonl` and set the `dataset_of_interest` to `en_3000-3840` tokens. This can be any custom value.\n\n  ```yaml\n  datasets:\n    .\n    .\n    .\n    metrics:\n      dataset_of_interest: en_1000-2000\n  ```\n\n#### Bring your own Endpoint (BYOE Configuration)\n\n- You can customize FMBench to use the BYOE mode when you want to bring an already deployed model either on AWS or your custom infrastructure.\n\n- Point the `ep_name` parameter in your configuration file to the `endpoint URL` so that `FMBench` can use it while making predictions. View [`here`](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/fmbench/configs/byoe/config-byo-custom-rest-predictor.yml#L199).\n  \n  ```yaml\n  ep_name: '<your-ep-url>' # public DNS/URL to send your request\n  ```\n- If you have an endpoint that has a custom inference format, then create a derived class from [FMBenchPredictor](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/fmbench/scripts/fmbench_predictor.py) abstract class and provide implementation for the constructor, the `get_predictions` method and the `endpoint_name` property. Specify the name of that file in the config file next to the `inference_script` parameter [here](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/e28af9bfa524a0e54205fc50e5f717d81c8d75a9/fmbench/configs/byoe/config-byo-custom-rest-predictor.yml#L212). No deployment script is needed since you are bringing your own endpoint.\n\n  ```yaml\n  # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n  # and Bedrock. You can also add your own. This is an example of a custom rest predictor\n  # that does a POST request on the endpoint URL with custom headers,\n  # parameters and authentication information\n  inference_script: custom_rest_predictor.py\n  ```\n\n- Place your custom FMBench predictor (custom_rest_predictor.py) in your EC2 `/tmp/fmbench-read/scripts` directory. View an example of an inference file that you can use here: https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/fmbench/scripts/custom_rest_predictor.py. \n\n- Set the `deploy` variable in the experiments section of the config file to `no` because the model does not have to be deployed since this is a `byoe` mode. Set the `2_deploy_model.ipynb` notebook in the `run_steps` section to `yes`. Even though the model is not deployed, the notebook will identify that `deploy` from the experiments section is set to `no` and just log the provided endpoint for further use.\n\n  ```yaml\n  ## section that enables container to run notebooks and python scripts automatically \n  run_steps:\n      0_setup.ipynb: yes\n      1_generate_data.ipynb: yes\n      2_deploy_model.ipynb: yes # Set the deploy notebook to yes. This will not deploy the model, but will identify that the `deploy` variable in the `experiments` section below is set to 'no', \n                                # and will just log the endpoint provided for further use in the benchmarking test\n      3_run_inference.ipynb: yes\n      4_model_metric_analysis.ipynb: yes\n      5_cleanup.ipynb: no\n  .\n  .\n  .\n  .\n  experiments:\n  - name: bring-your-own-sm-endpoint\n    model_id: # model id, version and image uri not needed for byo endpoint\n    model_version:\n    deploy: no # set deploy to \"no\" in the experiments section because the model does not have to be deployed since this is a byoe mode\n    model_name: <your-model-name>\n    ep_name: \"<your-endpoint-name>\"\n    instance_type:  \"<your-instance-type>\"\n  ```\n\n- Build FMBench as per instructions [here](https://aws-samples.github.io/foundation-model-benchmarking-tool/build.html). This will install a developer version of FMBench in your Python venv.\n  \nAfter following these steps, you will be able to run `FMBench` with your own endpoint. `FMBench` will utilize the custom `FMBench` predictor and run inferences against the endpoint. All raw inferences are saved in a `per_inference` directory and used in the report generation process. Follow the steps in the next section to bring your own dataset and prompt templates.\n\n\n#### Pricing Information\n\n`FMBench` measures model performance, which translates into inference latency, token throughput and cost per transaction. The cost is determined by `FMBench` in two ways: instance based pricing or token based pricing. All pricing information is stored in a [`pricing.yml`](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/fmbench/configs/pricing.yml) which contains [hourly instance based pricing](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/e28af9bfa524a0e54205fc50e5f717d81c8d75a9/fmbench/configs/pricing.yml#L2) (for example Amazon EC2 instances) and [token based pricing](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/e28af9bfa524a0e54205fc50e5f717d81c8d75a9/fmbench/configs/pricing.yml#L36) (for example Amazon Bedrock). The existing file contains several prices for instances on Amazon EC2 and SageMaker. To bring your own pricing, simply specify the name of your instance type followed by the custom hourly/token-based price and FMBench will use that pricing in the benchmarking test.\n\n  ```{.yaml}\n  pricing: pricing.yml # <your-custom-pricing.yml>\n  ```\n\nAdd your pricing in the `pricing.yml` file:\n\n  ```yaml\n  instance_based:\n    your-custom-instance-type: <your-pricing>\n  token_based:\n    <your-model-id>:\n      input-per-1k-tokens: <custom price per 1k input tokens>\n      output-per-1k-tokens: <custom price per 1k output tokens>\n  ```\n\n**Note**: Make sure the instance type specified in your `FMBench` config file matches the instance type specified in the `pricing.yml` file so that FMBench can correctly map the cost during the test. Place the pricing.yml file in the `/tmp/fmbench-read/configs` directory.\n\n\n\n#### Model Evaluations\n\nAccuracy is defined as percentage of responses generated by the `LLM` that match the ground truth included in the dataset (as a separate column). In order to determine if an `LLM` generated response matches the ground truth we ask other `LLMs` called the evaluator `LLMs` to compare the `LLM` output and the ground truth and provide a verdict if the `LLM` generated ground truth is correct or not given the ground truth. Here is the [link](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/fmbench/prompt_template/eval_criteria/claude_eval_prompt_templates/claude_eval_majority_vote.txt) to the `Anthropic Claude 3 Sonnet model` prompt being used as an evaluator (or a judge model). A combination of the `cosine similarity` and the LLM evaluator verdict decides if the `LLM` generated response is correct or incorrect. Finally, one `LLM` evaluator could be biased, could have inaccuracies so instead of relying on the judgement of a single evaluator, we rely on the majority vote of 3 different LLM evaluators. By default we use the `Anthropic Claude 3.5 Sonnet V2`, `Meta Llama3.3-70b Instruct` and the `Cohere Command R plus` model as LLM evaluators. See ***Pat Verga et al., \"Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models\", arXiv:2404.18796, 2024.*** for more details on using a `Panel of LLM Evaluators (PoLL)`. The following [file](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/fmbench/configs/model_eval_all_info.yml) in the configuration file contains judge model information, prompt templates used for evaluations, inference parameters, etc. \n\n  ```yaml\n  # name of the file that contains the model evaluation information\n  # for example, the prompt template names, the ground truth column name (if any), \n  # LLM panelist information, inference parameters, etc.\n  model_evaluations: model_eval_all_info.yml\n  ```\n\n  For more information on model evaluations using `FMBench` view this [notebook](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/fmbench/4_get_evaluations.ipynb) and this [documentation](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/docs/accuracy.md).\n\n  - This file contains information about metrics and the LLM judges (with their inference parameters) that will be used while evaluating candidate models. To add the evaluation step to `FMBench`, add it as a step under `run_steps` section in the configuration file (view step 4):\n\n  ```yaml\n  run_steps:\n      0_setup.ipynb: yes\n      1_generate_data.ipynb: yes\n      2_deploy_model.ipynb: no\n      3_run_inference.ipynb: yes\n      4_get_evaluations.ipynb: yes\n      5_model_metric_analysis.ipynb: yes\n      6_cleanup.ipynb: no\n  ```\n\n  - `FMBench`'s panel of LLM judges uses the model responses and compares it to the ground truth provided in the dataset. If there is a ground truth column, replace the following parameters with the name of the column. The ground truth and question column keys can be fetched from the Hugging Face dataset or your custom dataset. View an example below:\n\n  ![ground-truth-col-key](img/ground_truth_info.png)\n\n  Then, use the question and ground truth key from the dataset below in the configuration file. This will be used by `FMBench`'s evaluators to evaluate the correctness of models to be benchmarked.\n  \n  ```yaml\n  # Represents the column with the ground truth\n  ground_truth_col_key: answers\n  # Represents the column with questions/instructions\n  question_col_key: input\n  ```\n\n\n#### Benchmarking Thresholds & components\n\nThe `report` section allows you to set specific performance thresholds and constraints for your use case. These thresholds help determine whether a model deployment configuration meets your requirements:\n\n  ```yaml\n  report:\n      latency_budget: 3\n      cosine_similarity_budget: 0.3\n      accuracy_budget: 1\n      accuracy_error_rate_budget: 0\n      cost_per_10k_txn_budget: 200\n      error_rate_budget: 0\n  ```\n\n  In this use case, the user was interested in getting responses to questions within `3s` with a cost budget of `$200` per `10k` transactions. If the user has a more real-time application, they can set the `latency_budget` to `1s` or lower to get the most optimal model serving stack that satisfies that requirement. User's can also set accuracy thresholds in their report. If they are evaluating whether model responses are accurate compared to ground truth provided in the dataset, they can set an `accuracy budget` and a `cosine similarity` budget that are paired together to determine the accuracy of a response. \n\n\n**Run Steps Configuration**: The `FMBench` workflow consists of several sequential notebooks that handle different aspects of the benchmarking process, from setup to cleanup. Each step can be enabled or disabled using the `run_steps` configuration in the YAML file. While typically all steps would run in sequence, you have the flexibility to skip certain steps by setting them to `no` if you've already completed them or want to rerun specific analyses. For example, if you've already deployed your model and generated/collected inference data, you could set `2_deploy_model.ipynb` and `3_run_inference.ipynb` to `no` and only run the analysis notebooks with different parameters - this is particularly useful when you want to experiment with different performance thresholds (like adjusting latency budgets or cost constraints) without having to redeploy models or rerun inferences.\n\n  ```yaml\n  # steps to run, usually all of these would be\n  # set to yes so nothing needs to change here\n  # you could, however, bypass some steps for example\n  # set the 2_deploy_model.ipynb to no if you are re-running\n  # the same config file and the model is already deployed\n  run_steps:\n      0_setup.ipynb: yes\n      1_generate_data.ipynb: yes\n      2_deploy_model.ipynb: no\n      3_run_inference.ipynb: yes\n      4_get_evaluations.ipynb: yes\n      5_model_metric_analysis.ipynb: yes\n      6_cleanup.ipynb: no\n  ```\n\n\n### Resources:\n\n`FMBench` provides several configuration files for benchmarking models on Bedrock, SageMaker, EC2, Bring your own endpoint, EKS, etc. These configuration files can be found on the `FMBench` Github repo [here](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/main/src/fmbench/configs):\n\n![config-structure](img/config-structure.png)\n\n\n\n================================================"
  },
  {
    "filename": "deepseek.md",
    "path": "docs/deepseek.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# DeepSeek-R1\n\nThe distilled version of Deepseek-R1 models are now supported for both performance benchmarking and model evaluations \ud83c\udf89. You can use built in support for 4 different datasets: [`LongBench`](https://huggingface.co/datasets/THUDM/LongBench), [`Dolly`](https://huggingface.co/datasets/databricks/databricks-dolly-15k), [`OpenOrca`](https://huggingface.co/datasets/Open-Orca/OpenOrca) and [`ConvFinQA`](https://huggingface.co/datasets/AdaptLLM/finance-tasks/tree/refs%2Fconvert%2Fparquet/ConvFinQA). You can deploy the Deepseek-R1 distilled models on Amazon EC2, Amazon Bedrock or Amazon SageMaker.\n\nThe easiest way to benchmark the DeepSeek models is through the [`FMBench-orchestrator`](https://github.com/awslabs/fmbench-orchestrator) on Amazon EC2 VMs.\n\n## Benchmark Deepseek-R1 distilled models on Amazon EC2\n\n\ud83d\udc49 Make sure your account has enough service quota for vCPUs to run this benchmark. We would be using `g6e.xlarge`, `g6e.2xlarge`, `g6e.12xlarge` and `g6e.48xlarge` instances, if you do not have sufficient service quota then you can set `deploy: no` in the `configs/deepseek/deepseek-convfinqa.yml` (or other) file to disable some tests as needed.\n\nFollow instructions [here](https://github.com/awslabs/fmbench-orchestrator?tab=readme-ov-file#install-fmbench-orchestrator-on-ec2) to install the orchestrator. Once installed you can run Deepseek-r1 benchmarking with the [`ConvFinQA`](https://huggingface.co/datasets/AdaptLLM/finance-tasks/tree/refs%2Fconvert%2Fparquet/ConvFinQA) dataset the following command:\n\n```{.bashrc}\npython main.py --config-file configs/deepseek/deepseek-convfinqa.yml\n```\nChange the `--config-file` parameter to [`configs/deepseek/deepseek-longbench.yml`](https://github.com/aws-samples/fmbench-orchestrator/blob/main/configs/deepseek/deepseek-longbench.yml) or [`configs/deepseek/deepseek-openorca.yml`](https://github.com/aws-samples/fmbench-orchestrator/blob/main/configs/deepseek/deepseek-openorca.yml) to use other datasets for benchmarking. These orchestrator files test various Deepseek-R1 distilled models on `g6e` instances, edit this file as per your requirements. \n\n## Benchmark Deepseek-R1 quantized models on Amazon EC2\n\n\ud83d\udc49 Make sure your account has enough service quota for vCPUs to run this benchmark. We would be using `g6e.12xlarge` instance for this test.\n\n\n1. Create a `g6e.12xlarge` instance and run the `DeepSeek-R1 1.58b quantized` model on this instance by following the steps 1 through 8 described [here](https://github.com/aarora79/deepseek-r1-ec2?tab=readme-ov-file#quantized-models).\n\n1. Follow steps 1 through 5 [here](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#benchmarking-on-an-instance-type-with-nvidia-gpus-or-aws-chips) to setup `FMBench` on this instance.\n\n1. Next run the following command to benchmark LongBench \n\n    ```{.bashrc}\n    TMP_DIR=/tmp\n    fmbench --config-file $TMP_DIR/fmbench-read/configs/deepseek/config-deepseek-r1-quant1.58-longbench-byoe.yml --local-mode yes --write-bucket placeholder --tmp-dir $TMP_DIR > fmbench.log 2>&1\n    ```\n\n1. Once the run completes you should see the benchmarking results in a folder called `results-DeepSeek-R1-quant-1.58bit-g6e.12xl` present in your current directory.\n\n\n\n================================================"
  },
  {
    "filename": "ec2.md",
    "path": "docs/ec2.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# Run `FMBench` on Amazon EC2\n\nFor some enterprise scenarios it might be desirable to run `FMBench` directly on an EC2 instance with no dependency on S3. Here are the steps to do this:\n\n1. Have a `t3.xlarge` (or larger) instance in the `Running` stage. Make sure that the instance has at least 50GB of disk space and the IAM role associated with your EC2 instance has `AmazonSageMakerFullAccess` policy associated with it and `sagemaker.amazonaws.com` added to its Trust relationships.\n    ```{.bash}\n    {\n        \"Effect\": \"Allow\",\n        \"Principal\": {\n            \"Service\": \"sagemaker.amazonaws.com\"\n        },\n        \"Action\": \"sts:AssumeRole\"\n    }\n    ```\n\n1. Setup the `fmbench_python311` conda environment. This step required conda to be installed on the EC2 instance, see [instructions](https://www.anaconda.com/download) for downloading Anaconda.\n\n    ```{.bash}\n    conda create --name fmbench_python311 -y python=3.11 ipykernel\n    source activate fmbench_python311;\n    pip install -U fmbench\n    ```\n\n1. Create local directory structure needed for `FMBench` and copy all publicly available dependencies from the AWS S3 bucket for `FMBench`. This is done by running the `copy_s3_content.sh` script available as part of the `FMBench` repo. **Replace `/tmp` in the command below with a different path if you want to store the config files and the `FMBench` generated data in a different directory**.\n\n    ```{.bash}\n    # Replace \"/tmp\" with \"/path/to/your/custom/tmp\" if you want to use a custom tmp directory\n    TMP_DIR=\"/tmp\"\n    curl -s https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/copy_s3_content.sh | sh -s -- \"$TMP_DIR\"\n    ```\n\n\n1. Run `FMBench` with a packaged or a custom config file. The `--write-bucket` parameter value is just a placeholder and an actual S3 bucket is not required. You could set the `--tmp-dir` flag to an EFA path instead of `/tmp` if using a shared path for storing config files and reports.\n\n    ```{.bash}\n    fmbench --config-file $TMP_DIR/fmbench-read/configs/llama3/8b/config-ec2-llama3-8b.yml --local-mode yes --write-bucket placeholder --tmp-dir $TMP_DIR > fmbench.log 2>&1\n    ```\n\n1. Open a new Terminal and navigate to the `foundation-model-benchmarking-tool` directory and do a `tail` on `fmbench.log` to see a live log of the run.\n\n    ```{.bash}\n    tail -f fmbench.log\n    ```\n\n1. All metrics are stored in the `/tmp/fmbench-write` directory created automatically by the `fmbench` package. Once the run completes all files are copied locally in a `results-*` folder as usual.\n\n\n\n================================================"
  },
  {
    "filename": "evaluation.md",
    "path": "docs/evaluation.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n\n\n\n================================================"
  },
  {
    "filename": "features.md",
    "path": "docs/features.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# `FMBench` features\n\n**Support for Model Evaluations**: FMBench now adds support for evaluating candidate models using Majority Voting with a [Panel of LLM Evaluators](https://arxiv.org/abs/2404.18796). Customers can now use FMBench to evaluate model accuracy across open-source and custom datasets, thus FMBench now enables customers to not only measure performance (inference latency, cost, throughput) but also model accuracy.\n\n\n**Native support for LLM compilation and deployment on AWS Silicon**: FMBench now supports end-to-end compilation and model deployment on AWS Silicon. Customers no longer have to wait for models to be available for AWS Chips via SageMaker JumpStart and neither do they have to go through the process of compiling the model to Neuron themselves, FMBench does it all for them. We can simply put the relevant configuration options in the FMBench config file and it will compile and deploy the model on SageMaker ([config](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-inf2.yml)) or EC2 ([config](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-inf2-48xl-deploy-ec2.yml)).\n\n\n**Website for better user experience**: FMBench has a [website](https://aws-samples.github.io/foundation-model-benchmarking-tool/) now along with an [introduction video](https://youtu.be/yvRCyS0J90c). The website is fully searchable to ease common tasks such as installation, finding the right config file, benchmarking on various hosting platforms (EC2, EKS, Bedrock, Neuron, Docker), model evaluation, etc. This website was created based on feedback from several internal teams and external customers.\n\n\n**Native support for all AWS generative AI services**: FMBench now benchmarks and evaluates any Foundation Model (FM) deployed on any AWS Generative AI service, be it Amazon SageMaker, Amazon Bedrock, Amazon EKS, or Amazon EC2. We initially built FMBench for SageMaker, and later extended it to Bedrock and then based on customer requests extended it to support models on EKS and EC2 as well. See [list of config files](https://aws-samples.github.io/foundation-model-benchmarking-tool/manifest.html) supported out of the box, you can use these config files either as is or as templates for creating your own custom config.\n\n\n\n================================================"
  },
  {
    "filename": "gettingstarted.md",
    "path": "docs/gettingstarted.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# Getting started with `FMBench`\n\n`FMBench` is available as a Python package on [PyPi](https://pypi.org/project/fmbench) and is run as a command line tool once it is installed. All data that includes metrics, reports and results are stored in an Amazon S3 bucket.\n\nWhile technically you can run `FMBench` on any AWS compute but practically speaking we either run it on a SageMaker Notebook or on EC2. Both these options are described below.\n\n**Intro Video**\n\n[![FMBench Intro](img/fmbench-thumbnail.png)](https://www.youtube.com/watch?v=yvRCyS0J90c)\n\n![FMBench deployments](img/fmbench-deployment1.png)\n\n## `FMBench` in a client-server configuration on Amazon EC2\n\nOften times there might be a need where a platform team would like to have a bunch of LLM endpoints deployed in an account available permanently for data science teams or application teams to benchmark performance and accuracy for their specific use-case. They can take advantage of a special client-server configuration for `FMBench` where it can be used to deploy models on EC2 instances in one AWS account (called the server account) and run tests against these endpoints from `FMBench` deployed on EC2 instances in another AWS account (called the client AWS account).\n\nThis has the advantage that every team that wants to benchmark a set of LLMs does not first have to deploy the LLMs, a platform team can do that for them and have these LLMs available for a longer duration as these teams do their benchmarking, for example for their specific datasets, for their specific cost and performance criteria. Using `FMBench` in this way makes the process simpler for both teams as the platform team can use `FMBench` for easily deploying the models with full control on the configuration of the serving stack without having to write any LLM deployment code for EC2 and the data science teams or application teams can test with different datasets, performance criteria and inference parameters. As long as the security groups have an inbound rule to allow access to the model endpoint (typically TCP port 8080) an `FMBench` installation in the client AWS account should be able to access an endpoint in the server AWS account.\n\n![FMBench deployments client-server](img/fmbench-deployment2.png)\n\n\n================================================"
  },
  {
    "filename": "index.md",
    "path": "docs/index.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# Benchmark foundation models on AWS\n\n`FMBench` is a Python package for running performance benchmarks for **any Foundation Model (FM)** deployed on **any AWS Generative AI service**, be it **Amazon SageMaker**, **Amazon Bedrock**, **Amazon EKS**, or **Amazon EC2**. The FMs could be deployed on these platforms either directly through `FMbench`, or, if they are already deployed then also they could be benchmarked through the **Bring your own endpoint** mode supported by `FMBench`. \n\nHere are some salient features of `FMBench`:\n\n1. **Highly flexible**: in that it allows for using any combinations of instance types (`g5`, `p4d`, `p5`, `Inf2`), inference containers (`DeepSpeed`, `TensorRT`, `HuggingFace TGI` and others) and parameters such as tensor parallelism, rolling batch etc. as long as those are supported by the underlying platform. \n\n1. **Benchmark any model**: it can be used to be benchmark _open-source models_, _third party models_, and _proprietary models_ trained by enterprises on their own data.\n\n1. **Run anywhere**: it can be run on any AWS platform where we can run Python, such as Amazon EC2, Amazon SageMaker, or even the AWS CloudShell. _It is important to run this tool on an AWS platform so that internet round trip time does not get included in the end-to-end response time latency_.\n\n## The need for benchmarking\n\n<!-- markdown-link-check-disable -->\nCustomers often wonder what is the best AWS service to run FMs for _my specific use-case_ and _my specific price performance requirements_. While model evaluation metrics are available on several leaderboards ([`HELM`](https://crfm.stanford.edu/helm/lite/latest/#/leaderboard), [`LMSys`](https://chat.lmsys.org/?leaderboard)), but the price performance comparison can be notoriously hard to find and even more harder to trust. In such a scenario, we think it is best to be able to run performance benchmarking yourself on either on your own dataset or on a similar (task wise, prompt size wise) open-source datasets such as ([`LongBench`](https://huggingface.co/datasets/THUDM/LongBench), [`QMSum`](https://paperswithcode.com/dataset/qmsum)). This is the problem that [`FMBench`](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/main) solves.\n<!-- markdown-link-check-enable -->\n\n## [`FMBench`](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/main): an open-source Python package for FM benchmarking on AWS\n\n`FMBench` runs inference requests against endpoints that are either deployed through `FMBench` itself (as in the case of SageMaker) or are available either as a fully-managed endpoint (as in the case of Bedrock) or as bring your own endpoint. The metrics such as inference latency, transactions per-minute, error rates and cost per transactions are captured and presented in the form of a Markdown report containing explanatory text, tables and figures. The figures and tables in the report provide insights into what might be the best serving stack (instance type, inference container and configuration parameters) for a given FM for a given use-case.\n\nThe following figure gives an example of the price performance numbers that include inference latency, transactions per-minute and concurrency level for running the `Llama2-13b` model on different instance types available on SageMaker using prompts for Q&A task created from the [`LongBench`](https://huggingface.co/datasets/THUDM/LongBench) dataset, these prompts are between 3000 to 3840 tokens in length. **_Note that the numbers are hidden in this figure but you would be able to see them when you run `FMBench` yourself_**.\n\n![`Llama2-13b` on different instance types ](./img/business_summary.png)\n\nThe following table (also included in the report) provides information about the best available instance type for that experiment<sup>1</sup>.\n\n|Information\t|Value\t|\n|---\t|---\t|\n|experiment_name\t|llama2-13b-inf2.24xlarge\t|\n|payload_file\t|payload_en_3000-3840.jsonl\t|\n|instance_type\t|ml.inf2.24xlarge\t|\n|concurrency\t|**\t|\n|error_rate\t|**\t|\n|prompt_token_count_mean\t|3394\t|\n|prompt_token_throughput\t|2400\t|\n|completion_token_count_mean\t|31\t|\n|completion_token_throughput\t|15\t|\n|latency_mean\t|**\t|\n|latency_p50\t|**\t|\n|latency_p95\t|**\t|\n|latency_p99\t|**\t|\n|transactions_per_minute\t|**\t|\n|price_per_txn\t|**\t|\n\n<sup>1</sup> ** values hidden on purpose, these are available when you run the tool yourself.\n\nThe report also includes latency Vs prompt size charts for different concurrency levels. As expected, inference latency increases as prompt size increases but what is interesting to note is that the increase is much more at higher concurrency levels (and this behavior varies with instance types).\n\n![Effect of prompt size on inference latency for different concurrency levels](./img/latency_vs_tokens.png)\n\n## Determine the optimal model for your generative AI workload\n\nUse `FMBench` to determine model accuracy using a panel of LLM evaluators (PoLL [[1]](#1)). Here is one of the plots generated by `FMBench` to help answer the accuracy question for various FMs on Amazon Bedrock (the model ids in the charts have been blurred out on purpose, you can find them in the actual plot generated on running FMBench).\n\n![Accuracy trajectory with prompt size](img/accuracy_trajectory_per_payload.png)\n\n![Overall accuracy](img/overall_candidate_model_majority_voting_accuracy.png)\n\n\n## References\n<a id=\"1\">[1]</a> \n[Pat Verga et al., \"Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models\",    arXiv:2404.18796, 2024.](https://arxiv.org/abs/2404.18796)\n\n\n================================================"
  },
  {
    "filename": "mm_copies.md",
    "path": "docs/mm_copies.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# Running multiple model copies on Amazon EC2\n\nIt is possible to run multiple copies of a model if the tensor parallelism degree and the number of GPUs/Neuron cores on the instance allow it. For example if a model can fit into 2 GPU devices and there are 8 devices available then we could run 4 copies of the model on that instance. Some inference containers, such as the [DJL Serving LMI](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/index.html) automatically start multiple copies of the model within the same inference container for the scenario described in the example above. However, it is also possible to do this ourselves by running multiple containers and a load balancer through a Docker compose file. `FMBench` now supports this functionality by adding a single parameter called `model_copies` in the configuration file.\n\nFor example, here is a snippet from the [config-ec2-llama3-1-8b-p4-tp-2-mc-max](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/1db3cdd09ba4dafc095f3c5313fcd5dd1a48313c/src/fmbench/configs/llama3.1/8b/config-llama3.1-8b-trn1-32xl-deploy-tp-8-ec2.yml#L199) config file. The new parameters are `model_copies`, `tp_degree` and `shm_size` in the `inference_spec` section. **_Note that the `tp_degree` in the `inference_spec` and `option.tensor_parallel_degree` in the `serving.properties` section should be set to the same value_**.\n\n```{.bash}\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, \"1\", \"2\",..max\n      # set to 1 in the code if not configured,\n      # max: FMBench figures out the max number of model containers to be run\n      #      based on TP degree configured and number of neuron cores/GPUs available.\n      #      For example, if TP=2, GPUs=8 then FMBench will start 4 containers and 1 load balancer,\n      # auto: only supported if the underlying inference container would automatically \n      #       start multiple copies of the model internally based on TP degree and neuron cores/GPUs\n      #       available. In this case only a single container is created, no load balancer is created.\n      #       The DJL serving containers supports auto.  \n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n```\n\n## Considerations while setting the `model_copies` parameter\n\n1. The `model_copies` parameter is an EC2 only parameter, which means that you cannot use it when deploying models on SageMaker for example.\n\n1. If you are looking for the best (lowest) inference latency then you might get better results with setting the `tp_degree` and `option.tensor_parallel_degree` to the total number of GPUs/Neuron cores available on your EC2 instance and `model_copies` to `max` or `auto` or `1`, in other words, the model is being shared across all accelerators and there can be only 1 copy of the model that can run on that instance (therefore setting `model_copies` to `max` or `auto` or `1` all result in the same thing i.e. a single copy of the model running on that EC2 instance).\n\n1. If you are looking for the best (highest) transaction throughput while keeping the inference latency within a given latency budget then you might want to configure `tp_degree` and `option.tensor_parallel_degree` to the least number of GPUs/Neuron cores on which the model can run (for example for `Llama3.1-8b` that would be 2 GPUs or 4 Neuron cores) and set the `model_copies` to `max`. Let us understand this with an example, say you want to run `Llama3.1-8b` on a `p4de.24xlarge` instance type, you set `tp_degree` and `option.tensor_parallel_degree` to 2 and `model_copies` to `max`, `FMBench` will start 4 containers (as the `p4de.24xlarge` has 8 GPUs) and an Nginx load balancer that will round-robin the incoming requests to these 4 containers. In case of the DJL serving LMI you can achieve similar results by setting the `model_copies` to `auto` in which case `FMBench` will start a single container (and no load balancer since there is only one container) and then the DJL serving container will internally start 4 copies of the model within the same container and route the requests to these 4 copies internally. Theoretically you should expect the same performance but in our testing we have seen better performance with `model_copies` set to `max` and having an external (Nginx) container doing the load balancing.\n\n\n\n================================================"
  },
  {
    "filename": "neuron.md",
    "path": "docs/neuron.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# Benchmark foundation models for AWS Chips\n\nYou can use `FMBench` for benchmarking foundation model on [AWS Chips](https://aws.amazon.com/silicon-innovation/): Trainium 1, Inferentia 2. This can be done on Amazon SageMaker, Amazon EKS or on Amazon EC2. FMs need to be first compiled for [Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html) before they can be deployed on AWS Chips, this is made easier by SageMaker JumpStart which provides most of the FMs as a JumpStart Model that can be deployed on SageMaker directly, you can also compile models for Neuron yourself or do this through `FMBench` itself. All of these options are described below.\n\n\n## Benchmarking for AWS Chips on SageMaker\n\n1. Several FMs are available through SageMaker JumpStart already compiled for Neuron and ready to deploy. See [this link](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html) for more details.\n\n1. You can compile the model outside of `FMBench` using instructions available [here](https://github.com/aarora79/compile-llm-for-aws-silicon) and on the [Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html) documentation, deploy on SageMaker and use `FMBench` in the `bring your own endpoint` mode, see [this](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/src/fmbench/configs/byoe/config-model-byo-sagemaker-endpoint.yml) config file for an example.\n\n1. You can have `FMBench` compile and deploy the model on SageMaker for you. See this [Llama3-8b](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/859419e27760fd5ceeadf89361c06560ce4e79d5/src/fmbench/configs/llama3/8b/config-ec2-neuron-llama3-8b-inf2-48xl.yml) config file for example or this [Llama3.1-70b](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/859419e27760fd5ceeadf89361c06560ce4e79d5/src/fmbench/configs/llama3.1/70b/config-ec2-llama3-1-70b-inf2.yml). Search this website for \"inf2\" or \"trn1\" to find other examples. In this case `FMBench` will download the model from Hugging Face (you need to provide your HuggingFace token in the `/tmp/fmbench-read/scripts/hf_token.txt` file, the file simply contains the token without any formatting), compile it for neuron, upload the compiled model to S3 (you specify the bucket in the config file) and then deploy the model to a SageMaker endpoint.\n\n\n## Benchmarking for AWS Chips on EC2\n\nYou may want to benchmark models hosted directly on EC2. In this case both `FMBench` and the model are running on the same EC2 instance. `FMBench` will deploy the model for you on the EC2 instance. See this [Llama3.1-70b](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/859419e27760fd5ceeadf89361c06560ce4e79d5/src/fmbench/configs/llama3.1/70b/config-ec2-llama3-1-70b-inf2-48xl-deploy-ec2.yml) file for example or this [Llama3-8b](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/859419e27760fd5ceeadf89361c06560ce4e79d5/src/fmbench/configs/llama3/8b/config-ec2-llama3-8b-inf2-48xl.yml) file. In this case `FMBench` will download the model from Hugging Face (you need to provide your HuggingFace token in the `/tmp/fmbench-read/scripts/hf_token.txt` file, the file simply contains the token without any formatting), pull the inference container from the ECR repo and then run the container with the downloaded model, a local endpoint is provided that is then used by `FMBench` to run inference.\n\n\n\n================================================"
  },
  {
    "filename": "quarto.md",
    "path": "docs/quarto.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# Setting up Quarto\n\n`FMBench` uses [`Quarto`](https://quarto.org/) for generating reports. At the end of a run it downloads a Quarto container from `registry.gitlab.com/quarto-forge/docker/quarto quarto` and converts a markdown report into an HTML report. If however, download a Docker container is blocked in your environment you can install Quarto using the following steps. These are also described on the Quarto [website](https://quarto.org/docs/download/tarball.html).\n\nHere are the steps, copy paste them on a Linux based Amazon EC2 instance. This is required to be done one time only. \n\n```{.bash}\n# download Quarto tarball\nwget https://github.com/quarto-dev/quarto-cli/releases/download/v1.6.39/quarto-1.6.39-linux-amd64.tar.gz\n\n# replace the workspace path as appropriate for your environment\nWORKSPACE_PATH=~\n\nmkdir $WORKSPACE_PATH/opt\ntar -C $WORKSPACE_PATH/opt -xvzf quarto-1.6.39-linux-amd64.tar.gz\nmkdir -p $WORKSPACE_PATH/.local/bin\nln -s $WORKSPACE_PATH/opt/quarto-1.6.39/bin/quarto $WORKSPACE_PATH/.local/bin/quarto\n( echo \"\"; echo \"export PATH=\\$PATH:$WORKSPACE_PATH/.local/bin/quarto\" ; echo \"\" ) >> ~/.profile\nsource ~/.profile\n```\n\n\n\n================================================"
  },
  {
    "filename": "quickstart.md",
    "path": "docs/quickstart.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# Quickstart - run `FMBench` on SageMaker Notebook\n\n1. Each `FMBench` run works with a configuration file that contains the information about the model, the deployment steps, and the tests to run. A typical `FMBench` workflow involves either directly using an already provided config file from the [`configs`](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/main/src/fmbench/configs) folder in the `FMBench` GitHub repo or editing an already provided config file as per your own requirements (say you want to try benchmarking on a different instance type, or a different inference container etc.).\n\n    \ud83d\udc49 A simple config file with key parameters annotated is included in this repo, see [`config-llama2-7b-g5-quick.yml`](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/main/src/fmbench/configs/llama2/7b/config-llama2-7b-g5-quick.yml). This file benchmarks performance of Llama2-7b on an `ml.g5.xlarge` instance and an `ml.g5.2xlarge` instance. You can use this config file as it is for this Quickstart.\n\n1. Launch the AWS CloudFormation template included in this repository using one of the buttons from the table below. The CloudFormation template creates the following resources within your AWS account: Amazon S3 buckets, Amazon IAM role and an Amazon SageMaker Notebook with this repository cloned. A read S3 bucket is created which contains all the files (configuration files, datasets) required to run `FMBench` and a write S3 bucket is created which will hold the metrics and reports generated by `FMBench`. The CloudFormation stack takes about 5-minutes to create.\n\n   |AWS Region                |     Link        |\n   |:------------------------:|:-----------:|\n   |us-east-1 (N. Virginia)    | [<img src=\"./img/ML-FMBT-cloudformation-launch-stack.png\">](https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=fmbench&templateURL=https://aws-blogs-artifacts-public.s3.amazonaws.com/artifacts/ML-FMBT/template.yml) |\n   |us-west-2 (Oregon)    | [<img src=\"./img/ML-FMBT-cloudformation-launch-stack.png\">](https://console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/new?stackName=fmbench&templateURL=https://aws-blogs-artifacts-public.s3.amazonaws.com/artifacts/ML-FMBT/template.yml) |\n   |us-gov-west-1 (GovCloud West)    | [<img src=\"./img/ML-FMBT-cloudformation-launch-stack.png\">](https://us-gov-west-1.console.amazonaws-us-gov.com/cloudformation/home?region=us-gov-west-1#/stacks/new?stackName=fmbench&templateURL=https://aws-blogs-artifacts-public.s3.amazonaws.com/artifacts/ML-FMBT/template.yml) |\n\n1. Once the CloudFormation stack is created, navigate to SageMaker Notebooks and open the `fmbench-notebook`.\n\n1. On the `fmbench-notebook` open a Terminal and run the following commands.\n    ```{.bash}\n    conda create --name fmbench_python311 -y python=3.11 ipykernel\n    source activate fmbench_python311;\n    pip install -U fmbench\n    ```\n\n1. Now you are ready to `fmbench` with the following command line. We will use a sample config file placed in the S3 bucket by the CloudFormation stack for a quick first run.\n    \n    1. We benchmark performance for the `Llama2-7b` model on a `ml.g5.xlarge` and a `ml.g5.2xlarge` instance type, using the `huggingface-pytorch-tgi-inference` inference container. This test would take about 30 minutes to complete and cost about $0.20.\n    \n    1. It uses a simple relationship of 750 words equals 1000 tokens, to get a more accurate representation of token counts use the `Llama2 tokenizer` (instructions are provided in the next section). ***It is strongly recommended that for more accurate results on token throughput you use a tokenizer specific to the model you are testing rather than the default tokenizer. See instructions provided later in this document on how to use a custom tokenizer***.\n\n        ```{.bash}\n        account=`aws sts get-caller-identity | jq .Account | tr -d '\"'`\n        region=`aws configure get region`\n        fmbench --config-file s3://sagemaker-fmbench-read-${region}-${account}/configs/llama2/7b/config-llama2-7b-g5-quick.yml > fmbench.log 2>&1\n        ```\n\n    1. Open another terminal window and do a `tail -f` on the `fmbench.log` file to see all the traces being generated at runtime.\n\n        ```{.bash}\n        tail -f fmbench.log\n        ```\n\n    1. \ud83d\udc49 For streaming support on SageMaker and Bedrock checkout these config files:\n        1. [config-llama3-8b-g5-streaming.yml](src/configs/llama3/8b/config-llama3-8b-g5-streaming.yml)\n        1. [config-bedrock-llama3-streaming.yml](src/configs/bedrock/config-bedrock-llama3-streaming.yml)\n\n1. The generated reports and metrics are available in the `sagemaker-fmbench-write-<replace_w_your_aws_region>-<replace_w_your_aws_account_id>` bucket. The metrics and report files are also downloaded locally and in the `results` directory (created by `FMBench`) and the benchmarking report is available as a markdown file called `report.md` in the `results` directory. You can view the rendered Markdown report in the SageMaker notebook itself or download the metrics and report files to your machine for offline analysis.\n\n## `FMBench` on GovCloud\n\nNo special steps are required for running `FMBench` on GovCloud. The CloudFormation link for `us-gov-west-1` has been provided in the section above.\n\n1. Not all models available via Bedrock or other services may be available in GovCloud. The following commands show how to run `FMBench` to benchmark the [Amazon Titan Text Express](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-text-models.html#titantx-express) model in the GovCloud. See the [Amazon Bedrock GovCloud](https://docs.aws.amazon.com/govcloud-us/latest/UserGuide/govcloud-bedrock.html) page for more details.\n\n    ```{.bash}\n    account=`aws sts get-caller-identity | jq .Account | tr -d '\"'`\n    region=`aws configure get region`\n    fmbench --config-file s3://sagemaker-fmbench-read-${region}-${account}/configs/bedrock/config-bedrock-titan-text-express.yml > fmbench.log 2>&1\n    ```\n\n\n\n================================================"
  },
  {
    "filename": "releases.md",
    "path": "docs/releases.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# Releases\n\n## 2.1.4\n\n1. `Llama3.1-8b` config file for `p5en` instance type.\n1. Remove `vllm` from `pyproject.toml`.\n\n## 2.1.3\n\n1. SGLang support.\n\n## 2.1.1\n\n1. Optimized prompt templates for DeepSeek-R1 and Amazon Nova for `ConvFinQA` and `LongBench` datasets.\n\n## 2.1.0\n\n1. Deepseek-R1 distilled model support using [`vllm`](https://github.com/vllm-project/vllm).\n1. Evaluate Deepseek performance with `LongBench`, `OpenOrca`, `Dolly` and [`ConvFinQA`](https://huggingface.co/datasets/AdaptLLM/ConvFinQA) datasets.\n1. Replace `conda` with [`uv`](https://docs.astral.sh/uv/) for faster installs.\n\n## 2.0.27\n\n1. Ollama end to end support\n\n## 2.0.26\n\n1. Bug fix for missing HuggingFace token file.\n1. Config file enhancements\n\n## 2.0.25\n\n1. Fix bug with an alternate VariantName for SageMaker BYOE.\n\n## 2.0.24\n\n1. ARM benchmarking support (AWS Graviton 4).\n1. Relax IAM permission requirements for Amazon SageMaker bring your own endpoint.\n\n## 2.0.23\n\n1. Bug fixes for Amazon SageMaker BYOE.\n1. Additional config files.\n\n\n## 2.0.22\n1. Benchmarks for the [Amazon Nova](https://docs.aws.amazon.com/nova/latest/userguide/what-is-nova.html) family of models.\n1. Benchmarks for multi-modal models: LLama3.2-11B, Claude 3 Sonnet and Claude 3.5 Sonnet using the [ScienceQA](https://huggingface.co/datasets/derek-thomas/ScienceQA) dataset.\n\n## 2.0.21\n\n1. Dynamically get EC2 pricing from Boto3 API.\n1. Update pricing information and model id for Amazon Bedrock models.\n\n## 2.0.20\n\n1. Add `hf_tokenizer_model_id` parameter to automatically download tokenizers from Hugging Face.\n\n## 2.0.19\n1. Config files for `Llama3.1-1b` on AMD/Intel CPU instance types.\n1. Bug fixes for token counting for vLLM.\n\n# 2.0.18\n1. Delete SageMaker endpoint as soon as the run finishes.\n\n## 2.0.17\n1. Add support for embedding models through SageMaker jumpstart\n1. Add support for LLama 3.2 11b Vision Instruct benchmarking through FMBench\n1. Fix DJL Inference while deploying djl on EC2(424 Inference bug)\n\n## 2.0.16\n1. Update to torch 2.4 for compatibility with SageMaker Notebooks.\n\n## 2.0.15\n1. Support for [Ollama](https://github.com/ollama/ollama), see more details [here](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#benchmarking-models-on-ollama).\n1. Fix bugs with token counting.\n\n## 2.0.14\n\n1. `Llama3.1-70b` config files and more.\n1. Support for [`fmbench-orchestrator`](https://github.com/awslabs/fmbench-orchestrator).\n\n## 2.0.13\n\n1. Update `pricing.yml` additional config files.\n\n## 2.0.11\n\n1. `Llama3.2-1b` and `Llama3.2-3b` support on EC2 g5.\n1. `Llama3-8b` on EC2 `g6e` instances.\n\n## 2.0.9\n\n1. Triton-djl support for AWS Chips.\n1. Tokenizer files are now downloaded directly from Hugging Face (unless provided manually as before) \n\n## 2.0.7\n\n1. Support Triton-TensorRT for GPU instances and Triton-vllm for AWS Chips.\n1. Misc. bug fixes.\n\n## 2.0.6\n\n1. Run multiple model copies with the DJL serving container and an Nginx load balancer on Amazon EC2.\n1. Config files for `Llama3.1-8b` on `g5`, `p4de` and `p5` Amazon EC2 instance types.\n1. Better analytics for creating internal leaderboards.\n\n## 2.0.5\n\n1. Support for Intel CPU based instances such as `c5.18xlarge` and `m5.16xlarge`.\n\n## 2.0.4\n\n1. Support for AMD CPU based instances such as `m7a`.\n\n## 2.0.3\n\n1. Support for a EFA directory for benchmarking on EC2.\n\n## 2.0.2\n\n1. Code cleanup, minor bug fixes and report improvements.\n\n## 2.0.0\n\n1. \ud83d\udea8 Model evaluations done by a **Panel of LLM Evaluators** \ud83d\udea8\n\n## v1.0.52\n\n1. Compile for AWS Chips (Trainium, Inferentia) and deploy to SageMaker directly through `FMBench`.\n1. `Llama3.1-8b` and `Llama3.1-70b` config files for AWS Chips (Trainium, Inferentia).\n1. Misc. bug fixes.\n\n## v1.0.51\n1. `FMBench` has a [website](https://aws-samples.github.io/foundation-model-benchmarking-tool/index.html) now. Rework the README file to make it lightweight.\n1. `Llama3.1` config files for Bedrock.\n\n## v1.0.50\n1. `Llama3-8b` on Amazon EC2 `inf2.48xlarge` config file.\n1. Update to new version of DJL LMI (0.28.0).\n\n## v1.0.49\n1. Streaming support for Amazon SageMaker and Amazon Bedrock.\n1. Per-token latency metrics such as time to first token (TTFT) and mean time per-output token (TPOT).\n1. Misc. bug fixes.\n\n## v1.0.48\n1. Faster result file download at the end of a test run.\n1. `Phi-3-mini-4k-instruct` configuration file.\n1. Tokenizer and misc. bug fixes.\n\n\n## v1.0.47\n1. Run `FMBench` as a Docker container.\n1. Bug fixes for GovCloud support.\n1. Updated README for EKS cluster creation.\n\n## v1.0.46\n1. Native model deployment support for EC2 and EKS (i.e. you can now deploy and benchmark models on EC2 and EKS).\n1. FMBench is now available in GovCloud.\n1. Update to latest version of several packages.\n\n## v1.0.45\n1. Analytics for results across multiple runs.\n1. `Llama3-70b` config files for `g5.48xlarge` instances.\n\n## v1.0.44\n1. Endpoint metrics (CPU/GPU utilization, memory utiliztion, model latency) and invocation metrics (including errors) for SageMaker Endpoints.\n1. `Llama3-8b` config files for `g6` instances.\n\n## v1.0.42\n1. Config file for running `Llama3-8b` on all instance types except `p5`.\n1. Fix bug with business summary chart.\n1. Fix bug with deploying model using a DJL DeepSpeed container in the no S3 dependency mode.\n\n## v1.0.40\n1. Make it easy to run in the Amazon EC2 without any dependency on Amazon S3 dependency mode.\n\n## v1.0.39\n1. Add an internal `FMBench` website.\n\n## v1.0.38\n1. Support for running `FMBench` on Amazon EC2 without any dependency on Amazon S3.\n1. [`Llama3-8b-Instruct`](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) config file for `ml.p5.48xlarge`.\n\n## v1.0.37\n1. `g5`/`p4d`/`inf2`/`trn1` specific config files for [`Llama3-8b-Instruct`](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct).\n    1. `p4d` config file for both `vllm` and `lmi-dist`.\n\n## v1.0.36\n1. Fix bug at higher concurrency levels (20 and above).\n1. Support for instance count > 1.\n\n## v1.0.35\n\n1. Support for [Open-Orca](https://huggingface.co/datasets/Open-Orca/OpenOrca) dataset and corresponding prompts for Llama3, Llama2 and Mistral.\n\n## v1.0.34\n1. Don't delete endpoints for the bring your own endpoint case.\n1. Fix bug with business summary chart.\n\n## v1.0.32\n\n1. Report enhancements: New business summary chart, config file embedded in the report, version numbering and others.\n\n1. Additional config files: Meta Llama3 on Inf2, Mistral instruct with `lmi-dist` on `p4d` and `p5` instances.\n\n\n\n================================================"
  },
  {
    "filename": "resources.md",
    "path": "docs/resources.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n## Pending enhancements\n\nView the [ISSUES](https://github.com/aws-samples/foundation-model-benchmarking-tool/issues) on GitHub and add any you might think be an beneficial iteration to this benchmarking harness.\n\n## Security\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License\n\nThis library is licensed under the MIT-0 License. See the [LICENSE](./LICENSE) file.\n\n\n\n================================================"
  },
  {
    "filename": "results.md",
    "path": "docs/results.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# Results\n\nDepending upon the experiments in the config file, the `FMBench` run may take a few minutes to several hours. Once the run completes, you can find the report and metrics in the local `results-*` folder in the directory from where `FMBench` was run. The rpeort and metrics are also written to the write S3 bucket set in the [config file](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/src/fmbench/configs/config-mistral-7b-tgi-g5.yml#L12).\n\nHere is a screenshot of the `report.md` file generated by `FMBench`.\n![Report](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/img/results.gif?raw=true)\n\n\n\n================================================"
  },
  {
    "filename": "run_as_container.md",
    "path": "docs/run_as_container.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# Run `FMBench` as a Docker container\n\nYou can now run `FMBench` on any platform where you can run a Docker container, for example on an EC2 VM, SageMaker Notebook etc. The advantage is that you do not have to install anything locally, so no `conda` installs needed anymore. Here are the steps to do that.\n\n1. Create local directory structure needed for `FMBench` and copy all publicly available dependencies from the AWS S3 bucket for `FMBench`. This is done by running the `copy_s3_content.sh` script available as part of the `FMBench` repo. You can place model specific tokenizers and any new configuration files you create in the `/tmp/fmbench-read` directory that is created after running the following command. \n\n    ```{.bash}\n    curl -s https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/copy_s3_content.sh | sh\n    ```\n\n1. That's it! You are now ready to run the container.\n\n    ```{.bash}\n    # set the config file path to point to the config file of interest\n    CONFIG_FILE=https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/fmbench/configs/llama2/7b/config-llama2-7b-g5-quick.yml\n    docker run -v $(pwd)/fmbench:/app \\\n      -v /tmp/fmbench-read:/tmp/fmbench-read \\\n      -v /tmp/fmbench-write:/tmp/fmbench-write \\\n      aarora79/fmbench:v1.0.47 \\\n     \"fmbench --config-file ${CONFIG_FILE} --local-mode yes --write-bucket placeholder > fmbench.log 2>&1\"\n    ```\n    \n1. The above command will create a `fmbench` directory inside the current working directory. This directory contains the `fmbench.log` and the `results-*` folder that is created once the run finished.\n\n\n\n================================================"
  },
  {
    "filename": "simplified_config_files.md",
    "path": "docs/simplified_config_files.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# Simplified and Parameterized Config Files for FMBench\n\n## Introduction\n\nBenchmarking multiple models across various configurations on Amazon EC2 used to require creating and managing multiple configuration files. Now, with parameterized config files, you can manage deployments, inference, and benchmarking with a **single configuration file** and simply change the parameters (such as the `instance type`, `tp degree`, `batch size`, `tokenizer directory`, `prompt template`, `model id`) all via the command line.\n\nThis approach eliminates redundancy and streamlines benchmarking processes for models deployed on Amazon EC2 instances across various serving stacks.\n\n## Example: DJL Deployment Config File\n\nBelow is a generic config file for deploying models with DJL on EC2. Users can pass parameters dynamically to customize deployments.\n\n```yaml\nexperiments:\n  - name: {model_id}\n    model_id: {model_id}\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: {instance_type}\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes\n    instance_count: \n    deployment_script: ec2_deploy.py\n    inference_script: ec2_predictor.py\n    ec2:\n      model_loading_timeout: 2400\n    inference_spec:\n      parameter_set: ec2_djl\n      tp_degree: {tp_degree}\n      shm_size: 12g\n    serving.properties: |\n      option.tensor_parallel_degree={tp_degree}\n      option.max_rolling_batch_size={batch_size}\n      option.model_id={model_id}\n    payload_files:\n    - payload_en_1-500.jsonl\n    concurrency_levels:\n    - 1\n```\n\nNow, you can deploy any models using the [standard djl configuration file](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/src/fmbench/configs/generic/ec2/djl.yml). \n\n## Command Syntax\n\nDynamic parameters can be passed during runtime. Here's an example command:\n\n```{bash}\nfmbench --config-file $CONFIG_FILE_PATH \\\n        --local-mode yes \\\n        --write-bucket placeholder \\\n        --tmp-dir /tmp \\\n        -A model_id=mistralai/Mistral-7B-Instruct-v0.2 \\ # Mention your model id and other additional parameters below\n        -A instance_type=g6e.4xlarge \\ \n        -A tp_degree=1 \\\n        -A batch_size=4 \\\n        -A results_dir=Mistral-7B-Instruct-g6e.4xl \\\n        -A tokenizer_dir=mistral_tokenizer \\\n        -A prompt_template=prompt_template_mistral.txt \\\n        > $LOGFILE 2>&1\n```\n\nFor other generic configuration files, view [here](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/main/src/fmbench/configs/generic).\n\n\n\n================================================"
  },
  {
    "filename": "website.md",
    "path": "docs/website.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# Create a website for `FMBench` reports\n\nWhen you use `FMBench` as a tool for benchmarking your foundation models you would soon want to have an easy way to view all the reports in one place and search through the results, for example, \"`Llama3.1-8b` results on `trn1.32xlarge`\". An `FMBench` website provides a simple way of viewing these results.\n\nHere are the steps to setup a website using `mkdocs` and `nginx`. The steps below generate a self-signed certificate for SSL and use username and password for authentication. **It is strongly recommended that you use a valid SSL cert and a better authentication mechanism than username and password for your `FMBench` website**.\n\n1. Start an Amazon EC2 machine which will host the `FMBench` website. A `t3.xlarge` machine with an Ubuntu AMI say `ubuntu/images/hvm-ssd-gp3/ubuntu-noble-24.04-amd64-server-20240801` and 50GB storage is good enough. **Allow SSH and TCP port 443 traffic from anywhere into that machine**.\n\n1. SSH into that machine and install `conda`.\n\n    ```{.bash}\n    wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n    bash Miniconda3-latest-Linux-x86_64.sh -b  # Run the Miniconda installer in batch mode (no manual intervention)\n    rm -f Miniconda3-latest-Linux-x86_64.sh    # Remove the installer script after installation\n    eval \"$(/home/$USER/miniconda3/bin/conda shell.bash hook)\" # Initialize conda for bash shell\n    conda init  # Initialize conda, adding it to the shell  \n    ```\n\n1. Install `docker-compose`.\n\n    ```{.bash}\n    sudo apt-get update\n    sudo apt-get install --reinstall docker.io -y\n    sudo apt-get install -y docker-compose\n    sudo usermod -a -G docker $USER\n    newgrp docker\n    docker compose version \n    ```\n\n1. Setup the `fmbench_python311` conda environment and clone `FMBench` repo.\n\n    ```{.bash}\n    conda create --name fmbench_python311 -y python=3.11 ipykernel\n    source activate fmbench_python311\n    pip install -U fmbench mkdocs mkdocs-material mknotebooks\n    git clone https://github.com/aws-samples/foundation-model-benchmarking-tool.git\n    ```\n\n1. Get the `FMBench` results data from Amazon S3 or whichever storage system you used to store all the results.\n\n    ```{.bash}\n    curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n    sudo apt-get install unzip -y\n    unzip awscliv2.zip\n    sudo ./aws/install\n    FMBENCH_S3_BUCKET=your-fmbench-s3-bucket-name-here\n    aws s3 sync s3://$FMBENCH_S3_BUCKET $HOME/fmbench_data --exclude \"*.json\"\n    ```\n\n1. Create a directory for the `FMBench` website contents.\n\n    ```{.bash}\n    mkdir $HOME/fmbench_site\n    mkdir $HOME/fmbench_site/ssl\n    ```\n1. Setup SSL certs (we strongly encourage you to not use self-signed certs, this step here is just for demo purposes, get SSL certs the same way you get them for your current production workloads).\n\n    ```{.bash}\n    sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout $HOME/fmbench_site/ssl/nginx-selfsigned.key -out $HOME/fmbench_site/ssl/nginx-selfsigned.crt\n    ```\n\n1. Create an `.httpasswd` file. The `FMBench` website will use the `fmbench_admin` as a username and a password that you enter as part of the command below to allow login to the website.\n\n    ```{.bash}\n    sudo apt-get install apache2-utils -y\n    htpasswd -c $HOME/fmbench_site/.htpasswd fmbench_admin\n    ```\n\n1. Create the `mkdocs.yml` file for the website.\n\n    ```{.bash}\n    cd foundation-model-benchmarking-tool\n    cp website/index.md $HOME/fmbench_data/\n    cp -r img $HOME/fmbench_data/\n    python website/create_fmbench_website.py\n    mkdocs build -f website/mkdocs.yml --site-dir $HOME/fmbench_site/site\n    ```\n\n1. Update `nginx.conf` file. **Note the hostname that is printed out below, the `FMBench` website would be served at this address**.\n\n    ```{.bash}\n    TOKEN=`curl -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\"`\n    HOSTNAME=`curl -H \"X-aws-ec2-metadata-token: $TOKEN\" http://169.254.169.254/latest/meta-data/public-hostname`\n    echo \"hostname is: $HOSTNAME\"\n    sed \"s/__HOSTNAME__/$HOSTNAME/g\" website/nginx.conf.template > $HOME/fmbench_site/nginx.conf\n    ```\n\n1. Serve the website.\n\n    ```{.bash}\n    docker run --name fmbench-nginx -d -p 80:80 -p 443:443   -v $HOME/fmbench_site/site:/usr/share/nginx/html   -v $HOME/fmbench_site/nginx.conf:/etc/nginx/nginx.conf   -v $HOME/fmbench_site/ssl:/etc/nginx/ssl   -v $HOME/fmbench_site/.htpasswd:/etc/nginx/.htpasswd   nginx\n    ```\n\n1. Open a web browser and navigate to the hostname you noted in the step above, for example `https://<your-ec2-hostname>.us-west-2.compute.amazonaws.com`, ignore the security warnings if you used a self-signed SSL cert (replace this with a cert that you would normally use in your production websites) and then enter the username and password (the username would be `fmbench_admin` and password would be what you had set when running the `htpasswd` command). You should see a website as shown in the screenshot below.\n\n![website](./img/website.png)\n\n\n\n================================================"
  },
  {
    "filename": "workflow.md",
    "path": "docs/workflow.md",
    "directory": "docs",
    "extension": "md",
    "content": "================================================\n# Workflow for `FMBench`\n\nThe workflow for `FMBench` is as follows:\n\n```\nCreate configuration file\n        |\n        |-----> Deploy model on SageMaker/Use models on Bedrock/Bring your own endpoint\n                    |\n                    |-----> Run inference against deployed endpoint(s)\n                                     |\n                                     |------> Create a benchmarking report\n```\n\n1. Create a dataset of different prompt sizes and select one or more such datasets for running the tests.\n    1. Currently `FMBench` supports datasets from [LongBench](https://github.com/THUDM/LongBench) and filter out individual items from the dataset based on their size in tokens (for example, prompts less than 500 tokens, between 500 to 1000 tokens and so on and so forth). Alternatively, you can download the folder from [this link](https://huggingface.co/datasets/THUDM/LongBench/resolve/main/data.zip) to load the data.\n\n1. Deploy **any model** that is deployable on SageMaker on **any supported instance type** (`g5`, `p4d`, `Inf2`).\n    1. Models could be either available via SageMaker JumpStart (list available [here](https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html)) as well as models not available via JumpStart but still deployable on SageMaker through the low level boto3 (Python) SDK (Bring Your  Own Script).\n    1. Model deployment is completely configurable in terms of the inference container to use, environment variable to set, `setting.properties` file to provide (for inference containers such as DJL that use it) and instance type to use.\n\n1. Benchmark FM performance in terms of inference latency, transactions per minute and dollar cost per transaction for any FM that can be deployed on SageMaker.\n    1. Tests are run for each combination of the configured concurrency levels i.e. transactions (inference requests) sent to the endpoint in parallel and dataset. For example, run multiple datasets of say prompt sizes between 3000 to 4000 tokens at concurrency levels of 1, 2, 4, 6, 8 etc. so as to test how many transactions of what token length can the endpoint handle while still maintaining an acceptable level of inference latency.\n\n1. Generate a report that compares and contrasts the performance of the model over different test configurations and stores the reports in an Amazon S3 bucket.\n    1. The report is generated in the [Markdown](https://en.wikipedia.org/wiki/Markdown) format and consists of plots, tables and text that highlight the key results and provide an overall recommendation on what is the best combination of instance type and serving stack to use for the model under stack for a dataset of interest.\n    1. The report is created as an artifact of reproducible research so that anyone having access to the model, instance type and serving stack can run the code and recreate the same results and report.\n\n1. Multiple [configuration files](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/main/src/fmbench/configs) that can be used as reference for benchmarking new models and instance types.\n\n\n================================================\nSYMLINK: docs/configs -> configs\n================================================\n\n\n\n================================================"
  },
  {
    "filename": "fmbench.drawio",
    "path": "docs/img/fmbench.drawio",
    "directory": "docs/img",
    "extension": "drawio",
    "content": "================================================\n<mxfile host=\"app.diagrams.net\" agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36\" version=\"24.7.6\" pages=\"2\">\n  <diagram name=\"Page-1\" id=\"y2vha-sQEwUO__ylBviv\">\n    <mxGraphModel dx=\"1434\" dy=\"780\" grid=\"1\" gridSize=\"10\" guides=\"1\" tooltips=\"1\" connect=\"1\" arrows=\"1\" fold=\"1\" page=\"1\" pageScale=\"1\" pageWidth=\"850\" pageHeight=\"1100\" math=\"0\" shadow=\"0\">\n      <root>\n        <mxCell id=\"0\" />\n        <mxCell id=\"1\" parent=\"0\" />\n        <mxCell id=\"x3Ah9TazF13iL2Uz1pW2-19\" style=\"edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=0;exitY=0.5;exitDx=0;exitDy=0;entryX=1;entryY=0.5;entryDx=0;entryDy=0;\" edge=\"1\" parent=\"1\" source=\"x3Ah9TazF13iL2Uz1pW2-1\" target=\"x3Ah9TazF13iL2Uz1pW2-2\">\n          <mxGeometry relative=\"1\" as=\"geometry\" />\n        </mxCell>\n        <mxCell id=\"x3Ah9TazF13iL2Uz1pW2-20\" style=\"edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=0;exitY=0.5;exitDx=0;exitDy=0;entryX=1;entryY=0.5;entryDx=0;entryDy=0;\" edge=\"1\" parent=\"1\" source=\"x3Ah9TazF13iL2Uz1pW2-1\" target=\"x3Ah9TazF13iL2Uz1pW2-3\">\n          <mxGeometry relative=\"1\" as=\"geometry\" />\n        </mxCell>\n        <mxCell id=\"x3Ah9TazF13iL2Uz1pW2-21\" style=\"edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=0;exitY=0.5;exitDx=0;exitDy=0;entryX=1;entryY=0.5;entryDx=0;entryDy=0;\" edge=\"1\" parent=\"1\" source=\"x3Ah9TazF13iL2Uz1pW2-1\" target=\"x3Ah9TazF13iL2Uz1pW2-4\">\n          <mxGeometry relative=\"1\" as=\"geometry\" />\n        </mxCell>\n        <mxCell id=\"x3Ah9TazF13iL2Uz1pW2-1\" value=\"FMBench on Amazon SageMaker Notebook\" style=\"rounded=0;whiteSpace=wrap;html=1;fillColor=#ffe6cc;strokeColor=#d79b00;\" vertex=\"1\" parent=\"1\">\n          <mxGeometry x=\"250\" y=\"160\" width=\"120\" height=\"60\" as=\"geometry\" />\n        </mxCell>\n        <mxCell id=\"x3Ah9TazF13iL2Uz1pW2-2\" value=\"LLMs hosted on Amazon Bedrock\" style=\"rounded=0;whiteSpace=wrap;html=1;fillColor=#f5f5f5;fontColor=#333333;strokeColor=#666666;\" vertex=\"1\" parent=\"1\">\n          <mxGeometry x=\"40\" y=\"80\" width=\"120\" height=\"60\" as=\"geometry\" />\n        </mxCell>\n        <mxCell id=\"x3Ah9TazF13iL2Uz1pW2-3\" value=\"LLMs hosted on Amazon SageMaker\" style=\"rounded=0;whiteSpace=wrap;html=1;fillColor=#dae8fc;strokeColor=#6c8ebf;\" vertex=\"1\" parent=\"1\">\n          <mxGeometry x=\"40\" y=\"160\" width=\"120\" height=\"60\" as=\"geometry\" />\n        </mxCell>\n        <mxCell id=\"x3Ah9TazF13iL2Uz1pW2-4\" value=\"LLMs hosted on Amazon EKS\" style=\"rounded=0;whiteSpace=wrap;html=1;fillColor=#d5e8d4;strokeColor=#82b366;\" vertex=\"1\" parent=\"1\">\n          <mxGeometry x=\"40\" y=\"240\" width=\"120\" height=\"60\" as=\"geometry\" />\n        </mxCell>\n        <mxCell id=\"x3Ah9TazF13iL2Uz1pW2-7\" value=\"\" style=\"endArrow=none;html=1;rounded=0;exitX=1;exitY=1;exitDx=0;exitDy=0;\" edge=\"1\" parent=\"1\" source=\"fiXtwMIg6qrkt5U_o3Tw-2\">\n          <mxGeometry width=\"50\" height=\"50\" relative=\"1\" as=\"geometry\">\n            <mxPoint x=\"420\" y=\"330\" as=\"sourcePoint\" />\n            <mxPoint x=\"420\" y=\"70\" as=\"targetPoint\" />\n          </mxGeometry>\n        </mxCell>\n        <mxCell id=\"x3Ah9TazF13iL2Uz1pW2-23\" style=\"edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=0;exitY=0.5;exitDx=0;exitDy=0;entryX=1;entryY=0.5;entryDx=0;entryDy=0;\" edge=\"1\" source=\"x3Ah9TazF13iL2Uz1pW2-26\" target=\"x3Ah9TazF13iL2Uz1pW2-27\" parent=\"1\">\n          <mxGeometry relative=\"1\" as=\"geometry\" />\n        </mxCell>\n        <mxCell id=\"x3Ah9TazF13iL2Uz1pW2-24\" style=\"edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=0;exitY=0.5;exitDx=0;exitDy=0;entryX=1;entryY=0.5;entryDx=0;entryDy=0;\" edge=\"1\" source=\"x3Ah9TazF13iL2Uz1pW2-26\" target=\"x3Ah9TazF13iL2Uz1pW2-28\" parent=\"1\">\n          <mxGeometry relative=\"1\" as=\"geometry\" />\n        </mxCell>\n        <mxCell id=\"x3Ah9TazF13iL2Uz1pW2-25\" style=\"edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=0;exitY=0.5;exitDx=0;exitDy=0;entryX=1;entryY=0.5;entryDx=0;entryDy=0;\" edge=\"1\" source=\"x3Ah9TazF13iL2Uz1pW2-26\" target=\"x3Ah9TazF13iL2Uz1pW2-29\" parent=\"1\">\n          <mxGeometry relative=\"1\" as=\"geometry\" />\n        </mxCell>\n        <mxCell id=\"x3Ah9TazF13iL2Uz1pW2-26\" value=\"FMBench on Amazon EC2\" style=\"rounded=0;whiteSpace=wrap;html=1;fillColor=#ffe6cc;strokeColor=#d79b00;\" vertex=\"1\" parent=\"1\">\n          <mxGeometry x=\"670\" y=\"160\" width=\"120\" height=\"60\" as=\"geometry\" />\n        </mxCell>\n        <mxCell id=\"x3Ah9TazF13iL2Uz1pW2-27\" value=\"LLMs hosted on Amazon Bedrock\" style=\"rounded=0;whiteSpace=wrap;html=1;fillColor=#f5f5f5;fontColor=#333333;strokeColor=#666666;\" vertex=\"1\" parent=\"1\">\n          <mxGeometry x=\"460\" y=\"80\" width=\"120\" height=\"60\" as=\"geometry\" />\n        </mxCell>\n        <mxCell id=\"x3Ah9TazF13iL2Uz1pW2-28\" value=\"LLMs hosted on Amazon SageMaker\" style=\"rounded=0;whiteSpace=wrap;html=1;fillColor=#dae8fc;strokeColor=#6c8ebf;\" vertex=\"1\" parent=\"1\">\n          <mxGeometry x=\"460\" y=\"160\" width=\"120\" height=\"60\" as=\"geometry\" />\n        </mxCell>\n        <mxCell id=\"x3Ah9TazF13iL2Uz1pW2-29\" value=\"LLMs hosted on Amazon EKS\" style=\"rounded=0;whiteSpace=wrap;html=1;fillColor=#d5e8d4;strokeColor=#82b366;\" vertex=\"1\" parent=\"1\">\n          <mxGeometry x=\"460\" y=\"240\" width=\"120\" height=\"60\" as=\"geometry\" />\n        </mxCell>\n        <mxCell id=\"fiXtwMIg6qrkt5U_o3Tw-1\" value=\"&lt;b&gt;&lt;font style=&quot;font-size: 18px;&quot;&gt;FMBench deployment configurations&lt;/font&gt;&lt;/b&gt;&lt;div&gt;&lt;b&gt;&lt;font style=&quot;font-size: 15px;&quot;&gt;(LLMs are deployed by FMBench&lt;/font&gt;&lt;font style=&quot;font-size: 18px;&quot;&gt;)&lt;/font&gt;&lt;/b&gt;&lt;/div&gt;\" style=\"text;html=1;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\" vertex=\"1\" parent=\"1\">\n          <mxGeometry x=\"230\" y=\"10\" width=\"360\" height=\"30\" as=\"geometry\" />\n        </mxCell>\n        <mxCell id=\"fiXtwMIg6qrkt5U_o3Tw-2\" value=\"&lt;font style=&quot;font-size: 14px;&quot;&gt;Quickstart: FMBench is setup through AWS CloudFormation&lt;/font&gt;\" style=\"text;html=1;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\" vertex=\"1\" parent=\"1\">\n          <mxGeometry x=\"30\" y=\"330\" width=\"390\" height=\"30\" as=\"geometry\" />\n        </mxCell>\n        <mxCell id=\"fiXtwMIg6qrkt5U_o3Tw-3\" value=\"&lt;font style=&quot;font-size: 14px;&quot;&gt;DIY: FMBench is setup manually on Amazon EC2&lt;/font&gt;\" style=\"text;html=1;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\" vertex=\"1\" parent=\"1\">\n          <mxGeometry x=\"430\" y=\"330\" width=\"390\" height=\"30\" as=\"geometry\" />\n        </mxCell>\n      </root>\n    </mxGraphModel>\n  </diagram>\n  <diagram id=\"8b_q89SXEdJDEv58rKXf\" name=\"Page-2\">\n    <mxGraphModel dx=\"1434\" dy=\"780\" grid=\"1\" gridSize=\"10\" guides=\"1\" tooltips=\"1\" connect=\"1\" arrows=\"1\" fold=\"1\" page=\"1\" pageScale=\"1\" pageWidth=\"850\" pageHeight=\"1100\" math=\"0\" shadow=\"0\">\n      <root>\n        <mxCell id=\"0\" />\n        <mxCell id=\"1\" parent=\"0\" />\n        <mxCell id=\"nYGEgryt7huoeiD7q8NB-3\" value=\"\" style=\"rounded=0;whiteSpace=wrap;html=1;\" vertex=\"1\" parent=\"1\">\n          <mxGeometry x=\"235\" y=\"110\" width=\"180\" height=\"280\" as=\"geometry\" />\n        </mxCell>\n        <mxCell id=\"sXA2JP5MA5aewUyW0-Ma-2\" value=\"LLM-1 hosted on Amazon EC2 instance 1\" style=\"rounded=0;whiteSpace=wrap;html=1;fillColor=#fff2cc;strokeColor=#d6b656;\" vertex=\"1\" parent=\"1\">\n          <mxGeometry x=\"265\" y=\"140\" width=\"120\" height=\"60\" as=\"geometry\" />\n        </mxCell>\n        <mxCell id=\"sXA2JP5MA5aewUyW0-Ma-3\" style=\"edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;entryX=1;entryY=0.5;entryDx=0;entryDy=0;\" edge=\"1\" parent=\"1\" source=\"sXA2JP5MA5aewUyW0-Ma-6\" target=\"sXA2JP5MA5aewUyW0-Ma-2\">\n          <mxGeometry relative=\"1\" as=\"geometry\" />\n        </mxCell>\n        <mxCell id=\"sXA2JP5MA5aewUyW0-Ma-4\" style=\"edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;\" edge=\"1\" parent=\"1\" source=\"sXA2JP5MA5aewUyW0-Ma-6\" target=\"sXA2JP5MA5aewUyW0-Ma-7\">\n          <mxGeometry relative=\"1\" as=\"geometry\" />\n        </mxCell>\n        <mxCell id=\"nYGEgryt7huoeiD7q8NB-4\" value=\"\" style=\"rounded=0;whiteSpace=wrap;html=1;\" vertex=\"1\" parent=\"1\">\n          <mxGeometry x=\"470\" y=\"110\" width=\"180\" height=\"280\" as=\"geometry\" />\n        </mxCell>\n        <mxCell id=\"sXA2JP5MA5aewUyW0-Ma-5\" style=\"edgeStyle=orthogonalEdgeStyle;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;entryX=1;entryY=0.5;entryDx=0;entryDy=0;\" edge=\"1\" parent=\"1\" source=\"sXA2JP5MA5aewUyW0-Ma-6\" target=\"sXA2JP5MA5aewUyW0-Ma-8\">\n          <mxGeometry relative=\"1\" as=\"geometry\" />\n        </mxCell>\n        <mxCell id=\"sXA2JP5MA5aewUyW0-Ma-6\" value=\"FMBench on Amazon EC2\" style=\"rounded=0;whiteSpace=wrap;html=1;fillColor=#ffe6cc;strokeColor=#d79b00;\" vertex=\"1\" parent=\"1\">\n          <mxGeometry x=\"500\" y=\"220\" width=\"120\" height=\"60\" as=\"geometry\" />\n        </mxCell>\n        <mxCell id=\"sXA2JP5MA5aewUyW0-Ma-7\" value=\"LLM-2 hosted on Amazon EC2 instance 2\" style=\"rounded=0;whiteSpace=wrap;html=1;fillColor=#fff2cc;strokeColor=#d6b656;\" vertex=\"1\" parent=\"1\">\n          <mxGeometry x=\"265\" y=\"220\" width=\"120\" height=\"60\" as=\"geometry\" />\n        </mxCell>\n        <mxCell id=\"sXA2JP5MA5aewUyW0-Ma-8\" value=\"LLM-N hosted on Amazon EC2 instance N\" style=\"rounded=0;whiteSpace=wrap;html=1;fillColor=#fff2cc;strokeColor=#d6b656;\" vertex=\"1\" parent=\"1\">\n          <mxGeometry x=\"265\" y=\"310\" width=\"120\" height=\"60\" as=\"geometry\" />\n        </mxCell>\n        <mxCell id=\"nYGEgryt7huoeiD7q8NB-2\" value=\"&lt;b&gt;&lt;font style=&quot;font-size: 18px;&quot;&gt;FMBench client-server setup on Amazon EC2&lt;/font&gt;&lt;/b&gt;&lt;div&gt;&lt;b&gt;&lt;font style=&quot;font-size: 15px;&quot;&gt;(FMBench installed on all EC2 instance, LLMs are deployed by FMBench&lt;/font&gt;&lt;font style=&quot;font-size: 18px;&quot;&gt;)&lt;/font&gt;&lt;/b&gt;&lt;/div&gt;\" style=\"text;html=1;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\" vertex=\"1\" parent=\"1\">\n          <mxGeometry x=\"250\" y=\"40\" width=\"390\" height=\"30\" as=\"geometry\" />\n        </mxCell>\n        <mxCell id=\"nYGEgryt7huoeiD7q8NB-5\" value=\"&lt;font style=&quot;font-size: 14px;&quot;&gt;LLM hosting AWS account (server)&lt;/font&gt;\" style=\"text;html=1;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\" vertex=\"1\" parent=\"1\">\n          <mxGeometry x=\"265\" y=\"400\" width=\"115\" height=\"30\" as=\"geometry\" />\n        </mxCell>\n        <mxCell id=\"nYGEgryt7huoeiD7q8NB-6\" value=\"&lt;font style=&quot;font-size: 14px;&quot;&gt;Benchmarking AWS account (client)&lt;/font&gt;\" style=\"text;html=1;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;\" vertex=\"1\" parent=\"1\">\n          <mxGeometry x=\"500\" y=\"401\" width=\"130\" height=\"30\" as=\"geometry\" />\n        </mxCell>\n      </root>\n    </mxGraphModel>\n  </diagram>\n</mxfile>\n\n\n\n================================================"
  },
  {
    "filename": "ec2_instance_creation_steps.md",
    "path": "docs/misc/ec2_instance_creation_steps.md",
    "directory": "docs/misc",
    "extension": "md",
    "content": "================================================\n# Create an EC2 instance suitable for an LMI (Large Model Inference)\n\nFollow the steps below to create an EC2 instance for hosting a model in an LMI.\n\n1. On the homepage of AWS Console go to \u2018EC2\u2019 - it is likely in recently visited:\n   ![](../img/ec2connect1.png)\n\n1. If not found, go to the search bar on the top of the page. Type `ec2` into the search box and click the entry that pops up with name `EC2` :\n   ![](../img/ec2connect2.png)\n\n1. Click \u201cInstances\u201d:\n   ![](../img/ec2connect3.png)\n\n1. Click \"Launch Instances\":\n   ![](../img/ec2connect4.png)\n\n1. Type in a name for your instance (recommended to include your alias in the name), and then scroll down. Search for \u2018deep learning ami\u2019 in the box. Select the one that says **Deep Learning OSS Nvidia Driver AMI GPU PyTorch** for a GPU instance type, select **Deep Learning AMI Neuron (Ubuntu 22.04)** for an Inferential/Trainium instance type. **Your version number might be different**. \n    ![](../img/ec2connect5a.png)\n\n1. Name your instance _FMBenchInstance_.\n   \n1. Add a _fmbench-version_ tag to your instance.\n   ![](../img/ec2tag.png)\n   \n1. Scroll down to _Instance Type_. For large model inference, the g5.12xlarge is recommended.\n\n   ![](../img/ec2connect6.png)\n\n1. Make a key pair by clicking _Create new key pair_. Give it a name, keep all settings as is, and then click \u201cCreate key pair\u201d.\n   ![](../img/ec2connect7.png)\n   \n1. Skip over _Network settings_ (leave it as it is), going straight to _Configure storage_. 45 GB, the suggested amount, is not nearly enough, and using that will cause the LMI docker container to download for an arbitrarily long time and then error out. Change it to 100 GB or more:\n    ![](../img/ec2connect8.png)\n\n1. Create an IAM role to your instance called _FMBenchEC2Role_. Attach the following permission policies: `AmazonSageMakerFullAccess`, `AmazonBedrockFullAccess`.\n\n    Edit the trust policy to be the following:\n    ```\n    {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Effect\": \"Allow\",\n                \"Principal\": {\n                    \"Service\": \"ec2.amazonaws.com\"\n                },\n                \"Action\": \"sts:AssumeRole\"\n            },\n            {\n                \"Effect\": \"Allow\",\n                \"Principal\": {\n                    \"Service\": \"sagemaker.amazonaws.com\"\n                },\n                \"Action\": \"sts:AssumeRole\"\n            },\n            {\n                \"Effect\": \"Allow\",\n                \"Principal\": {\n                    \"Service\": \"bedrock.amazonaws.com\"\n                },\n                \"Action\": \"sts:AssumeRole\"\n            }\n        ]\n    }\n    ```\n    Select this role in the _IAM instance profile_ setting of your instance.\n    ![](../img/ec2-iam.png)\n    \n1. Then, we\u2019re done with the settings of the instance. Click _Launch Instance_ to finish. You can connect to your EC2 instance using any of these option\n    ![](../img/ec2connect10.png)\n\n\n\n================================================"
  },
  {
    "filename": "eks_cluster-creation_steps.md",
    "path": "docs/misc/eks_cluster-creation_steps.md",
    "directory": "docs/misc",
    "extension": "md",
    "content": "================================================\n# EKS cluster creation steps\n\nThe steps below create an EKS cluster called `trainium-inferentia`.\n\n1. Before we begin, ensure you have all the prerequisites in place to make the deployment process smooth and hassle-free. Ensure that you have installed the following tools on your machine: [aws-cli](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html), [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/) and [terraform](https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli). We use the [`DoEKS`](https://github.com/awslabs/data-on-eks/tree/main) repository as a guide to deploy the cluster infrastructure in an AWS account.\n\n1. Ensue that your account has enough `Inf2` on-demand VCPUs as most of the DoEKS blueprints utilize this specific instance. To increase service quota navigate to the service quota page for the region you are in [service quota](https://us-east-1.console.aws.amazon.com/servicequotas/home?region=us-east-1). Then select **services** under the left side menu and search for **Amazon Elastic Compute Cloud (Amazon EC2)**. This will bring up the service quota page, here search for `inf` and there should be an option for **Running On-Demand Inf instances**. Increase this quota to 300. \n\n1. Clone the [`DoEKS`](https://github.com/awslabs/data-on-eks) repository\n\n    ``` {.bash}\n    git clone https://github.com/awslabs/data-on-eks.git\n    ```\n\n1. Ensure that the region names are correct in [`variables.tf`](https://github.com/awslabs/data-on-eks/blob/d532720d0746959daa6d3a3f5925fc8be114ccc4/ai-ml/trainium-inferentia/variables.tf#L12) file before running the cluster creation script.\n\n1. Ensure that the ELB to be created would be external facing. Change the helm value from `internal` to `internet-facing` [here](https://github.com/awslabs/data-on-eks/blob/3ef55e21cf30b54341bb771a2bb2dbd1280c3edd/ai-ml/trainium-inferentia/helm-values/ingress-nginx-values.yaml#L8).\n\n1. Ensure that the IAM role you are using has the permissions needed to create the cluster. **While we expect the following set of permissions to work but the current recommendation is to also add the `AdminstratorAccess` permission to the IAM role. At a later date you could remove the  `AdminstratorAccess` and experiment with cluster creation without it.**\n\n    1. Attach the following managed policies: `AmazonEKSClusterPolicy`, `AmazonEKS_CNI_Policy`, and `AmazonEKSWorkerNodePolicy`.\n    1. In addition to the managed policies add the following as inline policy. Replace _your-account-id_ with the actual value of the AWS account id you are using.\n    \n    \n        ```{.bash}\n        {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Sid\": \"VisualEditor0\",\n                \"Effect\": \"Allow\",\n                \"Action\": [\n                    \"ec2:CreateVpc\",\n                    \"ec2:DeleteVpc\"\n                ],\n                \"Resource\": [\n                    \"arn:aws:ec2:*:your-account-id:ipv6pool-ec2/*\",\n                    \"arn:aws:ec2::your-account-id:ipam-pool/*\",\n                    \"arn:aws:ec2:*:your-account-id:vpc/*\"\n                ]\n            },\n            {\n                \"Sid\": \"VisualEditor1\",\n                \"Effect\": \"Allow\",\n                \"Action\": [\n                    \"ec2:ModifyVpcAttribute\",\n                    \"ec2:DescribeVpcAttribute\"\n                ],\n                \"Resource\": \"arn:aws:ec2:*:<your-account-id>:vpc/*\"\n            },\n            {\n                \"Sid\": \"VisualEditor2\",\n                \"Effect\": \"Allow\",\n                \"Action\": \"ec2:AssociateVpcCidrBlock\",\n                \"Resource\": [\n                    \"arn:aws:ec2:*:your-account-id:ipv6pool-ec2/*\",\n                    \"arn:aws:ec2::your-account-id:ipam-pool/*\",\n                    \"arn:aws:ec2:*:your-account-id:vpc/*\"\n                ]\n            },\n            {\n                \"Sid\": \"VisualEditor3\",\n                \"Effect\": \"Allow\",\n                \"Action\": [\n                    \"ec2:DescribeSecurityGroupRules\",\n                    \"ec2:DescribeNatGateways\",\n                    \"ec2:DescribeAddressesAttribute\"\n                ],\n                \"Resource\": \"*\"\n            },\n            {\n                \"Sid\": \"VisualEditor4\",\n                \"Effect\": \"Allow\",\n                \"Action\": [\n                    \"ec2:CreateInternetGateway\",\n                    \"ec2:RevokeSecurityGroupEgress\",\n                    \"ec2:CreateRouteTable\",\n                    \"ec2:CreateSubnet\"\n                ],\n                \"Resource\": [\n                    \"arn:aws:ec2:*:your-account-id:security-group/*\",\n                    \"arn:aws:ec2:*:your-account-id:internet-gateway/*\",\n                    \"arn:aws:ec2:*:your-account-id:subnet/*\",\n                    \"arn:aws:ec2:*:your-account-id:route-table/*\",\n                    \"arn:aws:ec2::your-account-id:ipam-pool/*\",\n                    \"arn:aws:ec2:*:your-account-id:vpc/*\"\n                ]\n            },\n            {\n                \"Sid\": \"VisualEditor5\",\n                \"Effect\": \"Allow\",\n                \"Action\": [\n                    \"ec2:AttachInternetGateway\",\n                    \"ec2:AssociateRouteTable\"\n                ],\n                \"Resource\": [\n                    \"arn:aws:ec2:*:your-account-id:vpn-gateway/*\",\n                    \"arn:aws:ec2:*:your-account-id:internet-gateway/*\",\n                    \"arn:aws:ec2:*:your-account-id:subnet/*\",\n                    \"arn:aws:ec2:*:your-account-id:route-table/*\",\n                    \"arn:aws:ec2:*:your-account-id:vpc/*\"\n                ]\n            },\n            {\n                \"Sid\": \"VisualEditor6\",\n                \"Effect\": \"Allow\",\n                \"Action\": \"ec2:AllocateAddress\",\n                \"Resource\": [\n                    \"arn:aws:ec2:*:your-account-id:ipv4pool-ec2/*\",\n                    \"arn:aws:ec2:*:your-account-id:elastic-ip/*\"\n                ]\n            },\n            {\n                \"Sid\": \"VisualEditor7\",\n                \"Effect\": \"Allow\",\n                \"Action\": \"ec2:ReleaseAddress\",\n                \"Resource\": \"arn:aws:ec2:*:your-account-id:elastic-ip/*\"\n            },\n            {\n                \"Sid\": \"VisualEditor8\",\n                \"Effect\": \"Allow\",\n                \"Action\": \"ec2:CreateNatGateway\",\n                \"Resource\": [\n                    \"arn:aws:ec2:*:your-account-id:subnet/*\",\n                    \"arn:aws:ec2:*:your-account-id:natgateway/*\",\n                    \"arn:aws:ec2:*:your-account-id:elastic-ip/*\"\n                ]\n            }\n        ]\n        }\n        ```\n1. Add the Role ARN and name here in the `variables.tf` file by updating [these lines](https://github.com/awslabs/data-on-eks/blob/d532720d0746959daa6d3a3f5925fc8be114ccc4/ai-ml/trainium-inferentia/variables.tf#L126). Move the structure inside the `defaut` list and replace the role ARN and name values with the values for the role you are using.\n\n1. Navigate into the `ai-ml/trainium-inferentia/` directory and run install.sh script.\n\n    ``` {.bash}\n    cd data-on-eks/ai-ml/trainium-inferentia/\n    ./install.sh\n    ```\n\n    Note: This step takes about 12-15 minutes to deploy the EKS infrastructure and cluster in the AWS account. To view more details on cluster creation, view an example here: [Deploy Llama3 on EKS](https://awslabs.github.io/data-on-eks/docs/gen-ai/inference/llama3-inf2) in the _prerequisites_ section.\n\n1. After the cluster is created, navigate to the **Karpenter EC2 node IAM role** called `karpenter-trainium-inferentia-XXXXXXXXXXXXXXXXXXXXXXXXX`. Attach the following inline policy to the role:\n\n    ``` {.bash}\n    {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Sid\": \"Statement1\",\n                \"Effect\": \"Allow\",\n                \"Action\": [\n                    \"iam:CreateServiceLinkedRole\"\n                ],\n                \"Resource\": \"*\"\n            }\n        ]\n    }\n    ```\n\n\n\n================================================"
  },
  {
    "filename": "the-diy-version-w-gory-details.md",
    "path": "docs/misc/the-diy-version-w-gory-details.md",
    "directory": "docs/misc",
    "extension": "md",
    "content": "================================================\n### The DIY version (with gory details)\n\nFollow the prerequisites below to set up your environment before running the code:\n\n1. **Python 3.11**: Setup a Python 3.11 virtual environment and install `FMBench`.\n\n    ```{.bash}\n    python -m venv .fmbench\n    pip install fmbench\n    ```\n\n1. **S3 buckets for test data, scripts, and results**: Create two buckets within your AWS account:\n\n    * _Read bucket_: This bucket contains `tokenizer files`, `prompt template`, `source data` and `deployment scripts` stored in a directory structure as shown below. `FMBench` needs to have read access to this bucket.\n    \n        ```{.bash}\n        s3://<read-bucket-name>\n            \u251c\u2500\u2500 source_data/\n            \u251c\u2500\u2500 source_data/<source-data-file-name>.json\n            \u251c\u2500\u2500 prompt_template/\n            \u251c\u2500\u2500 prompt_template/prompt_template.txt\n            \u251c\u2500\u2500 scripts/\n            \u251c\u2500\u2500 scripts/<deployment-script-name>.py\n            \u251c\u2500\u2500 tokenizer/\n            \u251c\u2500\u2500 tokenizer/tokenizer.json\n            \u251c\u2500\u2500 tokenizer/config.json\n        ```\n\n        * The details of the bucket structure is as follows:\n\n            1. **Source Data Directory**: Create a `source_data` directory that stores the dataset you want to benchmark with. `FMBench` uses `Q&A` datasets from the [`LongBench dataset`](https://github.com/THUDM/LongBench) or alternatively from [this link](https://huggingface.co/datasets/THUDM/LongBench/resolve/main/data.zip). _Support for bring your own dataset will be added soon_.\n\n                * Download the different files specified in the [LongBench dataset](https://github.com/THUDM/LongBench) into the `source_data` directory. Following is a good list to get started with:\n\n                    * `2wikimqa`\n                    * `hotpotqa`\n                    * `narrativeqa`\n                    * `triviaqa`\n                \n                    Store these files in the `source_data` directory.\n\n            1. **Prompt Template Directory**: Create a `prompt_template` directory that contains a `prompt_template.txt` file. This `.txt` file contains the prompt template that your specific model supports. `FMBench` already supports the [prompt template](src/fmbench/prompt_template/prompt_template.txt) compatible with `Llama` models.\n\n            1. **Scripts Directory**: `FMBench` also supports a `bring your own script (BYOS)` mode for deploying models that are not natively available via SageMaker JumpStart i.e. anything not included in [this](https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html) list. Here are the steps to use BYOS.\n\n                1. Create a Python script to deploy your model on a SageMaker endpoint. This script needs to have a `deploy` function that [`2_deploy_model.ipynb`](./src/fmbench/2_deploy_model.ipynb) can invoke. See [`p4d_hf_tgi.py`](./scripts/p4d_hf_tgi.py) for reference.\n\n                1. Place your deployment script in the `scripts` directory in your ***read bucket***. If your script deploys a model directly from HuggingFace and needs to have access to a HuggingFace auth token, then create a file called `hf_token.txt` and put the auth token in that file. The [`.gitignore`](.gitgnore) file in this repo has rules to not commit the `hf_token.txt` to the repo. Today, `FMBench` provides inference scripts for:\n\n                    * [All SageMaker Jumpstart Models](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models.html)\n                    * [Text-Generation-Inference (TGI) container supported models](https://huggingface.co/text-generation-inference)\n                    * [Deep Java Library DeepSpeed container supported models](https://docs.djl.ai/docs/serving/serving/docs/lmi/configurations_large_model_inference_containers.html)\n\n\n                    Deployment scripts for the options above are available in the [scripts](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/s3_metrics/scripts) directory, you can use these as reference for creating your own deployment scripts as well.\n\n            1. **Tokenizer Directory**: Place the `tokenizer.json`, `config.json` and any other files required for your model's tokenizer in the `tokenizer` directory. The tokenizer for your model should be compatible with the [`tokenizers`](https://pypi.org/project/tokenizers/) package. `FMBench` uses `AutoTokenizer.from_pretrained` to load the tokenizer.\n                >As an example, to use the `Llama 2 Tokenizer` for counting prompt and generation tokens for the `Llama 2` family of models: Accept the License here: [meta approval form](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and download the `tokenizer.json` and `config.json` files from [Hugging Face website](https://huggingface.co/meta-llama/Llama-2-7b/tree/main) and place them in the `tokenizer` directory.\n\n    * _Write bucket_: All prompt payloads, model endpoint and metrics generated by `FMBench` are stored in this bucket. `FMBench` requires write permissions to store the results in this bucket. No directory structure needs to be pre-created in this bucket, everything is created by `FMBench` at runtime.\n\n        ```{.bash}\n        s3://<write-bucket-name>\n            \u251c\u2500\u2500 <test-name>\n            \u251c\u2500\u2500 <test-name>/data\n            \u251c\u2500\u2500 <test-name>/data/metrics\n            \u251c\u2500\u2500 <test-name>/data/models\n            \u251c\u2500\u2500 <test-name>/data/prompts\n        ````\n\n\n\n================================================"
  },
  {
    "filename": "0_setup.ipynb",
    "path": "fmbench/0_setup.ipynb",
    "directory": "fmbench",
    "extension": "ipynb",
    "content": "================================================\n# Jupyter notebook converted to Python script.\n\n\"\"\"\n## Setup required for all notebooks\n---------------------\n*This notebook works best with the conda_python3 kernel on a ml.t3.medium machine*.\n\n**This step of our solution design covers running setup steps that need to be run prior to any other notebook being run.**\n\n1. Prerequisite: a `Python 3.11` conda environment.\n\n\"\"\"\n\n\"\"\"\n#### Import all of the necessary libraries below to run this notebook\n\"\"\"\n\n# if interactive mode is set to no -> pickup fmbench from Python installation path\n# if interactive mode is set to yes -> pickup fmbench from the current path (one level above this notebook)\n# if interactive mode is not defined -> pickup fmbench from the current path (one level above this notebook)\n# the premise is that if run non-interactively then it can only be run through main.py which will set interactive mode to no\nimport os\nimport sys\nif os.environ.get(\"INTERACTIVE_MODE_SET\", \"yes\") == \"yes\":\n    sys.path.append(os.path.dirname(os.getcwd()))\n\n\"\"\"\n#### Install packages listed in the requirements.txt file\n\"\"\"\n\n!{sys.executable} -m pip install -r requirements.txt\n\nimport sys\nimport time\nimport json\nimport boto3\nimport asyncio\nimport logging\nimport importlib.util\nimport fmbench.scripts\nfrom pathlib import Path\nfrom fmbench.utils import *\nfrom fmbench.globals import *\nfrom typing import Dict, List, Optional\nfrom sagemaker import get_execution_role\nimport importlib.resources as pkg_resources\nfrom botocore.exceptions import ClientError\nfrom botocore.exceptions import NoCredentialsError\n\n\"\"\"\n#### Pygmentize globals.py to view and use any of the globally initialized variables \n\"\"\"\n\n\"\"\"\n#### Set up a logger to log all messages while the code runs\n\"\"\"\n\n# Create a logger\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n# Remove existing handlers\nlogger.handlers.clear()\n\n# Add a simple handler\nhandler = logging.StreamHandler()\nformatter = logging.Formatter('[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s')\nhandler.setFormatter(formatter)\nlogger.addHandler(handler)\n\n\n\"\"\"\n### Load the config.yml file\n------\n\nThe config.yml file contains information that is used across this benchmarking environment, such as information about the aws account, prompts, payloads to be used for invocations, and model configurations like the version of the model, the endpoint name, model_id that needs to be deployed. Configurations also support the gives instance type to be used, for example: \"ml.g5.24xlarge\", the image uri, whether or not to deploy this given model, followed by an inference script \"jumpstart.py\" which supports the inference script for jumpstart models to deploy the model in this deploy notebook. \n\nView the contents of the config yml file below and how it is loaded and used throughout this notebook with deploying the model endpoints asynchronously.\n\"\"\"\n\n## Load the config.yml file referring to the globals.py file\nconfig = load_config(CONFIG_FILE)\n\n## configure the aws region and execution role\naws_region = config['aws']['region']\n\n\ntry:\n    sagemaker_execution_role = get_execution_role()\n    config['aws']['sagemaker_execution_role'] = sagemaker_execution_role\n    logger.info(f\"determined SageMaker exeuction role from get_execution_role\")\nexcept Exception as e:\n    logger.error(f\"could not determine SageMaker execution role, error={e}\")\n    logger.info(f\"going to look for execution role in config file..\")\n    sagemaker_execution_role = config['aws'].get('sagemaker_execution_role')\n    if sagemaker_execution_role is not None:\n        logger.info(f\"found SageMaker execution role in config file..\")\n\nlogger.info(f\"aws_region={aws_region}, execution_role={config['aws']['sagemaker_execution_role']}\")\nlogger.info(f\"config={json.dumps(config, indent=2)}\")\n\n\"\"\"\n#### Download any scripts from Amazon S3\n\nUsers can upload scripts to S3 which contain custom code for deployment and inference. We download these scripts here and place them in the `fmbench` package installation directory.\n\"\"\"\n\n# Initialize the S3 client\ns3_client = boto3.client('s3', region_name=aws_region)\n\n# Assuming fmbench is a valid Python package and scripts is a subdirectory within it\nscripts_dir = Path(pkg_resources.files('fmbench'), 'scripts')\nlogger.info(f\"Using fmbench.scripts directory: {scripts_dir}\")\n\n# Ensure the scripts directory exists\nscripts_dir.mkdir(parents=True, exist_ok=True)\n\nread_bucket = config['s3_read_data']['read_bucket']\nlogger.info(f\"the read bucket is --> {read_bucket} for reading the script files\")\nscripts_prefix = config['s3_read_data']['scripts_prefix']\nlogger.info(f\"the scripts directory is --> {scripts_prefix} for reading the script file names\")\nscript_files = config['s3_read_data'].get('script_files', [])\nif script_files is None:\n    script_files = []\nlogger.info(f\"Extracted script files that the user has provided --> {script_files}\")\n\n# Download script files to the fmbench.scripts directory\ntry:\n    for script_name in script_files:\n        # do os.path.join\n        s3_script_path = f\"{scripts_prefix}/{script_name}\"\n        ## take this out of the loop \n        logger.info(f\"the script path for where the scripts you have entered in s3 will be installed --> {s3_script_path}\")\n        local_script_path = scripts_dir / script_name\n        logger.info(f\"Downloading {s3_script_path} to {local_script_path}\")\n        txt = read_from_s3(read_bucket, s3_script_path)\n        if txt:\n            Path(local_script_path).write_text(txt)\n        else:\n            logger.error(f\"nothing read from {s3_script_path}\")\n        #s3_client.download_file(read_bucket, s3_script_path, str(local_script_path))\nexcept ClientError as error:\n    logger.error(f\"Failed to download script files: {error}\")\n\n\"\"\"\n#### Download any specified config .yml files from S3\n\"\"\"\n\n## Intialize the config dir that contains the config files for fmbench/to download config files from s3 into\nconfig_dir = Path(pkg_resources.files('fmbench'), 'configs')\nlogger.info(f\"Using fmbench.configs directory: {config_dir}\")\n\n# Ensure the scripts directory exists\nconfig_dir.mkdir(parents=True, exist_ok=True)\nlogger.info(f\"the read bucket is --> {read_bucket} for reading the config files\")\n# config prefix for all of the config files\nconfig_prefix = config['s3_read_data']['configs_prefix']\nlogger.info(f\"the configs directory is --> {config_prefix} for reading the config file names\")\n\n## all config files that reside within s3 as specified in the config file being used\nconfig_files = config['s3_read_data'].get('config_files', [])\nlogger.info(f\"Extracted config files that the user has provided --> {config_files}\")\n\n# Download config files to the fmbench.configs directory\ntry:\n    for config_file in config_files:\n        # do os.path.join\n        s3_config_file_path = f\"{config_prefix}/{config_file}\"\n        ## take this out of the loop \n        logger.info(f\"the config file path for where the config files you have entered in s3 will be installed --> {s3_config_file_path}\")\n        local_config_path = config_dir / config_file\n        logger.info(f\"Downloading {s3_config_file_path} to {local_config_path}\")\n        txt = read_from_s3(read_bucket, s3_config_file_path)\n        if txt:\n            Path(local_config_path).write_text(txt)\n        else:\n            logger.error(f\"nothing read from {s3_config_file_path}\")\n        #s3_client.download_file(read_bucket, s3_config_file_path, str(local_config_path))\nexcept ClientError as error:\n    logger.error(f\"Failed to download config files: {error}\")\n\n\"\"\"\n## Sanity checks on config file\n\"\"\"\n\n# check if dataset of interest is being tested in at least one experiment\nds_of_interest = config['metrics']['dataset_of_interest']\nfound = [True for e in config['experiments'] \\\n         if any([True for f in e['payload_files'] if ds_of_interest in f])]\nif not found:\n    error_message = f\"dataset {ds_of_interest} not found in any of the experiments, \\\n                      add it to the payload_files section of at least one experiment in the config file, exiting for now\"\n    logger.error(error_message)\n    ValueError(error_message)\n    sys.exit(1)\n\n\n\n================================================"
  },
  {
    "filename": "__init__.py",
    "path": "fmbench/__init__.py",
    "directory": "fmbench",
    "extension": "py",
    "content": "================================================\nimport importlib.metadata\n\ntry:\n    __version__ = importlib.metadata.version(\"fmbench\")\nexcept importlib.metadata.PackageNotFoundError:\n    # Package is not installed, fallback to a default version\n    __version__ = \"None\"\n\n\n\n================================================"
  },
  {
    "filename": "bring_your_own_dataset.ipynb",
    "path": "fmbench/bring_your_own_dataset.ipynb",
    "directory": "fmbench",
    "extension": "ipynb",
    "content": "================================================\n# Jupyter notebook converted to Python script.\n\n\"\"\"\n# Bring your own dataset\n\n---------\n*This notebook works best with the conda_python3 kernel on a ml.t3.medium machine*.\n\n### This part of our solution design includes \n\n- Creating your own `fmbench` compatible dataset from a [HuggingFace dataset](https://huggingface.co/docs/datasets/en/index).\n\n- Creating a prompt payload template compatible with your dataset.\n\n- Upload the dataset and the prompt payload to Amazon S3 from where it can be used by `fmbench`.\n\"\"\"\n\n# if interactive mode is set to no -> pickup fmbench from Python installation path\n# if interactive mode is set to yes -> pickup fmbench from the current path (one level above this notebook)\n# if interactive mode is not defined -> pickup fmbench from the current path (one level above this notebook)\n# the premise is that if run non-interactively then it can only be run through main.py which will set interactive mode to no\nimport os\nimport sys\nimport logging\nif os.environ.get(\"INTERACTIVE_MODE_SET\", \"yes\") == \"yes\":\n    sys.path.append(os.path.dirname(os.getcwd()))\n\nlogging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nimport pandas as pd\nfrom fmbench.utils import *\nfrom fmbench.globals import *\nfrom datasets import load_dataset\nconfig = load_config(CONFIG_FILE)\n\n\"\"\"\n## Convert HuggingFace dataset to jsonl format\n\n`fmbench` works with datasets in the [`JSON Lines`](https://jsonlines.org/) format. So here we show how to convert a HuggingFace dataset into JSON lines format.\n\"\"\"\n\n\"\"\"\nSet the `ds_name` to the HuggingFace dataset id, for example [`THUDM/LongBench`](https://huggingface.co/datasets/THUDM/LongBench), [`rajpurkar/squad_v2`](https://huggingface.co/datasets/rajpurkar/squad_v2), [`banking77`](https://huggingface.co/datasets/banking77) or other text datasets.\n\"\"\"\n\n\"\"\"\n#### Standard Text Datasets\n---\n \nIf you are using FMBench to benchmark models on standard text datasets, run the cells below to perform appropriate data preprocessing.\n\"\"\"\n\nds_id: str = \"rajpurkar/squad\"\nds_name: str = \"plain_text\"\nds_split: str = \"train\"\n# Take a random subset of the dataframe, adjust the value of `N` below as appropriate.\n# size of random subset of the data\nds_N: int = 100\n\n# another example\n# ds_id: str = \"THUDM/LongBench\"\n# ds_name: str = \"2wikimqa\"\n# ds_split: str = \"test\"\n# Take a random subset of the dataframe, adjust the value of `N` below as appropriate.\n# size of random subset of the data\n# ds_N: int = 200\n\n# another example\n# ds_id: str = \"banking77\"\n# ds_name: str = \"default\"\n# ds_split: str = \"train\"\n# Take a random subset of the dataframe, adjust the value of `N` below as appropriate.\n# size of random subset of the data\n# ds_N: int = 10000\n\nds_id: str = \"Open-Orca/OpenOrca\"\nds_name: str = \"default\"\nds_split: str = \"train\"\n# Take a random subset of the dataframe, adjust the value of `N` below as appropriate.\n# size of random subset of the data\nds_N: int = 100\n\n# Helper function to load the dataset and return the list of the dataset to convert into a df and then into a jsonl file\nimport logging\nimport itertools\nfrom datasets import load_dataset, Dataset\n\ndef load_hf_ds_subset(\n    ds_id: str,\n    ds_split: str = \"train\",\n    ds_N: int = 100\n) -> Dataset:\n    \"\"\"\n    Loads a subset of the Dolly dataset in streaming mode, taking the first ds_N examples.\n    \n    Args:\n        ds_id (str): The dataset identifier (default: 'databricks/databricks-dolly-15k').\n        ds_split (str): The split of the dataset to load (default: 'train').\n        ds_N (int): Number of examples to take from the dataset (default: 100).\n\n    Returns:\n        Dataset: A Hugging Face Dataset object containing ds_N examples.\n    \"\"\"\n    logger.info(\n        f\"Starting to load dataset with id='{ds_id}', split='{ds_split}', \"\n        f\"taking the first {ds_N} examples in streaming mode.\"\n    )\n    dataset_stream = load_dataset(\n        ds_id,\n        split=ds_split,\n        streaming=True\n    )\n    dataset_iter = itertools.islice(dataset_stream, ds_N)\n    # Convert to a list\n    dataset_list = list(dataset_iter)\n    # Convert the list to a regular (in-memory) Hugging Face Dataset\n    subset_dataset = Dataset.from_list(dataset_list)\n    logger.info(f\"Loaded {len(subset_dataset)} examples from the dataset.\")\n    return subset_dataset\n\n\"\"\"\n### Preprocess the Dolly Dataset\n---\n\nIn this section of the notebook, we will pre process the Dolly dataset. The `databricks/databricks-dolly-15k` is an open source dataset of instruction-following records generated by thousands of Databricks employees in several of the behavioral categories outlined in the InstructGPT paper, including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization. This dataset contains the `instruction` and the `context`. However, for some of the categories for example open QA, the context is not provided. To make this consistent, we will add the context in a new line within the instruction field so that we can have the Foundation Models (FMs) consistently evaluate all content without having empty contexts in some of the calls.\n\"\"\"\n\nds_id: str = \"databricks/databricks-dolly-15k\"\nds_split: str = \"train\"\n# Take a random subset of the dataframe, adjust the value of `N` below as appropriate.\n# size of random subset of the data\nds_N: int = 100\n\ndataset = load_hf_ds_subset(ds_id, ds_split, ds_N)\n\n# convert the dataset to a dataframe, for print it out and easy conversion to jsonl\ndf = pd.DataFrame(dataset)\ndf.head(10)\n\nprint(f\"dataset shape before random subset = {df.shape}\")\ndf = df.sample(n=ds_N)\nprint(f\"dataset shape before random subset = {df.shape}\")\n\n\"\"\"\n#### Merge the instruction and context\n---\n\nNow, we will add the context as a new line to the instruction column in the dataset.\n\"\"\"\n\n# 1. Append context to instruction only if context is not empty\ndf[\"instruction\"] = df.apply(\n    lambda row: row[\"instruction\"] + \"\\n\" + row[\"context\"] \n                if isinstance(row[\"context\"], str) and row[\"context\"].strip() != \"\" \n                else row[\"instruction\"],\n    axis=1\n)\ndf.drop(columns=[\"context\"], inplace=True)\ndf.head(10)\n\nlogger.info(f\"Preprocessed and loaded the dolly dataset. Going to load the jsonl line in the s3 bucket\")\njsonl_content = df.to_json(orient='records', lines=True)\nprint(jsonl_content[:1000])\n\nbucket: str = config['s3_read_data']['read_bucket']\nprefix: str = config['s3_read_data']['source_data_prefix']\nfile_name: str = f\"{ds_id}.jsonl\"\nwrite_to_s3(jsonl_content, bucket, prefix, \"\", file_name)\n\n\"\"\"\n### Preprocess the OpenOrca Dataset\n---\n\nIn this section of the notebook, we will pre process the OpenOrca dataset. This dataset contains the `system prompt` and the `question`. However, for some of the questions, the system prompt is not provided. To make this consistent, we will add the system prompt as a prefix to the question field so that we can have the Foundation Models (FMs) consistently evaluate all content without having empty system prompts in some of the calls.\n\"\"\"\n\nds_id: str = \"Open-Orca/OpenOrca\"\nds_split: str = \"train\"\n# Take a random subset of the dataframe, adjust the value of `N` below as appropriate.\n# size of random subset of the data\nds_N: int = 100\n\ndataset = load_hf_ds_subset(ds_id, ds_split, ds_N)\n\n# convert the dataset to a dataframe, for print it out and easy conversion to jsonl\ndf = pd.DataFrame(dataset)\ndf.head(10)\n\ndf[\"question\"] = df.apply(\n    lambda row: row[\"system_prompt\"] + \"\\n\" + row[\"question\"] \n                if isinstance(row[\"system_prompt\"], str) and row[\"system_prompt\"].strip() != \"\" \n                else row[\"question\"],\n    axis=1\n)\ndf.drop(columns=[\"system_prompt\"], inplace=True)\ndf.head(10)\n\nlogger.info(f\"Preprocessed and loaded the open orca dataset. Going to load the jsonl line in the s3 bucket\")\njsonl_content = df.to_json(orient='records', lines=True)\nprint(jsonl_content[:1000])\n\nbucket: str = config['s3_read_data']['read_bucket']\nprefix: str = config['s3_read_data']['source_data_prefix']\nfile_name: str = f\"{ds_id}.jsonl\"\nwrite_to_s3(jsonl_content, bucket, prefix, \"\", file_name)\n\n\"\"\"\n#### Image Datasets\n---\n\nIf you are using FMBench to benchmark models on an image dataset, run the cells below to load the image dataset and send the data to s3. This data will be used in the `1_generate_data.ipynb` notebook to convert the available image column (specified by the user) in the configuration file into `base64`. This will be used later in the benchmarking test while running inferences against the model endpoint.\n\"\"\"\n\n# Marqo/marqo-GS-10M DATASET: This is an image dataset without any questions\n\n# import itertools\n# from datasets import load_dataset, Dataset\n\n# # ds_id: str = \"HuggingFaceM4/WebSight\"\n# ds_id: str = \"Marqo/marqo-GS-10M\"\n# ds_name: str = \"default\"\n# # ds_name: str = \"v0.2\"\n# ds_split: str = \"in_domain\"\n# ds_N: int = 100\n\n# # Load the dataset in streaming mode so you don't have to load the entire dataset\n# dataset = load_dataset(ds_id, name=ds_name, split=ds_split, streaming=True)\n\n# # Take only the first ds_N examples\n# dataset_iter = itertools.islice(dataset, ds_N)\n\n# # Convert to a list and then to a regular dataset\n# dataset_list = list(dataset_iter)\n# dataset = Dataset.from_list(dataset_list)\n\nimport itertools\nfrom datasets import load_dataset, Dataset\n\nds_id: str = \"derek-thomas/ScienceQA\"\nds_name: str = \"default\"\nds_split: str = \"test\"\nds_N: int = 100\n\n# Load the dataset in streaming mode so you don't have to load the entire dataset\ndataset = load_dataset(ds_id, name=ds_name, split=ds_split, streaming=True)\n\n# Take only the first ds_N examples\ndataset_iter = itertools.islice(dataset, ds_N)\n\n# Convert to a list and then to a regular dataset\ndataset_list = list(dataset_iter)\ndataset = Dataset.from_list(dataset_list)\n\nlogger.info(f\"Loaded {len(dataset)} examples\")\n\ndataset\n\n# convert the dataset to a dataframe, for print it out and easy conversion to jsonl\ndf = pd.DataFrame(dataset)\ndf.head(10)\n\n# view one of the images in the dataset\ndf.image[10]\n\n# some datasets contain a field called column, we would like to call it\n# input to match it to the prompt template\ndf.rename(columns={\"question\": \"input\"}, inplace=True)\ndf.head()\n\nimport io\nimport json\nfrom PIL import Image\n\n# This is a custom JSON encoder class called PILImageEncoder that extends the built-in json.JSONEncoder class. \n# The purpose of this class is to enable JSON serialization of PIL (Python Imaging Library) Image objects, which are \n# not natively JSON serializable.\n\n# It checks if the object ( obj) is an instance of Image.Image (a PIL Image object).\n\n# If it is an Image object:\n# a. It creates a BytesIO buffer.\n# b. Saves the image to this buffer in JPEG format.\n# c. Converts the binary data in the buffer to a hexadecimal string.\n# d. Returns a dictionary with two keys: the hexadecimal string and the JPEG format\n\nclass PILImageEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, Image.Image):\n            buffered = io.BytesIO()\n            obj.save(buffered, format=\"JPEG\")\n            hex_data = buffered.getvalue().hex()\n            return {\n                'hex_data': hex_data,\n                'format': 'JPEG'\n            }\n        return super(PILImageEncoder, self).default(obj)\n\n\"\"\"\n### Subset the data\n\"\"\"\n\nprint(f\"dataset shape before random subset = {df.shape}\")\ndf = df.sample(n=ds_N)\nprint(f\"dataset shape before random subset = {df.shape}\")\n\n\"\"\"\nConvert to json lines format\n\"\"\"\n\n# if the image column is provided in the dataset, then use the PIL image encoder\nif config['datasets'].get('image_col') is not None:\n    logger.info(f\"The data is multimodal. Using the PILImageEncoder to encode the PIL image into jsonl files\")\n    jsonl_content = df.to_json(orient='records', lines=True, default_handler=PILImageEncoder().default)\nelse:\n    logger.info(f\"The data is standard text data, will convert to jsonl files.\")\n    jsonl_content = df.to_json(orient='records', lines=True)\nprint(jsonl_content[:1000])\n\n\"\"\"\n## Upload the dataset to s3\n\"\"\"\n\nbucket: str = config['s3_read_data']['read_bucket']\nprefix: str = config['s3_read_data']['source_data_prefix']\nfile_name: str = f\"{ds_id}.jsonl\"\nwrite_to_s3(jsonl_content, bucket, prefix, \"\", file_name)\n\n\"\"\"\n## Create a prompt template and upload it to S3\nThe prompt template is specific to the model under test and also the dataset being used. The variables used in the template, such as `context` and `input` must exist in the dataset being used so that this prompt template can be converted into an actual prompt.\n\"\"\"\n\n# dictionary containing the prompt template, it has a key by the name\n# of the dataset id which forces you to explicitly add your dataset here\n# otherwise no new prompt template will be uploaded and it wont accidently\n# end up overwriting an existing prompt template\nprompt_template = {}\n\n# LongBench\nprompt_template['THUDM-LongBench-llama2-mistral'] = \"\"\"<s>[INST] <<SYS>>\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context in the section demarcated by \"```\" to answer the question. If you don't know the answer just say that you don't know. Use three sentences maximum and keep the answer concise.\n<</SYS>>\n\n```\n{context}\n```\n\nQuestion: {input}\n\n[/INST]\nAnswer:\n\"\"\"\n\n# Open Orca\nprompt_template['Open-Orca-OpenOrca-llama2-mistral'] = \"\"\"<s>[INST] <<SYS>>\n\n{system_prompt}\n\n<</SYS>>\n\nContext and task: {input}\n\n[/INST]\n\"\"\"\n\nprompt_template['Open-Orca-OpenOrca-llama3'] = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n{system_prompt}\n\nContext and task: {input} \n\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\"\"\"\n\nbucket: str = config['s3_read_data']['read_bucket']\nprefix: str = config['s3_read_data']['prompt_template_dir']\nfor k in prompt_template.keys():\n    file_name: str = f\"prompt_template_{k}.txt\"\n    print(f\"writing {file_name} to s3://{bucket}/{prefix}/{file_name}\")\n    write_to_s3(prompt_template[k], bucket, prefix, \"\", file_name)\n\n\"\"\"\n## Scratchpad\n\"\"\"\n\n\"\"\"\n### Utility function for converting a line from container log to JSON format\n\nThe following is a line from CW log from a model container that provides all the information about the model that is not available anywhere else (not in Model or EndpointConfig or Endpoint description). This information is often necessary to know the low level settings about the model which may have been set while compiling the model.\n\"\"\"\n\nline=\"\"\"model_id_or_path='/tmp/.djl.ai/download/ae03dd100c208acd82b5dbed563c971de864c408' rolling_batch=<RollingBatchEnum.auto: 'auto'> tensor_parallel_degree=8 trust_remote_code=False enable_streaming=<StreamingEnum.false: 'false'> batch_size=4 max_rolling_batch_size=4 dtype=<Dtype.f16: 'fp16'> revision=None output_formatter=None waiting_steps=None is_mpi=False draft_model_id=None spec_length=0 neuron_optimize_level=None enable_mixed_precision_accumulation=False enable_saturate_infinity=False n_positions=4096 unroll=None load_in_8bit=False low_cpu_mem_usage=False load_split_model=True context_length_estimate=None amp='f16' quantize=None compiled_graph_path=None task=None save_mp_checkpoint_path=None group_query_attention=None model_loader=<TnXModelLoaders.tnx: 'tnx'> rolling_batch_strategy=<TnXGenerationStrategy.continuous_batching: 'continuous_batching'> fuse_qkv=False on_device_embedding=False attention_layout=None collectives_layout=None cache_layout=None partition_schema=None all_reduce_dtype=None cast_logits_dtype=None\"\"\"\nimport re\nimport json\npattern = r' (?=[^\\'\"])'\n\n\n# Split the string using the pattern\nresult = re.split(pattern, line)\nprint(\"\\n\".join([r for r in result]))\nparams= {}\nfor kv in result:\n    #print(kv.split('='))\n    k,v = kv.split('=')\n    params[k] = v\nprint(json.dumps(params, indent=2, default=str))\n\n\n\n================================================"
  },
  {
    "filename": "config_filepath.txt",
    "path": "fmbench/config_filepath.txt",
    "directory": "fmbench",
    "extension": "txt",
    "content": "================================================\nconfigs/multimodal/bedrock/config-llama-3-2-11b-vision-instruct-scienceqa.yml\n\n\n================================================"
  },
  {
    "filename": "defaults.py",
    "path": "fmbench/defaults.py",
    "directory": "fmbench",
    "extension": "py",
    "content": "================================================\nDEFAULT_BUCKET_WRITE: str = 'sagemaker-fmbench-write'\nDEFAULT_BUCKET_READ: str = 'sagemaker-fmbench-read'\nDEFAULT_LOCAL_WRITE: str = 'fmbench-write'\nDEFAULT_LOCAL_READ: str = 'fmbench-read'\nMAX_REQ_CNT: int = 105\nMIN_REQ_CNT: int = 5\n\n\n\n================================================"
  },
  {
    "filename": "globals.py",
    "path": "fmbench/globals.py",
    "directory": "fmbench",
    "extension": "py",
    "content": "================================================\nimport os\nimport yaml\nimport json\nimport boto3\nimport requests\nimport tempfile\nfrom enum import Enum\nfrom pathlib import Path\nfrom fmbench import defaults\nfrom datetime import datetime\nfrom typing import Optional, Dict\nimport importlib.resources as pkg_resources\n\n\nFMBENCH_PACKAGE_NAME: str = \"fmbench\"\n\n# This is the hf prefix to the source data file which acts as an identifier to whether\n# the dataset is a hugging face dataset or not\nHF_DATASET_PREFIX: str = \"hf:\"\nDEFAULT_IMAGE_FORMAT: str = \"JPEG\"\n\n# This is the default production variant name that is given to sagemaker endpoints\nDEFAULT_PRODUCTION_VARIANT_NAME: str = 'AllTraffic'\n\n# This is the ds_N default value of the number of rows to be processed from the hf dataset.\n# If this value is already given in the 'dataset' section of the config file, that will be used.\nDEFAULT_HF_DS_N_VALUE: int = 100\n\ncurrent_working_directory: str = Path.cwd()\n\nCONFIG_FILEPATH_FILE: str = current_working_directory / 'config_filepath.txt'\n\nPRICING_FALLBACK_YAML_PATH=\"https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/refs/heads/main/fmbench/configs/pricing_fallback.yml\"\n\n# S3 client initialization\ns3_client = boto3.client('s3')\nsession = boto3.session.Session()\nregion_name = session.region_name\nif region_name is None:\n    print(f\"boto3.session.Session().region_name is {region_name}, \"\n          f\"going to use an metadata api to determine region name\")\n    # THIS CODE ASSUMED WE ARE RUNNING ON EC2, for everything else\n    # the boto3 session should be sufficient to retrieve region name\n    resp = requests.put(\"http://169.254.169.254/latest/api/token\",\n                        headers={\"X-aws-ec2-metadata-token-ttl-seconds\": \"21600\"})\n    token = resp.text\n    region_name = requests.get(\"http://169.254.169.254/latest/meta-data/placement/region\",\n                               headers={\"X-aws-ec2-metadata-token\": token}).text\n    print(f\"region_name={region_name}, also setting the AWS_DEFAULT_REGION env var\")\n    os.environ[\"AWS_DEFAULT_REGION\"] = region_name\nprint(f\"region_name={region_name}\")\n\n# Configuring the role ARN -- extract the role name\ncaller = boto3.client('sts').get_caller_identity()\naccount_id = caller.get('Account')\nrole_arn_from_env = os.environ.get('FMBENCH_ROLE_ARN')\nif role_arn_from_env:\n    print(f\"role_arn_from_env={role_arn_from_env}, using it to set arn_string\")\n    arn_string = role_arn_from_env\nelse:\n    print(f\"role_arn_from_env={role_arn_from_env}, using current sts caller identity to set arn_string\")\n    arn_string = caller.get('Arn')\n    # if this is an assumed role then remove the assumed role related pieces\n    # because we are also using this role for deploying the SageMaker endpoint\n    # arn:aws:sts::015469603702:assumed-role/SSMDefaultRoleForOneClickPvreReporting/i-0c5bba16a8b3dac51\n    # should be converted to arn:aws:iam::015469603702:role/SSMDefaultRoleForOneClickPvreReporting\n    if \":assumed-role/\" in arn_string:\n        role_name = arn_string.split(\"/\")[-2]\n        arn_string = f\"arn:aws:iam::{account_id}:role/{role_name}\"\n        print(f\"the sts role is an assumed role, setting arn_string to {arn_string}\")\n    else:\n        arn_string = caller.get('Arn')\n\nROLE_NAME = arn_string.split('/')[-1]\n\ncurrent_config_file = os.environ.get(\"CONFIG_FILE_FMBENCH\")\n# if var is true, use that from cli\nif current_config_file is not None:\n    CONFIG_FILE = current_config_file\n    print(f\"config file current -> {CONFIG_FILE}, {current_config_file}\")\nelse:\n    CONFIG_FILE = Path(CONFIG_FILEPATH_FILE).read_text().strip()\n    print(f\"config file current -> {CONFIG_FILE}, {current_config_file}\")\n\n\nif CONFIG_FILE.startswith(\"s3://\"):\n    # Parse the S3 URL\n    bucket_name, key_path = CONFIG_FILE.replace(\"s3://\", \"\").split(\"/\", 1)\n    # Use boto3 to access the S3 service\n    s3 = boto3.resource('s3')\n    # Fetch the object from S3\n    obj = s3.Object(bucket_name, key_path)\n    # Read the object's content\n    CONFIG_FILE_CONTENT = obj.get()['Body'].read().decode('utf-8')\nelif CONFIG_FILE.startswith(\"https://\"):\n    # Fetch the content from the HTTPS URL\n    response = requests.get(CONFIG_FILE)\n    response.raise_for_status()  # Ensure we got a successful response\n    CONFIG_FILE_CONTENT = response.text\nelse:\n    CONFIG_FILE_CONTENT = Path(CONFIG_FILE).read_text()\n\n# check if the file is still parameterized and if so replace the parameters with actual values\n# if the file is not parameterized then the following statements change nothing\nwrite_bucket = os.environ.get(\"WRITE_BUCKET\", f\"{defaults.DEFAULT_BUCKET_WRITE}-{region_name}-{account_id}\")\n# check if the tmp dir is used as an argument if local mode is set to yes. If so, then use that as the temp file directory\n# else use the default `tempfile` option\ntmp_dir = os.environ.get(\"TMP_DIR\", tempfile.gettempdir())\n\n# Retrieve the serialized custom parameters from the environment variable\ncustom_params_str = os.environ.get('CUSTOM_PARAMS', '{}')\ntry:\n    custom_params = json.loads(custom_params_str)\nexcept json.JSONDecodeError:\n    logging.error(\"Failed to decode CUSTOM_PARAMS; ensure it is valid JSON.\")\n    custom_params = {}\n\nargs = dict(region=session.region_name,\n            role_arn=arn_string,\n            read_tmpdir=os.path.join(tmp_dir, defaults.DEFAULT_LOCAL_READ),\n            write_tmpdir=os.path.join(tmp_dir, defaults.DEFAULT_LOCAL_WRITE),\n            write_bucket=write_bucket,\n            read_bucket=f\"{defaults.DEFAULT_BUCKET_READ}-{region_name}-{account_id}\")\n# This updates the config file with the custom parameters that the user might provide and formats it into it.\n# For example, tp depree, instance type, batch size, etc.\nargs.update(custom_params)\nCONFIG_FILE_CONTENT = CONFIG_FILE_CONTENT.format(**args)\n\n# Load the configuration\nconfig = yaml.safe_load(CONFIG_FILE_CONTENT)\nlocal_mode = os.environ.get(\"LOCAL_MODE\")\nif local_mode == \"yes\":\n    print(\"globals.py, local_mode = yes\")\n    config['aws']['s3_and_or_local_file_system'] = 'local'\n    config['s3_read_data']['s3_or_local_file_system'] = 'local'\n    if config['s3_read_data'].get('local_file_system_path') is None:\n        config['s3_read_data']['local_file_system_path'] = os.path.join(tmp_dir, defaults.DEFAULT_LOCAL_READ)\n    if config['aws'].get('local_file_system_path') is None:\n        config['aws']['local_file_system_path'] = os.path.join(tmp_dir, defaults.DEFAULT_LOCAL_WRITE)\n\n# iterate through each experiment and populate the parameters section in the inference spec\nfor i in range(len(config['experiments'])):\n    # for the experiment at index i, look up the parameter set\n    # retrieve the parameter set from the inference_parameter section\n    # assign the parameters from that parameter set to a new key called\n    # parameters in that experiment\n    parameters = config['inference_parameters'][config['experiments'][i]['inference_spec']['parameter_set']]\n    config['experiments'][i]['inference_spec']['parameters'] = parameters\n    if config['experiments'][i].get('bucket') is None:\n        config['experiments'][i]['bucket'] = config['aws']['bucket']\nprint(f\"loaded config: {config}\")\n\n\n# get the model evaluation configuration file which contains information on the \n# ground truth, the method name, and directory structure being used\neval_config: Optional[Dict] = None\nconfig_dir = Path(pkg_resources.files('fmbench'), 'configs')\n# load the model evaluation configuration file based on the ground truth, \n# formatted into it from the main config file if any\nif 'model_evaluations' in config and config['model_evaluations'] is not None:\n    model_evaluation_common_file: str = config['model_evaluations']\n    ground_truth_col_key: Optional[str] = config['datasets'].get('ground_truth_col_key', None)\n    eval_module = Path(model_evaluation_common_file)\n    eval_file_path: str = os.path.join(config_dir, eval_module)\n    if ground_truth_col_key is not None:\n        with open(eval_file_path, 'r') as file:\n            model_eval_info = file.read()\n            # load the preliminary unformatted config file to fetch the method name and plug it into\n            # the prompt template file names\n            eval_config = yaml.safe_load(model_eval_info)\n            print(f\"loaded eval configuration file: {eval_config}\")\n    else:\n        eval_config=None\n        print(f\"Evalaution configuration file not found in the config file. Provide a valid eval configuration file name.\")\n\n# data directory and prompts\nPER_ACCOUNT_DIR: str = f\"{config['general']['name']}-{ROLE_NAME}\"\nDATA_DIR: str = os.path.join(PER_ACCOUNT_DIR, config['dir_paths']['data_prefix'])\nPROMPTS_DIR = os.path.join(DATA_DIR, config['dir_paths']['prompts_prefix'])\n\n# Metrics directory based on date and time\ncurrent_time = datetime.now()\n\n# Assuming current_time is a datetime object\nformatted_time = current_time.strftime(\"%Y/%m/%d/%H/%M\")\n\n# Split the formatted_time into components\nyear, month, day, hour, minute = formatted_time.split('/')\n\n# Construct the METRICS_DIR path\nMETRICS_DIR = f\"{DATA_DIR}/metrics/yyyy={year}/mm={month}/dd={day}/hh={hour}/mm={minute}\"\n\nMETRICS_PER_INFERENCE_DIR = os.path.join(METRICS_DIR, \"per_inference\")\nMETRICS_PER_CHUNK_DIR = os.path.join(METRICS_DIR, \"per_chunk\")\nMETRICS_PER_POLL_EVAL_DIR_NAME: str = \"per_poll_eval\"\n\nMETRICS_PER_INFERENCE_DIR = os.path.join(METRICS_DIR, \"per_inference\")\nMETRICS_PER_CHUNK_DIR = os.path.join(METRICS_DIR, \"per_chunk\")\nENDPOINT_METRICS_FNAME = \"endpoint_metrics.csv\"\nENDPOINT_METRICS_SUMMARIZED_FNAME = \"endpoint_metrics_summarized.csv\"\n\n# These are the column names that are present in the SageMaker and EC2 instance utilization metrics\n# SageMaker utilization metrics contain an EndpointName column\nSAGEMAKER_EP_NAME_COL: str = \"EndpointName\"\n# These are the list of quantiles that are measured as a part of the metric utilization calculation\n# for EC2 benchmarking\nUTILIZATION_QUANTILE_METRICS = [0, 0.25, 0.5, 0.75, 1.0]\n\n# Models directory based on date and time \nMODELS_DIR = f\"{DATA_DIR}/models\"\n\n# Use this to upload to the s3 bucket (extracted from the config file)\nBUCKET_NAME = config['aws']['bucket']\nREAD_BUCKET_NAME = config['s3_read_data']['read_bucket']\n\n# S3 prefix\nPREFIX_NAME = config['dir_paths']['data_prefix']\n\n# SOURCE data is where your actual data resides in s3\nSOURCE_DATA = config['s3_read_data']['source_data_prefix']\n\n# Read the prompt template that the user uploads\nPROMPT_TEMPLATE_S3_PREFIX = config['s3_read_data']['prompt_template_dir']\n\n# Initialize the scripts directory\nSCRIPTS_DIR: str = \"fmbench/scripts\"\n\n# Contruct the path to the evaluation prompt and the different rules in \n# the rules directory for respective subjective eval criteria\nif eval_config is not None:\n    EVAL_PROMPT_TEMPLATES: str = os.path.join(PROMPT_TEMPLATE_S3_PREFIX,\n                                              eval_config['model_evaluations']['model_eval_dir'].get('eval_prompts_dir', None))\n    EVAL_DIR: str = eval_config['model_evaluations']['model_eval_dir'].get('eval_prompts_dir', None)\n    EVAL_INSTRUCTIONS_DIR: str = eval_config['model_evaluations']['model_eval_dir'].get('eval_instructions_dir', None)\n\n# METADATA DIR TO HANDLE DYNAMIC S3 PATHS FOR METRICS/RESULTS\nMETADATA_DIR:str = config['dir_paths']['metadata_dir']\nMETRICS_PATH_FNAME: str = \"metrics_path.txt\"\n\n# Name of the .txt file where the HF token is stored\nHF_TOKEN_FNAME: str = \"hf_token.txt\"\n\nDIR_LIST = [DATA_DIR, PROMPTS_DIR, METRICS_DIR, MODELS_DIR, METRICS_PER_INFERENCE_DIR, METRICS_PER_CHUNK_DIR]\n\n# this is for custom tokenizers\nTOKENIZER_DIR_S3 = config['s3_read_data']['tokenizer_prefix']\nTOKENIZER = 'tokenizer'\n# we take the first experiment's model id as the model whose tokenizer is used for \n# counting tokens on the dataset. This is done just for ease of coding, this is just\n# token counting logic on the client side (does not impact the tokenizer the model uses)\n# NOTE: if tokenizer files are provided in the tokenizer directory then they take precedence\n# if the files are not present then we load the tokenizer for this model id from Hugging Face\nTOKENIZER_MODEL_ID = config['experiments'][0].get('model_id')\n\n# If the model that is being benchmarked does not necessarilly have an HF model id, but you\n# want to use the hf tokenizer for that model, then mention the \"hf_tokenizer_model_id\" in the\n# experiments section to load the tokenizer at runtime. The alternative option to this is\n# to provide the \"config.json\" and \"tokenizer.json\" files in the read directory. If neither are provided, \n# then the default 750-1000 tokens tokenizer will be used.\nHF_TOKENIZER_MODEL_ID = config['experiments'][0].get('hf_tokenizer_model_id')\n\nDEPLOYMENT_SCRIPT_S3 = config['s3_read_data']['scripts_prefix']\n\n_ = list(map(lambda x: os.makedirs(x, exist_ok=True), DIR_LIST))\n\n# Define the endpoint list as the config-general name plus the role arn for unique generation \n# from different roles in the same/different accounts\nENDPOINT_LIST_PATH: str = os.path.join(MODELS_DIR, \"endpoints.json\")\n\nREQUEST_PAYLOAD_FPATH: str = os.path.join(PROMPTS_DIR, \"payload.jsonl\")\nRESULTS_FPATH: str = os.path.join(METRICS_DIR, \"results.csv\")\n\n\nclass TRUNCATE_POLICY(str, Enum):\n    AT_PROMPT_TOKEN_LENGTH = 'at-prompt-token-length'\n\n# misc. metrics related\nPLACE_HOLDER: int = -1705338041\nRESULTS_DIR: str = f\"results-{config['general']['name']}\"\n\n# benchmarking - metric filenames\nCOUNTS_FNAME: str = \"experiment_counts.csv\"\nERROR_RATES_FNAME: str = \"error_rates.csv\"\nRESULTS_DESC_MD_FNAME: str = \"report.md\"\nSUMMARY_METRICS_W_PRICING_FNAME: str = \"summary_metrics_w_pricing.csv\"\nINSTANCE_PRICING_PER_HOUR_FNAME: str = \"instance_pricing_per_hour.csv\"\nSUMMARY_METRICS_FOR_DATASET_W_SCORES_FNAME: str = \"summary_metrics_for_dataset_w_scores.csv\"\nSUMMARY_METRICS_FOR_DATASET_W_SCORES_BEST_OPTION_FNAME: str = \"summary_metrics_for_dataset_best_option.csv\"\nSUMMARY_METRICS_FOR_DATASET_W_SCORES_BEST_OPTION_EACH_INSTANCE_TYPE_FNAME: str = \"summary_metrics_for_dataset_best_option_each_instance_type.csv\"\nSUMMARY_MODEL_ENDPOINT_COST_PER_INSTANCE: str = \"endpoint_per_instance_per_run_costs.csv\"\nBUSINESS_SUMMARY_PLOT_FNAME: str = \"business_summary.png\"\nBUSINESS_SUMMARY_PLOT_FNAME2: str = \"business_summary_barchart.png\"\nLATENCY_CHART_PLOT_FNAME: str = \"latency_summary_chart.png\"\n\n# evaluation - metric filenames\nPER_INFERENCE_FILE_WITH_COSINE_SIMILARITY_SCORES: str = \"per_inference_quantitative_eval_metrics.csv\"\nEVAL_COL_SUFFIX: str = '_eval_prompt'\nPROCESSED_EVAL_PROMPT_PAYLOADS: str = \"processed_eval_prompts_for_inference.csv\"\nMODEL_EVALUATION_JUDGE_COMPLETIONS_DIR: str = \"judge_model_eval_completions\"\nMODEL_EVAL_COMPLETIONS_CSV: str = \"raw_llm_as_a_judge_evals.csv\"\nLLM_JUDGE_PANEL_RESPONSE_SUMMARIES: str = \"llm_as_a_judge_per_eval_summary.csv\"\n# this csv contains all of the incorrect verdict responses from the PoLL\n# evaluation of responses using Max Voting. View this csv to get more insight\n# into where the model went wrong, and what to fix\nVERDICT_TYPE_BREAKDOWN_FOR_CORRECT_FILE: str = \"verdict_type_breakdown_for_correct.csv\"\nVERDICT_TYPE_BREAKDOWN_FOR_INCORRECT_FILE: str = \"verdict_type_breakdown_for_incorrect.csv\"\nPER_MODEL_ACCURACY_W_VERDICT_TYPE_FILE: str = \"per_model_accuracy_w_verdict_type.csv\"\nMAJORITY_VOTE_DF_RAW_RESULTS_FILE: str = \"majority_vote_results_raw.csv\"\nPER_PAYLOAD_MODEL_ACCURACY_MAJORITY_VOTING: str = \"per_payload_model_accuracy_majority_vote.csv\"\nPER_MODEL_ACCURACY_PER_EVAL_JUDGE: str = \"per_model_per_eval_judge_accuracy.csv\"\nCANDIDATE_MODEL_ACCURACY_FILE: str = \"candidate_model_accuracy.csv\"\nINCORRECT_VERDICT_RESPONSES_FILE: str = \"incorrect_verdict_responses.csv\"\nCORRECT_VERDICT_RESPONSES_FILE: str = \"correct_verdict_responses.csv\"\nSCORING_RESULT_COUNT_POLL: str = \"PoLL_result_count_correct_incorrect.csv\"\nPER_MODEL_ACCURACY_POLL: str = \"PoLL_per_model_accuracy.csv\"\nPER_PAYLOAD_PER_MODEL_POLL_ACCURACY: str = \"majority_vote_accuracy_per_payload_file.csv\"\nEVAL_COST_PER_JUDGE_MODEL: str = \"eval_cost_per_llm_evaluator.csv\"\n# contains all tt data of the LLM completion from the evaluation process\nALL_EVALUATIONS_IN_TXT: str = \"all_judges_evals.txt\"\n# contains the final analysis done by a final LLM in the loop to summarize\n# all evaluations done by panel of LLM evaluators on candidate model responses\nNEEDS_FURTHER_EVAL_FILE: str = \"responses_need_further_eval.txt\"\n# accuracy charts \nPER_PAYLOAD_FILE_ACCURACY_TRAJECTORY: str = \"accuracy_trajectory_per_payload.png\"\nOVERALL_CANDIDATE_MODEL_MAJORITY_VOTING_ACCURACY: str = \"overall_candidate_model_majority_voting_accuracy.png\"\n\n# plot filenames\nERROR_RATES_PLOT_TEXT: str = \"Error rates for different concurrency levels and instance types\"\nERROR_RATES_PLOT_FNAME: str = \"error_rates.png\"\nTOKENS_VS_LATENCY_PLOT_TEXT: str = \"Tokens vs latency for different concurrency levels and instance types\"\nTOKENS_VS_LATENCY_PLOT_FNAME: str = \"tokens_vs_latency.png\"\nCONCURRENCY_VS_INFERENCE_LATENCY_PLOT_FNAME: str = \"concurrency_vs_inference_latency.png\"\nCONCURRENCY_VS_INFERENCE_LATENCY_PLOT_TEXT: str = \"Concurrency Vs latency for different instance type for selected dataset\"\nLATENCY_BUDGET: int = 5\n\nOVERALL_RESULTS_MD: str = \"\"\"\n# {title}\n\n|**Last modified (UTC)** | **FMBench version**  |\n|---|---|\n|{dttm}|{fmbench_version}|\n\n\n## Summary\n\n{business_summary}\n\n## Per instance results\n\nThe following table provides the best combinations for running inference for different sizes prompts on different instance types. The following dataset(s) were used for this test: {datasets}.\n\n|Dataset   | Instance type   | Recommendation   |\n|---|---|---|\n\"\"\"\n\n# Dataset=`{dataset}`, instance_type=`{instance_type}`\nRESULT_DESC: str = \"\"\"The best option for staying within a latency budget of `{latency_budget} seconds` on a `{instance_type}` for the `{dataset}` dataset is a `concurrency level of {concurrency}`. A concurrency level of {concurrency} achieves an `median latency of {latency_median} seconds`, for an `average prompt size of {prompt_size} tokens` and `completion size of {completion_size} tokens` with `{tpm} transactions/minute`.\"\"\"\n\nRESULT_ROW: str = \"|`{dataset}`|`{instance_type}`|{desc}|\"\n\nRESULT_FAILURE_DESC: str = \"\"\"This experiment did not find any combination of concurrency level and other configuration settings that could provide a response within a latency budget of `{latency_budget} seconds` on a `{instance_type}` for the `{dataset}` dataset.\"\"\"\n\nPROMPT_TEMPLATE: str = \"\"\"<s>[INST] <<SYS>>\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context in the section demarcated by \"```\" to answer the question. If you don't know the answer just say that you don't know. Use three sentences maximum and keep the answer concise.\n<</SYS>>\n\n```\n{context}\n```\n\nQuestion: {input}\n\n[/INST]\nAnswer:\n\n\"\"\"\n\n# default for cost per txn to use if it comes to be None when txn per second is zero\nCOST_PER_TXN_PLACEHOLDER = 99\n\n\n================================================"
  },
  {
    "filename": "main.py",
    "path": "fmbench/main.py",
    "directory": "fmbench",
    "extension": "py",
    "content": "================================================\nimport os\nimport re\nimport sys\nimport yaml\nimport json\nimport time\nimport boto3\nimport logging\nimport argparse\nimport requests\nimport papermill as pm\nfrom typing import Dict\nfrom pathlib import Path\nfrom datetime import datetime\nfrom nbformat import NotebookNode\n\n# Setup logging\nlogging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Function to check whether the given uri is a valid s3 uri\ndef is_valid_s3_uri(s3_uri: str) -> bool:\n    pattern = re.compile(r's3://[^/]+/.+')\n    return bool(pattern.match(s3_uri))\n\n# Function to check whether the given uri is a valid https URL\ndef is_valid_http_url(url: str) -> bool:\n    return url.startswith(\"https://\") or url.startswith(\"http://\")\n\n# Function to get the s3_uri from the user and get the config file path, writing the txt file\ndef read_config(config_file_path: str) -> Dict:\n    if is_valid_s3_uri(config_file_path):\n        logger.info(f\"executing the config file found in {config_file_path}...\")\n\n        bucket, key = config_file_path.replace(\"s3://\", \"\").split(\"/\", 1)\n\n        # Get object from S3 and load YAML\n        # Create an S3 client\n        s3_client = boto3.client('s3')\n        response = s3_client.get_object(Bucket=bucket, Key=key)\n        config_content = yaml.safe_load(response[\"Body\"])\n    elif is_valid_http_url(config_file_path):\n        try:\n            logger.info(f\"loading config from HTTPS URL: {config_file_path}\")\n            response = requests.get(config_file_path)\n            response.raise_for_status()  # Raises a HTTPError for bad responses\n            config_content = yaml.safe_load(response.text)\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"error loading config from HTTPS URL: {e}\")\n            raise\n    else:\n        logger.info(f\"the provided URI '{config_file_path}' is not a valid S3 URI or HTTPS URL, assuming this is a local file\")\n        config_content = yaml.safe_load(Path(config_file_path).read_text())\n\n    # You can choose to write to config_path here if needed, otherwise just return the loaded content\n    logger.info(f\"loaded configuration: {json.dumps(config_content, indent=2)}\")\n    return config_content\n\n# Function to handle cell outputs\ndef output_handler(cell: NotebookNode, _):\n    if cell.cell_type == 'code':\n        for output in cell.get('outputs', []):\n            if output.output_type == 'stream':\n                print(output.text, end='')\n\n\ndef run_notebooks(config_file: str) -> None:\n    # Assume `read_config` function is defined elsewhere to load the config\n    config = read_config(config_file)\n\n    current_directory = Path(__file__).parent\n    logging.info(f\"Current directory is --> {current_directory}\")\n\n    output_directory = current_directory / \"executed_notebooks\"\n    if not output_directory.exists():\n        output_directory.mkdir()\n\n    all_notebooks = list(current_directory.glob('*.ipynb'))\n    logger.info(f\"Steps to be executed: {all_notebooks}\")\n\n    for step, execute in config['run_steps'].items():\n        if execute:\n            notebook_path = current_directory / step\n\n            if not notebook_path.exists():\n                matching_notebooks = [nb for nb in all_notebooks if nb.name[1:] == step[1:]]\n                if matching_notebooks:\n                    notebook_path = matching_notebooks[0]\n                else:\n                    logging.error(f\"No matching notebook found for step: {step}\")\n                    continue\n\n            logging.info(f\"Current step file --> {notebook_path.stem}\")\n\n            timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n            output_file = output_directory / f\"{notebook_path.stem}_{timestamp}.ipynb\"\n\n            try:\n                logging.info(f\"Executing {notebook_path.name}...\")\n                logger.info(f\"THE STEP BEING EXECUTED NOW: {step}\")\n                pm.execute_notebook(\n                    input_path=str(notebook_path),\n                    output_path=str(output_file),\n                    kernel_name='python3',\n                    parameters={},\n                    report_mode=True,  \n                    progress_bar=True,\n                    stdout_file=None, \n                    stderr_file=None,\n                    log_output=True,\n                    output_handler=output_handler \n                )\n                logger.info(f\"STEP EXECUTION COMPLETED: {step}\")\n            except FileNotFoundError as e:\n                logging.error(f\"File not found: {e.filename}\")\n                sys.exit(1)\n            except Exception as e:\n                logging.error(f\"Failed to execute {step}: {str(e)}\")\n                sys.exit(1)\n        else:\n            logging.info(f\"Skipping {step} as it is not marked for execution\")\n\n    logger.info(f\"FMBench has completed the benchmarking process. Check the \\\"results-*\\\" folder for results\")\n\n\ndef _parse_key_value(option):\n    try:\n        key, value = option.split('=', 1)\n        return key.strip(), value.strip()\n    except ValueError:\n        raise argparse.ArgumentTypeError(f\"Invalid format for -A option '{option}'. Expected format is key=value.\")\n\n# main function to run all of the fmbench process through a single command via this python package\ndef main():\n    parser = argparse.ArgumentParser(description='Run FMBench with a specified config file.')\n    parser.add_argument('--config-file', type=str, help='The S3 URI of your Config File', required=True)\n    role_help = 'The ARN of the role to be used for FMBench. If an \\\n                 Amazon SageMaker endpoint is being deployed \\\n                 through FMBench then this role would also be used \\\n                 by that endpoint'\n    parser.add_argument('--role-arn', type=str, default=None, required=False, help=role_help)\n    # add an option to run FMBench local mode. If local mode is set to yes, then FMBench uses read and write data locally and if no, \n    # then the test will continue to interact with S3\n    parser.add_argument('--local-mode', type=str, default=None, choices=['yes', 'no'], help='Specify if running in local mode or not. Options: yes, no. Default is no.')\n    # add an option to run FMBench with a custom tmp file argument. Users if running in local mode can configure a custom tmp file \n    # instead of using the default /tmp directory\n    parser.add_argument('--tmp-dir', type=str, default=None, required=False, help='An optional tmp directory if fmbench is running in local mode.')\n    parser.add_argument('--write-bucket', type=str, help='Write bucket that is used for sagemaker endpoints in local mode and storing metrics in s3 mode.')\n    # Add the generic -A option that can be used multiple times. The user can now provide the tp degree, \n    # the batch size, instance type and so on\n    parser.add_argument('-A', \n                       action='append',\n                       type=_parse_key_value,\n                       help='Generic options in key=value format (can be used multiple times)',\n                       dest='generic_options')\n\n\n    args = parser.parse_args()\n    print(f\"main, {args} = args\")\n\n    # Set the environment variable based on the parsed argument\n    os.environ[\"CONFIG_FILE_FMBENCH\"] = args.config_file\n    logger.info(f\"Config file specified: {args.config_file}\")\n    \n    # set env var to indicate that fmbench is being run from main and not interactively via a notebook\n    os.environ[\"INTERACTIVE_MODE_SET\"] = \"no\"\n\n    # Initialize a dictionary to hold custom parameters\n    custom_params = {}\n    # Process each -A argument\n    if args.generic_options:\n        for key, value in args.generic_options:\n            custom_params[key] = value\n    custom_params_str = json.dumps(custom_params)\n\n    # Set the serialized string as an environment variable\n    # We parse this as a custom json that gets appended into the \n    # args dictionary regarless of what parameters the user provides\n    # so that each parameter is not separately rendered in the globals.py\n    # or utils.py file and dynamically added in the formatting process\n    # into the config file\n    os.environ['CUSTOM_PARAMS'] = custom_params_str\n    logging.info(f\"Set environment variable CUSTOM_PARAMS={custom_params_str}\")\n\n    # set the environment variable for the local mode option\n    if args.local_mode:\n        print(f\"setting the LOCAL_MODE to {args.local_mode}\")\n        os.environ[\"LOCAL_MODE\"] = args.local_mode\n        if args.local_mode == \"yes\":\n            if args.write_bucket is None:\n                logger.error(\"Write bucket is not provided when local mode is set to 'yes'\")\n                sys.exit(1)\n            else:\n                # set the environment variable for the write bucket name to be configured and used for sagemaker endpoints and \n                # metrics stored in s3 mode with local mode is set to \"yes\" by the user\n                os.environ[\"WRITE_BUCKET\"] = args.write_bucket\n                logger.info(f\"Write bucket specified in local mode: {args.write_bucket}\")\n            if args.tmp_dir:\n                os.environ[\"TMP_DIR\"] = args.tmp_dir\n                logger.info(f\"tmp directory specified in local mode: {args.tmp_dir}\")\n            else:\n                logger.info(f\"Custom tmp file not provided.\")\n\n\n    # if a role arn is specified then set it as an env var so that the rest of the code\n    # can use it. This will then be used to set the templatized \"sagemaker_execution_role\"\n    # parameter in the config file\n    if args.role_arn:\n        print(f\"setting FMBENCH_ROLE_ARN env variable to {args.role_arn}\")\n        os.environ['FMBENCH_ROLE_ARN'] = args.role_arn\n\n    # Proceed with the rest of your script's logic, passing the config file as needed\n    run_notebooks(args.config_file)    \n\nif __name__ == \"__main__\":\n    main()\n\n\n\n\n================================================"
  },
  {
    "filename": "requirements.txt",
    "path": "fmbench/requirements.txt",
    "directory": "fmbench",
    "extension": "txt",
    "content": "================================================\nipywidgets==8.1.1\nsagemaker==2.203.0\ntransformers==4.36.2\npandas==2.1.4\ndatasets==2.16.1\nseaborn==0.13.1\ntomark==0.1.4\nboto3==1.34.69\nlitellm==1.34.0\n\n\n\n\n================================================"
  },
  {
    "filename": "utils.py",
    "path": "fmbench/utils.py",
    "directory": "fmbench",
    "extension": "py",
    "content": "================================================\nimport re\nimport os\nimport yaml\nimport json\nimport math\nimport boto3\nimport shutil\nimport logging\nimport requests\nimport tempfile\nimport posixpath\nimport unicodedata\nfrom pathlib import Path\nimport concurrent.futures\nfrom fmbench import globals\nfrom fmbench import defaults\nfrom transformers import AutoTokenizer\nfrom botocore.exceptions import NoCredentialsError\nfrom typing import Union, Dict, List, Tuple, Optional\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\n\n# utility functions\ndef load_config(config_file: Union[Path, str]) -> Dict:\n    \"\"\"\n    Load configuration from a local file or an S3 URI.\n\n    :param config_file: Path to the local file or S3 URI (s3://bucket/key)\n    :return: Dictionary with the loaded configuration\n    \"\"\"\n    session = boto3.session.Session()\n    region_name = session.region_name\n    if region_name is None:\n        print(\n            f\"boto3.session.Session().region_name is {region_name}, \"\n            f\"going to use an metadata api to determine region name\"\n        )\n        resp = requests.put(\n            \"http://169.254.169.254/latest/api/token\",\n            headers={\"X-aws-ec2-metadata-token-ttl-seconds\": \"21600\"},\n        )\n        token = resp.text\n        region_name = requests.get(\n            \"http://169.254.169.254/latest/meta-data/placement/region\",\n            headers={\"X-aws-ec2-metadata-token\": token},\n        ).text\n        os.environ[\"AWS_DEFAULT_REGION\"] = region_name\n    print(f\"region_name={region_name}\")\n    caller = boto3.client(\"sts\").get_caller_identity()\n    account_id = caller.get(\"Account\")\n    arn_string = caller.get(\"Arn\")\n    role_arn_from_env = os.environ.get(\"FMBENCH_ROLE_ARN\")\n    if role_arn_from_env:\n        print(f\"role_arn_from_env={role_arn_from_env}, using it to set arn_string\")\n        arn_string = role_arn_from_env\n    else:\n        print(\n            f\"role_arn_from_env={role_arn_from_env}, using current sts caller identity to set arn_string\"\n        )\n        # if this is an assumed role then remove the assumed role related pieces\n        # because we are also using this role for deploying the SageMaker endpoint\n        # arn:aws:sts::015469603702:assumed-role/SSMDefaultRoleForOneClickPvreReporting/i-0c5bba16a8b3dac51\n        # should be converted to arn:aws:iam::015469603702:role/SSMDefaultRoleForOneClickPvreReporting\n        if \":assumed-role/\" in arn_string:\n            role_name = arn_string.split(\"/\")[-2]\n            arn_string = f\"arn:aws:iam::{account_id}:role/{role_name}\"\n            print(\n                f\"the sts role is an assumed role, setting arn_string to {arn_string}\"\n            )\n\n    # check if the file is still parameterized and if so replace the parameters with actual values\n    # if the file is not parameterized then the following statements change nothing\n    write_bucket = os.environ.get(\n        \"WRITE_BUCKET\", f\"{defaults.DEFAULT_BUCKET_WRITE}-{region_name}-{account_id}\"\n    )\n\n    # Retrieve the serialized custom parameters from the environment variable\n    custom_params_str = os.environ.get('CUSTOM_PARAMS', '{}')\n    try:\n        custom_params = json.loads(custom_params_str)\n    except json.JSONDecodeError:\n        logging.error(\"Failed to decode CUSTOM_PARAMS; ensure it is valid JSON.\")\n        custom_params = {}\n\n    # check if the tmp dir is used as an argument if local mode is set to yes. If so, then use that as the temp file directory\n    # else use the default `tempfile` option\n    tmp_dir = os.environ.get(\"TMP_DIR\", tempfile.gettempdir())\n    args = dict(\n        region=session.region_name,\n        role_arn=arn_string,\n        read_tmpdir=os.path.join(tmp_dir, defaults.DEFAULT_LOCAL_READ),\n        write_tmpdir=os.path.join(tmp_dir, defaults.DEFAULT_LOCAL_WRITE),\n        write_bucket=write_bucket,\n        read_bucket=f\"{defaults.DEFAULT_BUCKET_READ}-{region_name}-{account_id}\"\n    )\n    # This updates the config file with the custom parameters that the user might provide and formats it into it.\n    # For example, tp depree, instance type, batch size, etc.\n    args.update(custom_params)\n    # Check if config_file is an S3 URI\n    if config_file.startswith(\"s3://\"):\n        try:\n            # Parse S3 URI\n            s3_client = boto3.client(\"s3\")\n            bucket, key = config_file.replace(\"s3://\", \"\").split(\"/\", 1)\n\n            # Get object from S3 and load YAML\n            response = s3_client.get_object(Bucket=bucket, Key=key)\n            content = response[\"Body\"].read().decode(\"utf-8\")\n\n        except NoCredentialsError:\n            print(\"AWS credentials not found.\")\n            raise\n        except Exception as e:\n            print(f\"Error loading config from S3: {e}\")\n            raise\n    # Check if config_file is an HTTPS URL\n    elif config_file.startswith(\"https://\"):\n        try:\n            response = requests.get(config_file)\n            response.raise_for_status()  # Raises a HTTPError if the response was an error\n            content = response.text\n        except requests.exceptions.RequestException as e:\n            print(f\"Error loading config from HTTPS URL: {e}\")\n            raise\n    else:\n        # Assume local file system if not S3 or HTTPS\n        try:\n            content = Path(config_file).read_text()\n        except Exception as e:\n            print(f\"Error loading config from local file system: {e}\")\n            raise\n\n    content = content.format(**args)\n    config = yaml.safe_load(content)\n    return config\n\n\ndef load_main_config(config_file) -> Dict:\n    config = load_config(config_file)\n    # iterate through each experiment and populate the parameters section in the inference spec\n    for i in range(len(config[\"experiments\"])):\n        # for the experiment at index i, look up the parameter set\n        # retrieve the parameter set from the inference_parameter section\n        # assign the parameters from that parameter set to a new key called\n        # parameters in that experiment\n        parameters = config[\"inference_parameters\"][\n            config[\"experiments\"][i][\"inference_spec\"][\"parameter_set\"]\n        ]\n        config[\"experiments\"][i][\"inference_spec\"][\"parameters\"] = parameters\n        if config[\"experiments\"][i].get(\"bucket\") is None:\n            config[\"experiments\"][i][\"bucket\"] = config[\"aws\"][\"bucket\"]\n    local_mode = os.environ.get(\"LOCAL_MODE\")\n    tmp_dir = os.environ.get(\"TMP_DIR\", tempfile.gettempdir())\n    if local_mode == \"yes\":\n        print(\"utils.py, local_mode = yes\")\n        config[\"aws\"][\"s3_and_or_local_file_system\"] = \"local\"\n        config[\"s3_read_data\"][\"s3_or_local_file_system\"] = \"local\"\n        if config[\"s3_read_data\"].get(\"local_file_system_path\") is None:\n            config[\"s3_read_data\"][\"local_file_system_path\"] = os.path.join(\n                tmp_dir, defaults.DEFAULT_LOCAL_READ\n            )\n        if config[\"aws\"].get(\"local_file_system_path\") is None:\n            config[\"aws\"][\"local_file_system_path\"] = os.path.join(\n                tmp_dir, defaults.DEFAULT_LOCAL_WRITE\n            )\n    return config\n\n\ndef count_tokens(text: str) -> int:\n    global _tokenizer\n    return _tokenizer.count_tokens(text)\n\n\ndef process_item(item, prompt_template_keys: List, prompt_fmt: str) -> Dict:\n    args = {}\n    for k in prompt_template_keys:\n        v = _normalize(item[k])\n        args[k] = v\n        args[f\"{k}_len\"] = _tokenizer.count_tokens(v)\n    prompt = prompt_fmt.format(**args)\n    prompt_len = count_tokens(prompt)\n    return args | {\"prompt\": prompt, \"prompt_len\": prompt_len}\n\n\ndef nt_to_posix(p: str) -> str:\n    return p.replace(\"\\\\\", \"/\")\n\n\ndef is_read_local() -> str:\n    is_read_local = globals.config.get(\"s3_read_data\").get(\"s3_or_local_file_system\")\n    logger.debug(f\"is_read_local: {is_read_local}\")\n    return is_read_local is not None and is_read_local == \"local\"\n\n\ndef _is_write_local_only():\n    is_write_local_only = globals.config.get(\"aws\").get(\"s3_and_or_local_file_system\")\n    logger.debug(f\"is_write_local_only: {is_write_local_only}\")\n    return is_write_local_only is not None and is_write_local_only == \"local\"\n\n\ndef _upload_file_to_local(local_path: str, s3_path: str) -> None:\n    dest = _get_local_write_path(s3_path)\n    shutil.copy(local_path, dest)\n\n\ndef upload_file_to_s3(bucket: str, local_path: str, s3_path: str) -> None:\n    if _is_write_local_or_both():\n        _upload_file_to_local(local_path, s3_path)\n    if _is_write_local_only():\n        return\n\n    s3 = boto3.resource(\"s3\")\n    try:\n        s3.Bucket(bucket).upload_file(local_path, s3_path)\n    except Exception as e:\n        logger.error(f\"upload_file_to_s3, An error occurred: {e}\")\n\n\ndef _write_to_local_read(data, dir1, dir2, file_name):\n    file_dir, actual_file_name = os.path.split(file_name)\n    # remove the hf prefix before sending the file to the local fmbench-read directory\n    file_dir = file_dir.removeprefix(globals.HF_DATASET_PREFIX)\n    logger.info(\n        f\"File directory: {file_dir}, file name to be written locally: {actual_file_name}\"\n    )\n    dir_path = _get_local_read_path(dir1 + \"/\" + dir2 + \"/\" + file_dir + \"/\")\n    logger.info(f\"dir path: {dir_path}\")\n    Path(dir_path).mkdir(parents=True, exist_ok=True)\n    file_path = dir_path + actual_file_name\n    if isinstance(data, str):\n        Path(file_path).write_text(data)\n    else:\n        Path(file_path).write_bytes(data)\n\n\ndef _write_to_local(data, dir1, dir2, file_name):\n    dir = _get_local_write_path(dir1 + \"/\" + dir2 + \"/\")\n    Path(dir).mkdir(parents=True, exist_ok=True)\n    file = dir + file_name\n    if type(data) == str:\n        Path(file).write_text(data)\n    else:\n        Path(file).write_bytes(data)\n\n\n# Function to write data to S3\ndef write_to_s3(data, bucket_name, dir1, dir2, file_name):\n    if _is_write_local_or_both():\n        # If the file name starts with 'hf:', then it means that the hugging face dataset\n        # is going to be loaded at runtime and is supposed to be sent to the /tmp/fmbench-read/source_data\n        # folder. In this case, it is written to the local fmbench-read directory.\n        if file_name.startswith(globals.HF_DATASET_PREFIX):\n            _write_to_local_read(data, dir1, dir2, file_name)\n        else:\n            _write_to_local(data, dir1, dir2, file_name)\n    if _is_write_local_only():\n        return\n\n    # Initialize S3 client\n    s3_client = boto3.client(\"s3\")\n\n    # Construct the S3 file path\n    s3_file_path = posixpath.join(nt_to_posix(dir1), nt_to_posix(dir2), file_name)\n    logger.debug(f\"write_to_s3, s3_file_path={s3_file_path}\")\n    try:\n        # Write the JSON data to the S3 bucket\n        s3_client.put_object(Bucket=bucket_name, Key=s3_file_path, Body=data)\n        return f\"s3://{bucket_name}/{s3_file_path}\"\n    except NoCredentialsError:\n        logger.error(\"write_to_s3, Error: AWS credentials not found.\")\n    except Exception as e:\n        logger.error(f\"write_to_s3, An error occurred: {e}\")\n\n\n# Function to concurrently write multiple data to S3\n# input_list: List[Tuple[data, bucket_name, dir1, dir2, file_name]\ndef write_multiple_to_s3(input_list: List[Tuple[None, str, str, str, str]]) -> None:\n    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n        tasks = []\n        for input_tuple in input_list:\n            tasks.append(executor.submit(lambda: write_to_s3(*input_tuple)))\n\n    for completed_task in concurrent.futures.as_completed(tasks):\n        completed_task.result()\n\n\ndef _read_from_local(s3_file_path: str) -> str:\n    try:\n        s3_file_path = nt_to_posix(_get_local_read_path(s3_file_path))\n        logger.debug(f\"get_local_object, key={s3_file_path}\")\n        return Path(s3_file_path).read_bytes().decode(\"utf-8\")\n    except FileNotFoundError as e:\n        logger.error(f\"read_from_local, An error occurred: {e}\")\n        return None\n\n\n## function to read from s3\ndef read_from_s3(bucket_name, s3_file_path):\n    if is_read_local():\n        return _read_from_local(s3_file_path)\n\n    # Initialize S3 client\n    s3_client = boto3.client(\"s3\")\n    s3_file_path = nt_to_posix(s3_file_path)\n\n    try:\n        # Fetch the object from S3\n        logger.debug(\n            f\"read_from_s3, reading file from bucket={bucket_name}, key={s3_file_path}\"\n        )\n        response = s3_client.get_object(Bucket=bucket_name, Key=s3_file_path)\n        return response[\"Body\"].read().decode(\"utf-8\")\n    except NoCredentialsError:\n        logger.error(\"read_from_s3, Error: AWS credentials not found.\")\n        return None\n    except Exception as e:\n        logger.error(f\"read_from_s3, An error occurred: {e}\")\n        return None\n\n\ndef _get_local_object(bucket: str, key: str, decode: bool) -> Optional[str]:\n    key = nt_to_posix(key)\n    logger.debug(f\"get_local_object, key={key}\")\n    if bucket == globals.config[\"s3_read_data\"][\"read_bucket\"]:\n        pathname = _get_local_read_path(key)\n    else:\n        pathname = _get_local_write_path(key)\n    if Path(pathname).is_file() is False:\n        return None\n    if decode:\n        return Path(pathname).read_bytes().decode(\"utf-8\")\n    else:\n        return Path(pathname).read_bytes()\n\n\n## gets a single s3 file\ndef get_s3_object(bucket: str, key: str, decode=\"True\") -> str:\n    if is_read_local():\n        return _get_local_object(bucket, key, decode)\n    try:\n        key = nt_to_posix(key)\n        logger.debug(f\"get_s3_object, bucket_name={bucket}, key={key}\")\n        # Create an S3 client\n        s3_client = boto3.client(\"s3\")\n        # Retrieve the object from S3\n        response = s3_client.get_object(Bucket=bucket, Key=key)\n        # Read the content of the file\n        if decode:\n            content = response[\"Body\"].read().decode(\"utf-8\")\n        else:\n            content = response[\"Body\"].read()\n    except Exception as e:\n        logger.error(f\"An error occurred while getting the object from s3: {e}\")\n        content=None\n    return content\n\n\ndef _list_local_files(bucket, prefix, suffix):\n    if bucket == globals.config[\"s3_read_data\"][\"read_bucket\"]:\n        dir = _get_local_read_path(prefix)\n    else:\n        dir = _get_local_write_path(prefix)\n    logger.info(f\"dir for listing local files: {dir}\")\n    # Recursively search for files with the suffix in all subdirectories\n    path_list = list(Path(dir).glob(\"**/*\" + suffix))\n    pathname_list = [str(item) for item in path_list]\n    if bucket == globals.config[\"s3_read_data\"][\"read_bucket\"]:\n        return_list = [\n            item.replace(_get_local_read_path(), \"\") for item in pathname_list\n        ]\n    else:\n        return_list = [\n            item.replace(_get_local_write_path(), \"\") for item in pathname_list\n        ]\n    return return_list\n\n\n# Function to list files in S3 bucket with a specific prefix\ndef list_s3_files(bucket, prefix, suffix=\".json\"):\n    if is_read_local():\n        return _list_local_files(bucket, prefix, suffix)\n\n    filter_key_by_suffix = lambda k, s: True if s is None else k.endswith(s)\n    s3_client = boto3.client(\"s3\")\n    next_continuation_token = None\n\n    return_list = []\n    while True:\n        if next_continuation_token is not None:\n            response = s3_client.list_objects_v2(\n                Bucket=bucket,\n                Prefix=nt_to_posix(prefix),\n                ContinuationToken=next_continuation_token,\n            )\n        else:\n            response = s3_client.list_objects_v2(\n                Bucket=bucket, Prefix=nt_to_posix(prefix)\n            )\n        return_list += [\n            item[\"Key\"]\n            for item in response.get(\"Contents\", [])\n            if filter_key_by_suffix(item[\"Key\"], suffix) is True\n        ]\n        logger.info(\n            f\"found {len(return_list)} items in bucket={bucket}, prefix={prefix}, suffix={suffix}\"\n        )\n        if response[\"IsTruncated\"] is True:\n            next_continuation_token = response[\"NextContinuationToken\"]\n        else:\n            break\n    logger.info(\n        f\"there are total of {len(return_list)} items in bucket={bucket}, prefix={prefix}, suffix={suffix}\"\n    )\n    return return_list\n\n\ndef _normalize(text, form=\"NFC\"):\n    # The files in LongBench contain nonstandard or irregular Unicode.\n    # For compatibility and safety we normalize them.\n    return unicodedata.normalize(form, str(text))\n\n\ndef _is_write_local_or_both():\n    is_write_local_or_both = globals.config.get(\"aws\").get(\n        \"s3_and_or_local_file_system\"\n    )\n    logger.debug(f\"is_write_local_or_both: {is_write_local_or_both}\")\n    return is_write_local_or_both is not None and (\n        is_write_local_or_both == \"local\" or is_write_local_or_both == \"both\"\n    )\n\n\ndef _get_local_read_path(dir_or_file: str = None) -> str:\n    if dir_or_file is not None:\n        local_read_path = (\n            globals.config[\"s3_read_data\"][\"local_file_system_path\"] + \"/\" + dir_or_file\n        )\n    else:\n        local_read_path = globals.config[\"s3_read_data\"][\"local_file_system_path\"] + \"/\"\n    logger.debug(f\"local_read_path: {local_read_path}\")\n    return local_read_path\n\n\ndef _get_local_write_path(dir_or_file: str = None) -> str:\n    if dir_or_file is not None:\n        local_write_path = (\n            globals.config[\"aws\"][\"local_file_system_path\"] + \"/\" + dir_or_file\n        )\n    else:\n        local_write_path = globals.config[\"aws\"][\"local_file_system_path\"] + \"/\"\n    logger.debug(f\"local_write_path: {local_write_path}\")\n    return local_write_path\n\n\ndef _download_multiple_files_from_local_write_path(prefix, local_dir):\n    src = _get_local_write_path(prefix)\n    print(\n        f\"_download_multiple_files_from_local_write_path, prefix={prefix}, src={src}, local_dir={local_dir}\"\n    )\n    shutil.copytree(src, local_dir, dirs_exist_ok=True)\n\n\ndef _download_multiple_files_from_local_read_path(prefix, local_dir):\n    src = _get_local_read_path(prefix)\n    print(\n        f\"_download_multiple_files_from_local_read_path, prefix={prefix}, src={src}, local_dir={local_dir}\"\n    )\n    if Path(src).is_dir():\n        shutil.copytree(src, local_dir, dirs_exist_ok=True)\n    else:\n        print(\n            f\"_download_multiple_files_from_local_read_path, {src} does not exist, no files to copy\"\n        )\n\n\ndef download_multiple_files_from_s3(bucket_name, prefix, local_dir):\n    if _is_write_local_or_both():\n        if bucket_name == globals.config[\"aws\"][\"bucket\"]:\n            return _download_multiple_files_from_local_write_path(prefix, local_dir)\n        elif bucket_name == globals.config[\"s3_read_data\"][\"read_bucket\"]:\n            return _download_multiple_files_from_local_read_path(prefix, local_dir)\n        else:\n            logger.error(\n                f\"bucket_name={bucket_name} which does not match write bucket={globals.config['aws']['bucket']} \"\n                f\"or read bucket={globals.config['s3_read_data']['read_bucket']}\"\n            )\n\n    \"\"\"Downloads files from an S3 bucket and a specified prefix to a local directory.\"\"\"\n    logger.info(\n        f\"download_multiple_files_from_s3, bucket_name={bucket_name}, prefix={prefix}, local_dir={local_dir}\"\n    )\n    s3_client = boto3.client(\"s3\")\n\n    # Ensure the local directory exists\n    if not os.path.exists(local_dir):\n        os.makedirs(local_dir)\n\n    # List and download files\n    try:\n        response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n        key_list = list_s3_files(bucket_name, prefix, suffix=None)\n        for file_key in key_list:\n            logger.debug(f\"file_key={file_key}, prefix={prefix}\")\n            local_file_key = file_key.replace(prefix, \"\")\n            parent_dir_in_s3 = os.path.dirname(local_file_key)\n            logger.debug(\n                f\"local_file_key={local_file_key}, parent_dir_in_s3={parent_dir_in_s3}\"\n            )\n            # the first char for parent_dir_in_s3 would always be a '/' so skip that\n            local_dir_to_create = os.path.join(local_dir, parent_dir_in_s3[1:])\n            os.makedirs(local_dir_to_create, exist_ok=True)\n            logger.debug(\n                f\"local_dir_to_create={local_dir_to_create}, local_file_key={local_file_key}\"\n            )\n            local_file_to_create = os.path.basename(local_file_key)\n            if file_key.endswith(\"/\"):\n                logger.info(f\"skipping file_key={file_key}\")\n                continue\n\n            local_file_path = os.path.join(local_dir_to_create, local_file_to_create)\n            logger.debug(\n                f\"bucket_name={bucket_name}, file_key={file_key}, local_file_path={local_file_path}\"\n            )\n            s3_client.download_file(bucket_name, file_key, local_file_path)\n            logger.debug(\n                f\"download_multiple_files_from_s3, Downloaded: {local_file_path}\"\n            )\n    except Exception as e:\n        logger.error(f\"An error occurred while downloading from S3: {e}\")\n\n\nclass CustomTokenizer:\n    \"\"\"A custom tokenizer class\"\"\"\n\n    TOKENS: int = 1000\n    WORDS: int = 750\n    HF_TOKEN_FNAME: str = os.path.join(\n        os.path.dirname(os.path.realpath(__file__)), \"scripts\", \"hf_token.txt\"\n    )\n    if Path(HF_TOKEN_FNAME).is_file() is True:\n        print(f\"{HF_TOKEN_FNAME} file found, going to set HF_TOKEN env var\")\n        HF_TOKEN: str = Path(HF_TOKEN_FNAME).read_text().strip()\n        os.environ[\"HF_TOKEN\"] = HF_TOKEN\n    else:\n        print(f\"{HF_TOKEN_FNAME} file not found\")\n\n    def __init__(self, bucket, prefix, local_dir, model_id, hf_model_id=None):\n        print(\n            f\"CustomTokenizer, based on HF transformers, {bucket} \"\n            f\"prefix: {prefix} local_dir: {local_dir}, model_id: {model_id}\"\n        )\n        # Check if the tokenizer files exist in s3 and if not, use the autotokenizer\n        download_multiple_files_from_s3(bucket, prefix, local_dir)\n        # Load the tokenizer from the local directory\n        all_files = [f for f in list(Path(local_dir).iterdir()) if f.name != \".keep\"]\n        dir_not_empty = len(all_files)\n        print(f\"CustomTokenizer, all_files = {all_files}\")\n        abs_path = Path(local_dir).absolute().resolve()\n        if dir_not_empty > 0:\n            print(\n                f\"loading the provided tokenizer from local_dir={local_dir}, abs_path={abs_path}\"\n            )\n            self.tokenizer = AutoTokenizer.from_pretrained(local_dir)\n            print(\n                f\"successfully loaded the tokenizer using AutoTokenizer.from_pretrained from {local_dir}\"\n            )\n        else:\n            print(f\"{local_dir} directory is empty\")\n            try:\n                if hf_model_id:\n                    print(f'going to download tokenizer from HF for \"{hf_model_id}\"')\n                    self.tokenizer = AutoTokenizer.from_pretrained(hf_model_id)\n                    print(\n                        f'successfully loaded the tokenizer using AutoTokenizer.from_pretrained from HF for \"{hf_model_id}\"'\n                    )\n                else:\n                    print(f'going to download tokenizer from HF for \"{model_id}\"')\n                    self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n                    print(\n                        f'successfully loaded the tokenizer using AutoTokenizer.from_pretrained from HF for \"{model_id}\"'\n                    )\n            except Exception as e:\n                print(\n                    f\"exception while loading tokenizer from HuggingFace, exception={e}\"\n                )\n                print(\n                    f\"no tokenizer provided, the {local_dir}, abs_path={abs_path} is empty, \"\n                    f\"using default tokenizer i.e. {self.WORDS} words = {self.TOKENS} tokens\"\n                )\n                self.tokenizer = None\n\n    def count_tokens(self, text):\n        if self.tokenizer is not None:\n            return len(self.tokenizer.encode(text))\n        else:\n            return int(math.ceil((self.TOKENS / self.WORDS) * len(text.split())))\n\n\n_tokenizer = CustomTokenizer(\n    globals.READ_BUCKET_NAME,\n    globals.TOKENIZER_DIR_S3,\n    globals.TOKENIZER,\n    globals.TOKENIZER_MODEL_ID,\n    globals.HF_TOKENIZER_MODEL_ID,\n)\n\n\n\n================================================"
  },
  {
    "filename": "model_eval_all_info.yml",
    "path": "fmbench/configs/model_eval_all_info.yml",
    "directory": "fmbench/configs",
    "extension": "yml",
    "content": "================================================\n# This file contains the evaluation information for majority voting. Here, we initialize\n# the embeddings model used to calculate quantitative metrics such as \n# cosine similarity. The other part of this evaluation is using subjective\n# evaluation methods: majority voting. In the case of when a ground truth\n# is provided, FMBench can use majority voting with the help of a 'panel of judges' to get a verdict [correct, incorrect].\n# For more information, view this paper: https://arxiv.org/pdf/2404.18796. Majority voting using a panel of LLM evaluators\n# helps in getting a 'close to human evaluation', reduces cost of evaluations, and eliminates intra model bias.\nmodel_evaluations:\n  ground_truth_col: {ground_truth}\n  question_col: {question}\n\n  PoLL_Composition_and_Voting: \n    method: majority_vote\n    # Set this variable to yes if you want to make partial correct/incorrect decisions based\n    # on quantitative metrics like cosine similarity, levenshtein score and token set ratio. Set\n    # this to yes only if you have a very direct QnA use case\n    use_quantitative_metrics: yes\n  \n  model_eval_dir:\n    # This is the directory in S3 and locally where all the evaluation instructions are stored for \n    # evaluating the candidate model responses using majority voting\n    eval_prompts_dir: eval_criteria\n    # the directory contains a folder that contains all the files with rules for evaluations\n    # and another directory that stores the standard prompt template that is used for evlauation\n    # of different answers at runtime. For example, `claude_eval_prompt_templates` contains the \n    # prompt template that claude will use for majority voting, etc.\n    eval_prompt_template_dir_list:\n    - claude_eval_prompt_templates\n    - llama3_eval_prompt_templates\n    - cohere_eval_prompt_templates\n    - mistral_eval_prompt_templates\n  \n    # These are the rules that are prefilled within the \n    # prompt templates evaluating for majority voting\n    eval_instructions_dir: eval_instructions\n    eval_instructions_files:\n    - evaluation_instructions_majority_vote.txt\n\n  # This represents the information that is used to get the quantitative metrics \n  # from the evaluation step. This includes calculating the cosine similarity. \n  # If a ground truth is provided, measure the cosine similarity against the ground truth, \n  # else measure it against the context provided. We use the `sentence-transformers/all-mpnet-base-v2`\n  # dataset. There is also an option to use the Titan embeddings model (WIP)\n  quantitative_eval_info:\n    embeddings_model_id:\n      model_id: sentence-transformers/all-mpnet-base-v2\n    # This contains information about quantitative metrics thresholds that need to be set while\n    # evaluating whether a candidate model response is correct or incorrect without parsing it through\n    # the panel of LLM evaluation procedure\n\n    # There are two cosine similarity verdict scores that are used, one to determine whether a candidate model\n    # response is incorrect and another to determine whether it is correct. If the incorrect threshold is met, for \n    # example if the LLM evaluator provides an incorrect verdict, the actual incorrectness will be defined once\n    # it also is below the incorrect cosine similarity threshold of for example 0.40. \n    # If the LLM evaluator provides a correct verdict and it exceeds the correctness cosine similarity score of\n    # 0.05 for example, then the answer is defined as correctly evaluated as \"correct\"\n    incorrect_verdict_cosine_similarity_threshold: 0.40\n    correct_verdict_cosine_similarity_threshold: 0.01\n  # This represents the information that is used to get subjective evaluations on the \n  # content that is generated. It uses an LLM as a judge (that is configurable) and evaluates\n  # each content from the inference step on different evaluation criteria. The information about \n  # the LLM as a judge panel is given below that is used in the majority voting\n  subjective_eval_info:\n    # this is the judge panel list that is used in the evaluation process\n    judge_panel_list:\n      # Information on judge 1 on the evaluation judge panel\n      - model_id: us.meta.llama3-3-70b-instruct-v1:0\n        # this is the prompt template that is used in the evaluation process\n        # based on the method: majority voting\n        eval_prompt_template_dir: \"llama3_eval_prompt_templates\"\n        eval_prompt_template_name: \"llama3_eval_{method_name}\"\n      # Information on judge 2 on the evaluation judge panel\n      - model_id: us.anthropic.claude-3-5-sonnet-20241022-v2:0\n        # this is the prompt template that is used in the evaluation process\n        # based on the method: majority voting\n        eval_prompt_template_dir: \"claude_eval_prompt_templates\"\n        eval_prompt_template_name: \"claude_eval_{method_name}\"\n      # Information on judge 3 on the evaluation judge panel\n      # We use the most powerful cohere model - cohere command R +\n      - model_id: cohere.command-r-plus-v1:0\n        # this is the prompt template that is used in the evaluation process\n        # based on the method: majority voting\n        eval_prompt_template_dir: \"cohere_eval_prompt_templates\"\n        eval_prompt_template_name: \"cohere_eval_{method_name}\"\n    # number of parallel calls made asyncronously to bedrock using Ray\n    run_parallel_inference_count: 5\n    # Common inference parameters used in the evaluation process\n    # We use LiteLLM for interfacing with Bedrock\n    inference_parameters:\n      temperature: 0.1\n      max_tokens: 300\n      top_p: 0.92\n      caching: False\n    \n\n\n\n================================================"
  },
  {
    "filename": "pricing.yml",
    "path": "fmbench/configs/pricing.yml",
    "directory": "fmbench/configs",
    "extension": "yml",
    "content": "================================================\npricing:\n  instance_based:\n    anthropic.claude-v3-sonnet-pt-nc: 88\n    ml.c5.2xlarge: 0.408\n    ml.c5.4xlarge: 0.816\n    ml.c5.xlarge: 0.204\n    ml.c7i.xlarge: 0.214\n    ml.g4dn.12xlarge: 4.89\n    ml.g4dn.16xlarge: 5.44\n    ml.g4dn.2xlarge: 0.94\n    ml.g4dn.4xlarge: 1.505\n    ml.g4dn.8xlarge: 2.72\n    ml.g4dn.xlarge: 0.7364\n    ml.g5.12xlarge: 7.09\n    ml.g5.24xlarge: 10.18\n    ml.g5.2xlarge: 1.515\n    ml.g5.48xlarge: 20.36\n    ml.g5.4xlarge: 2.03\n    ml.g5.8xlarge: 3.06\n    ml.g5.xlarge: 1.4084\n    ml.g6.12xlarge: 5.752\n    ml.g6.16xlarge: 4.246\n    ml.g6.24xlarge: 8.344\n    ml.g6.2xlarge: 1.222\n    ml.g6.48xlarge: 16.688\n    ml.inf2.24xlarge: 7.79\n    ml.inf2.48xlarge: 15.58\n    ml.inf2.8xlarge: 2.36\n    ml.inf2.xlarge: 0.99\n    ml.m5.xlarge: 0.23\n    ml.p3.2xlarge: 3.825\n    ml.p4d.24xlarge: 37.688\n    ml.p5.48xlarge: 113.068\n    ml.trn1.32xlarge: 28.497\n    p5e.48xlarge: 110.92\n  token_based:\n    ai21.j2-mid-v1:\n      input-per-1k-tokens: 0.0125\n      output-per-1k-tokens: 0.0125\n    ai21.j2-ultra-v1:\n      input-per-1k-tokens: 0.0188\n      output-per-1k-tokens: 0.0188\n    amazon.titan-text-express-v1:\n      input-per-1k-tokens: 0.0002\n      output-per-1k-tokens: 0.0006\n    amazon.titan-text-lite-v1:\n      input-per-1k-tokens: 0.00015\n      output-per-1k-tokens: 0.0002\n    anthropic.claude-3-5-sonnet-20240620-v1:0:\n      input-per-1k-tokens: 0.003\n      output-per-1k-tokens: 0.015\n    anthropic.claude-3-haiku-20240307-v1:0:\n      input-per-1k-tokens: 0.00025\n      output-per-1k-tokens: 0.00125\n    anthropic.claude-3-opus-20240229-v1:0:\n      input-per-1k-tokens: 0.015\n      output-per-1k-tokens: 0.075\n    anthropic.claude-3-sonnet-20240229-v1:0:\n      input-per-1k-tokens: 0.003\n      output-per-1k-tokens: 0.015\n    anthropic.claude-instant-v1:\n      input-per-1k-tokens: 0.0008\n      output-per-1k-tokens: 0.0024\n    anthropic.claude-v2:\n      input-per-1k-tokens: 0.008\n      output-per-1k-tokens: 0.024\n    anthropic.claude-v2:1:\n      input-per-1k-tokens: 0.008\n      output-per-1k-tokens: 0.024\n    cohere.command-light-text-v14:\n      input-per-1k-tokens: 0.0003\n      output-per-1k-tokens: 0.0006\n    cohere.command-r-plus-v1:0:\n      input-per-1k-tokens: 0.003\n      output-per-1k-tokens: 0.015\n    cohere.command-text-v14:\n      input-per-1k-tokens: 0.0015\n      output-per-1k-tokens: 0.002\n    meta.llama2-13b-chat-v1:\n      input-per-1k-tokens: 0.00075\n      output-per-1k-tokens: 0.001\n    meta.llama2-70b-chat-v1:\n      input-per-1k-tokens: 0.00195\n      output-per-1k-tokens: 0.00256\n    meta.llama3-1-405b-instruct-v1:0:\n      input-per-1k-tokens: 0.00532\n      output-per-1k-tokens: 0.016\n    us.meta.llama3-1-70b-instruct-v1:0:\n      input-per-1k-tokens: 0.00072\n      output-per-1k-tokens: 0.00072\n    us.meta.llama3-1-8b-instruct-v1:0:\n      input-per-1k-tokens: 0.00022\n      output-per-1k-tokens: 0.00022\n    meta.llama3-70b-instruct-v1:0:\n      input-per-1k-tokens: 0.00265\n      output-per-1k-tokens: 0.0035\n    meta.llama3-8b-instruct-v1:0:\n      input-per-1k-tokens: 0.0003\n      output-per-1k-tokens: 0.0006\n    mistral.mistral-7b-instruct-v0:2:\n      input-per-1k-tokens: 0.00015\n      output-per-1k-tokens: 0.0002\n    mistral.mixtral-8x7b-instruct-v0:1:\n      input-per-1k-tokens: 0.00045\n      output-per-1k-tokens: 0.0007\n    us.meta.llama3-2-11b-instruct-v1:0:\n      input-per-1k-tokens: 0.00016\n      output-per-1k-tokens: 0.00016\n    us.meta.llama3-2-1b-instruct-v1:0:\n      input-per-1k-tokens: 0.0001\n      output-per-1k-tokens: 0.0001\n    us.meta.llama3-2-3b-instruct-v1:0:\n      input-per-1k-tokens: 0.00015\n      output-per-1k-tokens: 0.00015\n    us.meta.llama3-2-90b-instruct-v1:0:\n      input-per-1k-tokens: 0.00072\n      output-per-1k-tokens: 0.00072\n    amazon.nova-micro-v1:0:\n      input-per-1k-tokens: 0.000035\n      output-per-1k-tokens: 0.00014\n    amazon.nova-lite-v1:0:\n      input-per-1k-tokens: 0.00006\n      output-per-1k-tokens: 0.00024\n    amazon.nova-pro-v1:0:\n      input-per-1k-tokens: 0.0008\n      output-per-1k-tokens: 0.0032\n\n\n\n================================================"
  },
  {
    "filename": "pricing_fallback.yml",
    "path": "fmbench/configs/pricing_fallback.yml",
    "directory": "fmbench/configs",
    "extension": "yml",
    "content": "================================================\npricing:\n  instance_based:\n    # Instance Based Pricing: SageMaker, EKS, Bedrock Provisioned Throughput, Bring your own endpoints that are priced hourly\n    # SageMaker Hourly Instance Pricing\n    ml.c5.xlarge: 0.204\n    ml.c5.2xlarge: 0.408\n    ml.c5.4xlarge: 0.816\n    ml.c7i.xlarge: 0.214\n    ml.m5.xlarge: 0.23\n    ml.g5.xlarge: 1.4084\n    ml.g5.4xlarge: 2.03\n    ml.g5.8xlarge: 3.06\n    ml.g5.2xlarge: 1.515\n    ml.g5.12xlarge: 7.09\n    ml.g5.24xlarge: 10.18\n    ml.g5.48xlarge: 20.36\n    ml.inf2.xlarge: 0.99\n    ml.inf2.8xlarge: 2.36\n    ml.inf2.24xlarge: 7.79\n    ml.inf2.48xlarge: 15.58\n    ml.trn1.32xlarge: 28.497\n    ml.p4d.24xlarge: 37.688\n    ml.p5.48xlarge: 113.068\n    ml.p3.2xlarge: 3.825\n    ml.g4dn.xlarge: 0.7364\n    ml.g4dn.2xlarge: 0.94\n    ml.g4dn.4xlarge: 1.505\n    ml.g4dn.8xlarge: 2.72\n    ml.g4dn.12xlarge: 4.89\n    ml.g4dn.16xlarge: 5.44\n    ml.g6.2xlarge: 1.222\n    ml.g6.16xlarge: 4.246\n    ml.g6.12xlarge: 5.752\n    ml.g6.24xlarge: 8.344\n    ml.g6.48xlarge: 16.688\n    anthropic.claude-v3-sonnet-pt-nc: 88\n    # corresponding hourly pricing for EC2 instances if your model is hosted on EC2\n    # all EC2 pricing is based on public on-demand pricing information that can be \n    # viewed in this link: https://aws.amazon.com/ec2/pricing/on-demand/\n    m5.16xlarge: 3.072\n    c5.18xlarge: 3.06\n    m7i.12xlarge: 2.419\n    m7i.24xlarge: 4.8384\n    m7a.4xlarge: 0.9274\n    m7a.16xlarge: 3.709\n    m7a.24xlarge: 5.564\n    m5.xlarge: 0.192\n    g5.xlarge: 1.006\n    g5.4xlarge: 1.624\n    g5.2xlarge: 1.212\n    g5.12xlarge: 5.672\n    g5.24xlarge: 8.144\n    g5.48xlarge: 16.288\n    inf2.xlarge: 0.7582\n    inf2.8xlarge: 1.96786\n    inf2.24xlarge: 6.49063\n    inf2.48xlarge: 12.98127\n    trn1.32xlarge: 21.50\n    p4d.24xlarge: 32.7726\n    p4de.24xlarge: 40.965\n    p5.48xlarge: 98.32\n    p5e.48xlarge: 110.92\n    p3.2xlarge: 3.06\n    g4dn.12xlarge: 3.912\n    g6.2xlarge: 0.9776\n    g6.4xlarge: 1.3512\n    g6.16xlarge: 3.3968\n    g6.12xlarge: 4.6016\n    g6.24xlarge: 6.6752\n    g6.48xlarge: 13.3504\n    g6e.2xlarge: 2.242\n    g6e.4xlarge: 3.1294\n    g6e.12xlarge: 10.493\n    g6e.16xlarge: 7.577\n    g6e.24xlarge: 15.066\n    g6e.48xlarge: 30.131\n    c8g.24xlarge: 3.828\n\n  token_based:\n    amazon.nova-micro-v1:0:\n      input-per-1k-tokens: 0.000035\n      output-per-1k-tokens: 0.00014\n    amazon.nova-lite-v1:0:\n      input-per-1k-tokens: 0.00006\n      output-per-1k-tokens: 0.00024\n    amazon.nova-pro-v1:0:\n      input-per-1k-tokens: 0.0008\n      output-per-1k-tokens: 0.0032\n    ai21.j2-mid-v1:\n      input-per-1k-tokens: 0.0125\n      output-per-1k-tokens: 0.0125\n    ai21.j2-ultra-v1:\n      input-per-1k-tokens: 0.0188\n      output-per-1k-tokens: 0.0188\n    amazon.titan-text-express-v1:\n      input-per-1k-tokens: 0.0002\n      output-per-1k-tokens: 0.0006\n    amazon.titan-text-lite-v1:\n      input-per-1k-tokens: 0.00015\n      output-per-1k-tokens: 0.0002\n    us.anthropic.claude-3-5-sonnet-20241022-v2:0: \n      input-per-1k-tokens: 0.003\n      output-per-1k-tokens: 0.015\n    anthropic.claude-3-5-sonnet-20241022-v2:0: \n      input-per-1k-tokens: 0.003\n      output-per-1k-tokens: 0.015\n    anthropic.claude-3-5-sonnet-20240620-v1:0:\n      input-per-1k-tokens: 0.003\n      output-per-1k-tokens: 0.015\n    anthropic.claude-3-haiku-20240307-v1:0:\n      input-per-1k-tokens: 0.00025\n      output-per-1k-tokens: 0.00125\n    anthropic.claude-3-opus-20240229-v1:0:\n      input-per-1k-tokens: 0.015\n      output-per-1k-tokens: 0.075\n    anthropic.claude-3-sonnet-20240229-v1:0:\n      input-per-1k-tokens: 0.003\n      output-per-1k-tokens: 0.015\n    anthropic.claude-instant-v1:\n      input-per-1k-tokens: 0.0008\n      output-per-1k-tokens: 0.0024\n    anthropic.claude-v2:\n      input-per-1k-tokens: 0.008\n      output-per-1k-tokens: 0.024\n    anthropic.claude-v2:1:\n      input-per-1k-tokens: 0.008\n      output-per-1k-tokens: 0.024\n    cohere.command-light-text-v14:\n      input-per-1k-tokens: 0.0003\n      output-per-1k-tokens: 0.0006\n    cohere.command-r-plus-v1:0:\n      input-per-1k-tokens: 0.003\n      output-per-1k-tokens: 0.015\n    cohere.command-text-v14:\n      input-per-1k-tokens: 0.0015\n      output-per-1k-tokens: 0.002\n    meta.llama3-3-70b-instruct-v1:0:\n      input-per-1k-tokens: 0.00072\n      output-per-1k-tokens: 0.00072\n    us.meta.llama3-3-70b-instruct-v1:0:\n      input-per-1k-tokens: 0.00072\n      output-per-1k-tokens: 0.00072\n    meta.llama2-13b-chat-v1:\n      input-per-1k-tokens: 0.00075\n      output-per-1k-tokens: 0.001\n    meta.llama2-70b-chat-v1:\n      input-per-1k-tokens: 0.00195\n      output-per-1k-tokens: 0.00256\n    meta.llama3-1-405b-instruct-v1:0:\n      input-per-1k-tokens: 0.00532\n      output-per-1k-tokens: 0.016\n    meta.llama3-1-70b-instruct-v1:0:\n      input-per-1k-tokens: 0.00072\n      output-per-1k-tokens: 0.00072\n    meta.llama3-1-8b-instruct-v1:0:\n      input-per-1k-tokens: 0.00022\n      output-per-1k-tokens: 0.00022\n    meta.llama3-70b-instruct-v1:0:\n      input-per-1k-tokens: 0.00265\n      output-per-1k-tokens: 0.0035\n    meta.llama3-8b-instruct-v1:0:\n      input-per-1k-tokens: 0.0003\n      output-per-1k-tokens: 0.0006\n    mistral.mistral-7b-instruct-v0:2:\n      input-per-1k-tokens: 0.00015\n      output-per-1k-tokens: 0.0002\n    mistral.mixtral-8x7b-instruct-v0:1:\n      input-per-1k-tokens: 0.00045\n      output-per-1k-tokens: 0.0007\n    us.meta.llama3-2-11b-instruct-v1:0:\n      input-per-1k-tokens: 0.00016\n      output-per-1k-tokens: 0.00016\n    us.meta.llama3-2-1b-instruct-v1:0:\n      input-per-1k-tokens: 0.0001\n      output-per-1k-tokens: 0.0001\n    us.meta.llama3-2-3b-instruct-v1:0:\n      input-per-1k-tokens: 0.00015\n      output-per-1k-tokens: 0.00015\n    us.meta.llama3-2-90b-instruct-v1:0:\n      input-per-1k-tokens: 0.00072\n      output-per-1k-tokens: 0.00072\n    us.amazon.nova-micro-v1:0:\n      input-per-1k-tokens: 0.000035\n      output-per-1k-tokens: 0.00014\n    us.amazon.nova-lite-v1:0:\n      input-per-1k-tokens: 0.00006\n      output-per-1k-tokens: 0.00024\n    us.amazon.nova-pro-v1:0:\n      input-per-1k-tokens: 0.0008\n      output-per-1k-tokens: 0.0032\n\n\n\n================================================"
  },
  {
    "filename": "config-bedrock-all-anthropic-models-longbench-data.yml",
    "path": "fmbench/configs/bedrock/config-bedrock-all-anthropic-models-longbench-data.yml",
    "directory": "fmbench/configs/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"fmbench-bedrock-anthropic-models\"      \n  model_name: \"All anthropic models available in Amazon Bedrock - max voting\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n## S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n  \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n  \n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n  prompt_template_dir: prompt_template\n  prompt_template_file: prompt_template_claude.txt ## add your desired prompt template type\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n \nmetrics:\n  dataset_of_interest: en_3000-4000\n  \npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n  \ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  # Experiment for claude 3 sonnet\n  - name: anthropic.claude-3-sonnet-20240229-v1:0\n    model_name: anthropic.claude-3-sonnet-20240229-v1:0\n    ep_name: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_type: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  # Experiment for claude 2.1\n  - name: anthropic.claude-v2:1\n    model_name: anthropic.claude-v2:1\n    ep_name: anthropic.claude-v2:1\n    instance_type: anthropic.claude-v2:1\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  # Experiment for claude instant\n  - name: anthropic.claude-instant-v1\n    model_name: anthropic.claude-instant-v1\n    ep_name: anthropic.claude-instant-v1\n    instance_type: anthropic.claude-instant-v1\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  # Experiment for claude v2\n  - name: anthropic.claude-v2\n    model_name: anthropic.claude-v2\n    ep_name: anthropic.claude-v2\n    instance_type: anthropic.claude-v2\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env:\n  # Experiment for claude 3 haiku\n  - name: anthropic.claude-3-haiku-20240307-v1:0\n    model_name: anthropic.claude-3-haiku-20240307-v1:0\n    ep_name: anthropic.claude-3-haiku-20240307-v1:0\n    instance_type: anthropic.claude-3-haiku-20240307-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n    \nreport:\n  latency_budget: 5\n  cost_per_10k_txn_budget: 350\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-bedrock-anthropic-models-OpenOrca.yml",
    "path": "fmbench/configs/bedrock/config-bedrock-anthropic-models-OpenOrca.yml",
    "directory": "fmbench/configs/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"fmbench-bedrock-anthropic-models\"      \n  model_name: \"All anthropic models available in Amazon Bedrock\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n## S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n  \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - Open-Orca/OpenOrca.jsonl\n  tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n  prompt_template_dir: prompt_template\n  prompt_template_file: prompt_template_claude_OpenOrca.txt ## add your desired prompt template type\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - system_prompt \n  ground_truth_col_key: response\n  question_col_key: input\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n \nmetrics:\n  dataset_of_interest: en_1000-2000\n\n  \npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n  \ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  # Experiment for claude 3 sonnet\n  - name: anthropic.claude-3-sonnet-20240229-v1:0\n    model_name: anthropic.claude-3-sonnet-20240229-v1:0\n    ep_name: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_type: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  # Experiment for claude 3 haiku\n  - name: anthropic.claude-3-haiku-20240307-v1:0\n    model_name: anthropic.claude-3-haiku-20240307-v1:0\n    ep_name: anthropic.claude-3-haiku-20240307-v1:0\n    instance_type: anthropic.claude-3-haiku-20240307-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n    \nreport:\n  latency_budget: 5\n  cost_per_10k_txn_budget: 350\n  error_rate_budget: 0\n  # change this based on your dataset size and your needs\n  accuracy_budget: 0.90\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-bedrock-claude.yml",
    "path": "fmbench/configs/bedrock/config-bedrock-claude.yml",
    "directory": "fmbench/configs/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"fmbench-bedrock-claude-sonnet\"      \n  model_name: \"Claude Sonnet available in Amazon Bedrock\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket}\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n\n    configs_prefix: configs\n    config_files:\n    - pricing.yml ## mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_claude.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: no\n    3_run_inference.ipynb: yes\n    4_get_evaluations.ipynb: yes\n    5_model_metric_analysis.ipynb: yes\n    6_cleanup.ipynb: no\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n \n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n\nmetrics:\n  dataset_of_interest: en_3000-4000\n\n  \npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n  \ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  - name: anthropic.claude-v3-sonnet-pt\n    model_name: anthropic.claude-v3-sonnet-pt\n    ep_name: <your-provisioned-throughput-arn>\n    instance_type: anthropic.claude-v3-sonnet-pt-nc\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  - name: anthropic.claude-3-sonnet-20240229-v1:0\n    model_name: anthropic.claude-3-sonnet-20240229-v1:0\n    ep_name: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_type: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n    \nreport:\n  latency_budget: 5\n  cost_per_10k_txn_budget: 350\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-bedrock-evals-only-conc-1.yml",
    "path": "fmbench/configs/bedrock/config-bedrock-evals-only-conc-1.yml",
    "directory": "fmbench/configs/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"latest-FMs-fmbench-bedrock\"      \n  model_name: \"FMs available in Amazon Bedrock\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket}\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n  \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n  \n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: llama2_tokenizer\n  \n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama2.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets: \n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the \n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n    \n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n\nmetrics:\n  dataset_of_interest: en_2000-3000\n  \npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n  \ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  - name: meta.llama3-1-70b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: meta.llama3-1-70b-instruct-v1:0\n    model_version: \n    model_name: meta.llama3-1-70b-instruct-v1:0\n    ep_name: meta.llama3-1-70b-instruct-v1:0\n    instance_type: meta.llama3-1-70b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    # - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  - name: meta.llama3-1-8b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: meta.llama3-1-8b-instruct-v1:0\n    model_version: \n    model_name: meta.llama3-1-8b-instruct-v1:0\n    ep_name: meta.llama3-1-8b-instruct-v1:0\n    instance_type: meta.llama3-1-8b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    # - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  - name: amazon.titan-text-express-v1\n    model_id: amazon.titan-text-express-v1\n    model_version: \"*\"\n    model_name: amazon.titan-text-express-v1\n    ep_name: amazon.titan-text-express-v1\n    instance_type: amazon.titan-text-express-v1\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    # - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env: \n  - name: mistral.mistral-7b-instruct-v0:2\n    model_id: mistral.mistral-7b-instruct-v0:2\n    model_version: \"*\"\n    model_name: mistral.mistral-7b-instruct-v0:2\n    ep_name: mistral.mistral-7b-instruct-v0:2\n    instance_type: mistral.mistral-7b-instruct-v0:2\n    image_uri:\n    deploy: no\n    instance_count: 1\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    # - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env:\n  - name: mistral.mixtral-8x7b-instruct-v0:1\n    model_id: mistral.mixtral-8x7b-instruct-v0:1\n    model_version: \"*\"\n    model_name: mistral.mixtral-8x7b-instruct-v0:1\n    ep_name: mistral.mixtral-8x7b-instruct-v0:1\n    instance_type: mistral.mixtral-8x7b-instruct-v0:1\n    image_uri:\n    deploy: no\n    instance_count: 1\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    # - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env:\n  - name: anthropic.claude-3-sonnet-20240229-v1:0\n    model_id: anthropic.claude-3-sonnet-20240229-v1:0\n    model_version: \"*\"\n    model_name: anthropic.claude-3-sonnet-20240229-v1:0\n    ep_name: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_type: anthropic.claude-3-sonnet-20240229-v1:0\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    # - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env:\n  - name: anthropic.claude-3-haiku-20240307-v1:0\n    model_id: anthropic.claude-3-haiku-20240307-v1:0\n    model_version: \"*\"\n    model_name: anthropic.claude-3-haiku-20240307-v1:0\n    ep_name: anthropic.claude-3-haiku-20240307-v1:0\n    instance_type: anthropic.claude-3-haiku-20240307-v1:0\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    # - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env:\n  - name: cohere.command-text-v14\n    model_id: cohere.command-text-v14\n    model_version: \"*\"\n    model_name: cohere.command-text-v14\n    ep_name: cohere.command-text-v14\n    instance_type: cohere.command-text-v14\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    # - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env:\n  - name: cohere.command-light-text-v14\n    model_id: cohere.command-light-text-v14\n    model_version: \"*\"\n    model_name: cohere.command-light-text-v14\n    ep_name: cohere.command-light-text-v14\n    instance_type: cohere.command-light-text-v14\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    # - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env:\n\nreport:\n  latency_budget: 2\n  cosine_similarity_budget: 0.3\n  accuracy_budget: 1\n  accuracy_error_rate_budget: 0\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n  \n\n\n================================================"
  },
  {
    "filename": "config-bedrock-haiku-sonnet-majority-voting.yml",
    "path": "fmbench/configs/bedrock/config-bedrock-haiku-sonnet-majority-voting.yml",
    "directory": "fmbench/configs/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"fmbench-bedrock-anthropic-models\"      \n  model_name: \"Sonnet and Haiku on Amazon Bedrock\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n## S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n  \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n  prompt_template_dir: prompt_template\n  prompt_template_file: prompt_template_claude.txt ## add your desired prompt template type\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n \nmetrics:\n  dataset_of_interest: en_3000-4000\n\n  \npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n  \ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  # Experiment for claude 3 sonnet\n  - name: anthropic.claude-3-sonnet-20240229-v1:0\n    model_name: anthropic.claude-3-sonnet-20240229-v1:0\n    ep_name: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_type: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  # Experiment for claude 3 haiku\n  - name: anthropic.claude-3-haiku-20240307-v1:0\n    model_name: anthropic.claude-3-haiku-20240307-v1:0\n    ep_name: anthropic.claude-3-haiku-20240307-v1:0\n    instance_type: anthropic.claude-3-haiku-20240307-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n    \nreport:\n  latency_budget: 5\n  cost_per_10k_txn_budget: 350\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n================================================"
  },
  {
    "filename": "config-bedrock-llama3-1-70b-streaming.yml",
    "path": "fmbench/configs/bedrock/config-bedrock-llama3-1-70b-streaming.yml",
    "directory": "fmbench/configs/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"fmbench-bedrock-llama3-1\"      \n  model_name: \"Llama3-1-70b Model on Amazon Bedrock\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: meta.llama3-1-70b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: meta.llama3-1-70b-instruct-v1:0\n    model_version: \n    model_name: meta.llama3-1-70b-instruct-v1:0\n    ep_name: meta.llama3-1-70b-instruct-v1:0\n    instance_type: meta.llama3-1-70b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: True\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-bedrock-llama3-1-8b-streaming.yml",
    "path": "fmbench/configs/bedrock/config-bedrock-llama3-1-8b-streaming.yml",
    "directory": "fmbench/configs/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"fmbench-bedrock-llama3-1\"      \n  model_name: \"Llama3-1-8b Model on Amazon Bedrock\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  \n  # There are two options to use the longbench dataset. The first option is to run the 'copy_s3_content.sh'\n  # script which will copy the jsonl files to the local/s3 path. The second option is given below. Users can now\n  # prefix the hf dataset with a \"hf:\" and append the split to the dataset id to load that dataset. For example, \n  # to load the \"2wikimqa_e\" split of the THUDM/LongBench dataset, specify \"hf:THUDM/LongBench/2wikimqa_e\" in the list below.\n  # FMBench will load the dataset, convert it into jsonl lines, and upload it to the local/s3 read source data directory\n  # at runtime. This eliminates the need to preprocess hugging face datasets to benchmark the models of choice. Now users\n  # can use any HF dataset on the fly without any data preparation.\n  source_data_files:\n  # Follow this format below: hf:dataset-id/subset-name/split-name. \n  # If there is no specified subset name, use \"default\"\n  - hf:THUDM/LongBench/2wikimqa_e/test\n  - hf:THUDM/LongBench/2wikimqa/test\n  - hf:THUDM/LongBench/hotpotqa_e/test\n  - hf:THUDM/LongBench/hotpotqa/test\n  - hf:THUDM/LongBench/narrativeqa/test\n  - hf:THUDM/LongBench/triviaqa_e/test\n  - hf:THUDM/LongBench/triviaqa/test\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: meta.llama3-1-8b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: meta.llama3-1-8b-instruct-v1:0\n    model_version: \n    model_name: meta.llama3-1-8b-instruct-v1:0\n    ep_name: meta.llama3-1-8b-instruct-v1:0\n    instance_type: meta.llama3-1-8b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: True\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-bedrock-llama3-1-no-streaming.yml",
    "path": "fmbench/configs/bedrock/config-bedrock-llama3-1-no-streaming.yml",
    "directory": "fmbench/configs/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"fmbench-bedrock-llama3-1\"      \n  model_name: \"Llama3-1-70b Model on Amazon Bedrock\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: meta.llama3-1-70b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: meta.llama3-1-70b-instruct-v1:0\n    model_version: \n    model_name: meta.llama3-1-70b-instruct-v1:0\n    ep_name: meta.llama3-1-70b-instruct-v1:0\n    instance_type: meta.llama3-1-70b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  - name: meta.llama3-1-8b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: meta.llama3-1-8b-instruct-v1:0\n    model_version: \n    model_name: meta.llama3-1-8b-instruct-v1:0\n    ep_name: meta.llama3-1-8b-instruct-v1:0\n    instance_type: meta.llama3-1-8b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-bedrock-llama3-1.yml",
    "path": "fmbench/configs/bedrock/config-bedrock-llama3-1.yml",
    "directory": "fmbench/configs/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"fmbench-bedrock-llama3-1\"      \n  model_name: \"Llama3-1 Models on Amazon Bedrock\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: no\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: us.meta.llama3-1-70b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: us.meta.llama3-1-70b-instruct-v1:0\n    hf_tokenizer_model_id: meta-llama/Llama-3.1-70B-Instruct\n    model_version: \n    model_name: us.meta.llama3-1-70b-instruct-v1:0\n    ep_name: us.meta.llama3-1-70b-instruct-v1:0\n    instance_type: us.meta.llama3-1-70b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  - name: us.meta.llama3-1-8b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: us.meta.llama3-1-8b-instruct-v1:0\n    hf_tokenizer_model_id: meta-llama/Llama-3.1-8B-Instruct\n    model_version: \n    model_name: us.meta.llama3-1-8b-instruct-v1:0\n    ep_name: us.meta.llama3-1-8b-instruct-v1:0\n    instance_type: us.meta.llama3-1-8b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-bedrock-llama3-streaming.yml",
    "path": "fmbench/configs/bedrock/config-bedrock-llama3-streaming.yml",
    "directory": "fmbench/configs/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"fmbench-bedrock-llama3-stream-eval-responses\"      \n  model_name: \"Llama3 on Bedrock\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: no\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: no\n  5_model_metric_analysis.ipynb: no\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: meta.llama3-70b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: meta.llama3-70b-instruct-v1:0\n    model_version: \n    model_name: meta.llama3-70b-instruct-v1:0\n    ep_name: meta.llama3-70b-instruct-v1:0\n    instance_type: meta.llama3-70b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: True\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    - 2\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  - name: meta.llama3-8b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: meta.llama3-1-8b-instruct-v1:0\n    model_version: \n    model_name: meta.llama3-8b-instruct-v1:0\n    ep_name: meta.llama3-8b-instruct-v1:0\n    instance_type: meta.llama3-8b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: True\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    - 2\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-bedrock-models-OpenOrca.yml",
    "path": "fmbench/configs/bedrock/config-bedrock-models-OpenOrca.yml",
    "directory": "fmbench/configs/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"fmbench-bedrock-latest-models\"      \n  model_name: \"All latest models available in Amazon Bedrock\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n## S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n  \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n  \n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - Open-Orca/OpenOrca.jsonl\n  tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n  prompt_template_dir: prompt_template\n  prompt_template_file: prompt_template_claude_OpenOrca.txt ## add your desired prompt template type\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: no\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: no\n  4_get_evaluations.ipynb: no\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - system_prompt  \n  ground_truth_col_key: response\n  question_col_key: input\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n\nmetrics:\n  dataset_of_interest: en_1000-2000\n\n\npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n  \ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  # Experiment for claude 3 sonnet\n  - name: anthropic.claude-3-sonnet-20240229-v1:0\n    model_name: anthropic.claude-3-sonnet-20240229-v1:0\n    ep_name: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_type: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  # Experiment for claude 3 haiku\n  - name: anthropic.claude-3-haiku-20240307-v1:0\n    model_name: anthropic.claude-3-haiku-20240307-v1:0\n    ep_name: anthropic.claude-3-haiku-20240307-v1:0\n    instance_type: anthropic.claude-3-haiku-20240307-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  - name: mistral.mistral-7b-instruct-v0:2\n    model_id: mistral.mistral-7b-instruct-v0:2\n    model_version: \"*\"\n    model_name: mistral.mistral-7b-instruct-v0:2\n    ep_name: mistral.mistral-7b-instruct-v0:2\n    instance_type: mistral.mistral-7b-instruct-v0:2\n    image_uri:\n    deploy: no\n    instance_count: 1\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n      stream: True\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  - name: mistral.mixtral-8x7b-instruct-v0:1\n    model_id: mistral.mixtral-8x7b-instruct-v0:1\n    model_version: \"*\"\n    model_name: mistral.mixtral-8x7b-instruct-v0:1\n    ep_name: mistral.mixtral-8x7b-instruct-v0:1\n    instance_type: mistral.mixtral-8x7b-instruct-v0:1\n    image_uri:\n    deploy: no\n    instance_count: 1\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  - name: meta.llama3-70b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: meta.llama3-70b-instruct-v1:0\n    model_version: \n    model_name: meta.llama3-70b-instruct-v1:0\n    ep_name: meta.llama3-70b-instruct-v1:0\n    instance_type: meta.llama3-70b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      stream: True\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  - name: meta.llama3-8b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: meta.llama3-8b-instruct-v1:0\n    model_version: \n    model_name: meta.llama3-8b-instruct-v1:0\n    ep_name: meta.llama3-8b-instruct-v1:0\n    instance_type: meta.llama3-8b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      stream: True\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n    \nreport:\n  latency_budget: 5\n  cost_per_10k_txn_budget: 350\n  error_rate_budget: 0\n  # change this based on your dataset size and your needs\n  accuracy_budget: 0.90\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-bedrock-titan-text-express.yml",
    "path": "fmbench/configs/bedrock/config-bedrock-titan-text-express.yml",
    "directory": "fmbench/configs/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"fmbench-bedrock\"      \n  model_name: \"FMs available in Amazon Bedrock\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket}\n  \n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n\n    configs_prefix: configs\n    config_files:\n    - pricing.yml ## mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_titan_text.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: no\n    3_run_inference.ipynb: yes\n    4_get_evaluations.ipynb: yes\n    5_model_metric_analysis.ipynb: yes\n    6_cleanup.ipynb: no\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # - text\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n \nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# name of the file that contains the model evaluation information\n# change to model_eval_avg pooling for average pooling use cases\nmodel_evaluations: model_eval_all_info.yml\n \npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n  \ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  - name: amazon.titan-text-express-v1\n    model_id: amazon.titan-text-express-v1\n    model_version: \"*\"\n    model_name: amazon.titan-text-express-v1\n    ep_name: amazon.titan-text-express-v1\n    instance_type: amazon.titan-text-express-v1\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env: \n    \nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n================================================"
  },
  {
    "filename": "config-bedrock.yml",
    "path": "fmbench/configs/bedrock/config-bedrock.yml",
    "directory": "fmbench/configs/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"latest-FMs-fmbench-bedrock\"      \n  model_name: \"FMs available in Amazon Bedrock\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket}\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n  \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n  \n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: llama2_tokenizer\n  \n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama2.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets: \n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the \n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 4000\n    max_length_in_tokens: 5000\n    payload_file: payload_en_4000-5000.jsonl\n  - language: en\n    min_length_in_tokens: 5000\n    max_length_in_tokens: 6000\n    payload_file: payload_en_5000-6000.jsonl\n    \n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n\nmetrics:\n  dataset_of_interest: en_2000-3000\n  \npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n  \ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  - name: meta.llama3-1-70b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: meta.llama3-1-70b-instruct-v1:0\n    model_version: \n    model_name: meta.llama3-1-70b-instruct-v1:0\n    ep_name: meta.llama3-1-70b-instruct-v1:0\n    instance_type: meta.llama3-1-70b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    - payload_en_4000-5000.jsonl\n    - payload_en_5000-6000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  - name: meta.llama3-1-8b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: meta.llama3-1-8b-instruct-v1:0\n    model_version: \n    model_name: meta.llama3-1-8b-instruct-v1:0\n    ep_name: meta.llama3-1-8b-instruct-v1:0\n    instance_type: meta.llama3-1-8b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    - payload_en_4000-5000.jsonl\n    - payload_en_5000-6000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  - name: amazon.titan-text-express-v1\n    model_id: amazon.titan-text-express-v1\n    model_version: \"*\"\n    model_name: amazon.titan-text-express-v1\n    ep_name: amazon.titan-text-express-v1\n    instance_type: amazon.titan-text-express-v1\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    - payload_en_4000-5000.jsonl\n    - payload_en_5000-6000.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env: \n  - name: mistral.mistral-7b-instruct-v0:2\n    model_id: mistral.mistral-7b-instruct-v0:2\n    model_version: \"*\"\n    model_name: mistral.mistral-7b-instruct-v0:2\n    ep_name: mistral.mistral-7b-instruct-v0:2\n    instance_type: mistral.mistral-7b-instruct-v0:2\n    image_uri:\n    deploy: no\n    instance_count: 1\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    - payload_en_4000-5000.jsonl\n    - payload_en_5000-6000.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env:\n  - name: mistral.mixtral-8x7b-instruct-v0:1\n    model_id: mistral.mixtral-8x7b-instruct-v0:1\n    model_version: \"*\"\n    model_name: mistral.mixtral-8x7b-instruct-v0:1\n    ep_name: mistral.mixtral-8x7b-instruct-v0:1\n    instance_type: mistral.mixtral-8x7b-instruct-v0:1\n    image_uri:\n    deploy: no\n    instance_count: 1\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    - payload_en_4000-5000.jsonl\n    - payload_en_5000-6000.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env:\n  - name: anthropic.claude-3-sonnet-20240229-v1:0\n    model_id: anthropic.claude-3-sonnet-20240229-v1:0\n    model_version: \"*\"\n    model_name: anthropic.claude-3-sonnet-20240229-v1:0\n    ep_name: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_type: anthropic.claude-3-sonnet-20240229-v1:0\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    - payload_en_4000-5000.jsonl\n    - payload_en_5000-6000.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env:\n  - name: anthropic.claude-3-haiku-20240307-v1:0\n    model_id: anthropic.claude-3-haiku-20240307-v1:0\n    model_version: \"*\"\n    model_name: anthropic.claude-3-haiku-20240307-v1:0\n    ep_name: anthropic.claude-3-haiku-20240307-v1:0\n    instance_type: anthropic.claude-3-haiku-20240307-v1:0\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    - payload_en_4000-5000.jsonl\n    - payload_en_5000-6000.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env:\n  - name: cohere.command-text-v14\n    model_id: cohere.command-text-v14\n    model_version: \"*\"\n    model_name: cohere.command-text-v14\n    ep_name: cohere.command-text-v14\n    instance_type: cohere.command-text-v14\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    - payload_en_4000-5000.jsonl\n    - payload_en_5000-6000.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env:\n  - name: cohere.command-light-text-v14\n    model_id: cohere.command-light-text-v14\n    model_version: \"*\"\n    model_name: cohere.command-light-text-v14\n    ep_name: cohere.command-light-text-v14\n    instance_type: cohere.command-light-text-v14\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    - payload_en_4000-5000.jsonl\n    - payload_en_5000-6000.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env:\n\nreport:\n  latency_budget: 2\n  cosine_similarity_budget: 0.3\n  accuracy_budget: 1\n  accuracy_error_rate_budget: 0\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n  \n\n\n================================================"
  },
  {
    "filename": "config-claude-3-5-sonnet-v2.yml",
    "path": "fmbench/configs/bedrock/config-claude-3-5-sonnet-v2.yml",
    "directory": "fmbench/configs/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"fmbench-bedrock-claude-3-5-sonnet-v2\"      \n  model_name: \"Claude Sonnet 3.5 V2\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket}\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n\n    configs_prefix: configs\n    config_files:\n    - pricing.yml ## mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # Format: hf:dataset-id/subset-name/split-name. Use   \"default\" if no subset name is provided.\n    - hf:THUDM/LongBench/2wikimqa_e/test\n    - hf:THUDM/LongBench/2wikimqa/test\n    - hf:THUDM/LongBench/hotpotqa_e/test\n    - hf:THUDM/LongBench/hotpotqa/test\n    - hf:THUDM/LongBench/narrativeqa/test\n    - hf:THUDM/LongBench/triviaqa_e/test\n    - hf:THUDM/LongBench/triviaqa/test\n    tokenizer_prefix: llama3_1_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_claude.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: no\n    3_run_inference.ipynb: yes\n    4_get_evaluations.ipynb: yes\n    5_model_metric_analysis.ipynb: yes\n    6_cleanup.ipynb: no\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n \n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n\nmetrics:\n  dataset_of_interest: en_3000-4000\n\n  \npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n  \ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  - name: anthropic.claude-3-5-sonnet-20241022-v2:0\n    model_name: anthropic.claude-3-5-sonnet-20241022-v2:0\n    ep_name: anthropic.claude-3-5-sonnet-20241022-v2:0\n    instance_type: anthropic.claude-3-5-sonnet-20241022-v2:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    env:\n    \nreport:\n  latency_budget: 5\n  cost_per_10k_txn_budget: 350\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-claude-dolly-dataset.yml",
    "path": "fmbench/configs/bedrock/config-claude-dolly-dataset.yml",
    "directory": "fmbench/configs/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"fmbench-bedrock-claude-sonnet-dolly-dataset\"      \n  model_name: \"Claude Sonnet available in Amazon Bedrock\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket}\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n\n    configs_prefix: configs\n    config_files:\n    - pricing.yml ## mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - hf:databricks/databricks-dolly-15k\n    tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_claude_dolly_dataset.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: no\n    3_run_inference.ipynb: yes\n    4_get_evaluations.ipynb: yes\n    5_model_metric_analysis.ipynb: yes\n    6_cleanup.ipynb: no\n\ndatasets:\n  prompt_template_keys:\n  - instruction\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n \n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n\nmetrics:\n  dataset_of_interest: en_1-500\n\n  \npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n  \ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  - name: anthropic.claude-3-sonnet-20240229-v1:0\n    model_name: anthropic.claude-3-sonnet-20240229-v1:0\n    ep_name: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_type: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n    \nreport:\n  latency_budget: 5\n  cost_per_10k_txn_budget: 350\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama-3-2-11b-databricks-dolly-15k.yml",
    "path": "fmbench/configs/bedrock/config-llama-3-2-11b-databricks-dolly-15k.yml",
    "directory": "fmbench/configs/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"fmbench-bedrock-llama3-2-11b-vision-instruct-dolly-dataset\"      \n  model_name: \"llama3-2 11b on Amazon Bedrock\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the https://huggingface.co/datasets/derek-thomas/ScienceQA\n  source_data_files:\n  - hf:databricks/databricks-dolly-15k/default/train\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3_databricks-dolly-15k.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  # This represents the question that is asked as a part of the\n  # image messages API format. If this is not provided, a description\n  # of the image will be provided instead\n  - instruction\n  - context\n  # This is the number of rows of the dataset that you want to load\n  # if this parameter is not given, it is defaulted to 100 rows\n  ds_N: 150\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1-500\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-2-11b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: us.meta.llama3-2-11b-instruct-v1:0\n    model_version: \n    model_name: us.meta.llama3-2-11b-instruct-v1:0\n    ep_name: us.meta.llama3-2-11b-instruct-v1:0\n    instance_type: us.meta.llama3-2-11b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    - 14\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 1\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama-3-2-1b-3b-no-evals.yml",
    "path": "fmbench/configs/bedrock/config-llama-3-2-1b-3b-no-evals.yml",
    "directory": "fmbench/configs/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"fmbench-bedrock-llama3.2-1b-3b\"      \n  model_name: \"llama3-2 Models on Amazon Bedrock\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: no\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-2-1b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: us.meta.llama3-2-1b-instruct-v1:0\n    model_version: \n    model_name: us.meta.llama3-2-1b-instruct-v1:0\n    ep_name: us.meta.llama3-2-1b-instruct-v1:0\n    instance_type: us.meta.llama3-2-1b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    # - 6\n    # - 8\n    # - 10\n    # - 12\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-2-3b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: us.meta.llama3-2-3b-instruct-v1:0\n    model_version: \n    model_name: us.meta.llama3-2-3b-instruct-v1:0\n    ep_name: us.meta.llama3-2-3b-instruct-v1:0\n    instance_type: us.meta.llama3-2-3b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    # - 6\n    # - 8\n    # - 10\n    # - 12\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  \nreport:\n  latency_budget: 1\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama-3-2-1b-3b.yml",
    "path": "fmbench/configs/bedrock/config-llama-3-2-1b-3b.yml",
    "directory": "fmbench/configs/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"fmbench-bedrock-llama3.2-1b-3b\"      \n  model_name: \"llama3-2 Models on Amazon Bedrock\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-2-1b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: us.meta.llama3-2-1b-instruct-v1:0\n    model_version: \n    model_name: us.meta.llama3-2-1b-instruct-v1:0\n    ep_name: us.meta.llama3-2-1b-instruct-v1:0\n    instance_type: us.meta.llama3-2-1b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-2-3b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: us.meta.llama3-2-3b-instruct-v1:0\n    model_version: \n    model_name: us.meta.llama3-2-3b-instruct-v1:0\n    ep_name: us.meta.llama3-2-3b-instruct-v1:0\n    instance_type: us.meta.llama3-2-3b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  \nreport:\n  latency_budget: 1\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama-3-2-all-models-longbench-hf-version.yml",
    "path": "fmbench/configs/bedrock/config-llama-3-2-all-models-longbench-hf-version.yml",
    "directory": "fmbench/configs/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"fmbench-bedrock-llama3-2-all-models\"      \n  model_name: \"llama3-2 Models on Amazon Bedrock\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  # Format: hf:dataset-id/subset-name/split-name. Use   \"default\" if no subset name is provided.\n  - hf:THUDM/LongBench/2wikimqa_e/test\n  - hf:THUDM/LongBench/2wikimqa/test\n  - hf:THUDM/LongBench/hotpotqa_e/test\n  - hf:THUDM/LongBench/hotpotqa/test\n  - hf:THUDM/LongBench/narrativeqa/test\n  - hf:THUDM/LongBench/triviaqa_e/test\n  - hf:THUDM/LongBench/triviaqa/test\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-2-1b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: us.meta.llama3-2-1b-instruct-v1:0\n    model_version: \n    model_name: us.meta.llama3-2-1b-instruct-v1:0\n    ep_name: us.meta.llama3-2-1b-instruct-v1:0\n    instance_type: us.meta.llama3-2-1b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-2-3b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: us.meta.llama3-2-3b-instruct-v1:0\n    model_version: \n    model_name: us.meta.llama3-2-3b-instruct-v1:0\n    ep_name: us.meta.llama3-2-3b-instruct-v1:0\n    instance_type: us.meta.llama3-2-3b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-2-90b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: us.meta.llama3-2-90b-instruct-v1:0\n    model_version: \n    model_name: us.meta.llama3-2-90b-instruct-v1:0\n    ep_name: us.meta.llama3-2-90b-instruct-v1:0\n    instance_type: us.meta.llama3-2-90b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-2-11b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: us.meta.llama3-2-11b-instruct-v1:0\n    model_version: \n    model_name: us.meta.llama3-2-11b-instruct-v1:0\n    ep_name: us.meta.llama3-2-11b-instruct-v1:0\n    instance_type: us.meta.llama3-2-11b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 1\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama-3-2-all-models.yml",
    "path": "fmbench/configs/bedrock/config-llama-3-2-all-models.yml",
    "directory": "fmbench/configs/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"fmbench-bedrock-llama3-2-all-models\"      \n  model_name: \"llama3-2 Models on Amazon Bedrock\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-2-1b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: us.meta.llama3-2-1b-instruct-v1:0\n    model_version: \n    model_name: us.meta.llama3-2-1b-instruct-v1:0\n    ep_name: us.meta.llama3-2-1b-instruct-v1:0\n    instance_type: us.meta.llama3-2-1b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-2-3b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: us.meta.llama3-2-3b-instruct-v1:0\n    model_version: \n    model_name: us.meta.llama3-2-3b-instruct-v1:0\n    ep_name: us.meta.llama3-2-3b-instruct-v1:0\n    instance_type: us.meta.llama3-2-3b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-2-90b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: us.meta.llama3-2-90b-instruct-v1:0\n    model_version: \n    model_name: us.meta.llama3-2-90b-instruct-v1:0\n    ep_name: us.meta.llama3-2-90b-instruct-v1:0\n    instance_type: us.meta.llama3-2-90b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-2-11b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: us.meta.llama3-2-11b-instruct-v1:0\n    model_version: \n    model_name: us.meta.llama3-2-11b-instruct-v1:0\n    ep_name: us.meta.llama3-2-11b-instruct-v1:0\n    instance_type: us.meta.llama3-2-11b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 1\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama-3-3-all-models-open-orca.yml",
    "path": "fmbench/configs/bedrock/config-llama-3-3-all-models-open-orca.yml",
    "directory": "fmbench/configs/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"fmbench-bedrock-llama3-3-all-models\"      \n  model_name: \"llama3-3 Models on Amazon Bedrock\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  # This is a preprocessed open orca file for accuracy benchmarking. To create this file, run the \n  # \"pre process dolly dataset\" section in the bring your own dataset notebook\n  - Open-Orca/OpenOrca.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3_OpenOrca_accuracy.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - question\n  ground_truth_col_key: response\n  question_col_key: question\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1000-2000\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-3-70b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: us.meta.llama3-3-70b-instruct-v1:0\n    hf_tokenizer_model_id: meta-llama/Llama-3.3-70B-Instruct\n    model_version: \n    model_name: us.meta.llama3-3-70b-instruct-v1:0\n    ep_name: us.meta.llama3-3-70b-instruct-v1:0\n    instance_type: us.meta.llama3-3-70b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama-3-3-all-models.yml",
    "path": "fmbench/configs/bedrock/config-llama-3-3-all-models.yml",
    "directory": "fmbench/configs/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"fmbench-bedrock-llama3-3-all-models\"      \n  model_name: \"llama3-3 Models on Amazon Bedrock\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-3-70b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: us.meta.llama3-3-70b-instruct-v1:0\n    hf_tokenizer_model_id: meta-llama/Llama-3.3-70B-Instruct\n    model_version: \n    model_name: us.meta.llama3-3-70b-instruct-v1:0\n    ep_name: us.meta.llama3-3-70b-instruct-v1:0\n    instance_type: us.meta.llama3-3-70b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 1\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-nova-all-models-convfinqa.yml",
    "path": "fmbench/configs/bedrock/config-nova-all-models-convfinqa.yml",
    "directory": "fmbench/configs/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: bedrock-nova-models-convfinqa\n  model_name: \"Nova models available in Amazon Bedrock\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket}\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n\n    configs_prefix: configs\n    config_files:\n    - pricing.yml ## mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - hf:AdaptLLM/finance-tasks/ConvFinQA/test\n    tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_nova_convfinqa.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: no\n    3_run_inference.ipynb: yes\n    4_get_evaluations.ipynb: yes\n    5_model_metric_analysis.ipynb: yes\n    6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  ground_truth_col_key: label\n  question_col_key: input\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n \n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n\nmetrics:\n  dataset_of_interest: en_1000-2000\n\n  \npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n  \ninference_parameters: \n  bedrock:\n    temperature: 0.6\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n    # Set use_boto3 to \"no\" if the model is supported by litellm and \n    # the litellm API will be used to invoke the model. If the model is on\n    # bedrock and is not supported by the latest version of litellm, then set\n    # the parameter below to \"yes\" and the bedrock converseAPI will be used.\n    use_boto3: no\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  # This is the NOVA micro model\n  - name: amazon.nova-micro-v1:0\n    model_name: amazon.nova-micro-v1:0\n    ep_name: amazon.nova-micro-v1:0\n    instance_type: amazon.nova-micro-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    concurrency_levels:\n    - 1\n    env:\n  - name: amazon.nova-lite-v1:0\n    model_name: amazon.nova-lite-v1:0\n    ep_name: amazon.nova-lite-v1:0\n    instance_type: amazon.nova-lite-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    concurrency_levels:\n    - 1\n    env:\n  - name: amazon.nova-pro-v1:0\n    model_name: amazon.nova-pro-v1:0\n    ep_name: amazon.nova-pro-v1:0\n    instance_type: amazon.nova-pro-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    - payload_en_4000-5000.jsonl\n    - payload_en_5000-6000.jsonl\n    concurrency_levels:\n    - 1\n    env:\n    \nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-nova-all-models-dolly-dataset.yml",
    "path": "fmbench/configs/bedrock/config-nova-all-models-dolly-dataset.yml",
    "directory": "fmbench/configs/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"fmbench-bedrock-nova-models-dolly-dataset\"      \n  model_name: \"Nova models available in Amazon Bedrock\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket}\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n\n    configs_prefix: configs\n    config_files:\n    - pricing.yml ## mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - databricks/databricks-dolly-15k.jsonl \n    tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_nova_dolly_dataset.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: no\n    3_run_inference.ipynb: yes\n    4_get_evaluations.ipynb: yes\n    5_model_metric_analysis.ipynb: yes\n    6_cleanup.ipynb: no\n\ndatasets:\n  prompt_template_keys:\n  - instruction\n  ground_truth_col_key: response\n  question_col_key: instruction\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n \n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n\nmetrics:\n  dataset_of_interest: en_500-1000\n\n  \npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n  \ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n    # Set use_boto3 to \"no\" if the model is supported by litellm and \n    # the litellm API will be used to invoke the model. If the model is on\n    # bedrock and is not supported by the latest version of litellm, then set\n    # the parameter below to \"yes\" and the bedrock converseAPI will be used.\n    use_boto3: no\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  # This is the NOVA micro model\n  - name: amazon.nova-micro-v1:0\n    model_name: amazon.nova-micro-v1:0\n    ep_name: amazon.nova-micro-v1:0\n    instance_type: amazon.nova-micro-v1:0 # amazon.nova-micro-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    concurrency_levels:\n    - 1\n    env:\n  - name: amazon.nova-lite-v1:0\n    model_name: amazon.nova-lite-v1:0\n    ep_name: amazon.nova-lite-v1:0\n    instance_type: amazon.nova-lite-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    concurrency_levels:\n    - 1\n    env:\n  - name: amazon.nova-pro-v1:0\n    model_name: amazon.nova-pro-v1:0\n    ep_name: amazon.nova-pro-v1:0\n    instance_type: amazon.nova-pro-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    concurrency_levels:\n    - 1\n    env:\n    \nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-nova-all-models-openarca.yml",
    "path": "fmbench/configs/bedrock/config-nova-all-models-openarca.yml",
    "directory": "fmbench/configs/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"fmbench-bedrock-nova-models-openorca-dataset-new\"      \n  model_name: \"Nova models available in Amazon Bedrock\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket}\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n\n    configs_prefix: configs\n    config_files:\n    - pricing.yml ## mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # This dataset is the custom open orca dataset which has the system prompt and the question columns\n    # merged. For a few rows, the system prompt is empty, so if there is a system prompt, it is appended to \n    # the question, else only the question is used. This is done to get the true accuracy measures of the model\n    # To get this custom jsonl file, run the `Preprocess the OpenOrca Dataset` section of the `bring_your_dataset`\n    # notebook within the FMBench repository.\n    - Open-Orca/OpenOrca.jsonl\n    tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_nova_open_orca.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: no\n    3_run_inference.ipynb: yes\n    4_get_evaluations.ipynb: yes\n    5_model_metric_analysis.ipynb: yes\n    6_cleanup.ipynb: no\n\ndatasets:\n  prompt_template_keys:\n  - question\n  ground_truth_col_key: response\n  question_col_key: question\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 4000\n    max_length_in_tokens: 5000\n    payload_file: payload_en_4000-5000.jsonl\n  - language: en\n    min_length_in_tokens: 5000\n    max_length_in_tokens: 6000\n    payload_file: payload_en_5000-6000.jsonl\n \n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n\nmetrics:\n  dataset_of_interest: en_1000-2000\n\n  \npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n  \ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 512\n    top_p: 0.92\n    caching: False\n    # Set use_boto3 to \"no\" if the model is supported by litellm and \n    # the litellm API will be used to invoke the model. If the model is on\n    # bedrock and is not supported by the latest version of litellm, then set\n    # the parameter below to \"yes\" and the bedrock converseAPI will be used.\n    use_boto3: no\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  # This is the NOVA micro model\n  - name: amazon.nova-micro-v1:0\n    model_name: amazon.nova-micro-v1:0\n    ep_name: amazon.nova-micro-v1:0\n    instance_type: amazon.nova-micro-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    concurrency_levels:\n    - 1\n    env:\n  - name: amazon.nova-lite-v1:0\n    model_name: amazon.nova-lite-v1:0\n    ep_name: amazon.nova-lite-v1:0\n    instance_type: amazon.nova-lite-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    concurrency_levels:\n    - 1\n    env:\n  - name: amazon.nova-pro-v1:0\n    model_name: amazon.nova-pro-v1:0\n    ep_name: amazon.nova-pro-v1:0\n    instance_type: amazon.nova-pro-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    - payload_en_4000-5000.jsonl\n    - payload_en_5000-6000.jsonl\n    concurrency_levels:\n    - 1\n    env:\n    \nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-nova-all-models.yml",
    "path": "fmbench/configs/bedrock/config-nova-all-models.yml",
    "directory": "fmbench/configs/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"fmbench-bedrock-nova-models\"      \n  model_name: \"Nova models available in Amazon Bedrock\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket}\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n\n    configs_prefix: configs\n    config_files:\n    - pricing.yml ## mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_nova.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: no\n    3_run_inference.ipynb: yes\n    4_get_evaluations.ipynb: yes\n    5_model_metric_analysis.ipynb: yes\n    6_cleanup.ipynb: no\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-3840.jsonl\n \n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n  \npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n  \ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n    # Set use_boto3 to \"no\" if the model is supported by litellm and \n    # the litellm API will be used to invoke the model. If the model is on\n    # bedrock and is not supported by the latest version of litellm, then set\n    # the parameter below to \"yes\" and the bedrock converseAPI will be used.\n    use_boto3: yes\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  # This is the NOVA micro model\n  - name: us.amazon.nova-micro-v1:0\n    model_name: us.amazon.nova-micro-v1:0\n    ep_name: us.amazon.nova-micro-v1:0\n    instance_type: us.amazon.nova-micro-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    env:\n  - name: us.amazon.nova-lite-v1:0\n    model_name: us.amazon.nova-lite-v1:0\n    ep_name: us.amazon.nova-lite-v1:0\n    instance_type: us.amazon.nova-lite-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    env:\n  - name: us.amazon.nova-pro-v1:0\n    model_name: us.amazon.nova-pro-v1:0\n    ep_name: us.amazon.nova-pro-v1:0\n    instance_type: us.amazon.nova-pro-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    env:\n    \nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-distilbert-base-uncased.yml",
    "path": "fmbench/configs/bert/config-distilbert-base-uncased.yml",
    "directory": "fmbench/configs/bert",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"distilbert-base-uncased-v1\"      \n  model_name: \"distilbert-base-uncased\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket}\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml ## mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - banking77.jsonl\n    tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_bert.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  #- input\n  #- context\n  - text\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n  - language: en    \n    min_length_in_tokens: 100\n    max_length_in_tokens: 250\n    payload_file: payload_en_100-250.jsonl\n  - language: en    \n    min_length_in_tokens: 10\n    max_length_in_tokens: 20\n    payload_file: payload_en_10-20.jsonl\n  - language: en    \n    min_length_in_tokens: 50\n    max_length_in_tokens: 150\n    payload_file: payload_en_50-150.jsonl\n \nmetrics:\n  dataset_of_interest: en_50-150\n  \npricing: pricing.yml\n  \ninference_parameters:\n  sagemaker:\n    ContentType: \"application/x-text\"\n    Accept: \"application/json;verbose\"\n\n# Model configurations for llama-2 7b for deploying on g5 x and 2x large instances\nexperiments:\n  - name: distilbert-base-uncased-ml-p3-2xlarge\n    model_id: huggingface-tc-distilbert-base-uncased\n    model_version: \"*\"\n    model_name: distilbert-base-uncased\n    ep_name: distilbert-base-uncased\n    instance_type: \"ml.p3.2xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-inference:1.10.2-transformers4.17.0-gpu-py38-cu113-ubuntu20.04'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      split_input_and_parameters: yes\n      parameter_set: sagemaker\n    payload_files:\n    #- payload_en_1-500.jsonl\n    - payload_en_50-150.jsonl\n\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n  - name: distilbert-base-uncased-ml-m5-xlarge\n    model_id: huggingface-tc-distilbert-base-uncased\n    model_version: \"*\"\n    model_name: distilbert-base-uncased\n    ep_name: distilbert-base-uncased\n    instance_type: \"ml.m5.xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-inference:1.13.1-transformers4.26.0-cpu-py39-ubuntu20.04'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      split_input_and_parameters: yes\n      parameter_set: sagemaker\n    payload_files:\n    #- payload_en_1-500.jsonl\n    - payload_en_50-150.jsonl\n\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\n\nreport:\n  latency_budget: 0.2\n  cost_per_10k_txn_budget: 5\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 100000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-byo-custom-rest-predictor-tinyllama.yml",
    "path": "fmbench/configs/byoe/config-byo-custom-rest-predictor-tinyllama.yml",
    "directory": "fmbench/configs/byoe",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# To run this configuration file, follow the steps here to deploy\n# the model first: https://github.com/madhurprash/EC2_tinyllama\ngeneral:\n  name: \"byo-REST-ep-tinyllama-ollama\"      \n  model_name: \"tinyllama-ollama\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  # Format: hf:dataset-id/subset-name/split-name. Use   \"default\" if no subset name is provided.\n  - hf:THUDM/LongBench/2wikimqa_e/test\n  - hf:THUDM/LongBench/2wikimqa/test\n  - hf:THUDM/LongBench/hotpotqa_e/test\n  - hf:THUDM/LongBench/hotpotqa/test\n  - hf:THUDM/LongBench/narrativeqa/test\n  - hf:THUDM/LongBench/triviaqa_e/test\n  - hf:THUDM/LongBench/triviaqa/test\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1-500\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: custom_pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  custom_rest:\n    temperature: 0.3\n    top_p: 0.3\n    stream: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"custom-model-byoe\"\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    model_id: # model id, version and image uri not needed for byo endpoint\n    hf_tokenizer_model_id: \n    model_version:\n    model_name: \"custom-model\"\n    # the ep_name will contain the endpoint url that is used to invoke your model and get the response\n    # in this case, we use ray serve with `NousResearch/Llama-2-13b-chat-hf` model deployed on an EKS cluster.\n    # the endpoint url format used in this example is \"http://<NLB_DNS_NAME>/serve/infer?sentence=<PROMPT_PAYLOAD>\"\n    ep_name: 'http://localhost:8080/v2/completions' # public DNS/URL to send your request\n    instance_type: \"g6e.xlarge\"\n    image_uri:\n    deploy: no #setting to no since the endpoint has already been deployed on an EKS cluster\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count: \n    deployment_script: \n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example of a custom rest predictor\n    # that does a POST request on the endpoint URL (given on line 164) with custom headers, \n    # parameters and authentication information\n    inference_script: custom_rest_predictor.py\n    # This is the inference spec with custom information. You can add/remove variables \n    # and access those in the predictor file to use them as accepted by your container.\n    inference_spec:\n      # This parameter is specified above with custom configurations\n      parameter_set: custom_rest\n      # This model id is appended to the prompt as the payload is sent to the model id\n      model_id: \"tinyllama\"\n      # All key-value pairs added in this headers section are transparently\n      # passed via the POST request to the model server. You can add any custom\n      # header parameters needed for your specific implementation.\n      headers:\n        custom_authentication_token: \"your-secret-token-here\"  # Replace with actual token\n        content-type: \"application/json\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 5\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n\n\n================================================"
  },
  {
    "filename": "config-byo-custom-rest-predictor.yml",
    "path": "fmbench/configs/byoe/config-byo-custom-rest-predictor.yml",
    "directory": "fmbench/configs/byoe",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench -\n# This config file uses custom payload functionalities, and points to \n# a \"custom_rest_predictor.py\" that does a request.post to an endpoint\n# url, with the model id, inference parameters and a few headers configured\n# within this config file\ngeneral:\n  name: \"<add-your-rest-ep-model-name-here>\"      \n  model_name: \"<your-custom-model-name>\"\n  \n# AWS and SageMaker settings\n# These are placeholder values that will be substituted\n# dynamically once FMBench runs the test\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\n# These directory paths point to the data, prompts, metrics and other\n# model and metadata information that will be stored in the FMBench write\n# bucket once the benchmarking test starts\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  # Add your custom pricing in this yml file which is located at \"configs/pricing.yml\" that contains\n  # instance hourly and token-based pricing. NOTE: The instance_type parameter in this config file should match\n  # the instance_type parameter in the pricing.yml file for FMBench to correctly map the values before\n  # calculating the cost to run and evaluate the models to be benchmarked.\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  \n  # FOR HUGGING FACE DATASETS:\n  # ---------------------------\n  # FMBench now supports benchmarking models using datasets from Hugging Face with a simplified prefixing method. \n  # To specify a Hugging Face dataset and its split, use the hf: prefix followed by the dataset identifier, subset name, and split name. \n  # If a subset name is not provided, it defaults to default. If a split name is not provided, FMBench automatically selects the next available split at runtime.\n  # FOR CUSTOM DATASETS:\n  # ---------------------------\n  # To use custom data, convert it into JSONL format. We provide a sample notebook to help convert Hugging Face or custom datasets into JSONL and upload them to an S3 bucket used by FMBench. \n  # Follow the steps in the https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/fmbench/bring_your_own_dataset.ipynb notebook to integrate your own dataset into FMBench. \n  # Place this JSONL file in the local fmbench-read/scripts directory in your FMBench EC2 instance or in the fmbench-read S3 bucket in the scripts directory.\n  source_data_files:\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  # Format: hf:dataset-id/subset-name/split-name. Use   \"default\" if no subset name is provided.\n  - hf:THUDM/LongBench/2wikimqa_e/test\n  - hf:THUDM/LongBench/2wikimqa/test\n  - hf:THUDM/LongBench/hotpotqa_e/test\n  - hf:THUDM/LongBench/hotpotqa/test\n  - hf:THUDM/LongBench/narrativeqa/test\n  - hf:THUDM/LongBench/triviaqa_e/test\n  - hf:THUDM/LongBench/triviaqa/test\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: tokenizer\n\n  # S3 prefix for prompt templates - this does not have to be changed\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  # If you have a custom prompt template, place the file in the local fmbench-read/scripts directory and point to it here\n  prompt_template_file: <your-prompt-template.txt>\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  # <replace with the keys in your dataset. If your dataset contains another key you want inputted into the prompt, then specify that here and it will\n  # be plugged into the prompt\n  - input\n  - context\n  # If you want to measure model accuracy, mention the column keys in the dataset that point to the question/task and the ground truth. \n  # This will be used by FMBench's LLM evaluators to evaluate the correctness of models to be benchmarked.\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1-500\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: custom_pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  custom_rest:\n    generation_config:\n      temperature: 0.3\n      top_p: 0.3\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"<your-model-name>\"\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    model_id: # model id, version and image uri not needed for byo endpoint\n    hf_tokenizer_model_id: \n    model_version:\n    model_name: \"<your-model-name>\"\n    # the ep_name will contain the endpoint url that is used to invoke your model and get the response\n    # in this case, we use ray serve with `NousResearch/Llama-2-13b-chat-hf` model deployed on an EKS cluster.\n    # the endpoint url format used in this example is \"http://<NLB_DNS_NAME>/serve/infer?sentence=<PROMPT_PAYLOAD>\"\n    ep_name: '<your-ep-url>' # public DNS/URL to send your request\n    instance_type: \"<your-instance-type>\"\n    image_uri:\n    deploy: no #setting to no since the endpoint has already been deployed on an EKS cluster\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count: \n    deployment_script: # No deployment script is needed since this is a byoe configuration file\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example of a custom rest predictor\n    # that does a POST request on the endpoint URL (given on line 164) with custom headers, \n    # parameters and authentication information\n    inference_script: custom_rest_predictor.py\n    # This is the inference spec with custom information. You can add/remove variables \n    # and access those in the predictor file to use them as accepted by your container.\n    inference_spec:\n      # This parameter is specified above with custom configurations\n      parameter_set: custom_rest\n      # This model id is appended to the prompt as the payload is sent to the model id\n      model_id: \"<custom-model-id>\"\n      # All key-value pairs added in this headers section are transparently\n      # passed via the POST request to the model server. You can add any custom\n      # header parameters needed for your specific implementation.\n      headers:\n        custom_client_id: <your-custom-client-id>\n        custom_authentication_token: \"<your-auth-token>\"  # Replace with actual token\n        content-type: \"application/json\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 5\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n\n\n================================================"
  },
  {
    "filename": "config-model-byo-sagemaker-endpoint.yml",
    "path": "fmbench/configs/byoe/config-model-byo-sagemaker-endpoint.yml",
    "directory": "fmbench/configs/byoe",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"byo-sagemaker-ep\"      \n  model_name: \"your-model\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  # Use S3 only, local file system only, or both (values are s3, local or both)\n  # If set to local or both, set the local_file_system_path parameter\n  s3_and_or_local_file_system: local\n  local_file_system_path: {write_tmpdir}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    # Use S3 only or local file system only (values are s3 or local)\n    # If set to local, set the local_file_system_path parameter\n    s3_or_local_file_system: local\n    local_file_system_path: {read_tmpdir}\n    # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama2.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: no\n\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 924\n    payload_file: payload_en_1-924.jsonl\n \nmetrics:\n  dataset_of_interest: en_1-924\n  \npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    return_full_text: False\n    #truncate: at-prompt-token-length\n\n# Model configurations for llama-2 7b for deploying on g5 x and 2x large instances\nexperiments:\n  - name: bring-your-own-sm-endpoint\n    model_id: # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: <your-model-name>\n    ep_name: \"<your-endpoint-name>\"\n    # If you are bringing your own endpoint and want to specify a custom variant name that is used to generate endpoint utilization metrics, \n    # mention that below. If not, the production variant name defaults to 'AllTraffic'\n    production_variant_name: AllTraffic\n    instance_type:  \"<your-instance-type>\"\n    image_uri:\n    deploy: no # setting to no since the endpoint has already been deployed\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    # - payload_en_1-500.jsonl\n    # - payload_en_500-1000.jsonl\n    # - payload_en_1000-2000.jsonl\n    # - payload_en_2000-3000.jsonl\n    - payload_en_1-924.jsonl\n\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\nreport:\n  latency_budget: 5\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n\n================================================"
  },
  {
    "filename": "config-deepseek-r1-ollama.yml",
    "path": "fmbench/configs/deepseek/config-deepseek-r1-ollama.yml",
    "directory": "fmbench/configs/deepseek",
    "extension": "yml",
    "content": "================================================\n\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: {results_dir}\n  model_name: {model_id}\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_deepseek_longbench.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_ollama:\n    model: {model_id}\n    stream: false\n    options:\n      temperature: 0.1\n      top_p: 0.92\n      top_k: 120  \n      num_predict: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: {model_id}\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    hf_tokenizer_model_id: deepseek-ai/DeepSeek-R1\n    model_id: {model_id} # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: {model_id}\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:11434/api/generate' \n    instance_type: {instance_type}\n    image_uri: ec2\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_ollama\n      container_type: ollama\n    # modify the serving properties to match your model and requirements\n    serving.properties: \n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-deepseek-r1-quant1.58-longbench-byoe.yml",
    "path": "fmbench/configs/deepseek/config-deepseek-r1-quant1.58-longbench-byoe.yml",
    "directory": "fmbench/configs/deepseek",
    "extension": "yml",
    "content": "================================================\n\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: {results_dir}\n  model_name: {model_id}\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_deepseek_longbench.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_llama_server:\n    model: {model_id}\n    temperature: 0.1\n    # top_p: 0.92\n    # top_k: 120  \n    # max_tokens: 512\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: {model_id}\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    hf_tokenizer_model_id: {hf_tokenizer_model_id}\n    model_id: {model_id} # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: {model_id}\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8080/completion' \n    instance_type: {instance_type}\n    image_uri:\n    deploy: no # setting to no because BYOE\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:      \n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_llama_server\n      container_type: llama_server\n      cli_params:\n    # modify the serving properties to match your model and requirements\n    serving.properties: \n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    #- 2\n    # - 3\n    #- 4\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n================================================"
  },
  {
    "filename": "config-deepseek-r1-sglang.yml",
    "path": "fmbench/configs/deepseek/config-deepseek-r1-sglang.yml",
    "directory": "fmbench/configs/deepseek",
    "extension": "yml",
    "content": "================================================\n\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: {results_dir}\n  model_name: {model_id}\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  # Format: hf:dataset-id/subset-name/split-name. Use   \"default\" if no subset name is provided.\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_deepseek_longbench.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_sglang:\n    model: default\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_tokens: 2048\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: {model_id}\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    hf_tokenizer_model_id: deepseek-ai/DeepSeek-R1\n    model_id: {model_id} # model id, version and image uri not needed for byo endpoint\n    model_version:\n    deploy: yes\n    model_name: {model_id}\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:30000/v1/chat/completions'\n    instance_type: {instance_type}\n    image_uri: lmsysorg/sglang:latest\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_sglang\n      container_type: sglang\n    # modify the serving properties to match your model and requirements\n    serving.properties: \n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-deepseek-r1-vllm-convfinqa.yml",
    "path": "fmbench/configs/deepseek/config-deepseek-r1-vllm-convfinqa.yml",
    "directory": "fmbench/configs/deepseek",
    "extension": "yml",
    "content": "================================================\n\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: {results_dir}\n  model_name: {model_id}\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - hf:AdaptLLM/finance-tasks/ConvFinQA/test\n  # - hf:THUDM/LongBench/narrativeqa/test\n\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_convfinqa.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  ground_truth_col_key: label\n  question_col_key: input\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1000-2000\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_vllm:\n    model: {model_id}\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_tokens: 2048\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: {model_id}\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    hf_tokenizer_model_id: {model_id}\n    model_id: {model_id} # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: {model_id}\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8000/v1/completions' \n    instance_type: {instance_type}\n    image_uri: vllm/vllm-openai:v0.7.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_vllm\n      container_type: vllm_gpu\n      cli_params: {cli_params}\n    # modify the serving properties to match your model and requirements\n    serving.properties: \n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-deepseek-r1-vllm-longbench.yml",
    "path": "fmbench/configs/deepseek/config-deepseek-r1-vllm-longbench.yml",
    "directory": "fmbench/configs/deepseek",
    "extension": "yml",
    "content": "================================================\n\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: {results_dir}\n  model_name: {model_id}\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_deepseek_longbench.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_vllm:\n    model: {model_id}\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_tokens: 2048\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: {model_id}\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    hf_tokenizer_model_id: {model_id}\n    model_id: {model_id} # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: {model_id}\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8000/v1/completions' \n    instance_type: {instance_type}\n    image_uri: vllm/vllm-openai:v0.7.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:      \n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_vllm\n      container_type: vllm_gpu\n      cli_params: {cli_params}\n    # modify the serving properties to match your model and requirements\n    serving.properties: \n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    #- 2\n    # - 3\n    #- 4\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n================================================"
  },
  {
    "filename": "config-deepseek-r1-vllm-openorca.yml",
    "path": "fmbench/configs/deepseek/config-deepseek-r1-vllm-openorca.yml",
    "directory": "fmbench/configs/deepseek",
    "extension": "yml",
    "content": "================================================\n\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: {results_dir}\n  model_name: {model_id}\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - OpenOrca.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_open_orca.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - question\n  ground_truth_col_key: response\n  question_col_key: question\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_500-1000\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_vllm:\n    model: {model_id}\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_tokens: 2048\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: {model_id}\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    hf_tokenizer_model_id: {model_id}\n    model_id: {model_id} # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: {model_id}\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8000/v1/completions' \n    instance_type: {instance_type}\n    image_uri: vllm/vllm-openai:v0.7.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:      \n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_vllm\n      container_type: vllm_gpu\n      cli_params: {cli_params}\n    # modify the serving properties to match your model and requirements\n    serving.properties: \n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    #- 2\n    # - 3\n    #- 4\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n================================================"
  },
  {
    "filename": "llama3-ray-service.yaml",
    "path": "fmbench/configs/eks_manifests/llama3-ray-service.yaml",
    "directory": "fmbench/configs/eks_manifests",
    "extension": "yaml",
    "content": "================================================\n#----------------------------------------------------------------------\n# NOTE: For deployment instructions, refer to the DoEKS website.\n#----------------------------------------------------------------------\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: llama3\n\n---\napiVersion: ray.io/v1\nkind: RayService\nmetadata:\n  name: llama3-service\n  namespace: llama3\nspec:\n  serviceUnhealthySecondThreshold: 900\n  deploymentUnhealthySecondThreshold: 900\n  serveConfigV2: |\n    applications:\n      - name: llama3\n        import_path: ray_serve_llama3:entrypoint\n        runtime_env:\n          env_vars:\n            MODEL_ID: \"meta-llama/Meta-Llama-3-8B-Instruct\"\n            HUGGING_FACE_HUB_TOKEN: $HUGGING_FACE_HUB_TOKEN\n            LD_LIBRARY_PATH: \"/home/ray/anaconda3/lib\"\n  rayClusterConfig:\n    rayVersion: '2.11.0'\n    headGroupSpec:\n      headService:\n        metadata:\n          name: llama3-service\n          namespace: llama3\n      rayStartParams:\n        dashboard-host: '0.0.0.0'\n      template:\n        spec:\n          containers:\n          - name: ray-head\n            image: public.ecr.aws/data-on-eks/ray-serve-inf2-llama3:latest\n            imagePullPolicy: Always\n            lifecycle:\n              preStop:\n                exec:\n                  command: [\"/bin/sh\", \"-c\", \"ray stop\"]\n            ports:\n            - containerPort: 6379\n              name: gcs-server\n            - containerPort: 8265\n              name: dashboard\n            - containerPort: 10001\n              name: client\n            - containerPort: 8000\n              name: serve\n            volumeMounts:\n            - mountPath: /tmp/ray\n              name: ray-logs\n            resources:\n              limits:\n                cpu: \"4\"\n                memory: \"20G\"\n              requests:\n                cpu: \"4\"\n                memory: \"20G\"\n            env:\n              - name: LD_LIBRARY_PATH\n                value: \"/home/ray/anaconda3/lib\"\n          nodeSelector:\n            instanceType: mixed-x86\n            provisionerType: Karpenter\n            workload: rayhead\n          volumes:\n          - name: ray-logs\n            emptyDir: {}\n\n    workerGroupSpecs:\n    - groupName: inf2-worker-group\n      replicas: 1\n      minReplicas: 1\n      maxReplicas: 1\n      rayStartParams: {}\n      template:\n        spec:\n          containers:\n          - name: ray-worker\n            image: public.ecr.aws/data-on-eks/ray-serve-inf2-llama3:latest\n            imagePullPolicy: Always\n            lifecycle:\n              preStop:\n                exec:\n                  command: [\"/bin/sh\", \"-c\", \"ray stop\"]\n            resources:\n              limits:\n                cpu: \"180\"\n                memory: \"700G\"\n                aws.amazon.com/neuron: \"12\"\n              requests:\n                cpu: \"180\"\n                memory: \"700G\"\n                aws.amazon.com/neuron: \"12\"\n            env:\n              - name: LD_LIBRARY_PATH\n                value: /home/ray/anaconda3/lib\n          nodeSelector:\n            instanceType: inferentia-inf2\n            provisionerType: Karpenter\n          tolerations:\n          - key: \"aws.amazon.com/neuron\"\n            operator: \"Exists\"\n            effect: \"NoSchedule\"\n          - key: \"hub.jupyter.org/dedicated\"\n            operator: \"Equal\"\n            value: \"user\"\n            effect: \"NoSchedule\"\n\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: llama3-ingress\n  namespace: llama3\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: \"/$1\"\nspec:\n  ingressClassName: nginx\n  rules:\n  - http:\n      paths:\n      - path: /dashboard/(.*)\n        pathType: ImplementationSpecific\n        backend:\n          service:\n            name: llama3-service\n            port:\n              number: 8265\n      - path: /serve/(.*)\n        pathType: ImplementationSpecific\n        backend:\n          service:\n            name: llama3-service\n            port:\n              number: 8000\n\n\n\n================================================"
  },
  {
    "filename": "mistral-ray-service.yaml",
    "path": "fmbench/configs/eks_manifests/mistral-ray-service.yaml",
    "directory": "fmbench/configs/eks_manifests",
    "extension": "yaml",
    "content": "================================================\n#----------------------------------------------------------------------\n# NOTE: For deployment instructions, refer to the DoEKS website.\n#----------------------------------------------------------------------\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: mistral\n\n---\napiVersion: ray.io/v1\nkind: RayService\nmetadata:\n  name: mistral-service\n  namespace: mistral\nspec:\n  serviceUnhealthySecondThreshold: 900\n  deploymentUnhealthySecondThreshold: 300\n  serveConfigV2: |\n    applications:\n      - name: mistral-deployment\n        import_path: \"ray_serve_mistral:entrypoint\"\n        route_prefix: \"/\"\n        runtime_env:\n          env_vars:\n            MODEL_ID: \"mistralai/Mistral-7B-Instruct-v0.2\"\n            NEURON_CC_FLAGS: \"-O1\"\n            HUGGING_FACE_HUB_TOKEN: $HUGGING_FACE_HUB_TOKEN\n        deployments:\n          - name: mistral-7b\n            autoscaling_config:\n              metrics_interval_s: 0.2\n              min_replicas: 2\n              max_replicas: 12\n              look_back_period_s: 2\n              downscale_delay_s: 30\n              upscale_delay_s: 2\n              target_num_ongoing_requests_per_replica: 1\n            graceful_shutdown_timeout_s: 5\n            max_concurrent_queries: 100\n            ray_actor_options:\n              num_cpus: 10\n              resources: {\"neuron_cores\": 2}\n  rayClusterConfig:\n    rayVersion: '2.11.0'\n    enableInTreeAutoscaling: true\n    headGroupSpec:\n      serviceType: NodePort\n      headService:\n        metadata:\n          name: mistral-service\n          namespace: mistral\n      rayStartParams:\n        dashboard-host: '0.0.0.0'\n      template:\n        spec:\n          containers:\n          - name: ray-head\n            image: public.ecr.aws/data-on-eks/ray2.11.0-py310-mistral7b-neuron:latest\n            imagePullPolicy: Always # Ensure the image is always pulled when updated\n            lifecycle:\n              preStop:\n                exec:\n                  command: [\"/bin/sh\", \"-c\", \"ray stop\"]\n            ports:\n            - containerPort: 6379\n              name: gcs-server\n            - containerPort: 8265\n              name: dashboard\n            - containerPort: 10001\n              name: client\n            - containerPort: 8000\n              name: serve\n            volumeMounts:\n            - mountPath: /tmp/ray\n              name: ray-logs\n            resources:\n              limits:\n                cpu: \"2\"\n                memory: \"20G\"\n              requests:\n                cpu: \"2\"\n                memory: \"20G\"\n          nodeSelector:\n            instanceType: mixed-x86\n            provisionerType: Karpenter\n            workload: rayhead\n          volumes:\n          - name: ray-logs\n            emptyDir: {}\n\n    workerGroupSpecs:\n    - groupName: inf2-worker-group\n      replicas: 1\n      minReplicas: 1\n      maxReplicas: 5\n      rayStartParams: {}\n      template:\n        spec:\n          containers:\n          - name: ray-worker\n            image: public.ecr.aws/data-on-eks/ray2.11.0-py310-mistral7b-neuron:latest\n            imagePullPolicy: Always # Ensure the image is always pulled when updated\n            lifecycle:\n              preStop:\n                exec:\n                  command: [\"/bin/sh\", \"-c\", \"ray stop\"]\n            # We are using 2 Neuron cores per HTTP request hence this configuration handles 6 requests per second\n            resources:\n              limits:\n                cpu: \"90\" # All vCPUs of inf2.24xlarge; 6vCPU daemonset overhead\n                memory: \"360G\" # All memory of inf2.24xlarge; 24G for daemonset overhead\n                aws.amazon.com/neuron: \"6\" # All Neuron cores of inf2.24xlarge\n              requests:\n                cpu: \"90\" # All vCPUs of inf2.24xlarge; 6vCPU daemonset overhead\n                memory: \"360G\" # All memory of inf2.24xlarge; 24G for daemonset overhead\n                aws.amazon.com/neuron: \"6\" # All Neuron cores of inf2.24xlarge\n          nodeSelector:\n            instanceType: inferentia-inf2\n            provisionerType: Karpenter\n          tolerations:\n          - key: \"aws.amazon.com/neuron\"\n            operator: \"Exists\"\n            effect: \"NoSchedule\"\n          - key: \"hub.jupyter.org/dedicated\"\n            operator: \"Equal\"\n            value: \"user\"\n            effect: \"NoSchedule\"\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: mistral-ingress\n  namespace: mistral\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: \"/$1\"\nspec:\n  ingressClassName: nginx\n  rules:\n  - http:\n      paths:\n      # Ray Dashboard\n      - path: /dashboard/(.*)\n        pathType: ImplementationSpecific\n        backend:\n          service:\n            name: mistral-service\n            port:\n              number: 8265\n  - http:\n      paths:\n      # Ray Serve\n      - path: /serve/(.*)\n        pathType: ImplementationSpecific\n        backend:\n          service:\n            name: mistral-service\n            port:\n              number: 8000\n\n\n================================================"
  },
  {
    "filename": "bge-base-en-v1-5-c5-embeddings.yml",
    "path": "fmbench/configs/embeddings/bge-base-en-v1-5-c5-embeddings.yml",
    "directory": "fmbench/configs/embeddings",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"bge-base-en-v1-5\"      \n  model_name: \"bge-base-en-v1-5\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - just_text.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_bert.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  prompt_template_keys:\n  - text\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 50\n    payload_file: payload_en_1-50.jsonl\n  - language: en    \n    min_length_in_tokens: 50\n    max_length_in_tokens: 100\n    payload_file: payload_en_50-100.jsonl\n  - language: en    \n    min_length_in_tokens: 100\n    max_length_in_tokens: 150\n    payload_file: payload_en_100-150.jsonl\n    \n\nmetrics:\n  dataset_of_interest: en_50-100\n  \npricing: pricing.yml\n\ninference_parameters:\n  embedding:\n    top_p: 0.92\n    \nexperiments:\n  - name: bge-base-en-v1-5-c5.xl\n    model_id: huggingface-sentencesimilarity-bge-base-en-v1-5\n    model_version: \"*\"\n    model_name: bge-base-en-v1.5\n    ep_name: bge-base-en-v1-5\n    instance_type: \"ml.c5.xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-inference:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: embedding\n      container_type: huggingface\n    payload_files:\n    - payload_en_1-50.jsonl\n    - payload_en_50-100.jsonl\n    - payload_en_100-150.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n    accept_eula: false\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"1200\"\n  \n  - name: bge-base-en-v1-5-c5.2xl\n    model_id: huggingface-sentencesimilarity-bge-base-en-v1-5\n    model_version: \"*\"\n    model_name: bge-base-en-v1.5\n    ep_name: bge-base-en-v1-5\n    instance_type: \"ml.c5.2xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-inference:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: embedding\n      container_type: huggingface\n    payload_files:\n    - payload_en_1-50.jsonl\n    - payload_en_50-100.jsonl\n    - payload_en_100-150.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n    accept_eula: false\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"1200\"\n  \n  - name: bge-base-en-v1-5-c5.4xl\n    model_id: huggingface-sentencesimilarity-bge-base-en-v1-5\n    model_version: \"*\"\n    model_name: bge-base-en-v1.5\n    ep_name: bge-base-en-v1-5\n    instance_type: \"ml.c5.4xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-inference:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: embedding\n      container_type: huggingface\n    payload_files:\n    - payload_en_1-50.jsonl\n    - payload_en_50-100.jsonl\n    - payload_en_100-150.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n\n    accept_eula: false\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"1200\"\n      # SM_NUM_GPUS: \"1\"\n      # MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      # SAGEMAKER_ENV: \"1\"\n      # HF_MODEL_ID: \"BAAI/bge-base-en-v1.5\"\n      # MAX_INPUT_LENGTH: \"4095\"\n      # MAX_TOTAL_TOKENS: \"4096\"\n    #   # SAGEMAKER_MODEL_SERVER_WORKERS: \"2\"\n\nreport:\n  latency_budget: 1\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "bge-base-en-v1-5-g5-embeddings.yml",
    "path": "fmbench/configs/embeddings/bge-base-en-v1-5-g5-embeddings.yml",
    "directory": "fmbench/configs/embeddings",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"bge-base-en-v1-5\"      \n  model_name: \"bge-base-en-v1-5\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - just_text.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_bert.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  prompt_template_keys:\n  - text\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 50\n    payload_file: payload_en_1-50.jsonl\n  - language: en    \n    min_length_in_tokens: 50\n    max_length_in_tokens: 100\n    payload_file: payload_en_50-100.jsonl\n  - language: en    \n    min_length_in_tokens: 100\n    max_length_in_tokens: 150\n    payload_file: payload_en_100-150.jsonl\n    \n\nmetrics:\n  dataset_of_interest: en_50-100\n  \npricing: pricing.yml\n\ninference_parameters:\n  embedding:\n    top_p: 0.92\n    \nexperiments:\n  - name: bge-base-en-v1-5-g5.xl\n    model_id: huggingface-sentencesimilarity-bge-base-en-v1-5\n    model_version: \"*\"\n    model_name: bge-base-en-v1.5\n    ep_name: bge-base-en-v1-5\n    instance_type: \"ml.g5.xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-inference:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: embedding\n      container_type: huggingface\n    payload_files:\n    - payload_en_1-50.jsonl\n    - payload_en_50-100.jsonl\n    - payload_en_100-150.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n    accept_eula: false\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"1200\"\n  \n  - name: bge-base-en-v1-5-g5.2xl\n    model_id: huggingface-sentencesimilarity-bge-base-en-v1-5\n    model_version: \"*\"\n    model_name: bge-base-en-v1.5\n    ep_name: bge-base-en-v1-5\n    instance_type: \"ml.g5.2xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-inference:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: embedding\n      container_type: huggingface\n    payload_files:\n    - payload_en_1-50.jsonl\n    - payload_en_50-100.jsonl\n    - payload_en_100-150.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n    accept_eula: false\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"1200\"\n      # SM_NUM_GPUS: \"1\"\n      # MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      # SAGEMAKER_ENV: \"1\"\n      # HF_MODEL_ID: \"BAAI/bge-base-en-v1.5\"\n      # MAX_INPUT_LENGTH: \"4095\"\n      # MAX_TOTAL_TOKENS: \"4096\"\n    #   # SAGEMAKER_MODEL_SERVER_WORKERS: \"2\"\n\nreport:\n  latency_budget: 1\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "bge-base-en-v1-5-g5-g4dn-c7-embeddings.yml",
    "path": "fmbench/configs/embeddings/bge-base-en-v1-5-g5-g4dn-c7-embeddings.yml",
    "directory": "fmbench/configs/embeddings",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"bge-base-en-v1-5\"      \n  model_name: \"bge-base-en-v1-5\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - just_text.jsonl\n    - banking77.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_bert.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  prompt_template_keys:\n  - text\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 50\n    payload_file: payload_en_1-50.jsonl\n  - language: en    \n    min_length_in_tokens: 50\n    max_length_in_tokens: 100\n    payload_file: payload_en_50-100.jsonl\n  - language: en    \n    min_length_in_tokens: 100\n    max_length_in_tokens: 150\n    payload_file: payload_en_100-150.jsonl\n    \n\nmetrics:\n  dataset_of_interest: en_1-50\n  \npricing: pricing.yml\n\ninference_parameters:\n  embedding:\n    top_p: 0.92\n    \nexperiments:\n  - name: bge-base-en-v1-5-g5.xl\n    model_id: huggingface-sentencesimilarity-bge-base-en-v1-5\n    model_version: \"*\"\n    model_name: bge-base-en-v1.5\n    ep_name: bge-base-en-v1-5\n    instance_type: \"ml.g5.xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-inference:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: embedding\n      container_type: huggingface\n    payload_files:\n    - payload_en_1-50.jsonl\n    - payload_en_50-100.jsonl\n    - payload_en_100-150.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n    accept_eula: false\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"1200\"\n      SM_NUM_GPUS: \"1\"\n  \n  - name: bge-base-en-v1-5-g5.2xl\n    model_id: huggingface-sentencesimilarity-bge-base-en-v1-5\n    model_version: \"*\"\n    model_name: bge-base-en-v1.5\n    ep_name: bge-base-en-v1-5\n    instance_type: \"ml.g4dn.2xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-inference:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: embedding\n      container_type: huggingface\n    payload_files:\n    - payload_en_1-50.jsonl\n    - payload_en_50-100.jsonl\n    - payload_en_100-150.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n    accept_eula: false\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"1200\"\n      SM_NUM_GPUS: \"1\"\n      # MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      # SAGEMAKER_ENV: \"1\"\n      # HF_MODEL_ID: \"BAAI/bge-base-en-v1.5\"\n      # MAX_INPUT_LENGTH: \"4095\"\n      # MAX_TOTAL_TOKENS: \"4096\"\n    #   # SAGEMAKER_MODEL_SERVER_WORKERS: \"2\"\n  \n  - name: bge-base-en-v1-5-g5.2xl\n    model_id: huggingface-sentencesimilarity-bge-base-en-v1-5\n    model_version: \"*\"\n    model_name: bge-base-en-v1.5\n    ep_name: bge-base-en-v1-5\n    instance_type: \"ml.c7i.xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-inference:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: embedding\n      container_type: huggingface\n    payload_files:\n    - payload_en_1-50.jsonl\n    - payload_en_50-100.jsonl\n    - payload_en_100-150.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n    accept_eula: false\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"1200\"\n\nreport:\n  latency_budget: 0.1\n  cost_per_10k_txn_budget: 5\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-gemma-2b-g5.yml",
    "path": "fmbench/configs/gemma/config-gemma-2b-g5.yml",
    "directory": "fmbench/configs/gemma",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"gemma-2b-v1\"      \n  model_name: \"gemma-2b\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - rajpurkar/squad.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: gemma_tokenizer\n  \n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_gemma.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the \n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 100\n    payload_file: payload_en_1-100.jsonl\n  - language: en\n    min_length_in_tokens: 100\n    max_length_in_tokens: 200\n    payload_file: payload_en_100-200.jsonl\n  - language: en\n    min_length_in_tokens: 150\n    max_length_in_tokens: 200\n    payload_file: payload_en_150-200.jsonl\n  - language: en\n    min_length_in_tokens: 200\n    max_length_in_tokens: 300\n    payload_file: payload_en_200-300.jsonl\n  - language: en\n    min_length_in_tokens: 300\n    max_length_in_tokens: 400\n    payload_file: payload_en_300-400.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_150-200\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: huggingface-llm-gemma-2b-huggingface-pytorch-tgi-inference:2.1.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: huggingface-llm-gemma-2b\n    model_version: \"1.*\"\n    model_name: gemma-2b\n    ep_name: gemma-2b\n    instance_type: \"ml.g5.2xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi1.4.2-gpu-py310-cu121-ubuntu22.04'\n    deploy: yes\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details    \n    deployment_script: jumpstart.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-100.jsonl\n    - payload_en_150-200.jsonl\n    - payload_en_200-300.jsonl\n    - payload_en_300-400.jsonl\n    #- payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10 \n    - 12\n    - 15\n    - 20\n    - 30\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"8191\"\n      MAX_TOTAL_TOKENS: \"8192\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\n# parameters related to how the final report is generated\nreport:\n  # markdown report title\n  title: \"Performance benchmarking results for Gemma-2b on `g5.2xlarge` using the HuggingFace TGI container\"\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 0.3\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "djl.yml",
    "path": "fmbench/configs/generic/ec2/djl.yml",
    "directory": "fmbench/configs/generic/ec2",
    "extension": "yml",
    "content": "================================================\n# Generic config file for djl deployment on EC2. This can be \n# used for any model that is deployed on djl on EC2. User provides\n# the model id, tokenizer directory name, prompt template and other\n# serving.properties in the command (tp degree, batch size) and those\n# parameters are used in this file.\ngeneral:\n  # if it is auto, then retrieve the model id and the instance type/container - \n  # derive the name based on that (?)\n  # if name: name, then provide the name as -A <model-id-instance>\n  name: {results_dir}\n  model_name: {model_id}\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: {tokenizer_dir} # llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: {prompt_template} # prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1-500\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: {model_id}\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: {model_id} # model id, version and image uri not needed for byo endpoint\n    hf_model_id: {model_id}\n    model_version:\n    model_name: {model_id}\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: {instance_type}\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: {tp_degree}\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree={tp_degree}\n      option.max_rolling_batch_size={batch_size}\n      option.model_id={model_id}\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    # - payload_en_500-1000.jsonl\n    # - payload_en_1000-2000.jsonl\n    # - payload_en_2000-3000.jsonl\n    # - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "llama3.1_djl.yml",
    "path": "fmbench/configs/generic/ec2/llama3.1_djl.yml",
    "directory": "fmbench/configs/generic/ec2",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  # if it is auto, then retrieve the model id and the instance type/container - \n  # derive the name based on that (?)\n  # if name: name, then provide the name as -A <model-id-instance>\n  name: {results_dir}\n  model_name: {model_id}\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: {model_id}\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: {model_id} # model id, version and image uri not needed for byo endpoint\n    hf_model_id: {model_id}\n    model_version:\n    model_name: {model_id}\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: {instance_type}\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: {tp_degree}\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree={tp_degree}\n      option.max_rolling_batch_size={batch_size}\n      option.model_id={model_id}\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "Qwen2.5_djl.yml",
    "path": "fmbench/configs/generic/ec2/Qwen2.5_djl.yml",
    "directory": "fmbench/configs/generic/ec2",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a Qwen2.5-72b deployed on ec2\ngeneral:\n  # if it is auto, then retrieve the model id and the instance type/container - \n  # derive the name based on that (?)\n  # if name: name, then provide the name as -A <model-id-instance>\n  name: {results_dir}\n  model_name: {model_id}\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_qwen.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: {model_id}\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: {model_id} # model id, version and image uri not needed for byo endpoint\n    hf_model_id: {model_id}\n    model_version:\n    model_name: {model_id}\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: {instance_type}\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a Qwen deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: {tp_degree}\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree={tp_degree}\n      option.max_rolling_batch_size={batch_size}\n      option.model_id={model_id}\n      # Setting this to 68832 because the default max model len that djl sets is 131702.\n      # For Qwen, the default max model len that djl sets (131072) cannot be larger than the\n      # maximum number of tokens that can be stored in the KV cache. For Qwen2.5-72b, the \n      # max tokens that can be stored in the KV cache is 68832\n      option.max_model_len={max_model_len}\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-bedrock-sagemaker-llama2.yml",
    "path": "fmbench/configs/llama2/13b/config-bedrock-sagemaker-llama2.yml",
    "directory": "fmbench/configs/llama2/13b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"fmbench-llama2\"      \n  model_name: \"Llama2 on Bedrock & SageMaker\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket}\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml ## mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama2.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  # - text\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n \nmetrics:\n  dataset_of_interest: en_3000-3840\n  \npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n  \ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n  sagemaker:\n    temperature: 0.1\n    do_sample: yes  \n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    return_full_text: False\n    #truncate: at-prompt-token-length\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  - name: llama2-13b-p4d.24xlarge-trt-inference\n    model_id: meta-llama/Llama-2-13b-chat-hf\n    model_version: \"*\"\n    model_name: meta-llama-Llama-2-13b-chat-hf\n    ep_name: llama-2-13b-p4d-24xlarge \n    download_from_hf_place_in_s3: None\n    model_s3_path: None\n    instance_type: \"ml.p4d.24xlarge\"    \n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-tensorrtllm0.8.0-cu122\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    serving.properties: |\n       engine=MPI\n       option.model_id=meta-llama/Llama-2-13b-chat-hf\n       option.tensor_parallel_degree=4\n       option.max_rolling_batch_size=64\n       option.rolling_batch=trtllm\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    accept_eula: true\n    env:\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      OPTION_MAX_INPUT_LEN: \"3840\"\n      OPTION_MAX_OUTPUT_LEN: \"256\"\n      OPTION_USE_CUSTOM_ALL_REDUCE: \"true\"\n      \n  - name: meta.llama2-13b-chat-v1\n    model_id: meta.llama2-13b-chat-v1\n    model_version: \"*\"\n    model_name: meta.llama2-13b-chat-v1\n    ep_name: meta.llama2-13b-chat-v1\n    instance_type: meta.llama2-13b-chat-v1\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 30\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n================================================"
  },
  {
    "filename": "config-byo-rest-ep-llama2-13b.yml",
    "path": "fmbench/configs/llama2/13b/config-byo-rest-ep-llama2-13b.yml",
    "directory": "fmbench/configs/llama2/13b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-2-13b-chat-hf deployed on an EKS cluster using ray serve\ngeneral:\n  name: \"byo-REST-ep\"      \n  model_name: \"llama-2-13b-chat\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama2.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 924\n    payload_file: payload_en_1-924.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1-924\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  rest:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-2-13b-chat-EKS-ray-serve\"\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    model_id: # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-2-13b-chat\"\n    # the ep_name will contain the endpoint url that is used to invoke your model and get the response\n    # in this case, we use ray serve with `NousResearch/Llama-2-13b-chat-hf` model deployed on an EKS cluster.\n    # the endpoint url format used in this example is \"http://<NLB_DNS_NAME>/serve/infer?sentence=<PROMPT_PAYLOAD>\"\n    ep_name: 'http://<NLB_DNS_NAME>/serve/infer?sentence=' # public DNS/URL to send your request\n    instance_type: \"ml.inf2.48xlarge\"\n    image_uri:\n    deploy: no #setting to no since the endpoint has already been deployed on an EKS cluster\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count: \n    deployment_script: \n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example of a rest predictor that handles\n    # inferences for a llama-2-13b-chat-hf model deployed on an EKS cluster using Ray\n    inference_script: rest_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: rest\n      timeout: 180\n      auth: # pass your authentication parameters here\n      # - auth_paramater_1: \n      # - auth_paramater_2: \n      # - auth_paramater_3: \n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-924.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10 \n    - 12 \n    - 15\n    - 18\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 5\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n\n\n================================================"
  },
  {
    "filename": "config-llama2-13b-inf2-g5-p4d.yml",
    "path": "fmbench/configs/llama2/13b/config-llama2-13b-inf2-g5-p4d.yml",
    "directory": "fmbench/configs/llama2/13b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama2-13b-inf2-g5-p4d-v2\"      \n  model_name: \"Llama2-13b\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama2.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n \nmetrics:\n  dataset_of_interest: en_3000-3840\n\n \npricing: pricing.yml\n\ninference_parameters: \n  sagemaker:\n    temperature: 0.1\n    do_sample: yes  \n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    return_full_text: False\n#   truncate: at-prompt-token-length\n\n# Model configurations\nexperiments: \n  - name: llama2-13b-inf2.24xlarge-djl-0.24.0-neuronx-sdk-2.14.1-bs=4-tpd=12\n    model_id: meta-textgenerationneuron-llama-2-13b-f\n    model_version: 1.0.0\n    model_name: llama2-13b-f\n    ep_name: llama-2-13b-inf2-24xlarge\n    instance_type: \"ml.inf2.24xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.26.0-neuronx-sdk2.16.0'    \n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n\n    accept_eula: true\n    env:\n      OPTION_DTYPE: fp16\n      OPTION_MAX_ROLLING_BATCH_SIZE: \"4\"\n      OPTION_N_POSITIONS: \"4096\"\n      OPTION_TENSOR_PARALLEL_DEGREE: \"12\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n      SAGEMAKER_TS_RESPONSE_TIMEOUT: \"120\"\n      SAGEMAKER_MODEL_SERVER_TIMEOUT: \"120\"\n  - name: llama2-13b-inf2.48xlarge-djl-0.24.0-neuronx-sdk-2.14.1-bs=4-tpd=24\n    model_id: meta-textgenerationneuron-llama-2-13b-f\n    model_version: 1.0.0\n    model_name: llama2-13b-f\n    ep_name: llama-2-13b-inf2-48xlarge    \n    instance_type: \"ml.inf2.48xlarge\"    \n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.26.0-neuronx-sdk2.16.0'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n\n    accept_eula: true\n    env:\n      OPTION_DTYPE: fp16\n      OPTION_MAX_ROLLING_BATCH_SIZE: \"4\"\n      OPTION_N_POSITIONS: \"4096\"\n      OPTION_TENSOR_PARALLEL_DEGREE: \"24\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n      SAGEMAKER_TS_RESPONSE_TIMEOUT: \"120\"\n      SAGEMAKER_MODEL_SERVER_TIMEOUT: \"120\"\n  # P4D Based Instance Model Configuration:\n  - name: llama2-13b-p4d.24xlarge-tgi-inference-2.0.1-tgi0.9.3-gpu-py39-cu118\n    model_id: meta-llama/Llama-2-13b-chat-hf\n    model_version: \"*\"\n    model_name: meta-llama-Llama-2-13b-chat-hf\n    ep_name: llama-2-13b-p4d-24xlarge    \n    instance_type: \"ml.p4d.24xlarge\"    \n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi1.4.5-gpu-py310-cu121-ubuntu22.04-v1.0\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_hf_tgi.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    accept_eula: true\n    env:\n      MODEL_LOADING_TIMEOUT: \"3600\"\n      NUMBER_OF_GPU: 8\n      INSTANCE_COUNT: 1\n      HEALTH_CHECK_TIMEOUT: 300\n  - name: llama2-13b-g5.12xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\n    model_id: meta-textgeneration-llama-2-13b-f\n    model_version: \"*\"\n    model_name: llama2-13b-f\n    ep_name: llama-2-13b-g5-12xlarge\n    instance_type: \"ml.g5.12xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi1.4.5-gpu-py310-cu121-ubuntu22.04-v1.0\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"4\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n  - name: llama2-13b-g5.24xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\n    model_id: meta-textgeneration-llama-2-13b-f\n    model_version: \"*\"\n    model_name: llama2-13b-f\n    ep_name: llama-2-13b-g5-24xlarge\n    instance_type: \"ml.g5.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi1.4.5-gpu-py310-cu121-ubuntu22.04-v1.0\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"4\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n  - name: llama2-13b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\n    model_id: meta-textgeneration-llama-2-13b-f\n    model_version: \"*\"\n    model_name: llama2-13b-f\n    ep_name: llama-2-13b-g5-48xlarge\n    instance_type: \"ml.g5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi1.4.5-gpu-py310-cu121-ubuntu22.04-v1.0\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"8\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama2-13b-inf2-g5.yml",
    "path": "fmbench/configs/llama2/13b/config-llama2-13b-inf2-g5.yml",
    "directory": "fmbench/configs/llama2/13b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama2-13b-inf2-g5-v1\"      \n  model_name: \"Llama2-13b\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama2.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n \nmetrics:\n  dataset_of_interest: en_3000-3840\n  \npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    return_full_text: False\n\n# Model configurations for llama-2 7b for deploying on g5 and inf2 instances\nexperiments:\n  # llama2-13b neuron on inf2\n  - name: llama2-13b-inf2.24xlarge-djl-inference:0.27.0-neuronx\n    model_id: meta-textgenerationneuron-llama-2-13b-f\n    model_version: \"*\"\n    model_name: llama2-13b\n    ep_name: llama-2-13b-inf2-24xl\n    instance_type: \"ml.inf2.24xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-neuronx-sdk2.18.1'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    accept_eula: true\n    env:\n      HF_MODEL_ID: /opt/ml/model\n      MODEL_CACHE_ROOT: /opt/ml/model\n      OPTION_DTYPE: fp16\n      OPTION_MAX_ROLLING_BATCH_SIZE: \"4\"\n      OPTION_N_POSITIONS: \"4096\"\n      OPTION_TENSOR_PARALLEL_DEGREE: \"12\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n      SAGEMAKER_TS_RESPONSE_TIMEOUT: \"120\"\n      SAGEMAKER_MODEL_SERVER_TIMEOUT: \"120\"\n\n\n  - name: llama2-13b-g5.24xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\n    model_id: meta-textgeneration-llama-2-13b-f\n    model_version: \"*\"\n    model_name: llama2-13b-f\n    ep_name: llama-2-13b-g5-24xlarge\n    instance_type: \"ml.g5.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi1.4.5-gpu-py310-cu121-ubuntu22.04-v1.0\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"4\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\n  - name: llama2-13b-g5.12xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\n    model_id: meta-textgeneration-llama-2-13b-f\n    model_version: \"*\"\n    model_name: llama2-13b-f\n    ep_name: llama-2-13b-g5-12xlarge\n    instance_type: \"ml.g5.12xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi1.4.5-gpu-py310-cu121-ubuntu22.04-v1.0\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"4\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 60\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama2-70b.yml",
    "path": "fmbench/configs/llama2/70b/config-ec2-llama2-70b.yml",
    "directory": "fmbench/configs/llama2/70b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama2-70b-g5.48xl-ec2\"      \n  model_name: \"Llama-2-70b-chat-hf\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama2.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 924\n    payload_file: payload_en_1-924.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-2-70b-chat-hf\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-2-70b-chat-hf # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-2-70b-chat-hf\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Llama-2-70b-chat-hf\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10 \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama2-70b-g5-p4d-tgi.yml",
    "path": "fmbench/configs/llama2/70b/config-llama2-70b-g5-p4d-tgi.yml",
    "directory": "fmbench/configs/llama2/70b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama2-70b-g5-p4d-tgi-v1\"\n  model_name: \"Llama2-70b\"\n\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama2.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n \nmetrics:\n  dataset_of_interest: en_3000-3840\n\n  \npricing: pricing.yml\n  \ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    #truncate: at-prompt-token-length\n\n# Model configurations\nexperiments:\n  - name: llama2-70b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\n    model_id: meta-textgeneration-llama-2-70b-f\n    model_version: \"*\"\n    model_name: llama2-70b-f\n    ep_name: llama-2-70b-g5-48xlarge\n    instance_type: \"ml.g5.48xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"8\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n  # P4D Based Instance Model Configuration - using the TGI container:\n  - name: llama2-70b-p4d.24xlarge-tgi-inference-2.0.1-tgi0.9.3-gpu-py39-cu118\n    model_id: meta-llama/Llama-2-70b-chat-hf\n    model_version: \"*\"\n    model_name: meta-llama-Llama-2-70b-chat-hf\n    ep_name: llama-2-13b-p4d-24xlarge    \n    instance_type: \"ml.p4d.24xlarge\"    \n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi0.9.3-gpu-py39-cu118-ubuntu20.04\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_hf_tgi.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    accept_eula: true\n    env:\n      MODEL_LOADING_TIMEOUT: \"3600\"\n      NUMBER_OF_GPU: 8\n      INSTANCE_COUNT: 1\n      HEALTH_CHECK_TIMEOUT: 300\n\nreport:\n  latency_budget: 5\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama2-70b-g5-p4d-trt.yml",
    "path": "fmbench/configs/llama2/70b/config-llama2-70b-g5-p4d-trt.yml",
    "directory": "fmbench/configs/llama2/70b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama2-70b-g5-p4d-trt-v1\"\n  model_name: \"Llama2-70b\"\n\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama2.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n  \n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n \nmetrics:\n  dataset_of_interest: en_3000-3840\n  \npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n  \ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    #truncate: at-prompt-token-length\n\n# Model configurations\nexperiments:\n  - name: llama2-70b-g5.48xlarge-huggingface-pytorch-tgi-inference:2.1.1-tgi1.4.0\n    model_id: meta-textgeneration-llama-2-70b-f\n    model_version: \"*\"\n    model_name: llama2-70b-f\n    ep_name: llama-2-70b-g5-48xlarge\n    instance_type: \"ml.g5.48xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi1.4.0-gpu-py310-cu121-ubuntu20.04'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"8\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n  - name: llama2-70b-chat-p4d.24xlarge-djl-inference-0.26.0-tensorrtllm0.7.1-cu122\n    model_id: meta-llama/Llama-2-70b-hf \n    model_version: \"*\"\n    model_name: llama2-70bdjl\n    ep_name: llama-2-70b-chat-p4d-24xlarge\n    download_from_hf_place_in_s3: None\n    model_s3_path: None\n    ## allow_patterns: [] use if you are downloading the model first for the first time\n    instance_type: \"ml.p4d.24xlarge\"    \n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.26.0-tensorrtllm0.7.1-cu122' ## New djl TENSORRTLLM container\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n       engine=MPI\n       option.model_id=meta-llama/Llama-2-70b-hf\n       option.tensor_parallel_degree=8\n       option.max_input_len=4000\n       option.max_output_len=100\n       option.use_custom_all_reduce=true\n       option.output_formatter=json\n       option.max_rolling_batch_size=8\n       option.model_loading_timeout=3600\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    accept_eula: true\n    env:\n      NUMBER_OF_GPU: \"8\"\n      INSTANCE_COUNT: \"1\"\n      HEALTH_CHECK_TIMEOUT: \"300\"\n\nreport:\n  title: \"Performance benchmarking results for Llama2-70b on `g5.48xlarge` (HF TGI) and `p4d.24xlarge` (DJL TensorRT)\"\n  latency_budget: 5\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama2-70b-inf2-g5.yml",
    "path": "fmbench/configs/llama2/70b/config-llama2-70b-inf2-g5.yml",
    "directory": "fmbench/configs/llama2/70b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama2-70b-g5-inf2-tgi-v1\"\n  model_name: \"Llama2-70b\"\n\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - rajpurkar/squad.jsonl\n    # - 2wikimqa_e.jsonl\n    # - 2wikimqa.jsonl\n    # - hotpotqa_e.jsonl\n    # - hotpotqa.jsonl\n    # - narrativeqa.jsonl\n    # - triviaqa_e.jsonl\n    # - triviaqa.jsonl\n    tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama2.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 924\n    payload_file: payload_en_1-924.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 1948\n    payload_file: payload_en_1000-1948.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n\nmetrics:\n  dataset_of_interest: en_1-924\n  \npricing: pricing.yml\n  \ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    return_full_text: False\n    #truncate: at-prompt-token-length\n\n# Model configurations\nexperiments:\n  - name: llama2-70b-g5.48xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\n    model_id: meta-textgeneration-llama-2-70b-f\n    model_version: \"*\"\n    model_name: llama2-70b-f\n    ep_name: llama-2-70b-g5-48xlarge\n    instance_type: \"ml.g5.48xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-924.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"8\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n  - name: llama2-70b-inf2.48xlarge-djl-0.24.0-neuronx-sdk-2.14.1-bs=4-tpd=24\n    model_id: meta-textgenerationneuron-llama-2-70b-f\n    model_version: 1.0.1\n    model_name: llama2-70b-f\n    ep_name: llama-2-70b-inf2-48xlarge    \n    instance_type: \"ml.inf2.48xlarge\"    \n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:1.13.1-optimum0.0.17-neuronx-py310-ubuntu22.04'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-924.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n\n\n    accept_eula: true\n    env:\n      \"SAGEMAKER_PROGRAM\": \"inference.py\"\n      \"ENDPOINT_SERVER_TIMEOUT\": \"3600\"\n      \"MODEL_CACHE_ROOT\": \"/opt/ml/model\"\n      \"SAGEMAKER_ENV\": \"1\"\n      \"HF_MODEL_ID\": \"/opt/ml/model\"\n      \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\"\n\nreport:\n  latency_budget: 5\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama2-7b-byo-sagemaker-endpoint.yml",
    "path": "fmbench/configs/llama2/7b/config-llama2-7b-byo-sagemaker-endpoint.yml",
    "directory": "fmbench/configs/llama2/7b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama2-7b-byo-sagemaker-ep\"      \n  model_name: \"Llama2-7b\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama2.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n\nmetrics:\n  dataset_of_interest: en_1-500\n  weights:\n    price_per_tx_wt: 0.65\n    latenct_wt: 0.35\n  \npricing:\n  ml.g5.xlarge: 1.006\n  ml.g5.2xlarge: 1.212\n  ml.g5.12xlarge: 7.09\n  ml.g5.24xlarge: 10.18\n  ml.g5.48xlarge: 20.36\n  ml.inf2.24xlarge: 7.79\n  ml.inf2.48xlarge: 15.58\n  ml.p4d.24xlarge: 37.688\n\ninference_parameters:\n  do_sample: yes\n  temperature: 0.1\n  top_p: 0.92\n  top_k: 120  \n  max_new_tokens: 100\n  truncate: at-prompt-token-length\n\n# Model configurations for llama-2 7b for deploying on g5 x and 2x large instances\nexperiments:\n  - name: llama2-7b-g5.2xlarge-bring-your-own-sm-endpoint\n    model_id: # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: llama2-7b-f\n    ep_name: <your-sagemaker-endpoint-name>\n    instance_type: \"ml.g5.2xlarge\"\n    image_uri:\n    deploy: no #setting to no since the endpoint has already been deployed\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\n  - name: llama2-7b-g5.2xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\n    model_id: meta-textgeneration-llama-2-7b-f\n    model_version: \"3.*\"\n    model_name: llama2-7b-f\n    ep_name: llama-2-7b-g5-2xlarge\n    instance_type: \"ml.g5.2xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\nreport:\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n\n================================================"
  },
  {
    "filename": "config-llama2-7b-g4dn-g5-trt.yml",
    "path": "fmbench/configs/llama2/7b/config-llama2-7b-g4dn-g5-trt.yml",
    "directory": "fmbench/configs/llama2/7b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"Llama2-7b-g4dn-g5\"\n  model_name: \"Llama2-7b\"\n \naws:\n  region: {region}\n  sagemaker_execution_role: {role_arn}\n  bucket: {write_bucket}\n\ndir_paths:\n    data_prefix: data\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata\n\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts\n    script_files:\n    - hf_token.txt\n    configs_prefix: configs\n    config_files:\n    - pricing.yml\n    source_data_prefix: source_data\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: tokenizer\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama2.txt\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n \nmetrics:\n  dataset_of_interest: en_3000-3840\n  \npricing: pricing.yml\n  \ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\nexperiments:\n  - name: llama2-7b-g5.xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\n    model_id: meta-textgeneration-llama-2-7b-f\n    model_version: \"3.*\"\n    model_name: llama2-7b-f\n    ep_name: llama-2-7b-g512xlarge\n    instance_type: \"ml.g5.12xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 5\n    - 10\n\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"4\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n  - name: Llama2-7b-g4dn-djl-inference-0.26.0-deepspeed0.12.6-cu121\n    model_id: meta-llama/Llama-2-7b-hf\n    model_version: \"*\"\n    model_name: Llama-2-7b-hf\n    ep_name: Llama-2-7b-hf-g4dn\n    download_from_hf_place_in_s3: yes\n    model_s3_path: s3://{write_bucket}/meta-llama/Llama-2-7b-hf\n    instance_type: \"ml.g4dn.12xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.26.0-deepspeed0.12.6-cu121\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n      engine=Python\n      option.tensor_parallel_degree=4\n      option.model_id=s3://{write_bucket}/meta-llama/Llama-2-7b-hf\n      option.max_rolling_batch_size=64\n      option.rolling_batch=vllm\n      option.dtype=fp16\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 5\n    - 10\n\n    accept_eula: true\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n  latency_vs_token_len_chart: \n    y_ticks:\n    title: \"Effect of token length on inference latency for \\\"meta-llama/Llama-2-70b-hf\\\"\"\n\n\n\n================================================"
  },
  {
    "filename": "config-llama2-7b-g5-no-s3-quick.yml",
    "path": "fmbench/configs/llama2/7b/config-llama2-7b-g5-no-s3-quick.yml",
    "directory": "fmbench/configs/llama2/7b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama2-7b-v1\"      \n  model_name: \"Llama2-7b\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # Use S3 only, local file system only, or both (values are s3, local or both)\n  # If set to local or both, set the local_file_system_path parameter\n  s3_and_or_local_file_system: local\n  local_file_system_path: {write_tmpdir}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # Use S3 only or local file system only (values are s3 or local)\n  # If set to local, set the local_file_system_path parameter\n  s3_or_local_file_system: local\n  local_file_system_path: {read_tmpdir}\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: tokenizer\n  \n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama2.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the \n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_2000-3000\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: llama2-7b-g5.xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: meta-textgeneration-llama-2-7b-f\n    model_version: \"3.*\"\n    model_name: llama2-7b-f\n    ep_name: llama-2-7b-g5xlarge\n    instance_type: \"ml.g5.xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04'\n    deploy: yes\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details    \n    deployment_script: jumpstart.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    #- payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\n  - name: llama2-7b-g5.2xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    model_id: meta-textgeneration-llama-2-7b-f\n    model_version: \"3.*\"\n    model_name: llama2-7b-f\n    ep_name: llama-2-7b-g5-2xlarge\n    instance_type: \"ml.g5.2xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04'\n    deploy: yes\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count: 1\n    deployment_script: jumpstart.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    #- payload_en_3000-3840.jsonl\n    \n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    # Added for models that require accepting a EULA\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\n# parameters related to how the final report is generated\nreport:\n  # markdown report title\n  title: \"Performance benchmarking results for Llama2-7b on `g5.xlarge` and `g5.2xlarge` using the HuggingFace TGI container\"\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n================================================"
  },
  {
    "filename": "config-llama2-7b-g5-quick.yml",
    "path": "fmbench/configs/llama2/7b/config-llama2-7b-g5-quick.yml",
    "directory": "fmbench/configs/llama2/7b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama2-7b-v1\"      \n  model_name: \"Llama2-7b\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n  \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: llama2_tokenizer\n  \n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama2.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the \n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n    \n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1000-2000\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: llama2-7b-g5.xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: meta-textgeneration-llama-2-7b-f\n    # Add the hugging face model id to load the specific tokenizer at runtime.\n    # To load this tokenizer, make sure to provide the hf_token.txt file.\n    hf_tokenizer_model_id: meta-llama/Llama-2-7b\n    model_version: \"3.*\"\n    model_name: llama2-7b-f\n    ep_name: llama-2-7b-g5-xlarge\n    instance_type: \"ml.g5.xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04'\n    deploy: yes\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details    \n    deployment_script: jumpstart.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n      stream: False\n      stop_token: \".</s>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    # - payload_en_2000-3000.jsonl\n    # - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\n  - name: llama2-7b-g5.2xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    model_id: meta-textgeneration-llama-2-7b-f\n    model_version: \"3.*\"\n    model_name: llama2-7b-f\n    ep_name: llama-2-7b-g5-2xlarge\n    instance_type: \"ml.g5.2xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04'\n    deploy: yes\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count: 1\n    deployment_script: jumpstart.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n      stream: False\n      stop_token: \".\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    # - payload_en_2000-3000.jsonl\n    # - payload_en_3000-3840.jsonl\n    \n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n\n    # Added for models that require accepting a EULA\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\n# parameters related to how the final report is generated\nreport:\n  # markdown report title\n  title: \"Performance benchmarking results for Llama2-7b on `g5.xlarge` and `g5.2xlarge` using the HuggingFace TGI container\"\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n\n================================================"
  },
  {
    "filename": "config-llama2-7b-inf2-g5.yml",
    "path": "fmbench/configs/llama2/7b/config-llama2-7b-inf2-g5.yml",
    "directory": "fmbench/configs/llama2/7b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama2-7b-inf2-g5-v1\"      \n  model_name: \"Llama2-7b\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama2.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n \nmetrics:\n  dataset_of_interest: en_3000-3840\n  \npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    return_full_text: False\n\n# Model configurations for llama-2 7b for deploying on g5 and inf2 instances\nexperiments:\n  # llama2-7b neuron on inf2\n  - name: llama2-7b-inf2.8xlarge-djl-inference:0.27.0-neuronx\n    model_id: meta-textgenerationneuron-llama-2-7b-f\n    model_version: \"*\"\n    model_name: llama2-7b\n    ep_name: llama2-7b-inf2-8xl\n    instance_type: \"ml.inf2.8xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-neuronx-sdk2.18.1'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n      OPTION_TENSOR_PARALLEL_DEGREE: \"2\"\n      OPTION_N_POSITIONS: \"4096\"\n      OPTION_DTYPE: \"fp16\"\n      OPTION_ROLLING_BATCH: \"auto\"\n      OPTION_MAX_ROLLING_BATCH_SIZE: \"1\"\n      OPTION_NEURON_OPTIMIZE_LEVEL: \"2\"\n  - name: llama2-7b-inf2.24xlarge-djl-inference:0.27.0-neuronx\n    model_id: meta-textgenerationneuron-llama-2-7b-f\n    model_version: \"*\"\n    model_name: llama2-7b\n    ep_name: llama2-7b-inf2-24xl\n    instance_type: \"ml.inf2.24xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-neuronx-sdk2.18.1'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n      OPTION_TENSOR_PARALLEL_DEGREE: \"2\"\n      OPTION_N_POSITIONS: \"4096\"\n      OPTION_DTYPE: \"fp16\"\n      OPTION_ROLLING_BATCH: \"auto\"\n      OPTION_MAX_ROLLING_BATCH_SIZE: \"1\"\n      OPTION_NEURON_OPTIMIZE_LEVEL: \"2\"\n      \n  # llama2-7b on hf tgi, on g5 instances\n  - name: llama2-7b-g5.2xlarge-huggingface-pytorch-tgi\n    model_id: meta-textgeneration-llama-2-7b-f\n    model_version: \"3.*\"\n    model_name: llama2-7b-f\n    ep_name: llama2-7b-g5-2x\n    instance_type: \"ml.g5.2xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi1.4.5-gpu-py310-cu121-ubuntu22.04-v1.0'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n  - name: llama2-7b-g5.12xlarge-huggingface-pytorch-tgi\n    model_id: meta-textgeneration-llama-2-7b-f\n    model_version: \"3.*\"\n    model_name: llama2-7b-f\n    ep_name: llama2-7b-g5-12x\n    instance_type: \"ml.g5.12xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi1.4.5-gpu-py310-cu121-ubuntu22.04-v1.0'         \n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n      \nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 30\n  error_rate_budget: 0.0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-70b-instruct.yml",
    "path": "fmbench/configs/llama3/70b/config-ec2-llama3-70b-instruct.yml",
    "directory": "fmbench/configs/llama3/70b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-70bdeployed on ec2\ngeneral:\n  name: \"llama3-70b-instruct-g5.48xl-ec2\"      \n  model_name: \"Meta-Llama-3-70B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 924\n    payload_file: payload_en_1-924.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_tokens: 4096\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-70B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-70B-Instruct\n    model_version:\n    model_name: Meta-Llama-3-70B-Instruct\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 24 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 24g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 7200\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3-70B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10 \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-neuron-llama3-70b-inf2-48xl-deploy-sm.yml",
    "path": "fmbench/configs/llama3/70b/config-ec2-neuron-llama3-70b-inf2-48xl-deploy-sm.yml",
    "directory": "fmbench/configs/llama3/70b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-70b-inf2.48xl-ec2\"      \n  model_name: \"llama3-70b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    #stop: '<|eot_id|>'\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-70b-instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    sagemaker_execution_role: {role_arn}\n    # S3 bucket to which metrics, plots and reports would be written to\n    bucket: {write_bucket} ## add the name of your desired bucket\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    prefix: \"lmi\"\n    model_id: meta-llama/Meta-Llama-3-70b-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3-70b-instruct\"\n    model_id_wo_repo: \"Meta-Llama-3-70b-Instruct\"\n    model_id_wo_repo_split: \"Meta-Llama-3-70b-Instruct-split\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name:  \n    instance_type: \"ml.inf2.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: neuron_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: sagemaker_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      batch_size: \"1\"\n      num_neuron_cores: \"24\"\n      neuron_version: \"2.19.1\"\n      model_loading_timeout: \"2400\"\n      instance_count: \"1\"\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=s3://{write_bucket}/lmi/Meta-Llama-3-70b-Instruct/Meta-Llama-3-70b-Instruct-split/\n      option.load_split_model=True\n      option.tensor_parallel_degree=24\n      option.n_positions=4096\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=8\n      option.dtype=fp16\n      option.model_loading_timeout=2400\n      option.neuron_optimize_level=2\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10 \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-70b-instruct-g5-48xl.yml",
    "path": "fmbench/configs/llama3/70b/config-llama3-70b-instruct-g5-48xl.yml",
    "directory": "fmbench/configs/llama3/70b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama3-70b-g5-48xl-v1\"      \n  model_name: \"llama3-70b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_70b_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes \n\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n \nmetrics:\n  dataset_of_interest: en_3000-3840\n  \npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\nexperiments:\n  - name: llama-3-70b-instruct-g5-48xl-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-70b-instruct\n    model_version: \"*\"\n    model_name: llama-3-70b-instruct\n    ep_name: llama-3-70b-instruct-g5-48xl\n    instance_type: \"ml.g5.48xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n\n    accept_eula: true\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      job_queue_size: 1000\n      max_dynamic_batch_size: 1\n      max_batch_delay: 100\n      max_idle_time: 60\n      load_on_devices: *\n      engine: MPI\n      mpi_mode: true\n      option.entryPoint: null\n      option.tensor_parallel_degree: 8\n      option.max_rolling_batch_size: 256\n      option.mpi_mode: true\n      option.model_id: /opt/ml/model\n      option.rolling_batch: lmi-dist\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"8\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"   \n  \nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-70b-instruct-g5-p4d.yml",
    "path": "fmbench/configs/llama3/70b/config-llama3-70b-instruct-g5-p4d.yml",
    "directory": "fmbench/configs/llama3/70b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama3-70b-p4d-g5-v1\"      \n  model_name: \"llama3-70b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_70b_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n \nmetrics:\n  dataset_of_interest: en_3000-3840\n  \npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\nexperiments:\n  - name: llama-3-70b-instruct-g5-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-70b-instruct\n    model_version: \"*\"\n    model_name: llama-3-70b-instruct\n    ep_name: llama-3-70b-instruct-g5\n    instance_type: \"ml.g5.48xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"8\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n      \n  - name: llama-3-70b-instruct-p4d-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-70b-instruct\n    model_version: \"*\"\n    model_name: llama-3-70b-instruct\n    ep_name: llama-3-70b-instruct-p4d\n    instance_type: \"ml.p4d.24xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    - 15\n\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"8\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-70b-instruct-p4d.yml",
    "path": "fmbench/configs/llama3/70b/config-llama3-70b-instruct-p4d.yml",
    "directory": "fmbench/configs/llama3/70b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama3-70b-p4d-v1\"      \n  model_name: \"llama3-70b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_70b_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n \nmetrics:\n  dataset_of_interest: en_3000-3840\n  \npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\nexperiments:\n      \n  - name: llama-3-70b-instruct-p4d-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-70b-instruct\n    model_version: \"*\"\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-70B-Instruct\n    model_name: llama-3-70b-instruct\n    ep_name: llama-3-70b-instruct-p4d\n    instance_type: \"ml.p4d.24xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    - 15\n\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"8\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================\nSYMLINK: fmbench/configs/llama3/70b/config-bedrock.yml -> config-bedrock.yml\n================================================\n\n\n\n================================================"
  },
  {
    "filename": "config-bedrock.yml",
    "path": "fmbench/configs/llama3/8b/config-bedrock.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"fmbench-bedrock\"      \n  model_name: \"FMs available in Amazon Bedrock\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket}\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml ## mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_titan_text.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: no\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: no\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  # - text\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n \nmetrics:\n  dataset_of_interest: en_3000-3840\n  \npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n  \ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  - name: mistral.mistral-7b-instruct-v0:2\n    model_id: mistral.mistral-7b-instruct-v0:2\n    model_version: \"*\"\n    model_name: mistral.mistral-7b-instruct-v0:2\n    ep_name: mistral.mistral-7b-instruct-v0:2\n    instance_type: mistral.mistral-7b-instruct-v0:2\n    image_uri:\n    deploy: no\n    instance_count: 1\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  - name: mistral.mixtral-8x7b-instruct-v0:1\n    model_id: mistral.mixtral-8x7b-instruct-v0:1\n    model_version: \"*\"\n    model_name: mistral.mixtral-8x7b-instruct-v0:1\n    ep_name: mistral.mixtral-8x7b-instruct-v0:1\n    instance_type: mistral.mixtral-8x7b-instruct-v0:1\n    image_uri:\n    deploy: no\n    instance_count: 1\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n    \n\n  - name: meta.llama2-13b-chat-v1\n    model_id: meta.llama2-13b-chat-v1\n    model_version: \"*\"\n    model_name: meta.llama2-13b-chat-v1\n    ep_name: meta.llama2-13b-chat-v1\n    instance_type: meta.llama2-13b-chat-v1\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  - name: meta.llama2-70b-chat-v1\n    model_id: meta.llama2-70b-chat-v1\n    model_version: \"*\"\n    model_name: meta.llama2-70b-chat-v1\n    ep_name: meta.llama2-70b-chat-v1\n    instance_type: meta.llama2-70b-chat-v1\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  - name: amazon.titan-text-lite-v1\n    model_id: amazon.titan-text-lite-v1\n    model_version: \"*\"\n    model_name: amazon.titan-text-lite-v1\n    ep_name: amazon.titan-text-lite-v1\n    instance_type: 'amazon.titan-text-lite-v1'\n    image_uri: \n    deploy: no \n    instance_count: \n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  - name: amazon.titan-text-express-v1\n    model_id: amazon.titan-text-express-v1\n    model_version: \"*\"\n    model_name: amazon.titan-text-express-v1\n    ep_name: amazon.titan-text-express-v1\n    instance_type: amazon.titan-text-express-v1\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env: \n  - name: anthropic.claude-3-sonnet-20240229-v1:0\n    model_id: anthropic.claude-3-sonnet-20240229-v1:0\n    model_version: \"*\"\n    model_name: anthropic.claude-3-sonnet-20240229-v1:0\n    ep_name: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_type: anthropic.claude-3-sonnet-20240229-v1:0\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  - name: anthropic.claude-3-haiku-20240307-v1:0\n    model_id: anthropic.claude-3-haiku-20240307-v1:0\n    model_version: \"*\"\n    model_name: anthropic.claude-3-haiku-20240307-v1:0\n    ep_name: anthropic.claude-3-haiku-20240307-v1:0\n    instance_type: anthropic.claude-3-haiku-20240307-v1:0\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  - name: cohere.command-text-v14\n    model_id: cohere.command-text-v14\n    model_version: \"*\"\n    model_name: cohere.command-text-v14\n    ep_name: cohere.command-text-v14\n    instance_type: cohere.command-text-v14\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  - name: cohere.command-light-text-v14\n    model_id: cohere.command-light-text-v14\n    model_version: \"*\"\n    model_name: cohere.command-light-text-v14\n    ep_name: cohere.command-light-text-v14\n    instance_type: cohere.command-light-text-v14\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  - name: ai21.j2-mid-v1\n    model_id: ai21.j2-mid-v1\n    model_version: \"*\"\n    model_name: ai21.j2-mid-v1\n    ep_name: ai21.j2-mid-v1\n    instance_type: ai21.j2-mid-v1\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  - name: ai21.j2-ultra-v1\n    model_id: ai21.j2-ultra-v1\n    model_version: \"*\"\n    model_name: ai21.j2-ultra-v1\n    ep_name: ai21.j2-ultra-v1\n    instance_type: ai21.j2-ultra-v1\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n    \nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-8b-c5-18xlarge.yml",
    "path": "fmbench/configs/llama3/8b/config-ec2-llama3-8b-c5-18xlarge.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-c5.18xl-ec2\"      \n  model_name: \"llama3-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_vllm:\n    model: meta-llama/Meta-Llama-3-8B-Instruct\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-8b-instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3-8b-instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8000/v1/completions' \n    instance_type: \"c5.18xlarge\"\n    image_uri: vllm-cpu-env\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # Privileged Mode makes the docker container run with root. \n      # This basically means that if you are root in a container you have the privileges of root on the host system\n      # Only need this if you need to set VLLM_CPU_OMP_THREADS_BIND env variable.\n      privileged_mode: yes      \n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting:\n      # This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_vllm\n      # if not set assume djl\n      container_type: vllm\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      MODEL_LOADING_TIMEOUT: 2400\n      VLLM_CPU_KVCACHE_SPACE: 40\n      # This instance has 72 CPUs, and we are allocating 70 of them to run this container.\n      # For more information, visit the following URL:\n      # https://docs.vllm.ai/en/latest/getting_started/cpu-installation.html#related-runtime-environment-variables\n      VLLM_CPU_OMP_THREADS_BIND: 0-69\n\nreport:\n  latency_budget: 25\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-8b-c8g-24xlarge.yml",
    "path": "fmbench/configs/llama3/8b/config-ec2-llama3-8b-c8g-24xlarge.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-c8g.24xl-ec2\"      \n  model_name: \"llama3-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_vllm:\n    model: meta-llama/Meta-Llama-3-8B-Instruct\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-8b-instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3-8b-instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8000/v1/completions' \n    instance_type: \"c8g.24xlarge\"\n    image_uri: vllm-cpu-env\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # Privileged Mode makes the docker container run with root. \n      # This basically means that if you are root in a container you have the privileges of root on the host system\n      # Only need this if you need to set VLLM_CPU_OMP_THREADS_BIND env variable.\n      privileged_mode: yes      \n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting:\n      # This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_vllm\n      # if not set assume djl\n      container_type: vllm\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      MODEL_LOADING_TIMEOUT: 2400\n      # This instance is equipped with 96 CPUs, and we are allocating 93 of them to run this container.\n      # For additional details, refer to the following URL:\n      # https://docs.vllm.ai/en/latest/getting_started/cpu-installation.html#related-runtime-environment-variables\n      VLLM_CPU_OMP_THREADS_BIND: 0-92\n\nreport:\n  latency_budget: 35\n  cost_per_10k_txn_budget: 60\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-8b-g6e-2xlarge.yml",
    "path": "fmbench/configs/llama3/8b/config-ec2-llama3-8b-g6e-2xlarge.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.2xl-ec2\"      \n  model_name: \"llama3-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-8b-instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3-8b-instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g6e.2xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      tp_degree: 1 \n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=1\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-8b-inf2-48xl.yml",
    "path": "fmbench/configs/llama3/8b/config-ec2-llama3-8b-inf2-48xl.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-8b-inf2.48xl-ec2\"      \n  model_name: \"llama3-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 924\n    payload_file: payload_en_1-924.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-8b-instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3-8b-instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"inf2.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.28.0-neuronx-sdk2.18.2\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and Inferentia settings for the instance\n      # '-u djl' tells the container to use the Deep Java Library (DJL) runtime\n      # '--device /dev/neuron0 ... /dev/neuron5' specifies the Inferentia devices to be used\n      gpu_or_neuron_setting: -u djl --device /dev/neuron0 --device /dev/neuron1 --device /dev/neuron2  --device /dev/neuron3  --device /dev/neuron4  --device /dev/neuron5 --device /dev/neuron6 --device /dev/neuron7 --device /dev/neuron8 --device /dev/neuron9 --device /dev/neuron10 --device /dev/neuron11 \n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.load_split_model=True\n      option.tensor_parallel_degree=8\n      option.n_positions=4096\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=8\n      option.dtype=fp16\n      option.model_loading_timeout=2400\n      option.neuron_optimize_level=3\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10 \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-8b-m5-16xlarge.yml",
    "path": "fmbench/configs/llama3/8b/config-ec2-llama3-8b-m5-16xlarge.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-m5.16xl-ec2\"      \n  model_name: \"llama3-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_vllm:\n    model: meta-llama/Meta-Llama-3-8B-Instruct\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-8b-instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3-8b-instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8000/v1/completions' \n    instance_type: \"m5.16xlarge\"\n    image_uri: vllm-cpu-env\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # Privileged Mode makes the docker container run with root. \n      # This basically means that if you are root in a container you have the privileges of root on the host system\n      # Only need this if you need to set VLLM_CPU_OMP_THREADS_BIND env variable.\n      privileged_mode: yes      \n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting:\n      # This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_vllm\n      # if not set assume djl\n      container_type: vllm\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      MODEL_LOADING_TIMEOUT: 2400\n      VLLM_CPU_KVCACHE_SPACE: 40\n      # This instance is equipped with 64 CPUs, and we are allocating 60 of them to run this container.\n      # For additional details, refer to the following URL:\n      # https://docs.vllm.ai/en/latest/getting_started/cpu-installation.html#related-runtime-environment-variables\n      VLLM_CPU_OMP_THREADS_BIND: 0-59\n\nreport:\n  latency_budget: 35\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-8b-m7a-16xlarge.yml",
    "path": "fmbench/configs/llama3/8b/config-ec2-llama3-8b-m7a-16xlarge.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-m7a.16xl-ec2\"      \n  model_name: \"llama3-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_vllm:\n    model: meta-llama/Meta-Llama-3-8B-Instruct\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-8b-instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3-8b-instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8000/v1/completions' \n    instance_type: \"m7a.16xlarge\"\n    image_uri: vllm-cpu-env\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:      \n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting:\n      # This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_vllm\n      # if not set assume djl\n      container_type: vllm\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      MODEL_LOADING_TIMEOUT: 2400\n      VLLM_CPU_KVCACHE_SPACE: 40\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-8b-m7a-24xlarge.yml",
    "path": "fmbench/configs/llama3/8b/config-ec2-llama3-8b-m7a-24xlarge.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-m7a.24xl-ec2\"      \n  model_name: \"llama3-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_vllm:\n    model: meta-llama/Meta-Llama-3-8B-Instruct\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-8b-instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3-8b-instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8000/v1/completions' \n    instance_type: \"m7a.24xlarge\"\n    image_uri: vllm-cpu-env\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # Privileged Mode makes the docker container run with root. \n      # This basically means that if you are root in a container you have the privileges of root on the host system\n      # Only need this if you need to set VLLM_CPU_OMP_THREADS_BIND env variable.\n      privileged_mode: yes      \n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting:\n      # This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_vllm\n      # if not set assume djl\n      container_type: vllm\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      MODEL_LOADING_TIMEOUT: 2400\n      VLLM_CPU_KVCACHE_SPACE: 192\n      # This instance is equipped with 96 CPUs, and we are allocating 93 of them to run this container.\n      # For additional details, refer to the following URL:\n      # https://docs.vllm.ai/en/latest/getting_started/cpu-installation.html#related-runtime-environment-variables\n      VLLM_CPU_OMP_THREADS_BIND: 0-92\n\nreport:\n  latency_budget: 35\n  cost_per_10k_txn_budget: 60\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-8b-m7i-12xlarge.yml",
    "path": "fmbench/configs/llama3/8b/config-ec2-llama3-8b-m7i-12xlarge.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-m7i.12xl-ec2\"      \n  model_name: \"llama3-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_vllm:\n    model: meta-llama/Meta-Llama-3-8B-Instruct\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-8b-instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3-8b-instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8000/v1/completions' \n    instance_type: \"m7i.12xlarge\"\n    image_uri: vllm-cpu-env\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # Privileged Mode makes the docker container run with root. \n      # This basically means that if you are root in a container you have the privileges of root on the host system\n      # Only need this if you need to set VLLM_CPU_OMP_THREADS_BIND env variable.\n      privileged_mode: yes      \n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting:\n      # This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_vllm\n      # if not set assume djl\n      container_type: vllm\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      VLLM_CPU_KVCACHE_SPACE: 40\n      # This instance has 48 CPUs, and we are allocating 40 of them to run this container.\n      # For more information, visit the following URL:\n      # https://docs.vllm.ai/en/latest/getting_started/cpu-installation.html#related-runtime-environment-variables\n      VLLM_CPU_OMP_THREADS_BIND: 0-40\n\nreport:\n  latency_budget: 25\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-8b-m7i-16xlarge.yml",
    "path": "fmbench/configs/llama3/8b/config-ec2-llama3-8b-m7i-16xlarge.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-m7i.16xl-ec2\"      \n  model_name: \"llama3-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_vllm:\n    model: meta-llama/Meta-Llama-3-8B-Instruct\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-8b-instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3-8b-instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8000/v1/completions' \n    instance_type: \"m7i.16xlarge\"\n    image_uri: vllm-cpu-env\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # Privileged Mode makes the docker container run with root. \n      # This basically means that if you are root in a container you have the privileges of root on the host system\n      # Only need this if you need to set VLLM_CPU_OMP_THREADS_BIND env variable.\n      privileged_mode: yes      \n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting:\n      # This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_vllm\n      # if not set assume djl\n      container_type: vllm\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      VLLM_CPU_KVCACHE_SPACE: 40\n      # This instance has 48 CPUs, and we are allocating 40 of them to run this container.\n      # For more information, visit the following URL:\n      # https://docs.vllm.ai/en/latest/getting_started/cpu-installation.html#related-runtime-environment-variables\n      VLLM_CPU_OMP_THREADS_BIND: 0-40\n\nreport:\n  latency_budget: 25\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-8b-m7i-24xlarge.yml",
    "path": "fmbench/configs/llama3/8b/config-ec2-llama3-8b-m7i-24xlarge.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-m7i.24xl-ec2\"      \n  model_name: \"llama3-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_vllm:\n    model: meta-llama/Meta-Llama-3-8B-Instruct\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-8b-instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3-8b-instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8000/v1/completions' \n    instance_type: \"m7i.24xlarge\"\n    image_uri: vllm-cpu-env\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # Privileged Mode makes the docker container run with root. \n      # This basically means that if you are root in a container you have the privileges of root on the host system\n      # Only need this if you need to set VLLM_CPU_OMP_THREADS_BIND env variable.\n      privileged_mode: yes      \n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting:\n      # This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_vllm\n      # if not set assume djl\n      container_type: vllm\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    #- 2\n    #- 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      VLLM_CPU_KVCACHE_SPACE: 40\n      # This instance has 48 CPUs, and we are allocating 40 of them to run this container.\n      # For more information, visit the following URL:\n      # https://docs.vllm.ai/en/latest/getting_started/cpu-installation.html#related-runtime-environment-variables\n      VLLM_CPU_OMP_THREADS_BIND: 0-40\n\nreport:\n  latency_budget: 25\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-8b-neuron-trn1-32xl-tp16-sm.yml",
    "path": "fmbench/configs/llama3/8b/config-ec2-llama3-8b-neuron-trn1-32xl-tp16-sm.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"config-llama3-8b-trn1-32xl-tp=16-bs=4\"      \n  model_name: \"llama3-8b\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data #/Open-Orca\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  #- OpenOrca.jsonl\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: llama3_tokenizer\n  \n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n  # prompt_template_Open-Orca-OpenOrca-llama3.txt\n\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the \n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  #- system_prompt\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: llama-3-8b-instruct-trn1-djl-inference:0.27.0-neuronx-sdk2.18.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.  \n    region: {region}\n    sagemaker_execution_role: {role_arn}\n    # S3 bucket to which metrics, plots and reports would be written to\n    bucket: {write_bucket} ## add the name of your desired bucket\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    prefix: \"lmi\"\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3-8b-instruct\"\n    model_id_wo_repo: \"Meta-Llama-3-8B-Instruct\"\n    model_id_wo_repo_split: \"Meta-Llama-3-8B-Instruct-split\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name:  \n    instance_type: \"ml.trn1.32xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: neuron_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: sagemaker_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      batch_size: \"1\"\n      num_neuron_cores: \"16\"\n      neuron_version: \"2.19.1\"\n      model_loading_timeout: \"2400\"\n      instance_count: \"1\"\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=s3://{write_bucket}/lmi/Meta-Llama-3-8B-Instruct/Meta-Llama-3-8B-Instruct-split/\n      option.load_split_model=True\n      option.tensor_parallel_degree=16\n      option.n_positions=4096\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=4\n      option.dtype=fp16\n      option.model_loading_timeout=2400\n  \n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # currently the llama3-8b jumpstart model is compiled with a batch size of 1\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 11\n    - 13\n    - 15\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      model_id_or_path='/tmp/.djl.ai/download/ae03dd100c208acd82b5dbed563c971de864c408'\n      rolling_batch=<RollingBatchEnum.auto: 'auto'>\n      tensor_parallel_degree=16\n      trust_remote_code=False\n      enable_streaming=<StreamingEnum.false: 'false'>\n      batch_size=4\n      max_rolling_batch_size=4\n      dtype=<Dtype.f16: 'fp16'>\n      revision=None\n      output_formatter=None\n      waiting_steps=None\n      is_mpi=False\n      draft_model_id=None\n      spec_length=0\n      neuron_optimize_level=None\n      enable_mixed_precision_accumulation=False\n      enable_saturate_infinity=False\n      n_positions=4096\n      unroll=None\n      load_in_8bit=False\n      low_cpu_mem_usage=False\n      load_split_model=True\n      context_length_estimate=None\n      amp='f16'\n      quantize=None\n      compiled_graph_path=None\n      task=None\n      save_mp_checkpoint_path=None\n      group_query_attention=None\n      model_loader=<TnXModelLoaders.tnx: 'tnx'>\n      rolling_batch_strategy=<TnXGenerationStrategy.continuous_batching: 'continuous_batching'>\n      fuse_qkv=False\n      on_device_embedding=False\n      attention_layout=None\n      collectives_layout=None\n      cache_layout=None\n      partition_schema=None\n      all_reduce_dtype=None\n      cast_logits_dtype=None\n\n# parameters related to how the final report is generated    \nreport:\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-8b-p4d-tp-2-mc-max.yml",
    "path": "fmbench/configs/llama3/8b/config-ec2-llama3-8b-p4d-tp-2-mc-max.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-p4d.24xl-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"p4d.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 10\n    - 30\n    - 35\n    - 40\n    - 50\n    - 60\n    - 65\n    - 70\n    - 75\n    - 100\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-8b-p4d-tp-4-mc-max.yml",
    "path": "fmbench/configs/llama3/8b/config-ec2-llama3-8b-p4d-tp-4-mc-max.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-p4d.24xl-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"p4d.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 10\n    - 30\n    - 35\n    - 40\n    - 50\n    - 60\n    - 65\n    - 70\n    - 75\n    - 100\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-8b-p4d-tp-8-mc-max.yml",
    "path": "fmbench/configs/llama3/8b/config-ec2-llama3-8b-p4d-tp-8-mc-max.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-p4d.24xl-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"p4d.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 10\n    - 30\n    - 35\n    - 40\n    - 50\n    - 60\n    - 65\n    - 70\n    - 75\n    - 100\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-8b-p4de-tp-2-mc-max.yml",
    "path": "fmbench/configs/llama3/8b/config-ec2-llama3-8b-p4de-tp-2-mc-max.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-p4de.24xl-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"p4de.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 10\n    - 30\n    - 35\n    - 40\n    - 50\n    - 60\n    - 65\n    - 70\n    - 75\n    - 100\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-8b-p4de-tp-4-mc-max.yml",
    "path": "fmbench/configs/llama3/8b/config-ec2-llama3-8b-p4de-tp-4-mc-max.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-p4de.24xl-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"p4de.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 10\n    - 30\n    - 35\n    - 40\n    - 50\n    - 60\n    - 65\n    - 70\n    - 75\n    - 100\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-8b-p4de-tp-8-mc-max.yml",
    "path": "fmbench/configs/llama3/8b/config-ec2-llama3-8b-p4de-tp-8-mc-max.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-p4de.24xl-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"p4de.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 10\n    - 30\n    - 35\n    - 40\n    - 50\n    - 60\n    - 65\n    - 70\n    - 75\n    - 100\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-8b-p5-tp-2-mc-max.yml",
    "path": "fmbench/configs/llama3/8b/config-ec2-llama3-8b-p5-tp-2-mc-max.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-p5.2xl-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 924\n    payload_file: payload_en_1-924.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"p5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 10\n    - 30\n    - 50\n    - 60\n    - 65\n    - 70\n    - 75\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-8b-p5-tp-8-mc-auto.yml",
    "path": "fmbench/configs/llama3/8b/config-ec2-llama3-8b-p5-tp-8-mc-auto.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-p5.48xl-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 924\n    payload_file: payload_en_1-924.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"p5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: auto\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 10\n    - 30\n    - 40\n    - 50\n    - 60\n    - 65\n    - 70\n    - 75\n    - 100\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-8b.yml",
    "path": "fmbench/configs/llama3/8b/config-ec2-llama3-8b.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g5.2xl-djl-ec2\"      \n  model_name: \"llama3-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-8b-instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3-8b-instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g5.2xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 1\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=1\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10 \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-neuron-llama3-8b-inf2-24xl-deploy-sm.yml",
    "path": "fmbench/configs/llama3/8b/config-ec2-neuron-llama3-8b-inf2-24xl-deploy-sm.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-8b-inf2.24xl-ec2\"      \n  model_name: \"llama3-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    #stop: '<|eot_id|>'\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-8b-instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    sagemaker_execution_role: {role_arn}\n    # S3 bucket to which metrics, plots and reports would be written to\n    bucket: {write_bucket} ## add the name of your desired bucket\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    prefix: \"lmi\"\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3-8b-instruct\"\n    model_id_wo_repo: \"Meta-Llama-3-8B-Instruct\"\n    model_id_wo_repo_split: \"Meta-Llama-3-8B-Instruct-split\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name:  \n    instance_type: \"ml.inf2.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.28.0-neuronx-sdk2.18.2\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: neuron_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: sagemaker_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      batch_size: \"4\"\n      num_neuron_cores: \"8\"\n      neuron_version: \"2.18\"\n      model_loading_timeout: \"2400\"\n      instance_count: \"1\"\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=s3://{write_bucket}/lmi/Meta-Llama-3-8B-Instruct/Meta-Llama-3-8B-Instruct-split/\n      option.load_split_model=True\n      option.tensor_parallel_degree=8\n      option.n_positions=4096\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=8\n      option.dtype=fp16\n      option.model_loading_timeout=2400\n      option.neuron_optimize_level=3\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10 \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-neuron-llama3-8b-inf2-48xl-deploy-sm.yml",
    "path": "fmbench/configs/llama3/8b/config-ec2-neuron-llama3-8b-inf2-48xl-deploy-sm.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-8b-inf2.48xl-ec2\"      \n  model_name: \"llama3-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    #stop: '<|eot_id|>'\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-8b-instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    sagemaker_execution_role: {role_arn}\n    # S3 bucket to which metrics, plots and reports would be written to\n    bucket: {write_bucket} ## add the name of your desired bucket\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    prefix: \"lmi\"\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3-8b-instruct\"\n    model_id_wo_repo: \"Meta-Llama-3-8B-Instruct\"\n    model_id_wo_repo_split: \"Meta-Llama-3-8B-Instruct-split\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name:  \n    instance_type: \"ml.inf2.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: neuron_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: sagemaker_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      batch_size: \"1\"\n      num_neuron_cores: \"24\"\n      neuron_version: \"2.19.1\"\n      model_loading_timeout: \"2400\"\n      instance_count: \"1\"\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=s3://{write_bucket}/lmi/Meta-Llama-3-8B-Instruct/Meta-Llama-3-8B-Instruct-split/\n      option.load_split_model=True\n      option.tensor_parallel_degree=24\n      option.n_positions=4096\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=8\n      option.dtype=fp16\n      option.model_loading_timeout=2400\n      option.neuron_optimize_level=2\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10 \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-eks-inf2.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-eks-inf2.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama3-8b-eks-inf2\"\n  model_name: \"llama3-8b\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: llama3_tokenizer\n  \n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the \n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1-500\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  eks_parameter_set:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: Llama-3-8B-Instruct-eks\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.  \n    model_id: Llama-3-8B-Instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: Llama-3-8B-Instruct\n    # the endpoint name is not needed, since it is created dynamically and used\n    # the deploy script for eks stores the endpoint url that is saved and used in the \n    # rest of the test\n    ep_name: \n    instance_type: \"ml.inf2.24xlarge\"\n    image_uri: 'public.ecr.aws/data-on-eks/ray-serve-inf2-llama3:latest'\n    deploy: yes\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script: eks_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: eks_predictor.py\n    # refers to the eks custom parameters for the model\n    eks:\n      # This is the cluster created on Kubernetes\n      # using do-eks: https://github.com/awslabs/data-on-eks/tree/main\n      # The cluster is created using the github repository above. If there\n      # is a cluster that you create, that is out of this repository or has a different\n      # name, replace the cluster name below with your custom cluster name\n      eks_cluster_name: trainium-inferentia\n      # Represents the logical grouping of EKS resources\n      # for the mistral model. All kubernetes resources related\n      # to mistral will be in this namespace. Change this to \n      # llama3/llama2 for your use case\n      eks_model_namespace: llama3 \n      # name of the manifest directory where all of the EKS manifest files\n      # reside\n      manifest_dir: configs/eks_manifests\n      # this is the yaml file to deploy the mistral model\n      manifest_file: llama3-ray-service.yaml\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: eks_parameter_set\n      # this is the url format that gets appended to the \n      # model endpoint URL to run inferences from\n      inference_url_format: /serve/infer?sentence=\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # currently the llama3-8b jumpstart model is compiled with a batch size of 1\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\n# parameters related to how the final report is generated    \nreport:\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 2\n  cost_per_10k_txn_budget: 30\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n  latency_vs_token_len_chart: \n    y_ticks: \n    # - 1\n    # - 2\n    # - 3\n    # - 4\n    title: \"Effect of token length on inference latency for \\\"Llama-3-8B-Instruct-eks\\\"\"\n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-g5-streaming.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-g5-streaming.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama3-8b-g5-inf2-stream\"      \n  model_name: \"llama3-8b\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: llama3_tokenizer\n  \n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the \n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: llama-3-8b-instruct-g5-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct-g5.12xl\n    ep_name: llama-3-8b-instruct-g5-12xl\n    instance_type: \"ml.g5.12xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n      stream: True\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 8\n    - 10\n\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n  - name: llama-3-8b-instruct-inf2-djl-inference:0.27.0-neuronx-sdk2.18.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.  \n    model_id: meta-textgenerationneuron-llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-inf2.48xl\n    ep_name: llama-3-8b-instruct-inf2-48xl\n    instance_type: \"ml.inf2.48xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-neuronx-sdk2.18.1'\n    deploy: yes\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script: jumpstart.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n      stream: True\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # currently the llama3-8b jumpstart model is compiled with a batch size of 1\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 8\n    - 10\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: '1'\n      SERVING_LOAD_MODELS: 'test::Python=/opt/ml/model'\n      OPTION_ENTRYPOINT: 'inference.py'\n      OPTION_TENSOR_PARALLEL_DEGREEL: '24'\n      OPTION_N_POSITIONS: '8192'\n      OPTION_ROLLING_BATCH: 'auto'\n      OPTION_LOAD_SPLIT_MODEL: 'False'\n      OPTION_NEURON_OPTIMIZE_LEVEL: '2'\n      OPTION_DTYPE: 'bf16'\n      OPTION_MAX_ROLLING_BATCH_SIZE: '8'\n      OPTION_MODEL_LOADING_TIMEOUT: '6000'\n      OPTION_TRUST_REMOTE_CODE: 'true'\n      OPTION_FUSE_QKV: 'true'\n      OPTION_ATTENTION_LAYOUT: 'BSH'\n      OPTION_GROUP_QUERY_ATTENTION: 'replicated-heads'\n\n# parameters related to how the final report is generated    \nreport:\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 5\n  cost_per_10k_txn_budget: 30\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-g5.12xl-tp-2-mc-max-djl-ec2.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-g5.12xl-tp-2-mc-max-djl-ec2.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g5.12xl-tp=2-mc=max-djl-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g5.12xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-g5.12xl-tp-2-mc-max-triton-ec2.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-g5.12xl-tp-2-mc-max-triton-ec2.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g5.12xl-tp=2-mc=max-triton-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    temperature: 0.1\n    do_sample: yes\n    top_p: 0.92\n    top_k: 120  \n    bad_words: \"\"\n    stop_words: \"<|eot_id|>\"\n    pad_id: 2\n    end_id: 2\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: http://localhost:8080/v2/models/ensemble/generate\n    instance_type: \"g5.12xlarge\"\n    image_uri: nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      batch_size: 64\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-g5.12xl-tp-4-mc-max-djl-ec2.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-g5.12xl-tp-4-mc-max-djl-ec2.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g5.12xl-tp=4-mc=max-djl-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g5.12xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-g5.12xl-tp-4-mc-max-triton-ec2.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-g5.12xl-tp-4-mc-max-triton-ec2.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g5.12xl-tp=4-mc=max-triton-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    temperature: 0.1\n    do_sample: yes\n    top_p: 0.92\n    top_k: 120  \n    bad_words: \"\"\n    stop_words: \"<|eot_id|>\"\n    pad_id: 2\n    end_id: 2\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: http://localhost:8080/v2/models/ensemble/generate\n    instance_type: \"g5.12xlarge\"\n    image_uri: nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n      batch_size: 256\n\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-g5.48xl-tp-8-mc-max-triton-ec2.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-g5.48xl-tp-8-mc-max-triton-ec2.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g5.48xl-tp=8-mc=max-triton-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    temperature: 0.1\n    do_sample: yes\n    top_p: 0.92\n    top_k: 120  \n    bad_words: \"\"\n    stop_words: \"<|eot_id|>\"\n    pad_id: 2\n    end_id: 2\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: http://localhost:8080/v2/models/ensemble/generate\n    instance_type: \"g5.48xlarge\"\n    image_uri: nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n      batch_size: 256\n\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-g5.4xl-tp-1-mc-max-triton-ec2.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-g5.4xl-tp-1-mc-max-triton-ec2.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g5.4xl-tp=1-mc=max-triton-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    temperature: 0.1\n    do_sample: yes\n    top_p: 0.92\n    top_k: 120  \n    bad_words: \"\"\n    stop_words: \"<|eot_id|>\"\n    pad_id: 2\n    end_id: 2\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: http://localhost:8080/v2/models/ensemble/generate\n    instance_type: \"g5.4xlarge\"\n    image_uri: nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 1\n      shm_size: 12g\n      batch_size: 64\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-g6e.12xl-tp-2-mc-max-djl-ec2.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-g6e.12xl-tp-2-mc-max-djl-ec2.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.12xl-tp=2-mc=max-djl-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g6e.12xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-g6e.12xl-tp-2-mc-max-triton-ec2.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-g6e.12xl-tp-2-mc-max-triton-ec2.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.12xl-tp=2-mc=max-triton-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    temperature: 0.1\n    do_sample: yes\n    top_p: 0.92\n    top_k: 120  \n    bad_words: \"\"\n    stop_words: \"<|eot_id|>\"\n    pad_id: 2\n    end_id: 2\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: http://localhost:8080/v2/models/ensemble/generate\n    instance_type: \"g6e.12xlarge\"\n    image_uri: nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      batch_size: 64\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-g6e.12xl-tp-4-mc-max-djl-ec2.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-g6e.12xl-tp-4-mc-max-djl-ec2.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.12xl-tp=4-mc=max-djl-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g6e.12xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-g6e.12xl-tp-4-mc-max-triton-ec2.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-g6e.12xl-tp-4-mc-max-triton-ec2.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.12xl-tp=4-mc=max-triton-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    temperature: 0.1\n    do_sample: yes\n    top_p: 0.92\n    top_k: 120  \n    bad_words: \"\"\n    stop_words: \"<|eot_id|>\"\n    pad_id: 2\n    end_id: 2\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: http://localhost:8080/v2/models/ensemble/generate\n    instance_type: \"g6e.12xlarge\"\n    image_uri: nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n      batch_size: 256\n\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-g6e.24xl-tp-2-mc-max-djl-ec2.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-g6e.24xl-tp-2-mc-max-djl-ec2.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.24xl-tp=2-mc=max-djl-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g6e.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-g6e.24xl-tp-2-mc-max-triton-ec2.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-g6e.24xl-tp-2-mc-max-triton-ec2.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.24xl-tp=2-mc=max-triton-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    temperature: 0.1\n    do_sample: yes\n    top_p: 0.92\n    top_k: 120  \n    bad_words: \"\"\n    stop_words: \"<|eot_id|>\"\n    pad_id: 2\n    end_id: 2\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: http://localhost:8080/v2/models/ensemble/generate\n    instance_type: \"g6e.24xlarge\"\n    image_uri: nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      batch_size: 64\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-g6e.24xl-tp-4-mc-max-djl-ec2.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-g6e.24xl-tp-4-mc-max-djl-ec2.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.24xl-tp=4-mc=max-djl-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g6e.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-g6e.24xl-tp-4-mc-max-triton-ec2.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-g6e.24xl-tp-4-mc-max-triton-ec2.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.24xl-tp=4-mc=max-triton-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    temperature: 0.1\n    do_sample: yes\n    top_p: 0.92\n    top_k: 120  \n    bad_words: \"\"\n    stop_words: \"<|eot_id|>\"\n    pad_id: 2\n    end_id: 2\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: http://localhost:8080/v2/models/ensemble/generate\n    instance_type: \"g6e.24xlarge\"\n    image_uri: nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n      batch_size: 256\n\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-g6e.2xl-tp-1-mc-max-triton-ec2.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-g6e.2xl-tp-1-mc-max-triton-ec2.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.2xl-tp=1-mc=max-triton-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    temperature: 0.1\n    do_sample: yes\n    top_p: 0.92\n    top_k: 120  \n    bad_words: \"\"\n    stop_words: \"<|eot_id|>\"\n    pad_id: 2\n    end_id: 2\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: http://localhost:8080/v2/models/ensemble/generate\n    instance_type: \"g6e.2xlarge\"\n    image_uri: nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 1\n      shm_size: 12g\n      batch_size: 64\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-g6e.48xl-tp-2-mc-max-djl-ec2.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-g6e.48xl-tp-2-mc-max-djl-ec2.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.48xl-tp=2-mc=max-djl-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g6e.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-g6e.48xl-tp-2-mc-max-triton-ec2.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-g6e.48xl-tp-2-mc-max-triton-ec2.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.48xl-tp=2-mc=max-triton-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    temperature: 0.1\n    do_sample: yes\n    top_p: 0.92\n    top_k: 120  \n    bad_words: \"\"\n    stop_words: \"<|eot_id|>\"\n    pad_id: 2\n    end_id: 2\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: http://localhost:8080/v2/models/ensemble/generate\n    instance_type: \"g6e.48xlarge\"\n    image_uri: nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      batch_size: 64\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n    - 11\n    - 12\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-g6e.48xl-tp-4-mc-max-djl-ec2.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-g6e.48xl-tp-4-mc-max-djl-ec2.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.48xl-tp=4-mc=max-djl-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g6e.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-g6e.48xl-tp-4-mc-max-triton-ec2.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-g6e.48xl-tp-4-mc-max-triton-ec2.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.48xl-tp=4-mc=max-triton-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    temperature: 0.1\n    do_sample: yes\n    top_p: 0.92\n    top_k: 120  \n    bad_words: \"\"\n    stop_words: \"<|eot_id|>\"\n    pad_id: 2\n    end_id: 2\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: http://localhost:8080/v2/models/ensemble/generate\n    instance_type: \"g6e.48xlarge\"\n    image_uri: nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n      batch_size: 256\n\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-g6e.48xl-tp-8-mc-max-djl-ec2.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-g6e.48xl-tp-8-mc-max-djl-ec2.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.48xl-tp=4-mc=max-djl-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g6e.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-g6e.48xl-tp-8-mc-max-triton-ec2.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-g6e.48xl-tp-8-mc-max-triton-ec2.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.48xl-tp=4-mc=max-triton-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    temperature: 0.1\n    do_sample: yes\n    top_p: 0.92\n    top_k: 120  \n    bad_words: \"\"\n    stop_words: \"<|eot_id|>\"\n    pad_id: 2\n    end_id: 2\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: http://localhost:8080/v2/models/ensemble/generate\n    instance_type: \"g6e.48xlarge\"\n    image_uri: nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n      batch_size: 256\n\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-g6e.4xl-tp-1-mc-max-djl-ec2.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-g6e.4xl-tp-1-mc-max-djl-ec2.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.4xl-ec2\"      \n  model_name: \"llama3-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-8b-instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3-8b-instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g6e.4xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      tp_degree: 1\n      shm_size: 12g\n      model_loading_timeout: 2400\n      model_copies: max\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=1\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-g6e.4xl-tp-1-mc-max-triton-ec2.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-g6e.4xl-tp-1-mc-max-triton-ec2.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.4xl-tp=1-mc=max-triton-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    temperature: 0.1\n    do_sample: yes\n    top_p: 0.92\n    top_k: 120  \n    bad_words: \"\"\n    stop_words: \"<|eot_id|>\"\n    pad_id: 2\n    end_id: 2\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: http://localhost:8080/v2/models/ensemble/generate\n    instance_type: \"g6e.4xlarge\"\n    image_uri: nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 1\n      shm_size: 12g\n      batch_size: 64\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-g6e.xl-tp-1-mc-max-triton-ec2.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-g6e.xl-tp-1-mc-max-triton-ec2.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.xl-tp=1-mc=max-triton-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    temperature: 0.1\n    do_sample: yes\n    top_p: 0.92\n    top_k: 120  \n    bad_words: \"\"\n    stop_words: \"<|eot_id|>\"\n    pad_id: 2\n    end_id: 2\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: http://localhost:8080/v2/models/ensemble/generate\n    instance_type: \"g6e.xlarge\"\n    image_uri: nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 1\n      shm_size: 12g\n      batch_size: 64\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-inf2-24xl-tp=8-bs=4-byoe.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-inf2-24xl-tp=8-bs=4-byoe.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"config-llama3-8b-inf2-24xl-tp=8-bs=4-byoe\"      \n  model_name: \"llama3-8b\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data #/Open-Orca\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  #- OpenOrca.jsonl\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: llama3_tokenizer\n  \n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n  # prompt_template_Open-Orca-OpenOrca-llama3.txt\n\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the \n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  #- system_prompt\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    #stop: '<|eot_id|>'\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: llama-3-8b-instruct-inf2-djl-inference:0.27.0-neuronx-sdk2.18.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.  \n    model_id: llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version:\n    model_name: llama3-8b-inf2.24xl\n    ep_name: <your-endpoint-name>\n    instance_type: \"ml.inf2.24xlarge\"\n    image_uri: \n    deploy: no\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script: \n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # currently the llama3-8b jumpstart model is compiled with a batch size of 1\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 15\n    - 20\n    - 30\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      model_id_or_path='/tmp/.djl.ai/download/ae03dd100c208acd82b5dbed563c971de864c408'\n      rolling_batch=<RollingBatchEnum.auto: 'auto'>\n      tensor_parallel_degree=8\n      trust_remote_code=False\n      enable_streaming=<StreamingEnum.false: 'false'>\n      batch_size=4\n      max_rolling_batch_size=4\n      dtype=<Dtype.f16: 'fp16'>\n      revision=None\n      output_formatter=None\n      waiting_steps=None\n      is_mpi=False\n      draft_model_id=None\n      spec_length=0\n      neuron_optimize_level=None\n      enable_mixed_precision_accumulation=False\n      enable_saturate_infinity=False\n      n_positions=4096\n      unroll=None\n      load_in_8bit=False\n      low_cpu_mem_usage=False\n      load_split_model=True\n      context_length_estimate=None\n      amp='f16'\n      quantize=None\n      compiled_graph_path=None\n      task=None\n      save_mp_checkpoint_path=None\n      group_query_attention=None\n      model_loader=<TnXModelLoaders.tnx: 'tnx'>\n      rolling_batch_strategy=<TnXGenerationStrategy.continuous_batching: 'continuous_batching'>\n      fuse_qkv=False\n      on_device_embedding=False\n      attention_layout=None\n      collectives_layout=None\n      cache_layout=None\n      partition_schema=None\n      all_reduce_dtype=None\n      cast_logits_dtype=None\n\n# parameters related to how the final report is generated    \nreport:\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 2\n  cost_per_10k_txn_budget: 25\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-inf2-48xl-tp=8-bs=4-byoe.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-inf2-48xl-tp=8-bs=4-byoe.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama3-8b-inf2-48xl-byoe\"      \n  model_name: \"llama3-8b\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data #/Open-Orca\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  #- OpenOrca.jsonl\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: llama3_tokenizer\n  \n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n  # prompt_template_Open-Orca-OpenOrca-llama3.txt\n\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the \n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  #- system_prompt\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: llama-3-8b-instruct-inf2-djl-inference:0.27.0-neuronx-sdk2.18.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.  \n    model_id: llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version:\n    model_name: llama3-8b-inf2.24xl\n    ep_name: <your-endpoint-name>\n    instance_type: \"ml.inf2.48xlarge\"\n    image_uri: \n    deploy: no\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script: \n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # currently the llama3-8b jumpstart model is compiled with a batch size of 1\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 8\n    - 10\n    - 15\n    - 20\n    - 30\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      model_id_or_path='/tmp/.djl.ai/download/ae03dd100c208acd82b5dbed563c971de864c408'\n      rolling_batch=<RollingBatchEnum.auto: 'auto'>\n      tensor_parallel_degree=8\n      trust_remote_code=False\n      enable_streaming=<StreamingEnum.false: 'false'>\n      batch_size=4\n      max_rolling_batch_size=4\n      dtype=<Dtype.f16: 'fp16'>\n      revision=None\n      output_formatter=None\n      waiting_steps=None\n      is_mpi=False\n      draft_model_id=None\n      spec_length=0\n      neuron_optimize_level=None\n      enable_mixed_precision_accumulation=False\n      enable_saturate_infinity=False\n      n_positions=4096\n      unroll=None\n      load_in_8bit=False\n      low_cpu_mem_usage=False\n      load_split_model=True\n      context_length_estimate=None\n      amp='f16'\n      quantize=None\n      compiled_graph_path=None\n      task=None\n      save_mp_checkpoint_path=None\n      group_query_attention=None\n      model_loader=<TnXModelLoaders.tnx: 'tnx'>\n      rolling_batch_strategy=<TnXGenerationStrategy.continuous_batching: 'continuous_batching'>\n      fuse_qkv=False\n      on_device_embedding=False\n      attention_layout=None\n      collectives_layout=None\n      cache_layout=None\n      partition_schema=None\n      all_reduce_dtype=None\n      cast_logits_dtype=None\n\n# parameters related to how the final report is generated    \nreport:\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-inf2-48xlarge-triton-djl.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-inf2-48xlarge-triton-djl.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-8b-inf2.48xl-ec2-triton-djl\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    top_k: 50  \n    max_tokens: 100\n    max_model_len: 4096\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct-triton\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8080/v2/models/Meta-Llama-3-8B-Instruct/generate' \n    instance_type: \"inf2.48xlarge\"\n    image_uri: tritonserver-neuronx:fmbench \n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 8000\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # For deploying a model using the triton inference container: \n      # mention the backend type, if any. Options for triton are: [djl, vllm or tensorrt]\n      backend: djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured,\n      # max: FMBench figures out the max number of model containers to be run\n      #      based on TP degree configured and number of neuron cores/GPUs available.\n      #      For example, if TP=2, GPUs=8 then FMBench will start 4 containers and 1 load balancer,\n      # auto: only supported if the underlying inference container would automatically \n      #       start multiple copies of the model internally based on TP degree and neuron cores/GPUs\n      #       available. In this case only a single container is created, no load balancer is created.\n      #       The DJL serving containers supports auto.  \n      model_copies: max\n      shm_size: 12g\n      # The model.json parameters are replaced within the model.json file\n      # for the triton on vllm/djl/tensorrt options. The model.json already contains\n      # the tp degree and model id from above in this config file. This is a dictionary\n      # that contains other variables, such as max_num_seqs, dtype, max_model_len, block_size and more\n      # For tensorrt, the tp degree, batch size and other relevant parameters are \n      # extracted directly from the inference spec.\n      container_params:\n        # tp degree is a mandatory parameter\n        tp_degree: 8\n        amp: \"f16\"\n        attention_layout: 'BSH'\n        collectives_layout: 'BSH'\n        context_length_estimate: 3072, 3584, 4096\n        max_rolling_batch_size: 8\n        model_loader: \"tnx\"\n        model_loading_timeout: 2400\n        n_positions: 4096\n        output_formatter: \"json\"\n        rolling_batch: \"auto\"\n        rolling_batch_strategy: \"continuous_batching\"\n        trust_remote_code: true\n        # modify the serving properties to match your model and requirements\n        serving.properties:\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 8\n    - 10\n    - 12\n    - 14\n    - 15\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-inf2-g5-byoe-w-openorca.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-inf2-g5-byoe-w-openorca.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama3-8b-inf2-g5-v1\"      \n  model_name: \"llama3-8b\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data/Open-Orca\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - OpenOrca.jsonl\n  \n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: llama3_tokenizer\n  \n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3_OpenOrca.txt\n\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the \n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  - system_prompt  \n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_2000-3000\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: llama-3-8b-instruct-inf2-djl-inference:0.27.0-neuronx-sdk2.18.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.  \n    model_id: llama-3-8b-instruct\n    model_version:\n    model_name: llama3-8b-inf2.24xl\n    ep_name: \"your-llama3-8b-inf2-24x-large-endpoint-name\"\n    instance_type: \"ml.inf2.24xlarge\"\n    image_uri: \n    deploy: no\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script: \n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # Make sure your Llama3 model is compiled for a batch size of 4\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  - name: llama-3-8b-instruct-g5-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct-g5.12xl\n    ep_name: llama-3-8b-instruct-g5-12xl\n    instance_type: \"ml.g5.12xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\n\n# parameters related to how the final report is generated    \nreport:\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-inf2-g5.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-inf2-g5.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama3-8b-inf2-g5-v1\"      \n  model_name: \"llama3-8b\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: llama3_tokenizer\n  \n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the \n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: llama-3-8b-instruct-inf2-djl-inference:0.27.0-neuronx-sdk2.18.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.  \n    model_id: meta-textgenerationneuron-llama-3-8b-instruct\n    # If you want to use an hf model id tokenizer for the model that is being benchmarked, \n    # add the model id below to the \"hf_tokenizer_model_id\" parameter. If this is not provided\n    # OR if the config.json and tokenizer.json are not loaded in the read directory, then the \n    # default 750-1000 tokens tokenizer will be used.\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-inf2.24xl\n    ep_name: llama-3-8b-instruct-inf2-24xl\n    instance_type: \"ml.inf2.24xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.28.0-neuronx-sdk2.18.2'\n    deploy: yes\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script: jumpstart.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # currently the llama3-8b jumpstart model is compiled with a batch size of 1\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 8\n    - 10\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: '1'\n      SERVING_LOAD_MODELS: 'test::Python=/opt/ml/model'\n      OPTION_ENTRYPOINT: 'inference.py'\n      OPTION_TENSOR_PARALLEL_DEGREEL: '12'\n      OPTION_N_POSITIONS: '8192'\n      OPTION_ROLLING_BATCH: 'auto'\n      OPTION_LOAD_SPLIT_MODEL: 'False'\n      OPTION_NEURON_OPTIMIZE_LEVEL: '2'\n      OPTION_DTYPE: 'bf16'\n      OPTION_MAX_ROLLING_BATCH_SIZE: '1'\n      OPTION_MODEL_LOADING_TIMEOUT: '6000'\n      OPTION_TRUST_REMOTE_CODE: 'true'\n      OPTION_FUSE_QKV: 'true'\n      OPTION_ATTENTION_LAYOUT: 'BSH'\n      OPTION_GROUP_QUERY_ATTENTION: 'replicated-heads'\n\n  - name: llama-3-8b-instruct-g5-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct-g5.12xl\n    ep_name: llama-3-8b-instruct-g5-12xl\n    instance_type: \"ml.g5.12xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.28.0-lmi10.0.0-cu124'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 8\n    - 10\n\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\n# parameters related to how the final report is generated    \nreport:\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 2\n  cost_per_10k_txn_budget: 30\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-instruct-all.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-instruct-all.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama3-8b-instruct-all\"      \n  model_name: \"llama3-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\nexperiments:\n  - name: llama-3-8b-instruct-g5-2xl-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct-g5.2xl\n    ep_name: llama-3-8b-instruct-g5-2xl\n    instance_type: \"ml.g5.2xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 5\n    - 8\n    - 10\n    - 16\n    - 20\n    accept_eula: true    \n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"      \n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      job_queue_size: 1000\n      max_dynamic_batch_size: 1\n      max_batch_delay: 100\n      max_idle_time: 60\n      load_on_devices: *\n      engine: MPI\n      mpi_mode: true\n      option.entryPoint: null\n      option.tensor_parallel_degree: 1\n      option.max_rolling_batch_size: 256\n      option.mpi_mode: true\n      option.model_id: /opt/ml/model\n      option.rolling_batch: lmi-dist\n\n  - name: llama-3-8b-instruct-g5-48xl-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct-g5.48xl\n    ep_name: llama-3-8b-instruct-g5-48xl\n    instance_type: \"ml.g5.48xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 5\n    - 8\n    - 10\n    - 16\n    - 20\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"8\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"2\"\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      job_queue_size: 1000\n      max_dynamic_batch_size: 1\n      max_batch_delay: 100\n      max_idle_time: 60\n      load_on_devices: *\n      engine: MPI\n      mpi_mode: true\n      option.entryPoint: null\n      option.tensor_parallel_degree: 8\n      option.max_rolling_batch_size: 256\n      option.mpi_mode: true\n      option.model_id: /opt/ml/model\n      option.rolling_batch: lmi-dist\n\n  - name: llama-3-8b-instruct-g5-24xl-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct-g5.24xl\n    ep_name: llama-3-8b-instruct-g5-24xl\n    instance_type: \"ml.g5.24xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 5\n    - 8\n    - 10\n    - 16\n    - 20\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      job_queue_size: 1000\n      max_dynamic_batch_size: 1\n      max_batch_delay: 100\n      max_idle_time: 60\n      load_on_devices: *\n      engine: MPI\n      mpi_mode: true\n      option.entryPoint: null\n      option.tensor_parallel_degree: 4\n      option.max_rolling_batch_size: 256\n      option.mpi_mode: true\n      option.model_id: /opt/ml/model\n      option.rolling_batch: lmi-dist\n\n  - name: llama-3-8b-instruct-g5-12xl-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct-g5.12xl\n    ep_name: llama-3-8b-instruct-g5-12xl\n    instance_type: \"ml.g5.12xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 5\n    - 8\n    - 10\n    - 15\n    - 20\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      job_queue_size: 1000\n      max_dynamic_batch_size: 1\n      max_batch_delay: 100\n      max_idle_time: 60\n      load_on_devices: *\n      engine: MPI\n      mpi_mode: true\n      option.entryPoint: null\n      option.tensor_parallel_degree: 4\n      option.max_rolling_batch_size: 256\n      option.mpi_mode: true\n      option.model_id: /opt/ml/model\n      option.rolling_batch: lmi-dist\n  - name: llama-3-8b-instruct-p4d-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct\n    ep_name: llama-3-8b-instruct-p4d\n    instance_type: \"ml.p4d.24xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 15\n    - 20\n\n    accept_eula: true\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      job_queue_size: 1000\n      max_dynamic_batch_size: 1\n      max_batch_delay: 100\n      max_idle_time: 60\n      load_on_devices: *\n      engine: MPI\n      mpi_mode: true\n      option.entryPoint: null\n      option.tensor_parallel_degree: 8\n      option.max_rolling_batch_size: 256\n      option.mpi_mode: true\n      option.model_id: /opt/ml/model\n      option.rolling_batch: lmi-dist\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      # SM_NUM_GPUS: \"1\"\n      # SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n  - name: llama-3-8b-instruct-trn1-32xl-djl-inference:0.27.0-neuronx-sdk2.18.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.  \n    model_id: llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version:\n    model_name: llama3-8b-trn1.32xl\n    ep_name: \"<your-endpoint-name>\"\n    instance_type: \"ml.trn1.32xlarge\"\n    image_uri: \n    deploy: no\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script: \n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # currently the llama3-8b jumpstart model is compiled with a batch size of 1\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 8\n    - 10\n    - 15\n    - 20\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      model_id_or_path='/tmp/.djl.ai/download/ae03dd100c208acd82b5dbed563c971de864c408'\n      rolling_batch=<RollingBatchEnum.auto: 'auto'>\n      tensor_parallel_degree=8\n      trust_remote_code=False\n      enable_streaming=<StreamingEnum.false: 'false'>\n      batch_size=4\n      max_rolling_batch_size=4\n      dtype=<Dtype.f16: 'fp16'>\n      revision=None\n      output_formatter=None\n      waiting_steps=None\n      is_mpi=False\n      draft_model_id=None\n      spec_length=0\n      neuron_optimize_level=None\n      enable_mixed_precision_accumulation=False\n      enable_saturate_infinity=False\n      n_positions=4096\n      unroll=None\n      load_in_8bit=False\n      low_cpu_mem_usage=False\n      load_split_model=True\n      context_length_estimate=None\n      amp='f16'\n      quantize=None\n      compiled_graph_path=None\n      task=None\n      save_mp_checkpoint_path=None\n      group_query_attention=None\n      model_loader=<TnXModelLoaders.tnx: 'tnx'>\n      rolling_batch_strategy=<TnXGenerationStrategy.continuous_batching: 'continuous_batching'>\n      fuse_qkv=False\n      on_device_embedding=False\n      attention_layout=None\n      collectives_layout=None\n      cache_layout=None\n      partition_schema=None\n      all_reduce_dtype=None\n      cast_logits_dtype=None\n  - name: llama-3-8b-instruct-inf2-48xl-djl-inference:0.27.0-neuronx-sdk2.18.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.  \n    model_id: llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version:\n    model_name: llama3-8b-inf2.24xl\n    ep_name: \"<your-endpoint-name>\"\n    instance_type: \"ml.inf2.48xlarge\"\n    image_uri: \n    deploy: no\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script: \n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # currently the llama3-8b jumpstart model is compiled with a batch size of 1\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 8\n    - 10\n    - 15\n    - 20\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      model_id_or_path='/tmp/.djl.ai/download/ae03dd100c208acd82b5dbed563c971de864c408'\n      rolling_batch=<RollingBatchEnum.auto: 'auto'>\n      tensor_parallel_degree=8\n      trust_remote_code=False\n      enable_streaming=<StreamingEnum.false: 'false'>\n      batch_size=4\n      max_rolling_batch_size=4\n      dtype=<Dtype.f16: 'fp16'>\n      revision=None\n      output_formatter=None\n      waiting_steps=None\n      is_mpi=False\n      draft_model_id=None\n      spec_length=0\n      neuron_optimize_level=None\n      enable_mixed_precision_accumulation=False\n      enable_saturate_infinity=False\n      n_positions=4096\n      unroll=None\n      load_in_8bit=False\n      low_cpu_mem_usage=False\n      load_split_model=True\n      context_length_estimate=None\n      amp='f16'\n      quantize=None\n      compiled_graph_path=None\n      task=None\n      save_mp_checkpoint_path=None\n      group_query_attention=None\n      model_loader=<TnXModelLoaders.tnx: 'tnx'>\n      rolling_batch_strategy=<TnXGenerationStrategy.continuous_batching: 'continuous_batching'>\n      fuse_qkv=False\n      on_device_embedding=False\n      attention_layout=None\n      collectives_layout=None\n      cache_layout=None\n      partition_schema=None\n      all_reduce_dtype=None\n      cast_logits_dtype=None\n\n  - name: llama-3-8b-instruct-inf2-24xl-djl-inference:0.27.0-neuronx-sdk2.18.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.  \n    model_id: llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version:\n    model_name: llama3-8b-inf2.24xl\n    ep_name: \"<your-endpoint-name>\"\n    instance_type: \"ml.inf2.24xlarge\"\n    image_uri: \n    deploy: no\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script: \n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # currently the llama3-8b jumpstart model is compiled with a batch size of 1\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 15\n    - 20\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      model_id_or_path='/tmp/.djl.ai/download/ae03dd100c208acd82b5dbed563c971de864c408'\n      rolling_batch=<RollingBatchEnum.auto: 'auto'>\n      tensor_parallel_degree=8\n      trust_remote_code=False\n      enable_streaming=<StreamingEnum.false: 'false'>\n      batch_size=4\n      max_rolling_batch_size=4\n      dtype=<Dtype.f16: 'fp16'>\n      revision=None\n      output_formatter=None\n      waiting_steps=None\n      is_mpi=False\n      draft_model_id=None\n      spec_length=0\n      neuron_optimize_level=None\n      enable_mixed_precision_accumulation=False\n      enable_saturate_infinity=False\n      n_positions=4096\n      unroll=None\n      load_in_8bit=False\n      low_cpu_mem_usage=False\n      load_split_model=True\n      context_length_estimate=None\n      amp='f16'\n      quantize=None\n      compiled_graph_path=None\n      task=None\n      save_mp_checkpoint_path=None\n      group_query_attention=None\n      model_loader=<TnXModelLoaders.tnx: 'tnx'>\n      rolling_batch_strategy=<TnXGenerationStrategy.continuous_batching: 'continuous_batching'>\n      fuse_qkv=False\n      on_device_embedding=False\n      attention_layout=None\n      collectives_layout=None\n      cache_layout=None\n      partition_schema=None\n      all_reduce_dtype=None\n      cast_logits_dtype=None\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-instruct-g5-12xl-4-instances.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-instruct-g5-12xl-4-instances.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama3-8b-g5-12xl-4-instances\"      \n  model_name: \"llama3-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n \nmetrics:\n  dataset_of_interest: en_500-1000\n  \npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\nexperiments:\n  - name: llama-3-8b-instruct-g5-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct-g5.12xl\n    ep_name: llama-3-8b-instruct-g5-12xl\n    instance_type: \"ml.g5.12xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 4\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_500-1000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 5\n    - 8\n    - 10\n    - 16\n    - 20\n    - 30\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      # SM_NUM_GPUS: \"8\"\n      # SAGEMAKER_MODEL_SERVER_WORKERS: \"2\"\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-instruct-g5-12xl.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-instruct-g5-12xl.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama3-8b-g5-12xl\"      \n  model_name: \"llama3-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\nexperiments:\n  - name: llama-3-8b-instruct-g5-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct-g5.12xl\n    ep_name: llama-3-8b-instruct-g5-12xl\n    instance_type: \"ml.g5.12xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 5\n    - 8\n    - 10\n    - 16\n    - 20\n    - 30\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      # SM_NUM_GPUS: \"8\"\n      # SAGEMAKER_MODEL_SERVER_WORKERS: \"2\"\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-instruct-g5-24xl.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-instruct-g5-24xl.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama3-8b-g5-24xl\"      \n  model_name: \"llama3-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\nexperiments:\n  - name: llama-3-8b-instruct-g5-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct-g5.24xl\n    ep_name: llama-3-8b-instruct-g5-24xl\n    instance_type: \"ml.g5.24xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 5\n    - 8\n    - 10\n    - 16\n    - 20\n    - 30\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      # SM_NUM_GPUS: \"8\"\n      # SAGEMAKER_MODEL_SERVER_WORKERS: \"2\"\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-instruct-g5-2xl.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-instruct-g5-2xl.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama3-8b-g5-2xl\"      \n  model_name: \"llama3-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\nexperiments:\n  - name: llama-3-8b-instruct-g5-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct-g5.2xl\n    ep_name: llama-3-8b-instruct-g5-2xl\n    instance_type: \"ml.g5.2xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 5\n    - 8\n    - 10\n    - 16\n    - 20\n    - 30\n    accept_eula: true    \n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"      \n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      job_queue_size: 1000\n      max_dynamic_batch_size: 1\n      max_batch_delay: 100\n      max_idle_time: 60\n      load_on_devices: *\n      engine: MPI\n      mpi_mode: true\n      option.entryPoint: null\n      option.tensor_parallel_degree: 1\n      option.max_rolling_batch_size: 256\n      option.mpi_mode: true\n      option.model_id: /opt/ml/model\n      option.rolling_batch: lmi-dist\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-instruct-g5-48xl.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-instruct-g5-48xl.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama3-8b-g5-48xl\"      \n  model_name: \"llama3-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\nexperiments:\n  - name: llama-3-8b-instruct-g5-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct-g5.48xl\n    ep_name: llama-3-8b-instruct-g5-48xl\n    instance_type: \"ml.g5.48xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 5\n    - 8\n    - 10\n    - 16\n    - 20\n    - 30\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"8\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"2\"\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-instruct-g5-p4d.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-instruct-g5-p4d.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama3-8b-p4d-g5-v1\"      \n  model_name: \"llama3-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n \nmetrics:\n  dataset_of_interest: en_3000-3840\n  \npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\nexperiments:\n  - name: llama-3-8b-instruct-g5-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct-g5.12xl\n    ep_name: llama-3-8b-instruct-g5-12xl\n    instance_type: \"ml.g5.12xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n      \n  - name: llama-3-8b-instruct-p4d-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct\n    ep_name: llama-3-8b-instruct-p4d\n    instance_type: \"ml.p4d.24xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    - 15\n\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-instruct-g6-12xl.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-instruct-g6-12xl.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama3-8b-g6-12xl-djl-lmi-dist\"      \n  model_name: \"llama3-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n \nmetrics:\n  dataset_of_interest: en_3000-3840\n  \npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\nexperiments:\n \n  - name: llama-3-8b-instruct-g6-12xl-djl-deepspeed0.12.6-cu121\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct\n    ep_name: llama-3-8b-instruct-g6-12xl\n    instance_type: \"ml.g6.12xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.max_rolling_batch_size=256\n      option.rolling_batch=lmi-dist\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 15\n    - 20\n    - 30\n\n    accept_eula: true\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      job_queue_size: 1000\n      max_dynamic_batch_size: 1\n      max_batch_delay: 100\n      max_idle_time: 60\n      load_on_devices: *\n      engine: MPI\n      mpi_mode: true\n      option.entryPoint: null\n      option.tensor_parallel_degree: 4\n      option.max_rolling_batch_size: 256\n      option.mpi_mode: true\n      option.model_id: /opt/ml/model\n      option.rolling_batch: lmi-dist\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      #HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      # SM_NUM_GPUS: \"1\"\n      # SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n      \nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-instruct-g6-24xl.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-instruct-g6-24xl.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama3-8b-g6-24xl-djl-lmi-dist\"      \n  model_name: \"llama3-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n \nmetrics:\n  dataset_of_interest: en_3000-3840\n  \npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\nexperiments:\n \n  - name: llama-3-8b-instruct-g6-24xl-djl-deepspeed0.12.6-cu121\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct\n    ep_name: llama-3-8b-instruct-g6-24xl\n    instance_type: \"ml.g6.24xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.max_rolling_batch_size=256\n      option.rolling_batch=lmi-dist\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 15\n    - 20\n    - 30\n\n    accept_eula: true\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      job_queue_size: 1000\n      max_dynamic_batch_size: 1\n      max_batch_delay: 100\n      max_idle_time: 60\n      load_on_devices: *\n      engine: MPI\n      mpi_mode: true\n      option.entryPoint: null\n      option.tensor_parallel_degree: 4\n      option.max_rolling_batch_size: 256\n      option.mpi_mode: true\n      option.model_id: /opt/ml/model\n      option.rolling_batch: lmi-dist\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      #HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      # SM_NUM_GPUS: \"1\"\n      # SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n      \nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-instruct-g6-48xl.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-instruct-g6-48xl.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama3-8b-g6-48xl-djl-lmi-dist\"      \n  model_name: \"llama3-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n \nmetrics:\n  dataset_of_interest: en_3000-3840\n  \npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\nexperiments:\n \n  - name: llama-3-8b-instruct-g6-48xl-djl-deepspeed0.12.6-cu121\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct\n    ep_name: llama-3-8b-instruct-g6-48xl\n    instance_type: \"ml.g6.48xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.max_rolling_batch_size=256\n      option.rolling_batch=lmi-dist\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 15\n    - 20\n    - 30\n\n    accept_eula: true\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      job_queue_size: 1000\n      max_dynamic_batch_size: 1\n      max_batch_delay: 100\n      max_idle_time: 60\n      load_on_devices: *\n      engine: MPI\n      mpi_mode: true\n      option.entryPoint: null\n      option.tensor_parallel_degree: 8\n      option.max_rolling_batch_size: 256\n      option.mpi_mode: true\n      option.model_id: /opt/ml/model\n      option.rolling_batch: lmi-dist\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      #HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      # SM_NUM_GPUS: \"1\"\n      # SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n      \nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-instruct-p4d-djl-lmi-dist.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-instruct-p4d-djl-lmi-dist.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama3-8b-p4d-djl-lmi-dist\"      \n  model_name: \"llama3-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n \nmetrics:\n  dataset_of_interest: en_3000-3840\n  \npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\nexperiments:\n \n  - name: llama-3-8b-instruct-p4d-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct\n    ep_name: llama-3-8b-instruct-p4d\n    instance_type: \"ml.p4d.24xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 15\n    - 20\n    - 30\n\n    accept_eula: true\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      job_queue_size: 1000\n      max_dynamic_batch_size: 1\n      max_batch_delay: 100\n      max_idle_time: 60\n      load_on_devices: *\n      engine: MPI\n      mpi_mode: true\n      option.entryPoint: null\n      option.tensor_parallel_degree: 8\n      option.max_rolling_batch_size: 256\n      option.mpi_mode: true\n      option.model_id: /opt/ml/model\n      option.rolling_batch: lmi-dist\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      # SM_NUM_GPUS: \"1\"\n      # SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n      \nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-instruct-p4d-djl-vllm.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-instruct-p4d-djl-vllm.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"Llama3-8B-Instruct-p4d-djl-vllm\"\n  model_name: \"Llama3-8B\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: mistral_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_mistral.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: no\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \npricing: pricing.yml\n  \ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n# Model configurations\nexperiments:\n  - name: Meta-Llama-3-8B-Instruct-p4d-vllm\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: Llama-3-8B-Instruct\n    ep_name: Llama-3-8B-Instruct-p4d-vllm\n    download_from_hf_place_in_s3: no\n    model_s3_path:\n    instance_type: \"ml.p4d.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n        engine=Python\n        option.tensor_parallel_degree=max\n        option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n        option.max_rolling_batch_size=64\n        option.rolling_batch=vllm\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 8\n    - 10\n    - 15\n    - 20\n    - 30\n    accept_eula: true\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n  latency_vs_token_len_chart: \n    y_ticks: \n    title: \"Effect of token length on inference latency for \\\"meta-llama/Meta-Llama-3-8B-Instruct\\\"\"\n\n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-instruct-p5-djl-lmi-dist.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-instruct-p5-djl-lmi-dist.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama3-8b-p5-djl-lmi-dist\"      \n  model_name: \"llama3-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n \nmetrics:\n  dataset_of_interest: en_3000-3840\n  \npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\nexperiments:\n \n  - name: llama-3-8b-instruct-p5-djl-deepspeed0.12.6-cu121\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct\n    ep_name: llama-3-8b-instruct-p5\n    instance_type: \"ml.p5.48xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.max_rolling_batch_size=256\n      option.rolling_batch=lmi-dist\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 15\n    - 20\n    - 30\n\n    accept_eula: true\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      job_queue_size: 1000\n      max_dynamic_batch_size: 1\n      max_batch_delay: 100\n      max_idle_time: 60\n      load_on_devices: *\n      engine: MPI\n      mpi_mode: true\n      option.entryPoint: null\n      option.tensor_parallel_degree: 8\n      option.max_rolling_batch_size: 256\n      option.mpi_mode: true\n      option.model_id: /opt/ml/model\n      option.rolling_batch: lmi-dist\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      # SM_NUM_GPUS: \"1\"\n      # SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n      \nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-trn1-32xl-tp-16-bs-4-byoe.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-trn1-32xl-tp-16-bs-4-byoe.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"config-llama3-8b-trn1-32xl-tp=16-bs=4-byoe\"      \n  model_name: \"llama3-8b\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data #/Open-Orca\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  #- OpenOrca.jsonl\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: llama3_tokenizer\n  \n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n  # prompt_template_Open-Orca-OpenOrca-llama3.txt\n\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the \n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  #- system_prompt\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    #stop: '<|eot_id|>'\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: llama-3-8b-instruct-inf2-djl-inference:0.27.0-neuronx-sdk2.18.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.  \n    model_id: llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version:\n    model_name: llama3-8b-trn1.32xl\n    ep_name: <your-endpoint-name>\n    instance_type: \"ml.trn1.32xlarge\"\n    image_uri: \n    deploy: no\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script: \n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # currently the llama3-8b jumpstart model is compiled with a batch size of 1\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 15\n    - 20\n    - 30\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      model_id_or_path='/tmp/.djl.ai/download/ae03dd100c208acd82b5dbed563c971de864c408'\n      rolling_batch=<RollingBatchEnum.auto: 'auto'>\n      tensor_parallel_degree=16\n      trust_remote_code=False\n      enable_streaming=<StreamingEnum.false: 'false'>\n      batch_size=4\n      max_rolling_batch_size=4\n      dtype=<Dtype.f16: 'fp16'>\n      revision=None\n      output_formatter=None\n      waiting_steps=None\n      is_mpi=False\n      draft_model_id=None\n      spec_length=0\n      neuron_optimize_level=None\n      enable_mixed_precision_accumulation=False\n      enable_saturate_infinity=False\n      n_positions=4096\n      unroll=None\n      load_in_8bit=False\n      low_cpu_mem_usage=False\n      load_split_model=True\n      context_length_estimate=None\n      amp='f16'\n      quantize=None\n      compiled_graph_path=None\n      task=None\n      save_mp_checkpoint_path=None\n      group_query_attention=None\n      model_loader=<TnXModelLoaders.tnx: 'tnx'>\n      rolling_batch_strategy=<TnXGenerationStrategy.continuous_batching: 'continuous_batching'>\n      fuse_qkv=False\n      on_device_embedding=False\n      attention_layout=None\n      collectives_layout=None\n      cache_layout=None\n      partition_schema=None\n      all_reduce_dtype=None\n      cast_logits_dtype=None\n\n# parameters related to how the final report is generated    \nreport:\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 2\n  cost_per_10k_txn_budget: 25\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-trn1-32xl-tp-8-bs-4-byoe.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-trn1-32xl-tp-8-bs-4-byoe.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama3-8b-trn1-byoe\"      \n  model_name: \"llama3-8b\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data #/Open-Orca\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  #- OpenOrca.jsonl\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: llama3_tokenizer\n  \n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n  # prompt_template_Open-Orca-OpenOrca-llama3.txt\n\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the \n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  #- system_prompt\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    #stop: '<|eot_id|>'\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: llama-3-8b-instruct-inf2-djl-inference:0.27.0-neuronx-sdk2.18.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.  \n    model_id: llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version:\n    model_name: llama3-8b-trn1.32xl\n    ep_name: <your-endpoint-name>\n    instance_type: \"ml.trn1.32xlarge\"\n    image_uri: \n    deploy: no\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script: \n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # currently the llama3-8b jumpstart model is compiled with a batch size of 1\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 8\n    - 10\n    - 15\n    - 20\n    - 30\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      model_id_or_path='/tmp/.djl.ai/download/ae03dd100c208acd82b5dbed563c971de864c408'\n      rolling_batch=<RollingBatchEnum.auto: 'auto'>\n      tensor_parallel_degree=8\n      trust_remote_code=False\n      enable_streaming=<StreamingEnum.false: 'false'>\n      batch_size=4\n      max_rolling_batch_size=4\n      dtype=<Dtype.f16: 'fp16'>\n      revision=None\n      output_formatter=None\n      waiting_steps=None\n      is_mpi=False\n      draft_model_id=None\n      spec_length=0\n      neuron_optimize_level=None\n      enable_mixed_precision_accumulation=False\n      enable_saturate_infinity=False\n      n_positions=4096\n      unroll=None\n      load_in_8bit=False\n      low_cpu_mem_usage=False\n      load_split_model=True\n      context_length_estimate=None\n      amp='f16'\n      quantize=None\n      compiled_graph_path=None\n      task=None\n      save_mp_checkpoint_path=None\n      group_query_attention=None\n      model_loader=<TnXModelLoaders.tnx: 'tnx'>\n      rolling_batch_strategy=<TnXGenerationStrategy.continuous_batching: 'continuous_batching'>\n      fuse_qkv=False\n      on_device_embedding=False\n      attention_layout=None\n      collectives_layout=None\n      cache_layout=None\n      partition_schema=None\n      all_reduce_dtype=None\n      cast_logits_dtype=None\n\n# parameters related to how the final report is generated    \nreport:\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-trn1-32xl-tp16-bs-4-ec2.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-trn1-32xl-tp16-bs-4-ec2.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-8b-trn1.32xl-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 15\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"trn1.32xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and Inferentia settings for the instance\n      # '-u djl' tells the container to use the Deep Java Library (DJL) runtime\n      # '--device /dev/neuron0 ... /dev/neuron5' specifies the Inferentia devices to be used\n      gpu_or_neuron_setting: -u djl --device /dev/neuron0 --device /dev/neuron1 --device /dev/neuron2 --device /dev/neuron3 --device /dev/neuron4 --device /dev/neuron5 --device /dev/neuron6 --device /dev/neuron7 --device /dev/neuron8 --device /dev/neuron9 --device /dev/neuron10 --device /dev/neuron11 --device /dev/neuron12 --device /dev/neuron13 --device /dev/neuron14 --device /dev/neuron15\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured,\n      # max: FMBench figures out the max number of model containers to be run\n      #      based on TP degree configured and number of neuron cores/GPUs available.\n      #      For example, if TP=2, GPUs=8 then FMBench will start 4 containers and 1 load balancer,\n      # auto: only supported if the underlying inference container would automatically \n      #       start multiple copies of the model internally based on TP degree and neuron cores/GPUs\n      #       available. In this case only a single container is created, no load balancer is created.\n      #       The DJL serving containers supports auto.  \n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 16\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.load_split_model=True\n      option.tensor_parallel_degree=16\n      option.n_positions=4096\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=4\n      option.dtype=fp16\n      option.model_loading_timeout=2400\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 8\n    - 10\n    - 11\n    - 12\n    - 13\n    - 14\n    - 15\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O3\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-trn1-32xlarge-triton-djl.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-trn1-32xlarge-triton-djl.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-8b-trn1.32xl-ec2-triton\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    top_k: 50  \n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct-triton\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8080/v2/models/Meta-Llama-3-8B-Instruct/generate' \n    instance_type: \"trn1.32xlarge\"\n    image_uri: tritonserver-neuronx:fmbench \n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 8000\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # For deploying a model using the triton inference container: \n      # mention the backend type, if any. Options for triton are: [djl, vllm or tensorrt]\n      backend: djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured,\n      # max: FMBench figures out the max number of model containers to be run\n      #      based on TP degree configured and number of neuron cores/GPUs available.\n      #      For example, if TP=2, GPUs=8 then FMBench will start 4 containers and 1 load balancer,\n      # auto: only supported if the underlying inference container would automatically \n      #       start multiple copies of the model internally based on TP degree and neuron cores/GPUs\n      #       available. In this case only a single container is created, no load balancer is created.\n      #       The DJL serving containers supports auto.  \n      model_copies: max\n      shm_size: 12g\n      # The model.json parameters are replaced within the model.json file\n      # for the triton on vllm/djl/tensorrt options. The model.json already contains\n      # the tp degree and model id from above in this config file. This is a dictionary\n      # that contains other variables, such as max_num_seqs, dtype, max_model_len, block_size and more\n      # For tensorrt, the tp degree, batch size and other relevant parameters are \n      # extracted directly from the inference spec.\n      container_params:\n        # tp degree is a mandatory parameter\n        tp_degree: 8\n        amp: \"f16\"\n        attention_layout: 'BSH'\n        collectives_layout: 'BSH'\n        context_length_estimate: 3072, 3584, 4096\n        max_rolling_batch_size: 8\n        model_loader: \"tnx\"\n        model_loading_timeout: 2400\n        n_positions: 4096\n        output_formatter: \"json\"\n        rolling_batch: \"auto\"\n        rolling_batch_strategy: \"continuous_batching\"\n        trust_remote_code: true\n        # modify the serving properties to match your model and requirements\n        serving.properties:\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 8\n    - 10\n    - 12\n    - 14\n    - 15\n    - 18\n    - 20\n    - 25\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-trn1-32xlarge-triton-vllm.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-trn1-32xlarge-triton-vllm.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-8b-trn1.32xl-ec2-triton\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    top_k: 50  \n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct-triton\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8080/v2/models/Meta-Llama-3-8B-Instruct/generate' \n    instance_type: \"trn1.32xlarge\"\n    image_uri: tritonserver-neuronx:fmbench \n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and Inferentia settings for the instance\n      # '-u djl' tells the container to use the Deep Java Library (DJL) runtime\n      # '--device /dev/neuron0 ... /dev/neuron5' specifies the Inferentia devices to be used\n      gpu_or_neuron_setting: -u triton --device /dev/neuron0 --device /dev/neuron1 --device /dev/neuron2 --device /dev/neuron3 --device /dev/neuron4 --device /dev/neuron5 --device /dev/neuron6 --device /dev/neuron7 --device /dev/neuron8 --device /dev/neuron9 --device /dev/neuron10 --device /dev/neuron11 --device /dev/neuron12 --device /dev/neuron13 --device /dev/neuron14 --device /dev/neuron15\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 8000\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # mention the backend type, if any\n      backend: vllm\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured,\n      # max: FMBench figures out the max number of model containers to be run\n      #      based on TP degree configured and number of neuron cores/GPUs available.\n      #      For example, if TP=2, GPUs=8 then FMBench will start 4 containers and 1 load balancer,\n      # auto: only supported if the underlying inference container would automatically \n      #       start multiple copies of the model internally based on TP degree and neuron cores/GPUs\n      #       available. In this case only a single container is created, no load balancer is created.\n      #       The DJL serving containers supports auto.  \n      model_copies: max\n      shm_size: 12g\n      model_loading_timeout: 2400\n      # The model.json parameters are replaced within the model.json file\n      # for the triton on vllm/djl/tensorrt options. The model.json already contains\n      # the tp degree and model id from above in this config file. This is a dictionary\n      # that contains other variables, such as max_num_seqs, dtype, max_model_len, block_size and more\n      # For tensorrt, the tp degree, batch size and other relevant parameters are \n      # extracted directly from the inference spec.\n      container_params:\n        # if you set the model_copies parameter then it is mandatory to set the \n        # tp_degree, shm_size, model_loading_timeout parameters\n        tp_degree: 8\n        # The model.json parameters are replaced within the model.json file\n        # for the triton on vllm option. The model.json already contains\n        # the tp degree and model id from above in this config file. This is a dictionary\n        # that contains other variables, such as max_num_seqs, dtype, max_model_len, block_size and more\n        max_num_seqs: 4\n        dtype: \"float16\"\n        # The max_model_len and block_size arguments are required to be same as\n        # max sequence length when targeting neuron device.\n        # Currently, this is a known limitation in continuous batching support\n        # in transformers-neuronx. View the latest vllm documentation: https://vllm.readthedocs.io/_/downloads/en/stable/pdf/\n        max_model_len: 4096\n        block_size: 4096\n        # modify the serving properties to match your model and requirements\n        serving.properties:\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n    - 7\n    - 8\n    - 9\n    - 10\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-8b-trn1.yml",
    "path": "fmbench/configs/llama3/8b/config-llama3-8b-trn1.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama3-8b-trn1\"      \n  model_name: \"llama3-8b\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data #/Open-Orca\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  #- OpenOrca.jsonl\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: llama3_tokenizer\n  \n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n  # prompt_template_Open-Orca-OpenOrca-llama3.txt\n\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: no\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the \n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  #- system_prompt\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    #stop: '<|eot_id|>'\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: llama-3-8b-instruct-trn1-djl-inference:0.27.0-neuronx-sdk2.18.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.  \n    model_id: meta-textgenerationneuron-llama-3-8b-instruct\n    model_version: \"*\"\n    model_name: llama3-8b-trn1.32xl\n    model_data:\n      S3DataSource:\n        S3Uri: \"s3://jumpstart-private-cache-prod-{region}/meta-textgenerationneuron/meta-textgenerationneuron-llama-3-8b-instruct/artifacts/inference-prepack/v1.0.0/\"\n        S3DataType: \"S3Prefix\"\n        CompressionType: \"None\"\n    ep_name: llama-3-8b-instruct-trn1-32xl\n    instance_type: \"ml.trn1.32xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-neuronx-sdk2.18.1'\n    deploy: yes\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script: sagemaker_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # currently the llama3-8b jumpstart model is compiled with a batch size of 1\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: '1'\n      SERVING_LOAD_MODELS: 'test::Python=/opt/ml/model'\n      OPTION_ENTRYPOINT: 'inference.py'\n      OPTION_TENSOR_PARALLEL_DEGREE: '16'\n      OPTION_N_POSITIONS: '4096'\n      OPTION_ROLLING_BATCH: 'auto'\n      OPTION_LOAD_SPLIT_MODEL: 'False'\n      OPTION_NEURON_OPTIMIZE_LEVEL: '3'\n      OPTION_DTYPE: 'bf16'\n      OPTION_MAX_ROLLING_BATCH_SIZE: '4'\n      OPTION_MODEL_LOADING_TIMEOUT: '6000'\n      OPTION_TRUST_REMOTE_CODE: 'true'\n      OPTION_FUSE_QKV: 'true'\n      OPTION_ATTENTION_LAYOUT: 'BSH'\n      OPTION_GROUP_QUERY_ATTENTION: 'replicated-heads'\n\n    accept_eula: true\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata:\n\n# parameters related to how the final report is generated    \nreport:\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "llama3-8b-inf2-24xl-byoe-g5-12xl.yml",
    "path": "fmbench/configs/llama3/8b/llama3-8b-inf2-24xl-byoe-g5-12xl.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama3-8b-inf2-24xl-byoe-g5-12xl\"      \n  model_name: \"llama3-8b\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: llama3_tokenizer\n  \n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the \n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: llama-3-8b-instruct-inf2-24xl-djl-inference:0.27.0-neuronx-sdk2.18.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.  \n    model_id: llama-3-8b-instruct\n    model_version:\n    model_name: llama3-8b-inf2.24xl\n    ep_name: meta-llama-Meta-Llama-3-8B-Instruct-inf-2024-06-07-12-29-27-768\n    instance_type: \"ml.inf2.24xlarge\"\n    image_uri: \n    deploy: no\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script: \n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # currently the llama3-8b jumpstart model is compiled with a batch size of 1\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      model_id_or_path='/tmp/.djl.ai/download/ae03dd100c208acd82b5dbed563c971de864c408'\n      rolling_batch=<RollingBatchEnum.auto: 'auto'>\n      tensor_parallel_degree=8\n      trust_remote_code=False\n      enable_streaming=<StreamingEnum.false: 'false'>\n      batch_size=4\n      max_rolling_batch_size=4\n      dtype=<Dtype.f16: 'fp16'>\n      revision=None\n      output_formatter=None\n      waiting_steps=None\n      is_mpi=False\n      draft_model_id=None\n      spec_length=0\n      neuron_optimize_level=None\n      enable_mixed_precision_accumulation=False\n      enable_saturate_infinity=False\n      n_positions=4096\n      unroll=None\n      load_in_8bit=False\n      low_cpu_mem_usage=False\n      load_split_model=True\n      context_length_estimate=None\n      amp='f16'\n      quantize=None\n      compiled_graph_path=None\n      task=None\n      save_mp_checkpoint_path=None\n      group_query_attention=None\n      model_loader=<TnXModelLoaders.tnx: 'tnx'>\n      rolling_batch_strategy=<TnXGenerationStrategy.continuous_batching: 'continuous_batching'>\n      fuse_qkv=False\n      on_device_embedding=False\n      attention_layout=None\n      collectives_layout=None\n      cache_layout=None\n      partition_schema=None\n      all_reduce_dtype=None\n      cast_logits_dtype=None\n\n  - name: llama-3-8b-instruct-g5-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct-g5.12xl\n    ep_name: llama-3-8b-instruct-g5-12xl\n    instance_type: \"ml.g5.12xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\n# parameters related to how the final report is generated    \nreport:\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "llama3-8b-inf2-48xl-byoe-g5-24xl.yml",
    "path": "fmbench/configs/llama3/8b/llama3-8b-inf2-48xl-byoe-g5-24xl.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama3-8b-inf2-48xl-byoe-g5-24xl\"      \n  model_name: \"llama3-8b\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: llama3_tokenizer\n  \n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the \n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: llama-3-8b-instruct-inf2-48xl-djl-inference:0.27.0-neuronx-sdk2.18.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.  \n    model_id: llama-3-8b-instruct\n    model_version:\n    model_name: llama3-8b-inf2.48xl\n    ep_name: meta-llama-Meta-Llama-3-8B-Instruct-inf-2024-06-07-14-27-28-017\n    instance_type: \"ml.inf2.48xlarge\"\n    image_uri: \n    deploy: no\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script: \n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # currently the llama3-8b jumpstart model is compiled with a batch size of 1\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      model_id_or_path='/tmp/.djl.ai/download/ae03dd100c208acd82b5dbed563c971de864c408'\n      rolling_batch=<RollingBatchEnum.auto: 'auto'>\n      tensor_parallel_degree=8\n      trust_remote_code=False\n      enable_streaming=<StreamingEnum.false: 'false'>\n      batch_size=4\n      max_rolling_batch_size=4\n      dtype=<Dtype.f16: 'fp16'>\n      revision=None\n      output_formatter=None\n      waiting_steps=None\n      is_mpi=False\n      draft_model_id=None\n      spec_length=0\n      neuron_optimize_level=None\n      enable_mixed_precision_accumulation=False\n      enable_saturate_infinity=False\n      n_positions=4096\n      unroll=None\n      load_in_8bit=False\n      low_cpu_mem_usage=False\n      load_split_model=True\n      context_length_estimate=None\n      amp='f16'\n      quantize=None\n      compiled_graph_path=None\n      task=None\n      save_mp_checkpoint_path=None\n      group_query_attention=None\n      model_loader=<TnXModelLoaders.tnx: 'tnx'>\n      rolling_batch_strategy=<TnXGenerationStrategy.continuous_batching: 'continuous_batching'>\n      fuse_qkv=False\n      on_device_embedding=False\n      attention_layout=None\n      collectives_layout=None\n      cache_layout=None\n      partition_schema=None\n      all_reduce_dtype=None\n      cast_logits_dtype=None\n\n  - name: llama-3-8b-instruct-g5-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct-g5.24xl\n    ep_name: llama-3-8b-instruct-g5-24xl\n    instance_type: \"ml.g5.24xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      # SM_NUM_GPUS: \"1\"\n      # SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\n# parameters related to how the final report is generated    \nreport:\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "llama3-8b-trn1-32xl-byoe-g5-24xl.yml",
    "path": "fmbench/configs/llama3/8b/llama3-8b-trn1-32xl-byoe-g5-24xl.yml",
    "directory": "fmbench/configs/llama3/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama3-8b-trn1-32xl-byoe-g5-24xl\"      \n  model_name: \"llama3-8b\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: llama3_tokenizer\n  \n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the \n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: llama-3-8b-instruct-trn1-32xl-djl-inference:0.27.0-neuronx-sdk2.18.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.  \n    model_id: llama-3-8b-instruct\n    model_version:\n    model_name: llama3-8b-trn1.32xl\n    ep_name: meta-llama-Meta-Llama-3-8B-Instruct-inf-2024-06-07-16-59-37-956\n    instance_type: \"ml.trn1.32xlarge\"\n    image_uri: \n    deploy: no\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script: \n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # currently the llama3-8b jumpstart model is compiled with a batch size of 1\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      model_id_or_path='/tmp/.djl.ai/download/ae03dd100c208acd82b5dbed563c971de864c408'\n      rolling_batch=<RollingBatchEnum.auto: 'auto'>\n      tensor_parallel_degree=8\n      trust_remote_code=False\n      enable_streaming=<StreamingEnum.false: 'false'>\n      batch_size=4\n      max_rolling_batch_size=4\n      dtype=<Dtype.f16: 'fp16'>\n      revision=None\n      output_formatter=None\n      waiting_steps=None\n      is_mpi=False\n      draft_model_id=None\n      spec_length=0\n      neuron_optimize_level=None\n      enable_mixed_precision_accumulation=False\n      enable_saturate_infinity=False\n      n_positions=4096\n      unroll=None\n      load_in_8bit=False\n      low_cpu_mem_usage=False\n      load_split_model=True\n      context_length_estimate=None\n      amp='f16'\n      quantize=None\n      compiled_graph_path=None\n      task=None\n      save_mp_checkpoint_path=None\n      group_query_attention=None\n      model_loader=<TnXModelLoaders.tnx: 'tnx'>\n      rolling_batch_strategy=<TnXGenerationStrategy.continuous_batching: 'continuous_batching'>\n      fuse_qkv=False\n      on_device_embedding=False\n      attention_layout=None\n      collectives_layout=None\n      cache_layout=None\n      partition_schema=None\n      all_reduce_dtype=None\n      cast_logits_dtype=None\n\n  - name: llama-3-8b-instruct-g5-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct-g5.24xl\n    ep_name: llama-3-8b-instruct-g5-24xl\n    instance_type: \"ml.g5.24xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      # SM_NUM_GPUS: \"1\"\n      # SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\n# parameters related to how the final report is generated    \nreport:\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-1-70b-inf2-48xl-deploy-ec2-djl.yml",
    "path": "fmbench/configs/llama3.1/70b/config-ec2-llama3-1-70b-inf2-48xl-deploy-ec2-djl.yml",
    "directory": "fmbench/configs/llama3.1/70b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-70b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-70b-inf2.48xl-ec2-djl\"      \n  model_name: \"llama3-1-70b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-70b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-70B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-70b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"inf2.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-70b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 3600\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 24\n      shm_size: 12g\n      model_loading_timeout: 3600\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=meta-llama/Llama-3.1-70b-Instruct\n      option.tensor_parallel_degree=24\n      option.n_positions=4096\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=8\n      option.dtype=fp16\n      option.model_loading_timeout=2400\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10 \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O2\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-1-70b-inf2-deploy-sm.yml",
    "path": "fmbench/configs/llama3.1/70b/config-ec2-llama3-1-70b-inf2-deploy-sm.yml",
    "directory": "fmbench/configs/llama3.1/70b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-70b-inf2.48xl-ec2\"      \n  model_name: \"llama3-1-70b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-70B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    sagemaker_execution_role: {role_arn}\n    # S3 bucket to which metrics, plots and reports would be written to\n    bucket: {write_bucket} ## add the name of your desired bucket\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    prefix: \"lmi\"\n    model_id: meta-llama/Llama-3.1-70B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-70B-Instruct\"\n    model_id_wo_repo: \"Meta-Llama-3.1-70B-Instruct\"\n    model_id_wo_repo_split: \"Meta-Llama-3.1-70B-Instruct-split\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name:\n    instance_type: \"ml.inf2.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:  \n    deployment_script: neuron_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: sagemaker_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      batch_size: \"1\"\n      num_neuron_cores: \"24\"\n      neuron_version: \"2.19.1\"\n      model_loading_timeout: \"2400\"\n      instance_count: \"1\"\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=s3://{write_bucket}/lmi/Meta-Llama-3.1-70B-Instruct/Meta-Llama-3.1-70B-Instruct-split/\n      option.load_split_model=True\n      option.tensor_parallel_degree=24\n      option.n_positions=8192\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=8\n      option.dtype=fp16\n      option.model_loading_timeout=2400\n      option.neuron_optimize_level=2\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10 \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-1-70b-p4de.24xl-deploy-ec2-large-prompts.yml",
    "path": "fmbench/configs/llama3.1/70b/config-ec2-llama3-1-70b-p4de.24xl-deploy-ec2-large-prompts.yml",
    "directory": "fmbench/configs/llama3.1/70b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-70b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-70b-p4de.24xl-ec2-synthetic-large-prompts\"      \n  model_name: \"llama3-1-70b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  # This dataset contains prompt sizes from 1k-2k-10k-20k and increments\n  # by 10k up until 100-110k tokens in prompt payload size.\n  # The prompt template used for this test is for summarization\n  - synthetic_data_large_prompts.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3_summarization.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en    \n    min_length_in_tokens: 2000\n    max_length_in_tokens: 10000\n    payload_file: payload_en_2000-10000.jsonl\n  - language: en    \n    min_length_in_tokens: 10000\n    max_length_in_tokens: 20000\n    payload_file: payload_en_10000-20000.jsonl\n  - language: en\n    min_length_in_tokens: 20000\n    max_length_in_tokens: 30000\n    payload_file: payload_en_20000-30000.jsonl\n  - language: en\n    min_length_in_tokens: 30000\n    max_length_in_tokens: 40000\n    payload_file: payload_en_30000-40000.jsonl\n  - language: en\n    min_length_in_tokens: 40000\n    max_length_in_tokens: 50000\n    payload_file: payload_en_40000-50000.jsonl\n  - language: en\n    min_length_in_tokens: 50000\n    max_length_in_tokens: 60000\n    payload_file: payload_en_50000-60000.jsonl\n  - language: en\n    min_length_in_tokens: 60000\n    max_length_in_tokens: 70000\n    payload_file: payload_en_60000-70000.jsonl\n  - language: en\n    min_length_in_tokens: 70000\n    max_length_in_tokens: 80000\n    payload_file: payload_en_70000-80000.jsonl\n  - language: en\n    min_length_in_tokens: 80000\n    max_length_in_tokens: 90000\n    payload_file: payload_en_80000-90000.jsonl\n  - language: en\n    min_length_in_tokens: 90000\n    max_length_in_tokens: 100000\n    payload_file: payload_en_90000-100000.jsonl\n  - language: en\n    min_length_in_tokens: 100000\n    max_length_in_tokens: 110000\n    payload_file: payload_en_100000-110000.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_80000-90000\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 512\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-70b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-70B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-70b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"p4de.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged witsh multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-70b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and Inferentia settings for the instance\n      # '-u djl' tells the container to use the Deep Java Library (DJL) runtime\n      # '--device /dev/neuron0 ... /dev/neuron5' specifies the Inferentia devices to be used\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-70b-Instruct\n      option.rolling_batch=lmi-dist\n      option.max_model_len=131072\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-10000.jsonl\n    - payload_en_10000-20000.jsonl\n    - payload_en_20000-30000.jsonl\n    - payload_en_30000-40000.jsonl\n    - payload_en_40000-50000.jsonl\n    - payload_en_50000-60000.jsonl\n    - payload_en_60000-70000.jsonl\n    - payload_en_70000-80000.jsonl\n    - payload_en_80000-90000.jsonl\n    - payload_en_90000-100000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 60\n  cost_per_10k_txn_budget: 7000\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-1-70b-p4de.24xl-deploy-ec2-longbench.yml",
    "path": "fmbench/configs/llama3.1/70b/config-ec2-llama3-1-70b-p4de.24xl-deploy-ec2-longbench.yml",
    "directory": "fmbench/configs/llama3.1/70b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-70b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-70b-p4de.24xl-ec2-longbench\"      \n  model_name: \"llama3-1-70b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-70b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-70B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-70b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"p4de.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged witsh multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-70b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-70b-Instruct\n      option.rolling_batch=lmi-dist\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 5\n    - 10\n    - 15\n    - 20\n    - 25\n    - 30\n    - 35\n    - 37\n    - 40\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 200\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-1-70b-p4de.24xl-deploy-ec2-summarization.yml",
    "path": "fmbench/configs/llama3.1/70b/config-ec2-llama3-1-70b-p4de.24xl-deploy-ec2-summarization.yml",
    "directory": "fmbench/configs/llama3.1/70b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-70b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-70b-p4de.24xl-ec2-synthetic-large-prompts\"      \n  model_name: \"llama3-1-70b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  # This dataset contains prompt sizes from 1k-2k-10k-20k and increments\n  # by 10k up until 100-110k tokens in prompt payload size.\n  # The prompt template used for this test is for summarization\n  - synthetic_data_large_prompts.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3_summarization.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en    \n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en    \n    min_length_in_tokens: 2500\n    max_length_in_tokens: 3500\n    payload_file: payload_en_2500-3500.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_2500-3500\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 300\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-70b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-70B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-70b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"p4de.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged witsh multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-70b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and Inferentia settings for the instance\n      # '-u djl' tells the container to use the Deep Java Library (DJL) runtime\n      # '--device /dev/neuron0 ... /dev/neuron5' specifies the Inferentia devices to be used\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-70b-Instruct\n      option.rolling_batch=lmi-dist\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_2500-3500.jsonl\n    - payload_en_3000-4000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 7000\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-1-70b-g5.48xl-tp-8-mc-max-djl.yml",
    "path": "fmbench/configs/llama3.1/70b/config-llama3-1-70b-g5.48xl-tp-8-mc-max-djl.yml",
    "directory": "fmbench/configs/llama3.1/70b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-70b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3.1-70b-g5.48xl-tp=8-mc=max-djl-ec2\"      \n  model_name: \"llama3-1-70b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-70b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-70B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-70b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged witsh multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-70b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-70b-Instruct\n      option.rolling_batch=lmi-dist\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 200\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-1-70b-g6.48xl-tp-8-mc-max-djl.yml",
    "path": "fmbench/configs/llama3.1/70b/config-llama3-1-70b-g6.48xl-tp-8-mc-max-djl.yml",
    "directory": "fmbench/configs/llama3.1/70b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-70b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3.1-70b-g6.48xl-tp=8-mc=max-djl-ec2\"      \n  model_name: \"Meta-Llama-3.1-70b-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-70b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-70B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-70b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g6.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged witsh multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-70b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-70b-Instruct\n      option.rolling_batch=lmi-dist\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 200\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-1-70b-g6e.24xl-tp-4-mc-max-djl.yml",
    "path": "fmbench/configs/llama3.1/70b/config-llama3-1-70b-g6e.24xl-tp-4-mc-max-djl.yml",
    "directory": "fmbench/configs/llama3.1/70b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-70b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3.1-70b-g6e.24xl-tp=4-mc=max-djl-ec2\"      \n  model_name: \"Meta-Llama-3.1-70b-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-70b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-70B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-70b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g6e.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged witsh multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-70b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-70b-Instruct\n      option.rolling_batch=lmi-dist\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n    - 7\n    - 8\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 200\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-1-70b-g6e.48xl-tp-8-mc-max-djl.yml",
    "path": "fmbench/configs/llama3.1/70b/config-llama3-1-70b-g6e.48xl-tp-8-mc-max-djl.yml",
    "directory": "fmbench/configs/llama3.1/70b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-70b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3.1-70b-g6e.48xl-tp=8-mc=max-djl-ec2\"      \n  model_name: \"Meta-Llama-3.1-70b-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-70b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-70B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-70b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g6e.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged witsh multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-70b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-70b-Instruct\n      option.rolling_batch=lmi-dist\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n    - 7\n    - 8\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 200\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-1-70b-inf2.48xl-deploy-ec2-triton.yml",
    "path": "fmbench/configs/llama3.1/70b/config-llama3-1-70b-inf2.48xl-deploy-ec2-triton.yml",
    "directory": "fmbench/configs/llama3.1/70b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-70b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-70b-inf2.48xl-ec2-triton\"      \n  model_name: \"llama3-1-70b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    top_k: 50  \n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-70b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-70B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-70b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8080/v2/models/Llama-3.1-70B-Instruct/generate'\n    instance_type: \"inf2.48xlarge\"\n    image_uri: tritonserver-neuronx:fmbench \n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-70b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 10000\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # For deploying a model using the triton inference container: \n      # mention the backend type, if any. Options for triton are: [djl, vllm or tensorrt]\n      backend: djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured,\n      # max: FMBench figures out the max number of model containers to be run\n      #      based on TP degree configured and number of neuron cores/GPUs available.\n      #      For example, if TP=2, GPUs=8 then FMBench will start 4 containers and 1 load balancer,\n      # auto: only supported if the underlying inference container would automatically \n      #       start multiple copies of the model internally based on TP degree and neuron cores/GPUs\n      #       available. In this case only a single container is created, no load balancer is created.\n      #       The DJL serving containers supports auto.  \n      model_copies: max\n      shm_size: 12g\n      # The model.json parameters are replaced within the model.json file\n      # for the triton on vllm/djl/tensorrt options. The model.json already contains\n      # the tp degree and model id from above in this config file. This is a dictionary\n      # that contains other variables, such as max_num_seqs, dtype, max_model_len, block_size and more\n      # For tensorrt, the tp degree, batch size and other relevant parameters are \n      # extracted directly from the inference spec.\n      container_params:\n        # tp degree is a mandatory parameter\n        tp_degree: 24\n        amp: \"f16\"\n        attention_layout: 'BSH'\n        collectives_layout: 'BSH'\n        context_length_estimate: 3072, 3584, 4096\n        max_rolling_batch_size: 8\n        model_loader: \"tnx\"\n        model_loading_timeout: 2400\n        n_positions: 4096\n        output_formatter: \"json\"\n        rolling_batch: \"auto\"\n        rolling_batch_strategy: \"continuous_batching\"\n        trust_remote_code: true\n        # modify the serving properties to match your model and requirements\n        serving.properties:\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6 \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O3\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 200\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-1-70b-inf2.48xl-triton-tp24.yml",
    "path": "fmbench/configs/llama3.1/70b/config-llama3-1-70b-inf2.48xl-triton-tp24.yml",
    "directory": "fmbench/configs/llama3.1/70b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-70b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-70b-inf2.48xl-ec2\"      \n  model_name: \"llama3-1-70b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    top_k: 50  \n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-70b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-70B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-70b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8080/v2/models/Llama-3.1-70B-Instruct/generate' \n    instance_type: \"inf2.48xlarge\"\n    image_uri: tritonserver-neuronx:fmbench \n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-70b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 10000\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # For deploying a model using the triton inference container: \n      # mention the backend type, if any. Options for triton are: [djl, vllm or tensorrt]\n      backend: djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured,\n      # max: FMBench figures out the max number of model containers to be run\n      #      based on TP degree configured and number of neuron cores/GPUs available.\n      #      For example, if TP=2, GPUs=8 then FMBench will start 4 containers and 1 load balancer,\n      # auto: only supported if the underlying inference container would automatically \n      #       start multiple copies of the model internally based on TP degree and neuron cores/GPUs\n      #       available. In this case only a single container is created, no load balancer is created.\n      #       The DJL serving containers supports auto.  \n      model_copies: max\n      shm_size: 12g\n      # The model.json parameters are replaced within the model.json file\n      # for the triton on vllm/djl/tensorrt options. The model.json already contains\n      # the tp degree and model id from above in this config file. This is a dictionary\n      # that contains other variables, such as max_num_seqs, dtype, max_model_len, block_size and more\n      # For tensorrt, the tp degree, batch size and other relevant parameters are \n      # extracted directly from the inference spec.\n      container_params:\n        # tp degree is a mandatory parameter\n        tp_degree: 24\n        amp: \"f16\"\n        attention_layout: 'BSH'\n        collectives_layout: 'BSH'\n        context_length_estimate: 3072, 3584, 4096\n        max_rolling_batch_size: 8\n        model_loader: \"tnx\"\n        model_loading_timeout: 2400\n        n_positions: 4096\n        output_formatter: \"json\"\n        rolling_batch: \"auto\"\n        rolling_batch_strategy: \"continuous_batching\"\n        neuron_optimize_level: 2\n        trust_remote_code: true\n        # modify the serving properties to match your model and requirements\n        serving.properties:\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6 \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O2\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 200\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-1-70b-p5-djl-lmi.yml",
    "path": "fmbench/configs/llama3.1/70b/config-llama3-1-70b-p5-djl-lmi.yml",
    "directory": "fmbench/configs/llama3.1/70b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-70b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-70b-p5.48xl-ec2-longbench\"      \n  model_name: \"llama3-1-70b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-70b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-70B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-70b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"p5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged witsh multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-70b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-70b-Instruct\n      option.rolling_batch=lmi-dist\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 10\n    - 15\n    - 20\n    - 25\n    - 30\n    - 33\n    - 35\n    - 40\n    - 45\n    - 50\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 200\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-1-70b-trn1.32xl-deploy-ec2-triton.yml",
    "path": "fmbench/configs/llama3.1/70b/config-llama3-1-70b-trn1.32xl-deploy-ec2-triton.yml",
    "directory": "fmbench/configs/llama3.1/70b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-70b-trn1.32xl-ec2-triton\"      \n  model_name: \"Meta-Llama-3-70B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    top_k: 50  \n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-70B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-70B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-70B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8080/v2/models/Llama-3.1-70B-Instruct/generate' \n    instance_type: \"trn1.32xlarge\"\n    image_uri: tritonserver-neuronx:fmbench \n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 10000\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # For deploying a model using the triton inference container: \n      # mention the backend type, if any. Options for triton are: [djl, vllm or tensorrt]\n      backend: djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured,\n      # max: FMBench figures out the max number of model containers to be run\n      #      based on TP degree configured and number of neuron cores/GPUs available.\n      #      For example, if TP=2, GPUs=8 then FMBench will start 4 containers and 1 load balancer,\n      # auto: only supported if the underlying inference container would automatically \n      #       start multiple copies of the model internally based on TP degree and neuron cores/GPUs\n      #       available. In this case only a single container is created, no load balancer is created.\n      #       The DJL serving containers supports auto.  \n      model_copies: max\n      shm_size: 12g\n      # The model.json parameters are replaced within the model.json file\n      # for the triton on vllm/djl/tensorrt options. The model.json already contains\n      # the tp degree and model id from above in this config file. This is a dictionary\n      # that contains other variables, such as max_num_seqs, dtype, max_model_len, block_size and more\n      # For tensorrt, the tp degree, batch size and other relevant parameters are \n      # extracted directly from the inference spec.\n      container_params:\n        # tp degree is a mandatory parameter\n        tp_degree: 32\n        amp: \"f16\"\n        attention_layout: 'BSH'\n        collectives_layout: 'BSH'\n        context_length_estimate: 3072, 3584, 4096\n        max_rolling_batch_size: 8\n        model_loader: \"tnx\"\n        model_loading_timeout: 2400\n        n_positions: 4096\n        output_formatter: \"json\"\n        rolling_batch: \"auto\"\n        rolling_batch_strategy: \"continuous_batching\"\n        trust_remote_code: true\n        # modify the serving properties to match your model and requirements\n        serving.properties:\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n    - 8\n    - 10\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O3\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 200\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-1-7b-inf2.48xl-triton-ec2.yml",
    "path": "fmbench/configs/llama3.1/70b/config-llama3-1-7b-inf2.48xl-triton-ec2.yml",
    "directory": "fmbench/configs/llama3.1/70b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-70b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-70b-inf2.48xl-ec2\"      \n  model_name: \"llama3-1-70b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    top_k: 50  \n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-70b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-70B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-70b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8080/v2/models/Llama-3.1-70B-Instruct/generate'\n    instance_type: \"inf2.48xlarge\"\n    image_uri: tritonserver-neuronx:fmbench \n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-70b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 10000\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # For deploying a model using the triton inference container: \n      # mention the backend type, if any. Options for triton are: [djl, vllm or tensorrt]\n      backend: djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured,\n      # max: FMBench figures out the max number of model containers to be run\n      #      based on TP degree configured and number of neuron cores/GPUs available.\n      #      For example, if TP=2, GPUs=8 then FMBench will start 4 containers and 1 load balancer,\n      # auto: only supported if the underlying inference container would automatically \n      #       start multiple copies of the model internally based on TP degree and neuron cores/GPUs\n      #       available. In this case only a single container is created, no load balancer is created.\n      #       The DJL serving containers supports auto.  \n      model_copies: max\n      shm_size: 12g\n      # The model.json parameters are replaced within the model.json file\n      # for the triton on vllm/djl/tensorrt options. The model.json already contains\n      # the tp degree and model id from above in this config file. This is a dictionary\n      # that contains other variables, such as max_num_seqs, dtype, max_model_len, block_size and more\n      # For tensorrt, the tp degree, batch size and other relevant parameters are \n      # extracted directly from the inference spec.\n      container_params:\n        # tp degree is a mandatory parameter\n        tp_degree: 24\n        amp: \"f16\"\n        attention_layout: 'BSH'\n        collectives_layout: 'BSH'\n        context_length_estimate: 3072, 3584, 4096\n        max_rolling_batch_size: 8\n        model_loader: \"tnx\"\n        model_loading_timeout: 2400\n        n_positions: 4096\n        output_formatter: \"json\"\n        rolling_batch: \"auto\"\n        rolling_batch_strategy: \"continuous_batching\"\n        trust_remote_code: true\n        # modify the serving properties to match your model and requirements\n        serving.properties:\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6 \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O3\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 200\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "client-config-ec2-llama3-1-8b.yml",
    "path": "fmbench/configs/llama3.1/8b/client-config-ec2-llama3-1-8b.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-8b-inf2.48xl-ec2\"      \n  model_name: \"llama3-1-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: no\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-8b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://ec2-54-172-72-84.compute-1.amazonaws.com:8080/invocations'\n    instance_type: inf2.48xlarge\n    image_uri:\n    deploy: no #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10 \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-1-8b-g6e-2xlarge-byoe-ollama.yml",
    "path": "fmbench/configs/llama3.1/8b/config-ec2-llama3-1-8b-g6e-2xlarge-byoe-ollama.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama3.1-8b deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g6e.2xl-ec2\"      \n  model_name: \"llama3.1-8b\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: no\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_ollama:\n    model: llama3.1:8b\n    stream: false\n    options:\n      temperature: 0.1\n      top_p: 0.92\n      top_k: 120  \n      num_predict: 100\n    \n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3.1-8b\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"meta-llama/Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:11434/api/generate' \n    instance_type: \"g6e.2xlarge\"\n    image_uri: None\n    deploy: no #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # Privileged Mode makes the docker container run with root. \n      # This basically means that if you are root in a container you have the privileges of root on the host system\n      # Only need this if you need to set VLLM_CPU_OMP_THREADS_BIND env variable.\n      privileged_mode: yes      \n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting:\n      # This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_ollama\n      # if not set assume djl\n      container_type: ollama\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-1-8b-inf2-48xl-deploy-ec2-tp24-bs12.yml",
    "path": "fmbench/configs/llama3.1/8b/config-ec2-llama3-1-8b-inf2-48xl-deploy-ec2-tp24-bs12.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-8b-inf2.48xl-ec2-tp24-bs12\"      \n  model_name: \"llama3-1-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1000-2000\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-8b-Instruct-tp24-bs12\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"inf2.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and Inferentia settings for the instance\n      # '-u djl' tells the container to use the Deep Java Library (DJL) runtime\n      # '--device /dev/neuron0 ... /dev/neuron5' specifies the Inferentia devices to be used\n      gpu_or_neuron_setting: -u djl --device /dev/neuron0 --device /dev/neuron1 --device /dev/neuron2 --device /dev/neuron3 --device /dev/neuron4 --device /dev/neuron5 --device /dev/neuron6 --device /dev/neuron7 --device /dev/neuron8 --device /dev/neuron9 --device /dev/neuron10 --device /dev/neuron11\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 24\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=meta-llama/Meta-Llama-3.1-8b-Instruct\n      option.tensor_parallel_degree=24\n      option.n_positions=4096\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=12\n      option.model_loading_timeout=2400\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10 \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O3\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-1-8b-inf2-48xl-deploy-ec2.yml",
    "path": "fmbench/configs/llama3.1/8b/config-ec2-llama3-1-8b-inf2-48xl-deploy-ec2.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-8b-inf2.48xl-ec2\"      \n  model_name: \"llama3-1-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-8b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"inf2.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and Inferentia settings for the instance\n      # '-u djl' tells the container to use the Deep Java Library (DJL) runtime\n      # '--device /dev/neuron0 ... /dev/neuron5' specifies the Inferentia devices to be used\n      gpu_or_neuron_setting: -u djl --device /dev/neuron0  --device /dev/neuron1  --device /dev/neuron2  --device /dev/neuron3  --device /dev/neuron4  --device /dev/neuron5 --device /dev/neuron6 --device /dev/neuron7 --device /dev/neuron8 --device /dev/neuron9 --device /dev/neuron10 --device /dev/neuron11 \n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=meta-llama/Meta-Llama-3.1-8b-Instruct\n      option.load_split_model=True\n      option.tensor_parallel_degree=8\n      option.n_positions=4096\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=8\n      option.dtype=fp16\n      option.model_loading_timeout=2400\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10 \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O3\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-1-8b-inf2.yml",
    "path": "fmbench/configs/llama3.1/8b/config-ec2-llama3-1-8b-inf2.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-8b-inf2.48xl-ec2\"      \n  model_name: \"llama3-1-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-8b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    sagemaker_execution_role: {role_arn}\n    # S3 bucket to which metrics, plots and reports would be written to\n    bucket: {write_bucket} ## add the name of your desired bucket\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    prefix: \"lmi\"\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8b-Instruct\"\n    model_id_wo_repo: \"Meta-Llama-3.1-8b-Instruct\"\n    model_id_wo_repo_split: \"Meta-Llama-3.1-8b-Instruct-split\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name:\n    instance_type: \"ml.inf2.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:  \n    deployment_script: neuron_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: sagemaker_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      batch_size: \"1\"\n      num_neuron_cores: \"24\"\n      neuron_version: \"2.19.1\"\n      model_loading_timeout: \"2400\"\n      instance_count: \"1\"\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=s3://{write_bucket}/lmi/Meta-Llama-3.1-8b-Instruct/Meta-Llama-3.1-8b-Instruct-split/\n      option.load_split_model=True\n      option.tensor_parallel_degree=24\n      option.n_positions=8192\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=8\n      option.dtype=fp16\n      option.model_loading_timeout=2400\n      option.neuron_optimize_level=2\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10 \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-1-8b-p4-tp-2-mc-max.yml",
    "path": "fmbench/configs/llama3.1/8b/config-ec2-llama3-1-8b-p4-tp-2-mc-max.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-p4de.24xl-ec2\"      \n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"p4de.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 10\n    - 30\n    - 35\n    - 40\n    - 50\n    - 60\n    - 65\n    - 70\n    - 75\n    - 100\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-1-8b-p4-tp-4-mc-max.yml",
    "path": "fmbench/configs/llama3.1/8b/config-ec2-llama3-1-8b-p4-tp-4-mc-max.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-p4de.24xl-ec2\"      \n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"p4de.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 10\n    - 30\n    - 35\n    - 40\n    - 50\n    - 60\n    - 65\n    - 70\n    - 75\n    - 100\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-1-8b-p4-tp-8-mc-max.yml",
    "path": "fmbench/configs/llama3.1/8b/config-ec2-llama3-1-8b-p4-tp-8-mc-max.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-p4de.24xl-ec2\"      \n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: no\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"p4de.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 10\n    - 30\n    - 35\n    - 40\n    - 50\n    - 60\n    - 65\n    - 70\n    - 75\n    - 100\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-1-8b-p5-tp-2-mc-max.yml",
    "path": "fmbench/configs/llama3.1/8b/config-ec2-llama3-1-8b-p5-tp-2-mc-max.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-p5.2xl-ec2\"      \n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: no\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 924\n    payload_file: payload_en_1-924.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"p5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    - 10\n    # - 12\n    # - 15\n    # - 20\n    # - 25\n    - 30\n    # - 40\n    - 50\n    - 60\n    - 65\n    - 70\n    - 75\n    #- 100\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-1-8b-tp-8-mc-auto-p5.yml",
    "path": "fmbench/configs/llama3.1/8b/config-ec2-llama3-1-8b-tp-8-mc-auto-p5.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-p5.48xl-ec2\"      \n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 924\n    payload_file: payload_en_1-924.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"p5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: auto\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    - 10\n    # - 12\n    # - 15\n    # - 20\n    # - 25\n    - 30\n    # - 40\n    # - 50\n    # - 60\n    # - 65\n    # - 70\n    # - 75\n    #- 100\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-ec2-llama3-1-8b-trn1-32xl-deploy-ec2-tp32-bs8.yml",
    "path": "fmbench/configs/llama3.1/8b/config-ec2-llama3-1-8b-trn1-32xl-deploy-ec2-tp32-bs8.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-8b-trn1-32xl-deploy-ec2-tp32-bs8\"      \n  model_name: \"llama3-1-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1000-2000\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-1-8b-trn1-32xl-deploy-ec2-tp32-bs8\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"trn1.32xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and Inferentia settings for the instance\n      # '-u djl' tells the container to use the Deep Java Library (DJL) runtime\n      # '--device /dev/neuron0 ... /dev/neuron5' specifies the Inferentia devices to be used\n      gpu_or_neuron_setting: -u djl --device /dev/neuron0 --device /dev/neuron1 --device /dev/neuron2 --device /dev/neuron3 --device /dev/neuron4 --device /dev/neuron5 --device /dev/neuron6 --device /dev/neuron7 --device /dev/neuron8 --device /dev/neuron9 --device /dev/neuron10 --device /dev/neuron11 --device /dev/neuron12 --device /dev/neuron13 --device /dev/neuron14 --device /dev/neuron15\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 32\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=meta-llama/Meta-Llama-3.1-8b-Instruct\n      option.tensor_parallel_degree=32\n      option.n_positions=4096\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=8\n      option.model_loading_timeout=2400\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10 \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O3\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-1-8b-g5.12xlarge-djl-lmi-sm.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3-1-8b-g5.12xlarge-djl-lmi-sm.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama3-1-8b-g5-12xlarge-djl-lmi\"      \n  model_name: \"llama3-1-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n \nmetrics:\n  dataset_of_interest: en_3000-3840\n  \npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n    return_full_text: False\n\nexperiments:\n \n  - name: llama-3-1-8b-instruct-g6-48xl\n    model_id: meta-llama/Llama-3.1-8B-Instruct\n    model_version: \"*\"\n    model_name: Llama-3-1-8B-Instruct\n    ep_name: Llama-3-1-8B-Instruct\n    instance_type: \"ml.g5.12xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124'\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.p\n      y\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      #HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      # SM_NUM_GPUS: \"1\"\n      # SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n      \nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama3-1-8b-p5en-djl.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3-1-8b-p5en-djl.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"llama3-1-8b-p5en-djl-lmi\"      \n  model_name: \"llama3-1-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n \nmetrics:\n  dataset_of_interest: en_3000-3840\n  \npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n    return_full_text: False\n\nexperiments:\n  - name: llama-3-1-8b-instruct-p5en-48xlarge\n    model_id: meta-llama/Llama-3.1-8B-Instruct\n    hf_tokenizer_model_id: meta-llama/Llama-3.1-8B-Instruct\n    model_version: \"*\"\n    model_name: Llama-3-1-8B-Instruct\n    ep_name: Llama-3-1-8B-Instruct\n    download_from_hf_place_in_s3: yes\n    model_s3_path: s3://{write_bucket}/meta-llama/Llama-3.1-8B-Instruct\n    instance_type: \"ml.p5en.48xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.31.0-lmi13.0.0-cu124'\n    deploy: yes\n    bucket: {write_bucket}\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=4\n      option.model_id=s3://{write_bucket}/meta-llama/Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      #HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      # SM_NUM_GPUS: \"1\"\n      # SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n      \nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-g5-ec2.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-g5-ec2.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-1-8b-g5.2xl-ec2\"      \n  model_name: \"llama3-1-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 924\n    payload_file: payload_en_1-924.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-1-8b-instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3-1-8b-instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g5.2xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 1\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=1\n      option.model_id=meta-llama/Meta-Llama-3.1-8b-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10 \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-g5.12xl-tp-2-mc-auto-ec2.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-g5.12xl-tp-2-mc-auto-ec2.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g5.12xl-ec2\"      \n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 924\n    payload_file: payload_en_1-924.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g5.12xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: auto\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-g5.12xl-tp-2-mc-max-ec2.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-g5.12xl-tp-2-mc-max-ec2.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g5.12xl-ec2\"      \n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 924\n    payload_file: payload_en_1-924.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g5.12xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-g5.12xl-tp-4-mc-max-ec2.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-g5.12xl-tp-4-mc-max-ec2.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g5.12xl-ec2\"      \n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 924\n    payload_file: payload_en_1-924.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g5.12xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-g5.24xl-tp-2-mc-auto-ec2.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-g5.24xl-tp-2-mc-auto-ec2.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g5.24xl-ec2\"      \n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g5.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: auto\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-g5.24xl-tp-2-mc-max-ec2.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-g5.24xl-tp-2-mc-max-ec2.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g5.24xl-ec2\"      \n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g5.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-g5.24xl-tp-4-mc-max-ec2.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-g5.24xl-tp-4-mc-max-ec2.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g5.24xl-ec2\"      \n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g5.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-g5.2xl-g5.4xl-sm.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-g5.2xl-g5.4xl-sm.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"Llama3-1-8b-g5\"\n  model_name: \"Llama3-1-8b\"\n \naws:\n  region: {region}\n  sagemaker_execution_role: {role_arn}\n  bucket: {write_bucket}\n\ndir_paths:\n    data_prefix: data\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata\n\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts\n    script_files:\n    - hf_token.txt\n    configs_prefix: configs\n    config_files:\n    - pricing.yml\n    source_data_prefix: source_data\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_1_tokenizer\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_get_evaluations.ipynb: no\n    5_model_metric_analysis.ipynb: yes\n    6_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n \n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\nmetrics:\n  dataset_of_interest: en_2000-3000\n  \npricing: pricing.yml\n  \ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\nexperiments:\n  - name: Llama3-1-8b-g5.2xl-djl-inference:0.29.0-lmi11.0.0-cu124\n    model_id: meta-llama/Llama-3.1-8B-Instruct\n    model_version: \"*\"\n    model_name: Meta-Llama-3-1-8B-Instruct\n    ep_name: Meta-Llama-3-1-8B-Instruct-g5-2xl\n    download_from_hf_place_in_s3: yes\n    model_s3_path: s3://{write_bucket}/meta-llama/Llama-3.1-8B-Instruct\n    instance_type: \"ml.g5.xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n      engine=Python\n      option.model_id=s3://{write_bucket}/meta-llama/Llama-3.1-8B-Instruct\n      option.dtype=fp16\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    # - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n\n    accept_eula: true\n    env:\n  - name: Llama3-1-8b-g5.4xl-djl-inference:0.29.0-lmi11.0.0-cu124\n    model_id: meta-llama/Llama-3.1-8B-Instruct\n    model_version: \"*\"\n    model_name: Meta-Llama-3-1-8B-Instruct\n    ep_name: Meta-Llama-3-1-8B-Instruct-g5-4xl\n    download_from_hf_place_in_s3: yes\n    model_s3_path: s3://{write_bucket}/meta-llama/Llama-3.1-8B-Instruct\n    instance_type: \"ml.g5.2xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n      engine=Python\n      option.model_id=s3://{write_bucket}/meta-llama/Llama-3.1-8B-Instruct\n      option.dtype=fp16\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    # - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    # - 4\n    # - 10\n\n    accept_eula: true\n    env:\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n  latency_vs_token_len_chart: \n    y_ticks:\n    title: \"Effect of token length on inference latency for \\\"meta-llama/Meta-Llama-3.1-8B-Instruct\\\"\"\n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-g5.2xl-tp-1-mc-max-ec2-conc-1-2.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-g5.2xl-tp-1-mc-max-ec2-conc-1-2.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g5.2xl-tp=1-mc=max-ec2\"      \n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_2000-3000\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g5.2xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 1\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=1\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-g5.2xl-tp-1-mc-max-ec2.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-g5.2xl-tp-1-mc-max-ec2.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g5.2xl-tp=1-mc=max-ec2\"      \n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    hf_model_id: meta-llama/Llama-3.1-8B-Instruct\n    model_version:\n    model_name: \"Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g5.2xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 1\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=1\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-g5.48xl-tp-2-mc-auto-ec2.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-g5.48xl-tp-2-mc-auto-ec2.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g5.48xl-ec2\"      \n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: auto\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-g5.48xl-tp-2-mc-max-ec2.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-g5.48xl-tp-2-mc-max-ec2.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g5.48xl-ec2\"      \n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-g5.48xl-tp-4-mc-max-ec2.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-g5.48xl-tp-4-mc-max-ec2.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g5.48xl-ec2\"      \n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-g5.48xl-tp-8-mc-max-ec2.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-g5.48xl-tp-8-mc-max-ec2.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g5.48xl-ec2\"      \n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-g5.xl-tp-1-mc-max-ec2-conc-1-2.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-g5.xl-tp-1-mc-max-ec2-conc-1-2.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g5.xl-tp=1-mc=max-ec2\"      \n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_2000-3000\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g5.xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 1\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=1\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-g5.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-g5.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"Llama3-1-8b-g5\"\n  model_name: \"Llama3-1-8b\"\n \naws:\n  region: {region}\n  sagemaker_execution_role: {role_arn}\n  bucket: {write_bucket}\n\ndir_paths:\n    data_prefix: data\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata\n\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts\n    script_files:\n    - hf_token.txt\n    configs_prefix: configs\n    config_files:\n    - pricing.yml\n    source_data_prefix: source_data\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_1_tokenizer\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_get_evaluations.ipynb: yes\n    5_model_metric_analysis.ipynb: yes\n    6_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n \n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \npricing: pricing.yml\n  \ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\nexperiments:\n  - name: Llama3-1-8b-g5.12xl-djl-inference:0.29.0-lmi11.0.0-cu124\n    model_id: meta-llama/Llama-3.1-8B-Instruct\n    model_version: \"*\"\n    model_name: Meta-Llama-3-1-8B-Instruct\n    ep_name: Meta-Llama-3-1-8B-Instruct-g5-12xl\n    download_from_hf_place_in_s3: no\n    model_s3_path: s3://{write_bucket}/meta-llama/Llama-3.1-8B-Instruct\n    instance_type: \"ml.g5.12xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n      engine=Python\n      option.model_id=s3://{write_bucket}/meta-llama/Llama-3.1-8B-Instruct\n      option.dtype=fp16\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 10\n\n    accept_eula: true\n    env:\n  - name: Llama3-1-8b-g5.24xl-djl-inference:0.29.0-lmi11.0.0-cu124\n    model_id: meta-llama/Llama-3.1-8B-Instruct\n    model_version: \"*\"\n    model_name: Meta-Llama-3-1-8B-Instruct\n    ep_name: Meta-Llama-3-1-8B-Instruct-g5-24xl\n    download_from_hf_place_in_s3: no\n    model_s3_path: s3://{write_bucket}/meta-llama/Llama-3.1-8B-Instruct\n    instance_type: \"ml.g5.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n      engine=Python\n      option.model_id=s3://{write_bucket}/meta-llama/Llama-3.1-8B-Instruct\n      option.dtype=fp16\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 10\n\n    accept_eula: true\n    env:\n  - name: Llama3-1-8b-g5.48xl-djl-inference:0.29.0-lmi11.0.0-cu124\n    model_id: meta-llama/Llama-3.1-8B-Instruct\n    model_version: \"*\"\n    model_name: Meta-Llama-3-1-8B-Instruct\n    ep_name: Meta-Llama-3-1-8B-Instruct-g5-48xl\n    download_from_hf_place_in_s3: no\n    model_s3_path: s3://{write_bucket}/meta-llama/Llama-3.1-8B-Instruct\n    instance_type: \"ml.g5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n      engine=Python\n      option.model_id=s3://{write_bucket}/meta-llama/Llama-3.1-8B-Instruct\n      option.dtype=fp16\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 10\n\n    accept_eula: true\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n  latency_vs_token_len_chart: \n    y_ticks:\n    title: \"Effect of token length on inference latency for \\\"meta-llama/Meta-Llama-3.1-8B-Instruct\\\"\"\n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-g6e.12xl-tp-2-mc-max-djl.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-g6e.12xl-tp-2-mc-max-djl.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g6e.12xl-tp=2-mc=max-djl-ec2\"      \n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g6e.12xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    - 14\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-g6e.12xl-tp-4-mc-max-djl.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-g6e.12xl-tp-4-mc-max-djl.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g6e.12xl-tp=4-mc=max-djl-ec2\"      \n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g6e.12xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-g6e.24xl-tp-2-mc-max-djl.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-g6e.24xl-tp-2-mc-max-djl.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g6e.24xl-tp=2-mc=max-djl-ec2\"      \n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g6e.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    - 14\n    - 15\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-g6e.24xl-tp-4-mc-max-djl.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-g6e.24xl-tp-4-mc-max-djl.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g6e.24xl-tp=4-mc=max-djl-ec2\"      \n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g6e.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-g6e.2xl-ollama.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-g6e.2xl-ollama.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g6e.2xl-ec2_ollama\"      \n  model_name: \"llama3.1:8b\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_ollama:\n    model: llama3.1:8b\n    stream: false\n    options:\n      temperature: 0.1\n      top_p: 0.92\n      top_k: 120  \n      num_predict: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3.1:8b\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    hf_tokenizer_model_id: meta-llama/Llama-3.1-8B-Instruct\n    model_id: llama3.1:8b # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: llama3.1:8b\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:11434/api/generate' \n    instance_type: \"g6e.2xlarge\"\n    image_uri: ec2\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_ollama\n      container_type: ollama\n    # modify the serving properties to match your model and requirements\n    serving.properties: \n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-g6e.2xl-tp-1-mc-max-djl.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-g6e.2xl-tp-1-mc-max-djl.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g6e.2xl-tp=1-mc=max-djl-ec2\"      \n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3.1-8b-instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g6e.2xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      tp_degree: 1\n      shm_size: 12g\n      model_loading_timeout: 2400\n      model_copies: max\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=1\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-g6e.48xl-tp-2-mc-max-djl.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-g6e.48xl-tp-2-mc-max-djl.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g6e.48xl-tp=2-mc=max-djl-ec2\"      \n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g6e.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    - 14\n    - 15\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-g6e.48xl-tp-4-mc-max-djl.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-g6e.48xl-tp-4-mc-max-djl.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g6e.48xl-tp=4-mc=max-djl-ec2\"      \n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g6e.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    - 14\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-g6e.48xl-tp-8-mc-max-djl.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-g6e.48xl-tp-8-mc-max-djl.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g6e.48xl-tp=8-mc=max-djl-ec2\"      \n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g6e.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    - 14\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-g6e.4xl-tp-1-mc-max-djl.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-g6e.4xl-tp-1-mc-max-djl.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g6e.4xl-tp=1-mc=max-djl-ec2\"      \n  model_name: \"llama3.1-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-8b-instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3.1-8b-instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g6e.4xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      tp_degree: 1\n      shm_size: 12g\n      model_loading_timeout: 2400\n      model_copies: max\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=1\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-inf2-48xl-deploy-tp-24-ec2.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-inf2-48xl-deploy-tp-24-ec2.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-8b-inf2.48xl-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    return_full_text: no\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"inf2.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and Inferentia settings for the instance\n      # '-u djl' tells the container to use the Deep Java Library (DJL) runtime\n      # '--device /dev/neuron0 ... /dev/neuron5' specifies the Inferentia devices to be used\n      gpu_or_neuron_setting: -u djl --device /dev/neuron0 --device /dev/neuron1 --device /dev/neuron2 --device /dev/neuron3 --device /dev/neuron4 --device /dev/neuron5 --device /dev/neuron6 --device /dev/neuron7 --device /dev/neuron8 --device /dev/neuron9 --device /dev/neuron10 --device /dev/neuron11\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 24\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.tensor_parallel_degree=24\n      option.n_positions=4096\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=4\n      option.model_loading_timeout=2400\n      option.group_query_attention=replicated-heads\n      option.attention_layout=BSH\n      option.fuse_qkv=True\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n    # - 7\n    # - 8\n    # - 9\n    # - 10\n    # - 11\n    # - 12\n    # - 15\n    # - 20\n    # - 25\n    # - 30\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O3\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-inf2-48xl-deploy-tp-8-ec2.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-inf2-48xl-deploy-tp-8-ec2.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-8b-inf2.48xl-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    return_full_text: no\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"inf2.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and Inferentia settings for the instance\n      # '-u djl' tells the container to use the Deep Java Library (DJL) runtime\n      # '--device /dev/neuron0 ... /dev/neuron5' specifies the Inferentia devices to be used\n      gpu_or_neuron_setting: -u djl --device /dev/neuron0 --device /dev/neuron1 --device /dev/neuron2 --device /dev/neuron3 --device /dev/neuron4 --device /dev/neuron5 --device /dev/neuron6 --device /dev/neuron7 --device /dev/neuron8 --device /dev/neuron9 --device /dev/neuron10 --device /dev/neuron11\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.tensor_parallel_degree=8\n      option.n_positions=4096\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=4\n      option.model_loading_timeout=2400\n      option.group_query_attention=replicated-heads\n      option.attention_layout=BSH\n      option.fuse_qkv=True\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O3\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-trn1-32xl-deploy-tp-8-ec2.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-trn1-32xl-deploy-tp-8-ec2.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-8b-trn1.32xl-ec2\"      \n  model_name: \"Meta-Llama-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    return_full_text: no\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"trn1.32xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and Inferentia settings for the instance\n      # '-u djl' tells the container to use the Deep Java Library (DJL) runtime\n      # '--device /dev/neuron0 ... /dev/neuron5' specifies the Inferentia devices to be used\n      gpu_or_neuron_setting: -u djl --device /dev/neuron0 --device /dev/neuron1 --device /dev/neuron2 --device /dev/neuron3 --device /dev/neuron4 --device /dev/neuron5 --device /dev/neuron6 --device /dev/neuron7 --device /dev/neuron8 --device /dev/neuron9 --device /dev/neuron10 --device /dev/neuron11 --device /dev/neuron12 --device /dev/neuron13 --device /dev/neuron14 --device /dev/neuron15\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured,\n      # max: FMBench figures out the max number of model containers to be run\n      #      based on TP degree configured and number of neuron cores/GPUs available.\n      #      For example, if TP=2, GPUs=8 then FMBench will start 4 containers and 1 load balancer,\n      # auto: only supported if the underlying inference container would automatically \n      #       start multiple copies of the model internally based on TP degree and neuron cores/GPUs\n      #       available. In this case only a single container is created, no load balancer is created.\n      #       The DJL serving containers supports auto.  \n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.tensor_parallel_degree=8\n      option.n_positions=4096\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=4\n      option.model_loading_timeout=2400\n      option.group_query_attention=replicated-heads\n      option.attention_layout=BSH\n      #option.fuse_qkv=True\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n    - 8\n    - 10\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O3\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.1-8b-trn32xl-triton-vllm.yml",
    "path": "fmbench/configs/llama3.1/8b/config-llama3.1-8b-trn32xl-triton-vllm.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-8b-trn1.32xl-ec2-triton\"      \n  model_name: \"Meta-Llama-1-3-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    top_k: 50  \n    max_tokens: 4096\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct-triton\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-8B\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8080/v2/models/Meta-Llama-3.1-8B/generate' \n    instance_type: \"trn1.32xlarge\"\n    image_uri: tritonserver-neuronx:fmbench\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and Inferentia settings for the instance\n      # '-u djl' tells the container to use the Deep Java Library (DJL) runtime\n      # '--device /dev/neuron0 ... /dev/neuron5' specifies the Inferentia devices to be used\n      gpu_or_neuron_setting: -u triton --device /dev/neuron0 --device /dev/neuron1 --device /dev/neuron2 --device /dev/neuron3 --device /dev/neuron4 --device /dev/neuron5 --device /dev/neuron6 --device /dev/neuron7 --device /dev/neuron8 --device /dev/neuron9 --device /dev/neuron10 --device /dev/neuron11 --device /dev/neuron12 --device /dev/neuron13 --device /dev/neuron14 --device /dev/neuron15\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 8000\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured,\n      # max: FMBench figures out the max number of model containers to be run\n      #      based on TP degree configured and number of neuron cores/GPUs available.\n      #      For example, if TP=2, GPUs=8 then FMBench will start 4 containers and 1 load balancer,\n      # auto: only supported if the underlying inference container would automatically \n      #       start multiple copies of the model internally based on TP degree and neuron cores/GPUs\n      #       available. In this case only a single container is created, no load balancer is created.\n      #       The DJL serving containers supports auto.  \n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      batch_size: 0\n      shm_size: 12g\n      model_loading_timeout: 2400\n      # The model.json parameters are replaced within the model.json file\n      # for the triton on vllm option. The model.json already contains\n      # the tp degree and model id from above in this config file. This is a dictionary\n      # that contains other variables, such as max_num_seqs, dtype, max_model_len, block_size and more\n      vllm_model_params:\n        max_num_seqs: 4\n        dtype: \"float16\"\n        # The max_model_len and block_size arguments are required to be same as\n        # max sequence length when targeting neuron device.\n        # Currently, this is a known limitation in continuous batching support\n        # in transformers-neuronx. View the latest vllm documentation: https://vllm.readthedocs.io/_/downloads/en/stable/pdf/\n        max_model_len: 8192\n        block_size: 8192\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n    - 7\n    - 8\n    - 9\n    - 10\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "server-config-ec2-llama3-1-8b-inf2-48xl-deploy-ec2.yml",
    "path": "fmbench/configs/llama3.1/8b/server-config-ec2-llama3-1-8b-inf2-48xl-deploy-ec2.yml",
    "directory": "fmbench/configs/llama3.1/8b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-8b-inf2.48xl-ec2\"      \n  model_name: \"llama3-1-8b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: no\n  1_generate_data.ipynb: no\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: no\n  4_model_metric_analysis.ipynb: no\n  5_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-8b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"inf2.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and Inferentia settings for the instance\n      # '-u djl' tells the container to use the Deep Java Library (DJL) runtime\n      # '--device /dev/neuron0 ... /dev/neuron5' specifies the Inferentia devices to be used\n      gpu_or_neuron_setting: -u djl --device /dev/neuron0  --device /dev/neuron1  --device /dev/neuron2  --device /dev/neuron3  --device /dev/neuron4 --device /dev/neuron5 --device /dev/neuron6 --device /dev/neuron7 --device /dev/neuron8 --device /dev/neuron9 --device /dev/neuron10 --device /dev/neuron11\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=meta-llama/Meta-Llama-3.1-8b-Instruct\n      option.load_split_model=True\n      option.tensor_parallel_degree=8\n      option.n_positions=4096\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=8\n      option.dtype=fp16\n      option.model_loading_timeout=2400\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10 \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O3\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.2-11b-g6e-2xl-tp-1-mc-max-djl-vllm-ec2.yml",
    "path": "fmbench/configs/llama3.2/11b/config-llama3.2-11b-g6e-2xl-tp-1-mc-max-djl-vllm-ec2.yml",
    "directory": "fmbench/configs/llama3.2/11b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.2-3b-g5.4xl-tp=1-mc=max-djl-ec2\"      \n  model_name: \"Meta-Llama-3.2-3B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.2_vision.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: no\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.2-11B-Vision-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.2-11B-Vision-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g6e.2xlarge\"\n    image_uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.30.0-lmi12.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.2-3b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:      \n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      container_type: djl\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 1\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      option.tensor_parallel_degree=1\n      option.max_rolling_batch_size=8\n      option.model_id=meta-llama/Llama-3.2-11B-Vision-Instruct\n      option.rolling_batch=vllm\n      option.enforce_eager=true\n      option.max_rolling_batch_prefill_tokens=4096\n      option.max_model_len=4096\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n    - 8\n    - 10\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.2-1b-g5.2xl-summarization-500-50.yml",
    "path": "fmbench/configs/llama3.2/1b/config-llama3.2-1b-g5.2xl-summarization-500-50.yml",
    "directory": "fmbench/configs/llama3.2/1b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"Meta-Llama-3.2-1b-Instruct-g5.2xl-tp=1-mc=max-triton-ec2\"      \n  model_name: \"Meta-Llama-3.2-1b-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1-500\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 50\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.2-1b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.2-1b-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.2-1b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g5.2xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.2-1b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:      \n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 1\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=1\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Llama-3.2-1b-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 0.1\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.2-1b-g5.2xl-tp-1-mc-max-djl-ec2.yml",
    "path": "fmbench/configs/llama3.2/1b/config-llama3.2-1b-g5.2xl-tp-1-mc-max-djl-ec2.yml",
    "directory": "fmbench/configs/llama3.2/1b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.2-1b-g5.2xl-tp=1-mc=max-djl-ec2\"      \n  model_name: \"Meta-Llama-3.2-1b-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - hf:databricks/databricks-dolly-15k\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_claude_dolly_dataset.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: no\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - instruction\n  - context\n  ground_truth_col_key: response\n  question_col_key: instruction\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 1000\n    payload_file: payload_en_1-1000.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1-1000\n  \n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 20\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.2-1b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.2-1b-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.2-1b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g5.4xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.2-1b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:      \n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 1\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=1\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Llama-3.2-1b-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-1000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 0.1\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.2-1b-g5.4xl-tp-1-mc-max-djl-ec2.yml",
    "path": "fmbench/configs/llama3.2/1b/config-llama3.2-1b-g5.4xl-tp-1-mc-max-djl-ec2.yml",
    "directory": "fmbench/configs/llama3.2/1b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.2-1b-g5.4xl-tp=1-mc=max-djl-ec2\"      \n  model_name: \"Meta-Llama-3.2-1b-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: no\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.2-1b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.2-1b-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.2-1b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g5.4xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.2-1b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:      \n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 1\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=1\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Llama-3.2-1b-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.2-1b-g6e.2xl-tp-1-mc-max-djl.yml",
    "path": "fmbench/configs/llama3.2/1b/config-llama3.2-1b-g6e.2xl-tp-1-mc-max-djl.yml",
    "directory": "fmbench/configs/llama3.2/1b",
    "extension": "yml",
    "content": "================================================\n\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.2-1b-g6e.2xl-tp=1-mc=max-djl-ec2\"      \n  model_name: \"Meta-Llama-3.2-8B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 500_token_prompts_synthetic_data.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 1100\n    payload_file: payload_en_1-1100.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1-1100\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 15\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.2-1b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.2-1b-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3.2-1b-instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g6e.2xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      tp_degree: 1\n      shm_size: 12g\n      model_loading_timeout: 2400\n      model_copies: max\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=1\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Llama-3.2-1b-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-1100.jsonl\n\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 0.1\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.2-1b-m5-16xlarge-ec2.yml",
    "path": "fmbench/configs/llama3.2/1b/config-llama3.2-1b-m5-16xlarge-ec2.yml",
    "directory": "fmbench/configs/llama3.2/1b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.2-1b-m5.16xl-ec2\"      \n  model_name: \"llama3.2-1b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_vllm:\n    model: meta-llama/Llama-3.2-1b-Instruct\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.2-1b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.2-1b-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.2-1b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8000/v1/completions' \n    instance_type: \"m5.16xlarge\"\n    image_uri: vllm-cpu-env\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # Privileged Mode makes the docker container run with root. \n      # This basically means that if you are root in a container you have the privileges of root on the host system\n      # Only need this if you need to set VLLM_CPU_OMP_THREADS_BIND env variable.\n      privileged_mode: yes      \n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting:\n      # This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_vllm\n      # if not set assume djl\n      container_type: vllm\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 3\n    # - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      MODEL_LOADING_TIMEOUT: 2400\n      VLLM_CPU_KVCACHE_SPACE: 40\n      # This instance is equipped with 64 CPUs, and we are allocating 60 of them to run this container.\n      # For additional details, refer to the following URL:\n      # https://docs.vllm.ai/en/latest/getting_started/cpu-installation.html#related-runtime-environment-variables\n      VLLM_CPU_OMP_THREADS_BIND: 0-59\n\nreport:\n  latency_budget: 15\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.2-1b-m7a-16xlarge-ec2.yml",
    "path": "fmbench/configs/llama3.2/1b/config-llama3.2-1b-m7a-16xlarge-ec2.yml",
    "directory": "fmbench/configs/llama3.2/1b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.2-1b-m7a.16xl-ec2\"      \n  model_name: \"llama3.2-1b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_vllm:\n    model: meta-llama/Llama-3.2-1b-Instruct\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.2-1b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.2-1b-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.2-1b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8000/v1/completions' \n    instance_type: \"m7a.16xlarge\"\n    image_uri: vllm-cpu-env\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:      \n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting:\n      # This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_vllm\n      # if not set assume djl\n      container_type: vllm\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      MODEL_LOADING_TIMEOUT: 2400\n      VLLM_CPU_KVCACHE_SPACE: 40\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.2-1b-m7a-24xlarge-ec2-summarization.yml",
    "path": "fmbench/configs/llama3.2/1b/config-llama3.2-1b-m7a-24xlarge-ec2-summarization.yml",
    "directory": "fmbench/configs/llama3.2/1b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.2-1b-m7a.24xl-ec2-summarization\"      \n  model_name: \"llama3.2-1b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - synthetic_data_large_prompts.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3_summarization.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en    \n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en    \n    min_length_in_tokens: 2500\n    max_length_in_tokens: 3500\n    payload_file: payload_en_2500-3500.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_2500-3500\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_vllm:\n    model: meta-llama/Llama-3.2-1b-Instruct\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.2-1b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.2-1b-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.2-1b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8000/v1/completions' \n    instance_type: \"m7a.24xlarge\"\n    image_uri: vllm-cpu-env\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # Privileged Mode makes the docker container run with root. \n      # This basically means that if you are root in a container you have the privileges of root on the host system\n      # Only need this if you need to set VLLM_CPU_OMP_THREADS_BIND env variable.\n      privileged_mode: yes      \n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting:\n      # This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_vllm\n      # if not set assume djl\n      container_type: vllm\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_2500-3500.jsonl\n    - payload_en_3000-4000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      MODEL_LOADING_TIMEOUT: 2400\n      VLLM_CPU_KVCACHE_SPACE: 192\n      # This instance is equipped with 96 CPUs, and we are allocating 93 of them to run this container.\n      # For additional details, refer to the following URL:\n      # https://docs.vllm.ai/en/latest/getting_started/cpu-installation.html#related-runtime-environment-variables\n      VLLM_CPU_OMP_THREADS_BIND: 0-92\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 60\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.2-1b-m7a-24xlarge-ec2.yml",
    "path": "fmbench/configs/llama3.2/1b/config-llama3.2-1b-m7a-24xlarge-ec2.yml",
    "directory": "fmbench/configs/llama3.2/1b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.2-1b-m7a.24xl-ec2\"      \n  model_name: \"llama3.2-1b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_vllm:\n    model: meta-llama/Llama-3.2-1b-Instruct\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.2-1b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.2-1b-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.2-1b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8000/v1/completions' \n    instance_type: \"m7a.24xlarge\"\n    image_uri: vllm-cpu-env\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # Privileged Mode makes the docker container run with root. \n      # This basically means that if you are root in a container you have the privileges of root on the host system\n      # Only need this if you need to set VLLM_CPU_OMP_THREADS_BIND env variable.\n      privileged_mode: yes      \n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting:\n      # This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_vllm\n      # if not set assume djl\n      container_type: vllm\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      MODEL_LOADING_TIMEOUT: 2400\n      VLLM_CPU_KVCACHE_SPACE: 192\n      # This instance is equipped with 96 CPUs, and we are allocating 93 of them to run this container.\n      # For additional details, refer to the following URL:\n      # https://docs.vllm.ai/en/latest/getting_started/cpu-installation.html#related-runtime-environment-variables\n      VLLM_CPU_OMP_THREADS_BIND: 0-92\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 60\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.2-1b-m7i-12xlarge-ec2.yml",
    "path": "fmbench/configs/llama3.2/1b/config-llama3.2-1b-m7i-12xlarge-ec2.yml",
    "directory": "fmbench/configs/llama3.2/1b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.2-1b-m7i.12xl-ec2\"      \n  model_name: \"llama3.2-1b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_vllm:\n    model: meta-llama/Llama-3.2-1b-Instruct\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.2-1b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.2-1b-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.2-1b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8000/v1/completions' \n    instance_type: \"m7i.12xlarge\"\n    image_uri: vllm-cpu-env\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # Privileged Mode makes the docker container run with root. \n      # This basically means that if you are root in a container you have the privileges of root on the host system\n      # Only need this if you need to set VLLM_CPU_OMP_THREADS_BIND env variable.\n      privileged_mode: yes      \n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting:\n      # This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_vllm\n      # if not set assume djl\n      container_type: vllm\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      VLLM_CPU_KVCACHE_SPACE: 40\n      # This instance has 48 CPUs, and we are allocating 40 of them to run this container.\n      # For more information, visit the following URL:\n      # https://docs.vllm.ai/en/latest/getting_started/cpu-installation.html#related-runtime-environment-variables\n      VLLM_CPU_OMP_THREADS_BIND: 0-40\n\nreport:\n  latency_budget: 10\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-llama3.2-3b-g5.4xl-tp-1-mc-max-djl-ec2.yml",
    "path": "fmbench/configs/llama3.2/3b/config-llama3.2-3b-g5.4xl-tp-1-mc-max-djl-ec2.yml",
    "directory": "fmbench/configs/llama3.2/3b",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.2-3b-g5.4xl-tp=1-mc=max-djl-ec2\"      \n  model_name: \"Meta-Llama-3.2-3B-Instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.2-3B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.2-3B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.2-3B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g5.4xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.2-3b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:      \n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 1\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=1\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Llama-3.2-3B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-mistral-7b-eks-inf2.yml",
    "path": "fmbench/configs/mistral/config-mistral-7b-eks-inf2.yml",
    "directory": "fmbench/configs/mistral",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"mistral-7b-eks-inf2\"\n  model_name: \"mistral7b\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: mistral_tokenizer\n  \n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_mistral.txt\n\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the \n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1-500\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  eks_parameter_set:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: Mistral-7B-Instruct-eks\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.  \n    model_id: Mistral-7B-Instruct-v0.2\n    model_version: \"*\"\n    model_name: mistral-7b-instruct\n    # the endpoint name is not needed, since it is created dynamically and used\n    # the deploy script for eks stores the endpoint url that is saved and used in the \n    # rest of the test\n    ep_name: \n    instance_type: \"ml.inf2.24xlarge\"\n    image_uri: 'public.ecr.aws/data-on-eks/ray2.11.0-py310-mistral7b-neuron:latest'\n    deploy: yes\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script: eks_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: eks_predictor.py\n    # refers to the eks custom parameters for the model\n    eks:\n      # This is the cluster created on Kubernetes\n      # using do-eks: https://github.com/awslabs/data-on-eks/tree/main\n      # The cluster is created using the github repository above. If there\n      # is a cluster that you create, that is out of this repository or has a different\n      # name, replace the cluster name below with your custom cluster name\n      eks_cluster_name: trainium-inferentia\n      # Represents the logical grouping of EKS resources\n      # for the mistral model. All kubernetes resources related\n      # to mistral will be in this namespace. Change this to \n      # llama3/llama2 for your use case\n      eks_model_namespace: mistral \n      # name of the manifest directory where all of the EKS manifest files\n      # reside\n      manifest_dir: configs/eks_manifests\n      # this is the yaml file to deploy the mistral model\n      manifest_file: mistral-ray-service.yaml\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: eks_parameter_set\n      # this is the url format that gets appended to the \n      # model endpoint URL to run inferences from\n      inference_url_format: /serve/infer?sentence=\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # currently the llama3-8b jumpstart model is compiled with a batch size of 1\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\n# parameters related to how the final report is generated    \nreport:\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 2\n  cost_per_10k_txn_budget: 30\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n  latency_vs_token_len_chart: \n    y_ticks: \n    # - 1\n    # - 2\n    # - 3\n    # - 4\n    title: \"Effect of token length on inference latency for \\\"Mistral-7B-Instruct-eks\\\"\"\n\n\n================================================"
  },
  {
    "filename": "config-mistral-7b-tgi-g5.yml",
    "path": "fmbench/configs/mistral/config-mistral-7b-tgi-g5.yml",
    "directory": "fmbench/configs/mistral",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"mistral-7b-tgi-g5-v1\"\n  model_name: \"mistral7b\"\n\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml ## mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: mistral_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_mistral.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n\nmetrics:\n  dataset_of_interest: en_2000-3000\n\npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n  \ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n# Model configurations\nexperiments:\n  - name: mistral-7b--instruct-g5-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\n    model_id: huggingface-llm-mistral-7b-instruct\n    model_version: \"*\"\n    model_name: mistral-7b-instruct\n    ep_name: mistral7b-g5-2xlarge\n    instance_type: \"ml.g5.2xlarge\"    \n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi1.4.0-gpu-py310-cu121-ubuntu20.04\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    #- payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    # - 6\n    # - 8\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"8191\"\n      MAX_TOTAL_TOKENS: \"8192\"\n      MAX_BATCH_PREFILL_TOKENS: '8191'\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\n\nreport:\n  latency_budget: 5\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-mistral-7b-trn1-32xl-triton.yml",
    "path": "fmbench/configs/mistral/config-mistral-7b-trn1-32xl-triton.yml",
    "directory": "fmbench/configs/mistral",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"mistral-7b-trn1.32xl-ec2-triton\"      \n  model_name: \"Mistral-7B-Instruct-V0.2\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: mistral_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_mistral.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Mistral-7B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: mistralai/Mistral-7B-Instruct-v0.2 #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Mistral-7B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8080/v2/models/Mistral-7B-Instruct-v0.2/generate' \n    instance_type: \"trn1.32xlarge\"\n    image_uri: tritonserver-neuronx:24.06-2.x\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and Inferentia settings for the instance\n      # '-u djl' tells the container to use the Deep Java Library (DJL) runtime\n      # '--device /dev/neuron0 ... /dev/neuron5' specifies the Inferentia devices to be used\n      gpu_or_neuron_setting: -u triton --device /dev/neuron0 --device /dev/neuron1 --device /dev/neuron2 --device /dev/neuron3 --device /dev/neuron4 --device /dev/neuron5 --device /dev/neuron6 --device /dev/neuron7 --device /dev/neuron8 --device /dev/neuron9 --device /dev/neuron10 --device /dev/neuron11 --device /dev/neuron12 --device /dev/neuron13 --device /dev/neuron14 --device /dev/neuron15\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 8000\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured,\n      # max: FMBench figures out the max number of model containers to be run\n      #      based on TP degree configured and number of neuron cores/GPUs available.\n      #      For example, if TP=2, GPUs=8 then FMBench will start 4 containers and 1 load balancer,\n      # auto: only supported if the underlying inference container would automatically \n      #       start multiple copies of the model internally based on TP degree and neuron cores/GPUs\n      #       available. In this case only a single container is created, no load balancer is created.\n      #       The DJL serving containers supports auto.  \n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      batch_size: 4\n      n_positions: 8192\n      shm_size: 12g\n      model_loading_timeout: 2400\n      # The max new tokens, context length of the model, and the neuron \n      # config are used within the model respository of the triton container\n      # during model deployment\n      max_new_tokens: 100\n      # context length for mistral is 8192\n      context_len: 8192\n      neuron_config:\n        max_length: 8192\n        top_k: 50\n        do_sample: true\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O3\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-mistral-instruct-AWQ-p4d.yml",
    "path": "fmbench/configs/mistral/config-mistral-instruct-AWQ-p4d.yml",
    "directory": "fmbench/configs/mistral",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"mistral-7b-instruct-AWQ\"\n  model_name: \"mistral7bInstruct-AWQ\"\n\n  \n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: tokenizer_mistral ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_mistral.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n  - language: en\n    min_length_in_tokens: 1600\n    max_length_in_tokens: 5000\n    payload_file: payload_en_1600-5000.jsonl\n\nmetrics:\n  dataset_of_interest: en_1000-2000\n\npricing: pricing.yml\n  \ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 200\n\n# Model configurations\nexperiments:\n  - name: mistral-Instruct7b-p4d-lmi-customer-drop\n    model_id: TheBloke/Mistral-7B-Instruct-v0.2-AWQ\n    model_version: \"*\"\n    model_name: mistral7bInstruct-AWQ\n    ep_name: mistral7bInstruct-p4dAWQ\n    download_from_hf_place_in_s3: yes\n    model_s3_path: s3://{write_bucket}/TheBloke/Mistral-7B-Instruct-v0.2-AWQ\n    instance_type: \"ml.p4d.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.26.0-deepspeed0.12.6-cu121\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n      engine=Python\n      option.tensor_parallel_degree=1\n      option.model_id=s3://{write_bucket}/TheBloke/Mistral-7B-Instruct-v0.2-AWQ\n      option.max_rolling_batch_size=64\n      option.rolling_batch=vllm\n    payload_files:\n    - payload_en_1000-2000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 5\n    - 10\n    - 20\n    - 30\n    env:\n\nreport:\n  latency_budget: 1\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n  latency_vs_token_len_chart: \n    y_ticks: \n    # - 1\n    # - 2\n    # - 3\n    # - 4\n    title: \"Effect of token length on inference latency for \\\"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\\\"\"\n\n\n\n================================================"
  },
  {
    "filename": "config-mistral-instruct-AWQ-p5-byo-ep.yml",
    "path": "fmbench/configs/mistral/config-mistral-instruct-AWQ-p5-byo-ep.yml",
    "directory": "fmbench/configs/mistral",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"mistral-7b-p5-instruct-AWQ-byo-ep\"\n  model_name: \"mistral7bInstruct-AWQ\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: tokenizer_mistral ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_mistral.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n  - language: en\n    min_length_in_tokens: 1600\n    max_length_in_tokens: 5000\n    payload_file: payload_en_1600-5000.jsonl\n\nmetrics:\n  dataset_of_interest: en_1000-2000\n  weights:\n    price_per_tx_wt: 0.65\n    latenct_wt: 0.35\n  \npricing:\n  ml.g5.2xlarge: 1.515\n  ml.g5.12xlarge: 7.09\n  ml.g5.24xlarge: 10.18\n  ml.g5.48xlarge: 20.36\n  ml.inf2.24xlarge: 7.79\n  ml.inf2.48xlarge: 15.58\n  ml.p4d.24xlarge: 37.688\n  ml.p5.48xlarge: 98.32\n  \ninference_parameters:\n  do_sample: yes\n  temperature: 0.1\n  top_p: 0.92\n  top_k: 120  \n  max_new_tokens: 100\n\n# Model configurations\nexperiments:\n  - name: mistral-Instruct7b-p5-byo-sagemaker-ep\n    model_id: \n    model_version: \n    model_name: mistral7bInstruct-AWQ\n    ep_name: <your-sagemaker-endpoint-name> # enter the name of the existing endpoint name already deployed on sagemaker\n    instance_type: \"ml.p5.48xlarge\"\n    image_uri:\n    deploy: no #setting to no since the endpoint has already been deployed\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n\n    concurrency_levels:\n    - 1\n    - 5\n    - 30\n    env:\n    \nreport:\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n  latency_vs_token_len_chart: \n    y_ticks: \n    # - 1\n    # - 2\n    # - 3\n    # - 4\n    title: \"Effect of token length on inference latency for \\\"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\\\"\"\n\n\n\n\n================================================"
  },
  {
    "filename": "config-mistral-instruct-AWQ-p5.yml",
    "path": "fmbench/configs/mistral/config-mistral-instruct-AWQ-p5.yml",
    "directory": "fmbench/configs/mistral",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"mistral-7b-p5-instruct-AWQ\"\n  model_name: \"mistral7bInstruct-AWQ\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: mistral_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_mistral.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n  - language: en\n    min_length_in_tokens: 1600\n    max_length_in_tokens: 5000\n    payload_file: payload_en_1600-5000.jsonl\n\nmetrics:\n  dataset_of_interest: en_1000-2000\n  \npricing: pricing.yml\n  \ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n# Model configurations\nexperiments:\n  - name: mistral-Instruct7b-p5\n    model_id: TheBloke/Mistral-7B-Instruct-v0.2-AWQ\n    model_version: \"*\"\n    model_name: mistral7bInstruct-AWQ\n    ep_name: mistral7bInstruct-P5AWQ\n    download_from_hf_place_in_s3: yes\n    model_s3_path: s3://{write_bucket}/TheBloke/Mistral-7B-Instruct-v0.2-AWQ\n    instance_type: \"ml.p5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.26.0-deepspeed0.12.6-cu121\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n      engine=Python\n      option.tensor_parallel_degree=1\n      option.model_id=s3://{write_bucket}/TheBloke/Mistral-7B-Instruct-v0.2-AWQ\n      option.max_rolling_batch_size=64\n      option.rolling_batch=vllm\n    payload_files:\n    - payload_en_1000-2000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 5\n    - 10\n    - 20\n    - 30\n    env:\n\nreport:\n  latency_budget: 1\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n  latency_vs_token_len_chart: \n    y_ticks: \n    # - 1\n    # - 2\n    # - 3\n    # - 4\n    title: \"Effect of token length on inference latency for \\\"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\\\"\"\n\n\n\n\n================================================"
  },
  {
    "filename": "config-mistral-instruct-p4d.yml",
    "path": "fmbench/configs/mistral/config-mistral-instruct-p4d.yml",
    "directory": "fmbench/configs/mistral",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"Mistral-7B-Instruct-v0-2-p4d\"\n  model_name: \"Mistral-7B-Instruct-v0-2\"\n \naws:\n  region: {region}\n  sagemaker_execution_role: {role_arn}\n  bucket: {write_bucket}\n\ndir_paths:\n    data_prefix: data\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata\n\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts\n    script_files:\n    - hf_token.txt\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: tokenizer_mistral\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_mistral.txt\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n  - language: en\n    min_length_in_tokens: 1600\n    max_length_in_tokens: 5000\n    payload_file: payload_en_1600-5000.jsonl\n\nmetrics:\n  dataset_of_interest: en_1000-2000\n  \npricing: pricing.yml\n  \ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 200\n\nexperiments:\n  - name: Mistral-7B-Instruct-v0.2-p4d\n    model_id: mistralai/Mistral-7B-Instruct-v0.2\n    model_version: \"*\"\n    model_name: Mistral-7B-Instruct-v0-2\n    ep_name: Mistral-7B-Instruct-v0-2\n    download_from_hf_place_in_s3: None\n    model_s3_path: None\n    instance_type: \"ml.p4d.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.26.0-deepspeed0.12.6-cu121\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n      engine=Python\n      option.tensor_parallel_degree=2\n      option.model_id=mistralai/Mistral-7B-Instruct-v0.2\n      option.max_rolling_batch_size=64\n      option.rolling_batch=vllm\n      option.dtype=fp16\n    payload_files:\n    - payload_en_1000-2000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 5\n    - 10\n    - 20\n    - 30\n    env:\n\nreport:\n  latency_budget: 1\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n  latency_vs_token_len_chart: \n    y_ticks:\n    title: \"Effect of token length on inference latency for \\\"mistralai/Mistral-7B-Instruct-v0.2\\\"\"\n\n\n\n================================================"
  },
  {
    "filename": "config-mistral-instruct-v1-p5-trtllm.yml",
    "path": "fmbench/configs/mistral/config-mistral-instruct-v1-p5-trtllm.yml",
    "directory": "fmbench/configs/mistral",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"mistral-7b-p5-instructV1-tp4-trt\"\n  model_name: \"mistral7bInstruct\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: mistral_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_mistral.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n  - language: en\n    min_length_in_tokens: 1600\n    max_length_in_tokens: 5000\n    payload_file: payload_en_1600-5000.jsonl\n\nmetrics:\n  dataset_of_interest: en_1000-2000\n  \npricing: pricing.yml\n  \ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n# Model configurations\nexperiments:\n  - name: mistral-Instruct7b-p5-trt-version-1\n    model_id: mistralai/Mistral-7B-Instruct-v0.1\n    model_version: \"*\"\n    model_name: Mistral7BInstructv1\n    ep_name: mistral7bInstruct-P5\n    download_from_hf_place_in_s3: no\n    model_s3_path: s3://{write_bucket}/mistralai/Mistral-7B-Instruct-v0.1\n    instance_type: \"ml.p5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-tensorrtllm0.8.0-cu122\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n        engine=MPI\n        option.tensor_parallel_degree=4\n        option.model_id=mistralai/Mistral-7B-Instruct-v0.1\n        option.max_rolling_batch=64\n        option.max_input_len=8192\n        option.max_output_len=8192\n        option.max_num_tokens=100000\n        option.use_custom_all_reduce=true\n        option.num_engine_workers=1\n        option.num_checkpoint_workers=1\n        \n    payload_files:\n    - payload_en_1000-2000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 5\n    - 10\n    - 20\n    - 30\n    accept_eula: true\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n  latency_vs_token_len_chart: \n    y_ticks: \n    title: \"Effect of token length on inference latency for \\\"mistralai/Mistral-7B-Instruct-v0.1\\\"\"\n\n\n\n================================================"
  },
  {
    "filename": "config-mistral-instruct-v2-p4d-lmi-dist.yml",
    "path": "fmbench/configs/mistral/config-mistral-instruct-v2-p4d-lmi-dist.yml",
    "directory": "fmbench/configs/mistral",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"Mistral-7B-Instruct-v2-p4d-lmi-dist\"\n  model_name: \"Mistral-7B\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: mistral_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_mistral.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n  - language: en\n    min_length_in_tokens: 1600\n    max_length_in_tokens: 5000\n    payload_file: payload_en_1600-5000.jsonl\n\nmetrics:\n  dataset_of_interest: en_1000-2000\n  \npricing: pricing.yml\n  \ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n# Model configurations\nexperiments:\n  - name: Mistral-7B-Instruct-v0.2-p4d-lmi-dist\n    model_id: mistralai/Mistral-7B-Instruct-v0.2\n    model_version: \"*\"\n    model_name: Mistral7B\n    ep_name: Mistral7B\n    download_from_hf_place_in_s3: no\n    model_s3_path: s3://{write_bucket}/mistralai/Mistral-7B-Instruct-v0.2\n    instance_type: \"ml.p4d.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n        engine=MPI\n        option.tensor_parallel_degree=2\n        option.model_id=mistralai/Mistral-7B-Instruct-v0.2\n        option.max_rolling_batch_size=64\n        option.rolling_batch=lmi-dist\n    payload_files:\n    - payload_en_1000-2000.jsonl\n    concurrency_levels:\n    - 1\n    - 15\n    - 30\n    accept_eula: true\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n  latency_vs_token_len_chart: \n    y_ticks: \n    title: \"Effect of token length on inference latency for \\\"mistralai/Mistral-7B-Instruct-v0.2\\\"\"\n\n\n\n\n================================================"
  },
  {
    "filename": "config-mistral-instruct-v2-p4d-trtllm.yml",
    "path": "fmbench/configs/mistral/config-mistral-instruct-v2-p4d-trtllm.yml",
    "directory": "fmbench/configs/mistral",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"mistral-7b-p4d-instruct-tp4-trt\"\n  model_name: \"mistral7bInstruct\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: mistral_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_mistral.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n  - language: en\n    min_length_in_tokens: 1600\n    max_length_in_tokens: 5000\n    payload_file: payload_en_1600-5000.jsonl\n\nmetrics:\n  dataset_of_interest: en_1000-2000\n  \npricing: pricing.yml\n  \ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n# Model configurations\nexperiments:\n  - name: mistral-Instruct7b-p4d-tp4-trt\n    model_id: mistralai/Mistral-7B-Instruct-v0.2\n    model_version: \"*\"\n    model_name: Mistral7BInstruct\n    ep_name: mistral7bInstruct-P4d\n    download_from_hf_place_in_s3: no\n    model_s3_path: s3://{write_bucket}/mistralai/Mistral-7B-Instruct-v0.2\n    instance_type: \"ml.p4d.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-tensorrtllm0.8.0-cu122\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n        engine=MPI\n        option.tensor_parallel_degree=4\n        option.model_id=mistralai/Mistral-7B-Instruct-v0.2\n        option.max_rolling_batch=64\n        option.max_input_len=8192\n        option.max_output_len=8192\n        option.max_num_tokens=100000\n        option.use_custom_all_reduce=true\n        option.num_engine_workers=1\n        option.num_checkpoint_workers=1\n    payload_files:\n    - payload_en_1000-2000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 5\n    - 10\n    - 20\n    - 30\n    accept_eula: true\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n  latency_vs_token_len_chart: \n    y_ticks: \n    title: \"Effect of token length on inference latency for \\\"mistralai/Mistral-7B-Instruct-v0.2\\\"\"\n\n\n\n================================================"
  },
  {
    "filename": "config-mistral-instruct-v2-p5-lmi-dist.yml",
    "path": "fmbench/configs/mistral/config-mistral-instruct-v2-p5-lmi-dist.yml",
    "directory": "fmbench/configs/mistral",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"Mistral-7B-Instruct-v2-lmi-dist\"\n  model_name: \"Mistral-7B\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: mistral_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_mistral.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n  - language: en\n    min_length_in_tokens: 1600\n    max_length_in_tokens: 5000\n    payload_file: payload_en_1600-5000.jsonl\n\nmetrics:\n  dataset_of_interest: en_1000-2000\n  \npricing: pricing.yml\n  \ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n# Model configurations\nexperiments:\n  - name: Mistral-7B-Instruct-v0.2\n    model_id: mistralai/Mistral-7B-Instruct-v0.2\n    model_version: \"*\"\n    model_name: Mistral7B\n    ep_name: Mistral7B\n    download_from_hf_place_in_s3: no\n    model_s3_path: s3://{write_bucket}/mistralai/Mistral-7B-Instruct-v0.2\n    instance_type: \"ml.p5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n        engine=MPI\n        option.tensor_parallel_degree=2\n        option.model_id=mistralai/Mistral-7B-Instruct-v0.2\n        option.max_rolling_batch_size=64\n        option.rolling_batch=lmi-dist\n    payload_files:\n    - payload_en_1000-2000.jsonl\n    concurrency_levels:\n    - 1\n    - 15\n    - 30\n    accept_eula: true\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n  latency_vs_token_len_chart: \n    y_ticks: \n    title: \"Effect of token length on inference latency for \\\"mistralai/Mistral-7B-Instruct-v0.2\\\"\"\n\n\n\n\n================================================"
  },
  {
    "filename": "config-mistral-instruct-v2-p5-trtllm.yml",
    "path": "fmbench/configs/mistral/config-mistral-instruct-v2-p5-trtllm.yml",
    "directory": "fmbench/configs/mistral",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"mistral-7b-p5-instructv2-tp4-trt\"\n  model_name: \"mistral7bInstruct\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: mistral_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_mistral.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically \nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n  - language: en\n    min_length_in_tokens: 1600\n    max_length_in_tokens: 5000\n    payload_file: payload_en_1600-5000.jsonl\n\nmetrics:\n  dataset_of_interest: en_1000-2000\n  \npricing: pricing.yml\n  \ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n# Model configurations\nexperiments:\n  - name: mistral-Instruct7b-p5-tp4-trt\n    model_id: mistralai/Mistral-7B-Instruct-v0.2\n    model_version: \"*\"\n    model_name: Mistral7BInstruct\n    ep_name: mistral7bInstruct-P5\n    download_from_hf_place_in_s3: no\n    model_s3_path: s3://{write_bucket}/mistralai/Mistral-7B-Instruct-v0.2\n    instance_type: \"ml.p5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-tensorrtllm0.8.0-cu122\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n        engine=MPI\n        option.tensor_parallel_degree=4\n        option.model_id=mistralai/Mistral-7B-Instruct-v0.2\n        option.max_rolling_batch=64\n        option.max_input_len=8192\n        option.max_output_len=8192\n        option.max_num_tokens=100000\n        option.use_custom_all_reduce=true\n        option.num_engine_workers=1\n        option.num_checkpoint_workers=1\n        \n    payload_files:\n    - payload_en_1000-2000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 5\n    - 10\n    - 20\n    - 30\n    accept_eula: true\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 200\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n  latency_vs_token_len_chart: \n    y_ticks: \n    title: \"Effect of token length on inference latency for \\\"mistralai/Mistral-7B-Instruct-v0.2\\\"\"\n\n\n\n================================================"
  },
  {
    "filename": "config-mistral-trn1-32xl-deploy-ec2-tp32.yml",
    "path": "fmbench/configs/mistral/config-mistral-trn1-32xl-deploy-ec2-tp32.yml",
    "directory": "fmbench/configs/mistral",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"mistral-7b-trn1.32xl-ec2\"      \n  model_name: \"mistral-7b-instruct\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: mistral_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_mistral.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1000-2000\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    return_full_text: no\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Mistral-7B-Instruct-v0.2\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: mistralai/Mistral-7B-Instruct-v0.2 #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Mistral-7B-Instruct-v0.2\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"trn1.32xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and Inferentia settings for the instance\n      # '-u djl' tells the container to use the Deep Java Library (DJL) runtime\n      # '--device /dev/neuron0 ... /dev/neuron5' specifies the Inferentia devices to be used\n      gpu_or_neuron_setting: -u djl --device /dev/neuron0 --device /dev/neuron1 --device /dev/neuron2 --device /dev/neuron3 --device /dev/neuron4 --device /dev/neuron5 --device /dev/neuron6 --device /dev/neuron7 --device /dev/neuron8 --device /dev/neuron9 --device /dev/neuron10 --device /dev/neuron11 --device /dev/neuron12 --device /dev/neuron13 --device /dev/neuron14 --device /dev/neuron15\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 32\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=mistralai/Mistral-7B-Instruct-v0.2\n      option.tensor_parallel_degree=32\n      option.n_positions=4096\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=8\n      option.model_loading_timeout=2400\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 5\n    - 8\n    - 10\n    - 15\n    - 20\n    - 25\n    - 30\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O3\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-mistral-v3-inf2-48xl-deploy-ec2-tp24.yml",
    "path": "fmbench/configs/mistral/config-mistral-v3-inf2-48xl-deploy-ec2-tp24.yml",
    "directory": "fmbench/configs/mistral",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"mistral-7b-v3-inf2.48xl-ec2\"      \n  model_name: \"mistral-7b-instruct-v3\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: mistral_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_mistral.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n    return_full_text: no\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Mistral-7B-Instruct-v0.3\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: mistralai/Mistral-7B-Instruct-v0.3 #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Mistral-7B-Instruct-v0.3\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"inf2.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and Inferentia settings for the instance\n      # '-u djl' tells the container to use the Deep Java Library (DJL) runtime\n      # '--device /dev/neuron0 ... /dev/neuron5' specifies the Inferentia devices to be used\n      gpu_or_neuron_setting: -u djl --device /dev/neuron0 --device /dev/neuron1 --device /dev/neuron2 --device /dev/neuron3 --device /dev/neuron4 --device /dev/neuron5 --device /dev/neuron6 --device /dev/neuron7 --device /dev/neuron8 --device /dev/neuron9 --device /dev/neuron10 --device /dev/neuron11\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 6\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=mistralai/Mistral-7B-Instruct-v0.3\n      option.tensor_parallel_degree=6\n      option.n_positions=4096\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=4\n      option.model_loading_timeout=2400\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n    - 7\n    - 8\n    - 9\n    - 10\n    - 15\n    - 20\n    # - 25\n    # - 30\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O3\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-mixtral-8x7b-g6e.48xl-ec2.yml",
    "path": "fmbench/configs/mixtral/config-mixtral-8x7b-g6e.48xl-ec2.yml",
    "directory": "fmbench/configs/mixtral",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \ngeneral:\n  name: \"Mixtral-8x7B-Instruct-v0.1-AWQ-ec2\"      \n  model_name: \"Mixtral-8x7B-Instruct-v0.1-AWQ\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: mistral_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_mistral.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 4500\n    max_length_in_tokens: 5500\n    payload_file: payload_en_4500-5500.jsonl\n\n\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_4500-5500\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Mixtral-8x7B-Instruct-v0.1-AWQ\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: ybelkada/Mixtral-8x7B-Instruct-v0.1-AWQ # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Mixtral-8x7B-Instruct-v0.1-AWQ\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g6e.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes. \n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=64\n      option.model_id=ybelkada/Mixtral-8x7B-Instruct-v0.1-AWQ\n      option.rolling_batch=vllm\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    - payload_en_4500-5500.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2.5\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-claude-scienceqa.yml",
    "path": "fmbench/configs/multimodal/bedrock/config-claude-scienceqa.yml",
    "directory": "fmbench/configs/multimodal/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"fmbench-bedrock-claude-realworldqa\"      \n  model_name: \"Anthropic vision models on Amazon Bedrock\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the https://huggingface.co/datasets/derek-thomas/ScienceQA\n  # This dataset contains images containing science questions and associated ground truth responses to the questions.\n  # when this notebook looks for this dataset - if it is prefixed with hf, and if the token is already there\n  # if the token is there and the hf: is there, then download the dataset from hf first and then process it -\n  source_data_files: \n  # If a split is specified in the dataset identifier and exists in the loaded dataset, \n  # it uses that split. If no split is specified (e.g., hf:derek-thomas/ScienceQA), it will default to\n  # the first available split. In the given example below, users can specify the 'train' and 'validation' splits\n  # of the 'derek-thomas/ScienceQA' dataset. If none are provided, then the first available split will be used.\n  \n  # Follow this format below: hf:dataset-id/subset-name/split-name. \n  # If there is no specified subset name, use \"default\"\n  - hf:derek-thomas/ScienceQA/default/train\n  - hf:derek-thomas/ScienceQA/default/validation\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_claude_images_ScienceQA.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: no\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  # This represents the question that is asked as a part of the\n  # image messages API format. If this is not provided, a description\n  # of the image will be provided instead\n  - question\n  # If you want to benchmark a multimodal model on an image dataset, \n  # then it an image_col parameter is required. This parameter refers to the \n  # name of the column in the dataset that contains the images to be used \n  # during the benchmarking process. If this column is not provided, the \n  # standard text generation benchmark process is be used in the FMBench run.\n  image_col: image\n  ground_truth_col_key: solution\n  question_col_key: question\n  # This is the number of rows of the dataset that you want to load\n  # if this parameter is not given, it is defaulted to 100 rows\n  ds_N: 150\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1-500\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: anthropic.claude-3-sonnet-20240229-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: anthropic.claude-3-sonnet-20240229-v1:0\n    model_version: \n    model_name: anthropic.claude-3-sonnet-20240229-v1:0\n    ep_name: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_type: anthropic.claude-3-sonnet-20240229-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  - name: anthropic.claude-3-5-sonnet-20240620-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: anthropic.claude-3-5-sonnet-20240620-v1:0\n    model_version: \n    model_name: anthropic.claude-3-5-sonnet-20240620-v1:0\n    ep_name: anthropic.claude-3-5-sonnet-20240620-v1:0\n    instance_type: anthropic.claude-3-5-sonnet-20240620-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\nreport:\n  latency_budget: 1\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama-3-2-11b-vision-instruct-marqo-GS-10M.yml",
    "path": "fmbench/configs/multimodal/bedrock/config-llama-3-2-11b-vision-instruct-marqo-GS-10M.yml",
    "directory": "fmbench/configs/multimodal/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"fmbench-bedrock-llama3-2-11b-vision-instruct-marqo-GS-10M\"      \n  model_name: \"llama3-2 11b on Amazon Bedrock\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the marqo-GS-10M\n  # This dataset contains images without any questions. In this case, the same prompt\n  # template \"prompt_template_llama3_images_marqo-GS-10M\" will be used across all of the images in \n  # this hugging face dataset. It will ask to describe and mention the key components of each image.\n  source_data_files: \n  # If a split is specified in the dataset identifier and exists in the loaded dataset, \n  # it uses that split. If no split is specified (e.g., Marqo/marqo-GS-10M), it will default to\n  # the first available split. In the given example below, users can specify the 'in-domain' and 'novel_document' splits\n  # of the 'Marqo/marqo-GS-10M' dataset. If none are provided, then the first available split will be used.\n  \n  # Follow this format below: hf:dataset-id/subset-name/split-name. \n  # If there is no specified subset name, use \"default\"\n  - hf:Marqo/marqo-GS-10M/default/in-domain\n  - hf:Marqo/marqo-GS-10M/default/novel_document\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3_images_marqo-GS-10M.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: no\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  # This represents the question that is asked as a part of the\n  # image messages API format. If this is not provided, a description\n  # of the image will be provided instead\n  image_col: image\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1-500\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-2-11b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: us.meta.llama3-2-11b-instruct-v1:0\n    model_version: \n    model_name: us.meta.llama3-2-11b-instruct-v1:0\n    ep_name: us.meta.llama3-2-11b-instruct-v1:0\n    instance_type: us.meta.llama3-2-11b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    - 14\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 1\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama-3-2-11b-vision-instruct-scienceqa.yml",
    "path": "fmbench/configs/multimodal/bedrock/config-llama-3-2-11b-vision-instruct-scienceqa.yml",
    "directory": "fmbench/configs/multimodal/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"fmbench-bedrock-llama3-2-11b-vision-instruct-realworldqa\"      \n  model_name: \"llama3-2 11b on Amazon Bedrock\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the https://huggingface.co/datasets/derek-thomas/ScienceQA\n  # This dataset contains images containing science questions and associated ground truth responses to the questions.\n  # when this notebook looks for this dataset - if it is prefixed with hf, and if the token is already there\n  # if the token is there and the hf: is there, then download the dataset from hf first and then process it -\n  source_data_files: \n  # If a split is specified in the dataset identifier and exists in the loaded dataset, \n  # it uses that split. If no split is specified (e.g., hf:derek-thomas/ScienceQA), it will default to\n  # the first available split. In the given example below, users can specify the 'train' and 'validation' splits\n  # of the 'derek-thomas/ScienceQA' dataset. If none are provided, then the first available split will be used.\n  \n  # Follow this format below: hf:dataset-id/subset-name/split-name. \n  # If there is no specified subset name, use \"default\"\n  - hf:derek-thomas/ScienceQA/default/train\n  - hf:derek-thomas/ScienceQA/default/validation\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3_images_ScienceQA.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: no\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  # This represents the question that is asked as a part of the\n  # image messages API format. If this is not provided, a description\n  # of the image will be provided instead\n  - question\n  # If you want to benchmark a multimodal model on an image dataset, \n  # then it an image_col parameter is required. This parameter refers to the \n  # name of the column in the dataset that contains the images to be used \n  # during the benchmarking process. If this column is not provided, the \n  # standard text generation benchmark process is be used in the FMBench run.\n  image_col: image\n  # This is the number of rows of the dataset that you want to load\n  # if this parameter is not given, it is defaulted to 100 rows\n  ds_N: 150\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1-500\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-2-11b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: us.meta.llama3-2-11b-instruct-v1:0\n    model_version: \n    model_name: us.meta.llama3-2-11b-instruct-v1:0\n    ep_name: us.meta.llama3-2-11b-instruct-v1:0\n    instance_type: us.meta.llama3-2-11b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    - 14\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 1\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-llama-3-2-claude-models-scienceqa.yml",
    "path": "fmbench/configs/multimodal/bedrock/config-llama-3-2-claude-models-scienceqa.yml",
    "directory": "fmbench/configs/multimodal/bedrock",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"fmbench-bedrock-vision-models-realworldqa\"      \n  model_name: \"llama3-2 11b & Claude vision models on Amazon Bedrock\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the https://huggingface.co/datasets/derek-thomas/ScienceQA\n  # This dataset contains images containing science questions and associated ground truth responses to the questions.\n  # when this notebook looks for this dataset - if it is prefixed with hf, and if the token is already there\n  # if the token is there and the hf: is there, then download the dataset from hf first and then process it -\n  source_data_files: \n  # If a split is specified in the dataset identifier and exists in the loaded dataset, \n  # it uses that split. If no split is specified (e.g., hf:derek-thomas/ScienceQA), it will default to\n  # the first available split. In the given example below, users can specify the 'train' and 'validation' splits\n  # of the 'derek-thomas/ScienceQA' dataset. If none are provided, then the first available split will be used.\n  \n  # Follow this format below: hf:dataset-id/subset-name/split-name. \n  # If there is no specified subset name, use \"default\"\n  - hf:derek-thomas/ScienceQA/default/train\n  - hf:derek-thomas/ScienceQA/default/validation\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3_images_ScienceQA.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: no\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  # This represents the question that is asked as a part of the\n  # image messages API format. If this is not provided, a description\n  # of the image will be provided instead\n  - question\n  # If you want to benchmark a multimodal model on an image dataset, \n  # then it an image_col parameter is required. This parameter refers to the \n  # name of the column in the dataset that contains the images to be used \n  # during the benchmarking process. If this column is not provided, the \n  # standard text generation benchmark process is be used in the FMBench run.\n  image_col: image\n  # This is the number of rows of the dataset that you want to load\n  # if this parameter is not given, it is defaulted to 100 rows\n  ds_N: 150\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1-500\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any), \n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-2-11b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: us.meta.llama3-2-11b-instruct-v1:0\n    model_version: \n    model_name: us.meta.llama3-2-11b-instruct-v1:0\n    ep_name: us.meta.llama3-2-11b-instruct-v1:0\n    instance_type: us.meta.llama3-2-11b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  - name: anthropic.claude-3-sonnet-20240229-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: anthropic.claude-3-sonnet-20240229-v1:0\n    model_version: \n    model_name: anthropic.claude-3-sonnet-20240229-v1:0\n    ep_name: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_type: anthropic.claude-3-sonnet-20240229-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  - name: anthropic.claude-3-5-sonnet-20240620-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.    \n    model_id: anthropic.claude-3-5-sonnet-20240620-v1:0\n    model_version: \n    model_name: anthropic.claude-3-5-sonnet-20240620-v1:0\n    ep_name: anthropic.claude-3-5-sonnet-20240620-v1:0\n    instance_type: anthropic.claude-3-5-sonnet-20240620-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the \n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    \n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 1\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n================================================"
  },
  {
    "filename": "config-Nous-Hermes3-1-70b-g5.48xl-tp-8-mc-max-djl.yml",
    "path": "fmbench/configs/NousResearchHermes70B/config-Nous-Hermes3-1-70b-g5.48xl-tp-8-mc-max-djl.yml",
    "directory": "fmbench/configs/NousResearchHermes70B",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-70b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3.1-70b-g5.48xl-tp=8-mc=max-djl-ec2\"      \n  model_name: \"Hermes-3-Llama-3.1-70B\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Hermes-3-Llama-3.1-70B\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: NousResearch/Hermes-3-Llama-3.1-70B #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Hermes-3-Llama-3.1-70B\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations' \n    instance_type: \"g5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged witsh multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-70b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 24000\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that \n      # the code will determine how many copies can be loaded based on TP and \n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the \n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 24000\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=4\n      option.model_id=NousResearch/Hermes-3-Llama-3.1-70B\n      option.rolling_batch=lmi-dist\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 200\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-Nous-Hermes3-1-70b-inf2.48xl-triton-tp24.yml",
    "path": "fmbench/configs/NousResearchHermes70B/config-Nous-Hermes3-1-70b-inf2.48xl-triton-tp24.yml",
    "directory": "fmbench/configs/NousResearchHermes70B",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-70b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-70b-inf2.48xl-ec2\"      \n  model_name: \"Hermes-3-Llama-3.1-70B\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    top_k: 50  \n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Hermes-3-Llama-3.1-70B\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: NousResearch/Hermes-3-Llama-3.1-70B #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Hermes-3-Llama-3.1-70B\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8080/v2/models/Hermes-3-Llama-3.1-70B/generate' \n    instance_type: \"inf2.48xlarge\"\n    image_uri: tritonserver-neuronx:fmbench \n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-70b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 10000\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # For deploying a model using the triton inference container: \n      # mention the backend type, if any. Options for triton are: [djl, vllm or tensorrt]\n      backend: djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured,\n      # max: FMBench figures out the max number of model containers to be run\n      #      based on TP degree configured and number of neuron cores/GPUs available.\n      #      For example, if TP=2, GPUs=8 then FMBench will start 4 containers and 1 load balancer,\n      # auto: only supported if the underlying inference container would automatically \n      #       start multiple copies of the model internally based on TP degree and neuron cores/GPUs\n      #       available. In this case only a single container is created, no load balancer is created.\n      #       The DJL serving containers supports auto.  \n      model_copies: max\n      shm_size: 12g\n      # The model.json parameters are replaced within the model.json file\n      # for the triton on vllm/djl/tensorrt options. The model.json already contains\n      # the tp degree and model id from above in this config file. This is a dictionary\n      # that contains other variables, such as max_num_seqs, dtype, max_model_len, block_size and more\n      # For tensorrt, the tp degree, batch size and other relevant parameters are \n      # extracted directly from the inference spec.\n      container_params:\n        # tp degree is a mandatory parameter\n        tp_degree: 24\n        amp: \"f16\"\n        attention_layout: 'BSH'\n        collectives_layout: 'BSH'\n        context_length_estimate: 3072, 3584, 4096\n        max_rolling_batch_size: 8\n        model_loader: \"tnx\"\n        model_loading_timeout: 24000\n        n_positions: 4096\n        output_formatter: \"json\"\n        rolling_batch: \"auto\"\n        rolling_batch_strategy: \"continuous_batching\"\n        neuron_optimize_level: 2\n        trust_remote_code: true\n        # modify the serving properties to match your model and requirements\n        serving.properties:\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6 \n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O2\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 200\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n\n================================================"
  },
  {
    "filename": "config-Nous-Hermes3-1-70b-trn1.32xl-deploy-ec2-triton.yml",
    "path": "fmbench/configs/NousResearchHermes70B/config-Nous-Hermes3-1-70b-trn1.32xl-deploy-ec2-triton.yml",
    "directory": "fmbench/configs/NousResearchHermes70B",
    "extension": "yml",
    "content": "================================================\n# config file for a rest endpoint supported on fmbench - \n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-70b-trn1.32xl-ec2-triton\"      \n  model_name: \"Hermes-3-Llama-3.1-70B\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n  \n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml \n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters: \n  ec2_djl:\n    top_k: 50  \n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Hermes-3-Llama-3.1-70B\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference. \n    #from huggingface to grab\n    model_id: NousResearch/Hermes-3-Llama-3.1-70B #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Hermes-3-Llama-3.1-70B\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8080/v2/models/Hermes-3-Llama-3.1-70B/generate' \n    instance_type: \"trn1.32xlarge\"\n    image_uri: tritonserver-neuronx:fmbench \n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count: \n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. \n      model_loading_timeout: 10000\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # For deploying a model using the triton inference container: \n      # mention the backend type, if any. Options for triton are: [djl, vllm or tensorrt]\n      backend: djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured,\n      # max: FMBench figures out the max number of model containers to be run\n      #      based on TP degree configured and number of neuron cores/GPUs available.\n      #      For example, if TP=2, GPUs=8 then FMBench will start 4 containers and 1 load balancer,\n      # auto: only supported if the underlying inference container would automatically \n      #       start multiple copies of the model internally based on TP degree and neuron cores/GPUs\n      #       available. In this case only a single container is created, no load balancer is created.\n      #       The DJL serving containers supports auto.  \n      model_copies: max\n      shm_size: 12g\n      # The model.json parameters are replaced within the model.json file\n      # for the triton on vllm/djl/tensorrt options. The model.json already contains\n      # the tp degree and model id from above in this config file. This is a dictionary\n      # that contains other variables, such as max_num_seqs, dtype, max_model_len, block_size and more\n      # For tensorrt, the tp degree, batch size and other relevant parameters are \n      # extracted directly from the inference spec.\n      container_params:\n        # tp degree is a mandatory parameter\n        tp_degree: 32\n        amp: \"f16\"\n        attention_layout: 'BSH'\n        collectives_layout: 'BSH'\n        context_length_estimate: 3072, 3584, 4096\n        max_rolling_batch_size: 8\n        model_loader: \"tnx\"\n        model_loading_timeout: 2400\n        n_positions: 4096\n        output_formatter: \"json\"\n        rolling_batch: \"auto\"\n        rolling_batch_strategy: \"continuous_batching\"\n        trust_remote_code: true\n        # modify the serving properties to match your model and requirements\n        serving.properties:\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n    - 8\n    - 10\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O3\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 200\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025  \n\n\n================================================"
  },
  {
    "filename": "config-phi-3-g5.yml",
    "path": "fmbench/configs/phi/config-phi-3-g5.yml",
    "directory": "fmbench/configs/phi",
    "extension": "yml",
    "content": "================================================\ngeneral:\n  name: \"phi-3-v1\"      \n  model_name: \"phi-3\"\n  \n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts\n  \n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n    \n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - rajpurkar/squad.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: phi_tokenizer\n  \n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_gemma.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the \n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  - context\n  \n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en    \n    min_length_in_tokens: 1\n    max_length_in_tokens: 100\n    payload_file: payload_en_1-100.jsonl\n  - language: en\n    min_length_in_tokens: 100\n    max_length_in_tokens: 200\n    payload_file: payload_en_100-200.jsonl\n  - language: en\n    min_length_in_tokens: 150\n    max_length_in_tokens: 200\n    payload_file: payload_en_150-200.jsonl\n  - language: en\n    min_length_in_tokens: 200\n    max_length_in_tokens: 300\n    payload_file: payload_en_200-300.jsonl\n  - language: en\n    min_length_in_tokens: 300\n    max_length_in_tokens: 400\n    payload_file: payload_en_300-400.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but \n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_150-200\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120  \n    max_new_tokens: 10\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: huggingface-llm-phi-3-mini-4k-instruct-huggingface-pytorch-tgi-inference:2.1.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: huggingface-llm-phi-3-mini-4k-instruct\n    model_version: \"1.*\"\n    model_name: phi-3\n    ep_name: phi-3\n    instance_type: \"ml.g5.2xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.3.0-tgi2.0.3-gpu-py310-cu121-ubuntu22.04'\n    deploy: yes\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details    \n    deployment_script: jumpstart.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-100.jsonl\n    - payload_en_150-200.jsonl\n    - payload_en_200-300.jsonl\n    - payload_en_300-400.jsonl\n    #- payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10 \n    - 12\n    - 15\n    - 20\n    - 30\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"8191\"\n      MAX_TOTAL_TOKENS: \"8192\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\n# parameters related to how the final report is generated\nreport:\n  # markdown report title\n  title: \"Performance benchmarking results for Phi-3 on `g5.2xlarge` using the HuggingFace TGI container\"\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 0.3\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n\n\n\n================================================"
  },
  {
    "filename": "prompt_template_bert.txt",
    "path": "fmbench/prompt_template/prompt_template_bert.txt",
    "directory": "fmbench/prompt_template",
    "extension": "txt",
    "content": "================================================\n{text}\n\n\n\n================================================"
  },
  {
    "filename": "prompt_template_claude.txt",
    "path": "fmbench/prompt_template/prompt_template_claude.txt",
    "directory": "fmbench/prompt_template",
    "extension": "txt",
    "content": "================================================\nHuman: You are an assistant for question-answering tasks. Use the following pieces of retrieved context in the section demarcated by \"```\" to answer the question. If you don't know the answer just say that you don't know. Use three sentences maximum and keep the answer concise.\n\n```\n{context}\n```\n\nQuestion: {input}\n\nAnalyze the question carefully, and answer the question accurately with only the required information from the context. There can be multiple question answer pairs in the context. Answer the final question in the question section above after the context.\n\nAssistant:\n\n\n\n\n================================================"
  },
  {
    "filename": "prompt_template_claude_dolly_dataset.txt",
    "path": "fmbench/prompt_template/prompt_template_claude_dolly_dataset.txt",
    "directory": "fmbench/prompt_template",
    "extension": "txt",
    "content": "================================================\nHuman: You are an assistant for question-answering tasks. Use the following pieces of retrieved context in the section demarcated by \"```\" to answer the question. If you don't know the answer just say that you don't know. Use three sentences maximum and keep the answer concise.\n\n```\n{context}\n```\n\nInstruction: {instruction} \n\nAnalyze the question carefully, and answer the question accurately with only the required information from the context. Answer the final question in the question section above after the context.\n\nAssistant:\n\n\n\n\n================================================"
  },
  {
    "filename": "prompt_template_claude_images_ScienceQA.txt",
    "path": "fmbench/prompt_template/prompt_template_claude_images_ScienceQA.txt",
    "directory": "fmbench/prompt_template",
    "extension": "txt",
    "content": "================================================\nHuman: You are an assistant for question-answering tasks based on images. Your response should: Answer the question in maximum three concise sentences. If you don't know the answer, explicitly state that.\n\nSupport your answer by briefly describing the key visual elements from the image that led to your answer. Include all relevant details from the image in your analysis. If no question or image is provided, state this fact and do not fabricate an answer.\n\nRefer to the question to be answered in the <question></question> xml tags given\nbelow:\n\n<question>\nQuestion: {question}\n</question>\n\nAssistant:\n\n\n================================================"
  },
  {
    "filename": "prompt_template_claude_OpenOrca.txt",
    "path": "fmbench/prompt_template/prompt_template_claude_OpenOrca.txt",
    "directory": "fmbench/prompt_template",
    "extension": "txt",
    "content": "================================================\nHuman: {system_prompt}. Use the following pieces of retrieved context and task in the <context_and_task></context_and_task> xml tags to answer the question. If you don't know the answer just say that you don't know. Use three sentences maximum and keep the answer concise.\n\n<context_and_task>\nContext and task: {input} \n</context_and_task>\n\nGiven the context and task, give the direct answer to the question.\n\nAssistant:\n\n\n\n================================================"
  },
  {
    "filename": "prompt_template_convfinqa.txt",
    "path": "fmbench/prompt_template/prompt_template_convfinqa.txt",
    "directory": "fmbench/prompt_template",
    "extension": "txt",
    "content": "================================================\n{input}\n\n\n\n================================================"
  },
  {
    "filename": "prompt_template_deepseek_longbench.txt",
    "path": "fmbench/prompt_template/prompt_template_deepseek_longbench.txt",
    "directory": "fmbench/prompt_template",
    "extension": "txt",
    "content": "================================================\n<\uff5cUser\uff5c>\n<think>\nThere can be multiple question answer pairs in the context.\nAs soon as you find the first question in the text below immediately stop reading any further and just answer the question.\nAlways start your response with \"<think>\" at the beginning of every output and think step by step.\nKeep your thinking process short and your answers concise, do not overthink.\nMake sure to always provide an answer, if you do not know the answer then say I do not known but never leave the answer field empty in your response.\n</think>\n\n<answer>\nPut your final answer in one line starting with the word Answer:\n</answer>\n\nHere is the text for you to work on:\n\n<text>\n{input}\n\n{context}\n</text>\n\n<\uff5cAssistant\uff5c>\n\n\n================================================"
  },
  {
    "filename": "prompt_template_gemma.txt",
    "path": "fmbench/prompt_template/prompt_template_gemma.txt",
    "directory": "fmbench/prompt_template",
    "extension": "txt",
    "content": "================================================\nContext: {context}\n\nQuestion: {input}\n\nAnalyze the question carefully, and answer the question accurately with only the required information from the context. There can be multiple question answer pairs in the context. Answer the final question in the question section above after the context.\n\nAnwser: \n\n\n================================================"
  },
  {
    "filename": "prompt_template_llama2.txt",
    "path": "fmbench/prompt_template/prompt_template_llama2.txt",
    "directory": "fmbench/prompt_template",
    "extension": "txt",
    "content": "================================================\n<s>[INST] <<SYS>>\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context in the section demarcated by \"```\" to answer the question. If you don't know the answer just say that you don't know. Use three sentences maximum and keep the answer concise.\n<</SYS>>\n\n```\n{context}\n```\n\nQuestion: {input}\n\nAnalyze the question carefully, and answer the question accurately with only the required information from the context. There can be multiple question answer pairs in the context. Answer the final question in the question section above after the context.\n\n[/INST]\nAnswer:\n\n\n\n\n================================================"
  },
  {
    "filename": "prompt_template_llama2_Mistral_OpenOrca.txt",
    "path": "fmbench/prompt_template/prompt_template_llama2_Mistral_OpenOrca.txt",
    "directory": "fmbench/prompt_template",
    "extension": "txt",
    "content": "================================================\n<s>[INST] <<SYS>>\n\n{system_prompt}\n\n<</SYS>>\n\nContext and task: {input}\n\n[/INST]\n\n\n================================================"
  },
  {
    "filename": "prompt_template_llama3.2_vision.txt",
    "path": "fmbench/prompt_template/prompt_template_llama3.2_vision.txt",
    "directory": "fmbench/prompt_template",
    "extension": "txt",
    "content": "================================================\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context in the section demarcated by \"```\" to answer the question. \nThe context may contain multiple question answer pairs as an example. Only answer the final question provided in the question section below.\nIf you dont know the answer just say that you dont know. Use three sentences maximum and be concise with your answer.\n\n```\n{context}\n```\n\nQuestion: {input} \n\n\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n\n\n================================================"
  },
  {
    "filename": "prompt_template_llama3.txt",
    "path": "fmbench/prompt_template/prompt_template_llama3.txt",
    "directory": "fmbench/prompt_template",
    "extension": "txt",
    "content": "================================================\n<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context in the section demarcated by \"```\" to answer the question. \nThe context may contain multiple question answer pairs as an example. Only answer the final question provided in the question section below.\nIf you dont know the answer just say that you dont know. Use three sentences maximum and be concise with your answer.\n\n```\n{context}\n```\n\nQuestion: {input} \n\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n\n\n================================================"
  },
  {
    "filename": "prompt_template_llama3_databricks-dolly-15k.txt",
    "path": "fmbench/prompt_template/prompt_template_llama3_databricks-dolly-15k.txt",
    "directory": "fmbench/prompt_template",
    "extension": "txt",
    "content": "================================================\n<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context in the section demarcated by \"```\" to answer the question. Only answer the question provided in the question section below.\nIf you dont know the answer just say that you dont know. Use three sentences maximum and be concise with your answer.\n\n```\n{context}\n```\n\nInstruction: {instruction} \n\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n\n\n================================================"
  },
  {
    "filename": "prompt_template_llama3_images_marqo-GS-10M.txt",
    "path": "fmbench/prompt_template/prompt_template_llama3_images_marqo-GS-10M.txt",
    "directory": "fmbench/prompt_template",
    "extension": "txt",
    "content": "================================================\n<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nYou are an assistant for generating descriptions on the image provided in the data. Describe the image, \nincluding the key details and other aspects. Do not make up an answer and be accurate. Only refer to the \ncontent of the image provided in the data.\n\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n\n\n================================================"
  },
  {
    "filename": "prompt_template_llama3_images_ScienceQA.txt",
    "path": "fmbench/prompt_template/prompt_template_llama3_images_ScienceQA.txt",
    "directory": "fmbench/prompt_template",
    "extension": "txt",
    "content": "================================================\n<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nYou are an assistant for question-answering tasks based on the image provided in the data. \nIf you dont know the answer just say that you dont know. Use three sentences maximum and be concise with your answer.\n\nQuestion: {question} \n\nTo support your answer, give a short description of the key elements in the image and how they answer\nthe question. Do not miss out any key details from the image.\n\nIf there is no question provided, then mention that no image is provided, do not make up an answer.\n\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n\n\n================================================"
  },
  {
    "filename": "prompt_template_llama3_OpenOrca.txt",
    "path": "fmbench/prompt_template/prompt_template_llama3_OpenOrca.txt",
    "directory": "fmbench/prompt_template",
    "extension": "txt",
    "content": "================================================\n<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n{system_prompt}\n\nContext and task: {input} \n\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n================================================"
  },
  {
    "filename": "prompt_template_llama3_OpenOrca_accuracy.txt",
    "path": "fmbench/prompt_template/prompt_template_llama3_OpenOrca_accuracy.txt",
    "directory": "fmbench/prompt_template",
    "extension": "txt",
    "content": "================================================\n<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n{question} \n\nFollow the instructions below while giving a response:\n\n1. Do not repeat the question in your response. Your response should only contain what is being asked and not the question itself.\n\n2. Analyze the task carefully and only provide the direct response to the task/question. Do not be too descriptive, only respond with what is being asked for.\n\n\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n================================================"
  },
  {
    "filename": "prompt_template_llama3_summarization.txt",
    "path": "fmbench/prompt_template/prompt_template_llama3_summarization.txt",
    "directory": "fmbench/prompt_template",
    "extension": "txt",
    "content": "================================================\n<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\nYou are an assistant for summarization tasks. Use the following pieces of retrieved context in the section demarcated by \"```\" to summarize the context.\n\nContext: {input} \n\nSummarize the entire context. Your summary should be detailed, long and should cover all important key aspects of the context.\n\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n\n================================================"
  },
  {
    "filename": "prompt_template_mistral.txt",
    "path": "fmbench/prompt_template/prompt_template_mistral.txt",
    "directory": "fmbench/prompt_template",
    "extension": "txt",
    "content": "================================================\n<s>[INST]\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context in the section demarcated by \"```\" to answer the question. If you don't know the answer just say that you don't know. Use three sentences maximum and keep the answer concise.\n\n```\n{context}\n```\n\nQuestion: {input}\n\nAnalyze the question carefully, and answer the question accurately with only the required information from the context. There can be multiple question answer pairs in the context. Answer the final question in the question section above after the context.\n\n[/INST]\nAnswer:\n\n\n\n\n================================================"
  },
  {
    "filename": "prompt_template_nova.txt",
    "path": "fmbench/prompt_template/prompt_template_nova.txt",
    "directory": "fmbench/prompt_template",
    "extension": "txt",
    "content": "================================================\nTask: Answer questions based on provided context accurately and concisely\n\nContext information:\n\n- Context will be provided below\n- Question will follow after the context\n- Context may contain multiple question-answer pairs\n\nModel Instructions:\n\n- Analyze the given context carefully\n- Focus only on answering the specific question asked\n- If answer cannot be found in context, state \"I don't know\"\n- Base response solely on information provided in context\n\nResponse style and format requirements:\n\n- Provide maximum three sentences\n- Keep answers concise and to the point\n\nRefer to the context below.\n\n{context}\n\nQuestion: {input}\n\nExpected output: [Direct answer to the question in 1-3 sentences based on context]\n\n\n================================================"
  },
  {
    "filename": "prompt_template_nova_convfinqa.txt",
    "path": "fmbench/prompt_template/prompt_template_nova_convfinqa.txt",
    "directory": "fmbench/prompt_template",
    "extension": "txt",
    "content": "================================================\n{input}\nThink step by step first and then answer. Follow below format when responding\nResponse Schema:\n<thinking>\n( your thinking goes here )\n</thinking>\n<answer>\n( your answer goes here )\n</answer>\n\n\n================================================"
  },
  {
    "filename": "prompt_template_nova_dolly_dataset.txt",
    "path": "fmbench/prompt_template/prompt_template_nova_dolly_dataset.txt",
    "directory": "fmbench/prompt_template",
    "extension": "txt",
    "content": "================================================\n{instruction}\n\n\n================================================"
  },
  {
    "filename": "prompt_template_nova_open_orca.txt",
    "path": "fmbench/prompt_template/prompt_template_nova_open_orca.txt",
    "directory": "fmbench/prompt_template",
    "extension": "txt",
    "content": "================================================\n{question}\n\nFollow the instructions below while giving a response:\n\n1. Do not repeat the question in your response. Your response should only contain what is being asked and not the question itself.\n\n2. Analyze the task carefully and only provide the direct response to the task/question. Do not be too descriptive, only respond with what is being asked for.\n\n\n\n================================================"
  },
  {
    "filename": "prompt_template_open_orca.txt",
    "path": "fmbench/prompt_template/prompt_template_open_orca.txt",
    "directory": "fmbench/prompt_template",
    "extension": "txt",
    "content": "================================================\n{question}\n\n\n\n================================================"
  },
  {
    "filename": "prompt_template_qwen.txt",
    "path": "fmbench/prompt_template/prompt_template_qwen.txt",
    "directory": "fmbench/prompt_template",
    "extension": "txt",
    "content": "================================================\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context in the section demarcated by # to answer the question. If you don't know the answer just say that you don't know. Use three sentences maximum and keep the answer concise.\n\nContext:\n{context}\n\nQuestion: {input}\n\nAnalyze the question carefully, and answer the question accurately with only the required information from the context. There can be multiple question answer pairs in the context. Answer the final question in the question section above after the context.\n\nAnswer:\n\n\n================================================"
  },
  {
    "filename": "prompt_template_titan_text.txt",
    "path": "fmbench/prompt_template/prompt_template_titan_text.txt",
    "directory": "fmbench/prompt_template",
    "extension": "txt",
    "content": "================================================\nYou are an assistant for question-answering tasks. Use the following pieces of retrieved context in the section demarcated by # to answer the question. If you don't know the answer just say that you don't know. Use three sentences maximum and keep the answer concise.\n\n#\n{context}\n#\n\nQuestion: {input}\n\nAnalyze the question carefully, and answer the question accurately with only the required information from the context. There can be multiple question answer pairs in the context. Answer the final question in the question section above after the context.\n\nAnswer:\n\n\n================================================"
  },
  {
    "filename": "evaluation_instructions_majority_vote.txt",
    "path": "fmbench/prompt_template/eval_criteria/evaluation_instructions_majority_vote.txt",
    "directory": "fmbench/prompt_template/eval_criteria",
    "extension": "txt",
    "content": "================================================\n1. Your response should be a JSON containing two elements: \"verdict\" and \"explanation\". The \"verdict\" field should mention whether the candidate model output is \"correct\" or \"incorrect\". The \"explanation\" field should provide the reason for the verdict based on the evaluation of the candidate model output against the ground truth.\n\n2. If the ground truth is a comma-separated list with multiple options for responses, the verdict is correct if the candidate model output \naligns or matches with any of the options in the comma separated list and answers the question in the similar way as the ground truth. \nIt does not have to match all, but if it aligns or matches with one of the options, then the verdict is correct. If the ground truth contains a single option for a response, \nthen only compare the candidate model response with that to check if the candidate model response is correct or not.\n\n3. The candidate model output does not have to use the exact wording of the ground truth but should correctly answer the question in a semantically equivalent manner. If there is more content in the candidate model response than required but it answers the question correctly and aligns or matches\nwith the ground truth, the verdict is \"correct\". Otherwise, it is \"incorrect\".\n\n4. If the candidate model output does not match or align with the ground truth, or any option in the ground truth if the ground truth is \na comma separated list of options, then the verdict is \"incorrect\".\n\n5. If the candidate model output has more context than required compared to the ground truth in answering the question, but still contains the relevant answer \nlike the ground truth, then the verdict is correct.\n\n\n\n================================================"
  },
  {
    "filename": "claude_eval_majority_vote.txt",
    "path": "fmbench/prompt_template/eval_criteria/claude_eval_prompt_templates/claude_eval_majority_vote.txt",
    "directory": "fmbench/prompt_template/eval_criteria/claude_eval_prompt_templates",
    "extension": "txt",
    "content": "================================================\nHuman: Your role is to evaluate the correctness of the candidate model output provided in the <candidate model output></candidate model output> \ntags based on whether it aligns with the ground truth answer provided in the <ground_truth></ground_truth> xml tags in answering the \nquestion in the <question></question> xml tags.\n\nRefer to the question that you have to use while evaluating the correctness of the candidate model response in alignment to the ground truth:\n<question>\n{question}\n</question> \n\nRefer to the candidate model response to be evaluated in the <candidate model output></candidate model output> tags below:\n<candidate model output>\n{answer}\n</candidate model output> \n\nRefer to the ground truth below in the <ground_truth></ground_truth> xml tags while evaluating the candidate model output:\n<ground_truth>\n{ground_truth}\n</ground_truth> \n\nFollow the instructions below while giving your evaluation of the candidate model output in the <evaluation_instructions></evaluation_instructions>\ntags:\n\n<evaluation_instructions>\n{rules}\n</evaluation_instructions>\n\nYour response should only be in JSON format. Your response should NOT have any tags, and should start with the starting bracket of the JSON\nstructure and end with the ending bracket. There should only be the JSON in your response without any other words outside of it, should not\ncontain any tags, only the JSON structure.\n\nAssistant: Sure, here is my evaluation in JSON:\n\n\n================================================"
  },
  {
    "filename": "cohere_eval_majority_vote.txt",
    "path": "fmbench/prompt_template/eval_criteria/cohere_eval_prompt_templates/cohere_eval_majority_vote.txt",
    "directory": "fmbench/prompt_template/eval_criteria/cohere_eval_prompt_templates",
    "extension": "txt",
    "content": "================================================\nYour role is to evaluate the correctness of the candidate model output provided in the candidate model output section\nbased on whether it aligns with the ground truth answer provided in the ground truth section in answering the \nquestion in the question section.\n\nRefer to the question that you have to use while evaluating the correctness of the candidate model response in alignment to the ground truth:\nQuestion: {question}\n\nRefer to the candidate model response to be evaluated below:\ncandidate model response: {answer}\n\n\nRefer to the ground truth to the question in the context below in the ground_truth section:\nground_truth: {ground_truth}\n\nFollow the instructions below while giving your evaluation in the evaluation_instructions section below:\n\n## evaluation_instructions:\n{rules}\n\n## Response Instructions:\n\nOnly give your response in a JSON format. Do not prefix your response with ``` json .....  ``` and so on. \n\nDo not add anything else but the format above in your evaluation response. Your response should just start with \nan opening JSON bracket, the fields, and then a closing JSON bracket, nothing else.\n\n\n\n================================================"
  },
  {
    "filename": "llama3_eval_majority_vote.txt",
    "path": "fmbench/prompt_template/eval_criteria/llama3_eval_prompt_templates/llama3_eval_majority_vote.txt",
    "directory": "fmbench/prompt_template/eval_criteria/llama3_eval_prompt_templates",
    "extension": "txt",
    "content": "================================================\n<|begin_of_text|><|start_header_id|>user<|end_header_id|>\nYour role is to evaluate the correctness of the candidate model output provided in the candidate model output section\nbased on whether it aligns with the ground truth answer provided in the ground truth section in answering the \nquestion in the question section.\n\nRefer to the question that you have to use while evaluating the correctness of the candidate model response in alignment to the ground truth:\nQuestion: {question}\n\nRefer to the candidate model response to be evaluated below:\ncandidate model response: {answer}\n\n\nRefer to the ground truth to the question in the context below in the ground_truth section:\nground_truth: {ground_truth}\n\nFollow the instructions below while giving your evaluation in the evaluation_instructions section below:\n\nevaluation_instructions:\n{rules}\n\nDo not add anything else in your response. Give your response only in a correct formatted json starting with an \nopening and ending with a bracket.\n\nDo not add any pre filler words. Your response should contain the JSON structured output only.\n\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n\n================================================"
  },
  {
    "filename": "mistral_eval_majority_vote.txt",
    "path": "fmbench/prompt_template/eval_criteria/mistral_eval_prompt_templates/mistral_eval_majority_vote.txt",
    "directory": "fmbench/prompt_template/eval_criteria/mistral_eval_prompt_templates",
    "extension": "txt",
    "content": "================================================\n<s>[INST]\nYour role is to evaluate the correctness of the candidate model output provided in the candidate model output section\nbased on whether it aligns with the ground truth answer provided in the ground truth section in answering the \nquestion in the question section.\n\nRefer to the question that you have to use while evaluating the correctness of the candidate model response in alignment to the ground truth:\nQuestion: {question}\n\n\nRefer to the candidate model response to be evaluated below:\ncandidate model response: {answer}\n\n\nRefer to the ground truth to the question in the context below in the ground_truth section:\nground_truth: {ground_truth}\n\nFollow the instructions below while giving your evaluation in the evaluation_instructions section below:\n\nevaluation_instructions:\n{rules}\n\nOnly give your response in a JSON format. Do not prefix your response with ``` json .....  ``` and so on. \n\nDo not add anything else in your response. Give your response only in a correct formatted json starting with an \nopening and ending with a bracket. there should be no `````` in your response. your response\nshould only and only contain the json without any additional characters. Your response\nshould only be the JSON because it will be parsed later for downstream tasks. If you\nprovide anything other than or in addition to the JSON response, it will throw errors \nwhile performing downstream tasks.\n\nDo not add any pre filler words. Your response should contain the JSON structured output only.\n\n[/INST]\nAnswer:\n\n\n================================================"
  },
  {
    "filename": "bedrock_predictor_converseAPI.py",
    "path": "fmbench/scripts/bedrock_predictor_converseAPI.py",
    "directory": "fmbench/scripts",
    "extension": "py",
    "content": "================================================\nimport time\nimport boto3\nimport logging\nfrom typing import Dict, Optional\n\n# set a logger\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nBEDROCK_RUNTIME: str = \"bedrock-runtime\"\n\ndef invoke_bedrock_converse(\n    endpoint_name: str,\n    messages: list,\n    temperature: float,\n    max_tokens: int,\n    top_p: float,\n    system_prompts: list = [{\"text\": \"You are a helpful AI assistant.\"}]\n) -> Dict:\n    \"\"\"\n    Simple function to invoke Bedrock's converse API.\n    Args:\n        endpoint_name: The name of the Bedrock endpoint\n        messages: List of message dictionaries\n        temperature: Temperature parameter for inference\n        max_tokens: Maximum tokens to generate\n        top_p: Top-p parameter for inference\n        system_prompts: System prompts to use (default provided)\n    Returns:\n        Dict containing response data\n    \"\"\"\n    response: Optional[Dict] = None\n    bedrock_client = boto3.client(BEDROCK_RUNTIME)\n    inference_config = {\n        \"temperature\": temperature,\n        \"maxTokens\": max_tokens,\n        \"topP\": top_p,\n    }\n    st = time.perf_counter()\n    response = bedrock_client.converse(\n        modelId=endpoint_name,\n        messages=messages,\n        system=system_prompts,\n        inferenceConfig=inference_config\n    )\n    latency = time.perf_counter() - st\n    return response, latency\n\n\n\n================================================"
  },
  {
    "filename": "constants.py",
    "path": "fmbench/scripts/constants.py",
    "directory": "fmbench/scripts",
    "extension": "py",
    "content": "================================================\nfrom enum import Enum\nfrom typing import List\n\nCONTAINER_TYPE_DJL: str = 'djl'\nCONTAINER_TYPE_VLLM: str = 'vllm'\nCONTAINER_TYPE_VLLM_GPU: str = 'vllm_gpu'\nCONTAINER_TYPE_TRITON: str = 'triton'\nCONTAINER_TYPE_SGLANG: str = 'sglang'\nCONTAINER_TYPE_OLLAMA: str = 'ollama'\nCONTAINER_TYPE_LLAMA_SERVER: str = 'llama_server'\nCONTAINER_TYPE_HUGGINGFACE: str = 'huggingface'\nTRITON_INFERENCE_SCRIPT_VLLM: str = '/scripts/triton/triton-vllm-neuronx.sh'\nTRITON_INFERENCE_SCRIPT_DJL: str = '/scripts/triton/triton-djl-python-neuronx.sh'\nTRITON_CONTENT_DIR_NAME_VLLM: str = 'triton/vllm'\nTRITON_CONTENT_DIR_NAME_DJL: str = 'triton/djl'\nTRITON_SERVE_SCRIPT: str = \"triton_serve_model.sh\"\nAWS_CHIPS_PREFIX_LIST: List[str] = [\"inf2\", \"trn1\"]\nIS_NEURON_INSTANCE = lambda instance_type: any([instance_type.startswith(p) for p in AWS_CHIPS_PREFIX_LIST])\n\nclass ACCELERATOR_TYPE(str, Enum):\n    NEURON = 'neuron'\n    NVIDIA = \"nvidia\"\n\nclass BACKEND(str, Enum):\n    VLLM_BACKEND = 'vllm'\n    DJL_BACKEND = 'djl'\n    TENSORRT_BACKEND = 'tensorrt'\n\nclass MODEL_COPIES(str, Enum):\n    AUTO = 'auto'\n    MAX = \"max\"\n    \n    \n# These variables represent the platform where a specific\n# endpoint is deployed.\nPLATFORM_SAGEMAKER: str = \"sagemaker\"\nPLATFORM_EKS: str = \"eks\"\nPLATFORM_EC2: str = \"ec2\"\nPLATFORM_BEDROCK: str = \"bedrock\"\nPLATFORM_EXTERNAL: str = \"external\"\n\n# inference server listen port\nBASE_PORT_FOR_CONTAINERS: int = 8000\nLISTEN_PORT: int = 8080\n\n# This is the file where the EC2 instance utilization metrics are stored\nEC2_SYSTEM_METRICS_FNAME: str = \"EC2_system_metrics.csv\"\nEC2_UTILIZATION_METRICS_INTERVAL: int = 5\n\n\n\n================================================"
  },
  {
    "filename": "custom_rest_predictor.py",
    "path": "fmbench/scripts/custom_rest_predictor.py",
    "directory": "fmbench/scripts",
    "extension": "py",
    "content": "================================================\nimport os\nimport json\nimport math\nimport time\nimport boto3\nimport logging\nimport requests\nimport pandas as pd\nfrom datetime import datetime\nfrom fmbench.scripts import constants\nfrom fmbench.utils import count_tokens\nfrom typing import Dict, Optional, List\nfrom fmbench.scripts.fmbench_predictor import (FMBenchPredictor,\n                                               FMBenchPredictionResponse)\n\n# set a logger\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass CustomRestPredictor(FMBenchPredictor):\n    \"\"\"\n    This is a custom rest predictor that does a POST request on the endpoint\n    specified in the configuration file with custom headers, authentication parameters\n    and the model_id. This rest predictor can be used with custom parameters. View an \n    example of the parameters passed in this config file: configs/byoe/config-byo-custom-rest-predictor.yml\n    \"\"\"\n    def __init__(self,\n                 endpoint_name: str,\n                 inference_spec: Optional[Dict],\n                 metadata: Optional[Dict]):\n        try:\n            \"\"\"\n            Initialize the endpoint name and the inference spec. The endpoint name here points to the\n            endpoint name in the config file, which is the endpoint url to do a request.post on. The \n            inference spec contains the different auth, headers, inference parameters that are \n            passed into this script from the config file.\n            \"\"\"\n            self._endpoint_name: str = endpoint_name\n            self._inference_spec: Dict = inference_spec \n        except Exception as e:\n            logger.error(f\"create_predictor, exception occured while creating predictor \"\n                         f\"for endpoint_name={self._endpoint_name}, exception={e}\")\n        logger.info(f\"_endpoint_name={self._endpoint_name}, _inference_spec={self._inference_spec}\")\n\n    def get_prediction(self, payload: Dict) -> FMBenchPredictionResponse:\n        # Initialize some variables, including the response, latency, streaming variables, prompt and completion tokens.\n        response_json: Optional[Dict] = None\n        response: Optional[str] = None\n        latency: Optional[float] = None\n        # Streaming can be enabled if the model is deployed on SageMaker or Bedrock\n        TTFT: Optional[float] = None\n        TPOT: Optional[float] = None\n        TTLT: Optional[float] = None\n        prompt_tokens: Optional[int] = None\n        completion_tokens: Optional[int] = None\n        # This is the generated text from the model prediction\n        generated_text: Optional[str] = None\n        \n        try:\n            # define the generation config, custom headers and model id from the inference spec\n            # that will be used in the payload while FMBench makes predictions on the model endpoint\n            inference_param_set = self._inference_spec.get(\"parameters\")\n            headers = self._inference_spec.get(\"headers\")\n            model_id = self._inference_spec.get(\"model_id\")\n            # Prepare the request body - in this request body, we are providing the generation config, prompt\n            # and the model id as given in the inference spec within the FMBench config file. If the inference\n            # parameter set does not exist, then just send in the request without the inference specifications\n            if inference_param_set:\n                request_body = {\n                    \"model_id\": model_id,\n                    \"prompt\": payload['inputs'],\n                } | inference_param_set\n            else:\n                logger.info(f\"Using the request body without the generation_config variable\")\n                request_body = {\n                    \"model_id\": model_id,\n                    \"prompt\": payload['inputs']\n                }\n            # Start the timer to measure the latency of the prediction made to the endpoint\n            st = time.perf_counter()\n            # Make POST request including the headers, the request body, and the endpoint url.\n            response = requests.post(\n                self._endpoint_name,\n                headers=headers,\n                json=request_body\n            )\n            # measure the total latency to make the POST request to the endpoint\n            latency = time.perf_counter() - st\n            response.raise_for_status()\n            response_data = response.json()\n            # Extract the generated text from the completions array\n            if response_data.get(\"completions\"):\n                # This is custom to the endpoint based on the completion format. This will change\n                # based on how your inference container responds to requests.\n                generated_text = response_data[\"completions\"][0].get(\"text\", \"\")\n            response_json = dict(generated_text=generated_text)\n            # Get completion tokens from the usage information if available\n            # otherwise fall back to counting tokens\n            if response_data.get(\"usage\"):\n                # This is assuming the response data contains a usage field with prompt and input tokens\n                completion_tokens = response_data[\"usage\"].get(\"completion_tokens\")\n                prompt_tokens = response_data[\"usage\"].get(\"prompt_tokens\")\n                logger.info(f\"Found 'usage' field in the response data. Prompt tokens: {prompt_tokens}, Completion tokens: {completion_tokens}\")\n            else:\n                # This uses the count tokens function. If you have an hf tokenizer to be used, then \n                # place your hf_token.txt file in the fmbench-read/scripts directory and mention the \n                # hf model id in the experiments section of the config file in the \"hf_tokenizer_model_id\"\n                # paramter. The count_tokens function will use that custom tokenizer. If you have a custom\n                # tokenizer, then place the \"tokenizer.json\" and \"config.json\" files in the fmbench-read/scripts\n                # directory and FMBench will use that. If none of these options are available, FMBench will use\n                # the default 750-1000 tokens tokenizer.\n                prompt_tokens = count_tokens(payload[\"inputs\"])\n                completion_tokens = count_tokens(generated_text) \n                logger.info(f\"Using the default tokenizer to count the prompt and input tokens. Prompt tokens: {prompt_tokens}, Completion tokens: {completion_tokens}\")\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"get_prediction, exception occurred while getting prediction for payload={payload} \"\n                        f\"from predictor={self._endpoint_name}, response={response}, exception={e}\")\n        return FMBenchPredictionResponse(\n            response_json=response_json,\n            latency=latency,\n            time_to_first_token=TTFT,\n            time_per_output_token=TPOT,\n            time_to_last_token=TTLT,\n            completion_tokens=completion_tokens,\n            prompt_tokens=prompt_tokens\n        )\n        \n    @property\n    def endpoint_name(self) -> str:\n        \"\"\"The endpoint name property.\"\"\"\n        return self._endpoint_name\n\n    # The rest ep is deployed on an instance that incurs hourly cost hence, the calculcate cost function\n    # computes the cost of the experiment on an hourly basis. If your instance has a different pricing structure\n    # modify this function.\n    def calculate_cost(self,\n                       instance_type: str,\n                       instance_count: int,\n                       pricing: Dict,\n                       duration: float,\n                       prompt_tokens: int,\n                       completion_tokens: int) -> float:\n        \"\"\"Calculate the cost of each experiment run.\"\"\"\n        experiment_cost: Optional[float] = None\n        try:\n            instance_based_pricing = pricing['pricing']['instance_based']\n            hourly_rate = instance_based_pricing.get(instance_type, None)\n            logger.info(f\"the hourly rate for running on {instance_type} is {hourly_rate}, instance_count={instance_count}\")\n            # calculating the experiment cost for instance based pricing\n            instance_count = instance_count if instance_count else 1\n            experiment_cost = (hourly_rate / 3600) * duration * instance_count\n        except Exception as e:\n            logger.error(f\"exception occurred during experiment cost calculation, exception={e}\")\n        return experiment_cost\n    \n    def get_metrics(self,\n                    start_time: datetime,\n                    end_time: datetime,\n                    period: int = 60) -> pd.DataFrame:\n        # not implemented\n        return None\n\n    def shutdown(self) -> None:\n        \"\"\"Represents the function to shutdown the predictor\n           cleanup the endpooint/container/other resources\n        \"\"\"\n        return None\n    \n    @property\n    def inference_parameters(self) -> Dict:\n        \"\"\"The inference parameters property.\"\"\"\n        return self._inference_spec.get(\"parameters\")\n\n    @property\n    def platform_type(self) -> Dict:\n        \"\"\"The inference parameters property.\"\"\"\n        return constants.PLATFORM_EXTERNAL\n    \ndef create_predictor(endpoint_name: str, inference_spec: Optional[Dict], metadata: Optional[Dict]):\n    return CustomRestPredictor(endpoint_name, inference_spec, metadata)\n\n\n\n================================================"
  },
  {
    "filename": "deploy_w_djl_serving.py",
    "path": "fmbench/scripts/deploy_w_djl_serving.py",
    "directory": "fmbench/scripts",
    "extension": "py",
    "content": "================================================\n\"\"\"\nDeploys a model from HuggingFace on Amazon SageMaker using the DJL DeepSpeek LMI\n(https://github.com/deepjavalibrary/djl-serving).\n\n1. Configuration is read from the configured serving.properties file.\n2. A hf_token.txt file is required to download the model from Hugging Face.\n\"\"\"\n# Import necessary libraries\nimport os\nimport glob\nimport time\nimport boto3\nimport fmbench\nimport logging\nimport tarfile\nimport tempfile\nimport sagemaker\nfrom pathlib import Path\nfrom urllib.parse import urlparse\nfrom fmbench.scripts import constants\nfrom sagemaker.utils import name_from_base\nfrom huggingface_hub import snapshot_download\nfrom typing import Dict, List, Tuple, Optional\n\n# set a logger\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Initialize the platform where this script deploys the model\nPLATFORM: str = constants.PLATFORM_SAGEMAKER\n\n# globals\nHF_TOKEN_FNAME: str = os.path.join(os.path.dirname(os.path.realpath(__file__)),\n                                   \"hf_token.txt\")\n\n# Initialize your S3 client for your model uploading\ns3_client = boto3.client('s3')\n\n# session/account specific variables\nsess = sagemaker.session.Session()\n# Define the location of your s3 prefix for model artifacts\nregion: str = sess._region_name\n\n# bucket to house model artifacts\ntry:\n    default_bucket = sess.default_bucket()\nexcept Exception as e:\n    logger.error(f\"exception occured while getting default_bucket, exception={e}\")\n    default_bucket = None\n\n# Define your account id\naccount_id: str = sess.account_id()\n\n# Initialize the sagemaker and sagemaker runtime clients\nsm_client = boto3.client(\"sagemaker\")\n\ntag = [\n    {\n        'Key': 'fmbench-version',\n        'Value': fmbench.__version__\n    }\n]\ndef _download_model(model_id: str,\n                    local_model_path: str,\n                    allow_patterns: Optional[List] = [\"*\"]) -> str:\n    \"\"\"\n    Download the model files locally\n    \"\"\"\n    local_model_path = Path(local_model_path)\n    print(f\"Local model path: {local_model_path}\")\n    local_model_path.mkdir(exist_ok=True)\n    print(f\"Created the local directory: {local_model_path}\")\n\n    model_download_path = snapshot_download(\n        repo_id=model_id,\n        cache_dir=local_model_path,\n        allow_patterns=allow_patterns,\n        use_auth_token=Path(HF_TOKEN_FNAME).read_text().strip()\n    )\n    print(f\"Uncompressed model downloaded into ... -> {model_download_path}\")\n    return model_download_path\n\ndef _upload_dir(localDir: str, awsInitDir: str, bucketName: str, tag: str =\"*.*\"):\n    s3 = boto3.resource('s3')\n    p = Path(localDir)\n    # Iterate over all directories and files within localDir\n    for path in p.glob('**/*'):\n        if path.is_file():\n            rel_path = path.relative_to(p)\n            awsPath = os.path.join(awsInitDir, str(rel_path)).replace(\"\\\\\", \"/\")\n            logger.info(f\"Uploading {path} to s3://{bucketName}/{awsPath}\")\n            logger.info(f\"path: {path}, bucket name: {bucketName}, awsPath: {awsPath}\")\n            s3.meta.client.upload_file(path, bucketName, awsPath)\n\ndef _create_and_upload_model_artifact(serving_properties_path: str,\n                                      bucket: str,\n                                      prefix: str) -> str:\n    \"\"\"\n    Create the model artifact with the updated serving properties within the directory\n    \"\"\"\n    # Create a tar.gz file containing only the serving.properties file\n    tar_file_path = os.path.join(Path(serving_properties_path).parent, 'model.tar.gz')\n    with tarfile.open(tar_file_path, \"w:gz\") as tar:\n        # Add the serving.properties file\n        tar.add(serving_properties_path, arcname='serving.properties')\n\n    # Upload the tar.gz file to S3\n    key = f\"{prefix}/model.tar.gz\"\n    s3_client.upload_file(tar_file_path, bucket, key)\n    model_tar_gz_path: str = f\"s3://{bucket}/{key}\"\n    logger.info(f\"uploaded model.tar.gz to {model_tar_gz_path}\")\n    return model_tar_gz_path\n\ndef _create_model(experiment_config: Dict,\n                  inference_image_uri: str,\n                  s3_model_artifact: str,\n                  role_arn: str) -> Tuple[str, str]:\n    \"\"\"\n    # Function to create the SageMaker model\n    \"\"\"\n    model_name = name_from_base(experiment_config['model_name'])\n    env = experiment_config.get('env')\n\n    # HF token required for gated model downloads form HF\n    hf_token_file_path = Path(HF_TOKEN_FNAME)\n    hf_dict: Optional[Dict] = None\n    if hf_token_file_path.is_file() is True:\n        logger.info(f\"hf_token file path: {hf_token_file_path} is a file\")\n        hf_dict = dict(HUGGING_FACE_HUB_TOKEN=hf_token_file_path.read_text().strip())\n    else:\n        logger.info(f\"hf_token file path: {hf_token_file_path} is not a file\")\n        \n    # this gets passed as an env var\n    if env:\n        if hf_dict:\n            # both env and hf_dict exists, so we do a union\n            env = env | hf_dict\n    else:\n        if hf_dict:\n            # env var did not exist, but hf_dict did so that\n            # is now the env var\n            env = hf_dict\n\n    if env:\n        pc = dict(Image=inference_image_uri,\n                  ModelDataUrl=s3_model_artifact,\n                  Environment=env)\n    else:\n        pc = dict(Image=inference_image_uri,\n                  ModelDataUrl=s3_model_artifact)\n    \n    logger.info(f\"Fmbench Version Tag is {tag}\")\n    create_model_response = sm_client.create_model(\n        ModelName=model_name,\n        ExecutionRoleArn=role_arn,\n        PrimaryContainer=pc,\n        Tags=tag\n    )\n    return model_name, create_model_response[\"ModelArn\"]\n\n\ndef _deploy_endpoint(experiment_config: Dict,\n                     model_name: str) -> Tuple[str, str]:\n    \"\"\"\n    Function to create and deploy the endpoint\n    \"\"\"    \n    \n    endpoint_config_name = name_from_base(experiment_config['ep_name'])\n    endpoint_name = name_from_base(experiment_config['ep_name'])\n\n    _ = sm_client.create_endpoint_config(\n        EndpointConfigName=endpoint_config_name,\n        ProductionVariants=[\n            {\n                \"VariantName\": \"variant1\",\n                \"ModelName\": model_name,\n                \"InstanceType\": experiment_config[\"instance_type\"],\n                \"InitialInstanceCount\": 1,\n                \"ModelDataDownloadTimeoutInSeconds\": 3600,\n                \"ContainerStartupHealthCheckTimeoutInSeconds\": 3600,\n            },\n        ],\n    )\n\n    logger.info(f\"Fmbench Version Tag is {tag}\")\n    create_endpoint_response = sm_client.create_endpoint(\n        EndpointName=endpoint_name, \n        EndpointConfigName=endpoint_config_name, \n        Tags=tag\n    )\n    return endpoint_name, create_endpoint_response['EndpointArn']\n\n\ndef _check_endpoint_status(endpoint_name: str) -> str:\n    \"\"\"\n    Function to check the status of the endpoint\n    \"\"\"\n    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n    status = resp[\"EndpointStatus\"]\n    while status == \"Creating\":\n        time.sleep(60)\n        resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n        status = resp[\"EndpointStatus\"]\n    return status\n\n\ndef deploy(experiment_config: Dict, role_arn: str) -> Dict:\n    \"\"\"\n    Function to deploy the model and create the endpoint\n    \"\"\"\n\n    if experiment_config.get(\"download_from_hf_place_in_s3\") is True:\n        with tempfile.TemporaryDirectory() as local_model_path:\n            logger.info(f\"created temporary directory {local_model_path}\")\n        local_model_path = _download_model(experiment_config['model_id'],\n                                           local_model_path)\n        logger.info(f\"going to upload model files to {experiment_config['model_s3_path']}\")\n\n        o = urlparse(experiment_config['model_s3_path'], allow_fragments=False)\n        _upload_dir(local_model_path, o.path.lstrip('/'), o.netloc) \n        logger.info(f\"local model path: {local_model_path}, o.path: {o.path}, o.netloc: {o.netloc}\")\n\n        model_artifact = experiment_config['model_s3_path']\n        logger.info(f\"Uncompressed model downloaded into ... -> {model_artifact}\")\n\n    logger.info(\"preparing model artifact...\")\n\n    # handle serving.properties, we read it from the config and then write it to\n    # a local file\n    write_bucket = experiment_config.get('bucket', default_bucket)\n    logger.info(f\"write bucket for inserting model.tar.gz into: {write_bucket}\")\n    properties = experiment_config[\"serving.properties\"].format(write_bucket=write_bucket)\n    dir_path = os.path.dirname(os.path.realpath(__file__))\n    serving_properties_path = os.path.join(dir_path, \"serving.properties\")\n    Path(serving_properties_path).write_text(properties)\n    logger.info(f\"written the following serving.properties \\\n                  content={properties} to {serving_properties_path}\")\n\n    # create and upload the model.tar.gz, note that this file is just a placeholder\n    # it is not the actual model, the actual model binaries are in s3 or HuggingFace\n    # and the container will download them when the model endpoint is being created\n    logger.info(f\"uploading model.tar.gz to S3,bucket={write_bucket}, \\\n                  prefix={experiment_config['model_id']}\")\n    model_artifact = _create_and_upload_model_artifact(serving_properties_path,\n                                                      write_bucket,\n                                                      experiment_config['model_id'])\n    logger.info(f\"model uploaded to: {model_artifact}\")\n\n    inference_image_uri = experiment_config['image_uri']\n    logger.info(f\"inference image URI: {inference_image_uri}\")\n\n    # create model\n    model_name, model_arn = _create_model(experiment_config,\n                                          inference_image_uri,\n                                          model_artifact,\n                                          role_arn)\n    logger.info(f\"created Model: {model_arn}\")\n\n    # deploy model\n    endpoint_name, _ = _deploy_endpoint(experiment_config, model_name)\n    logger.info(f\"deploying endpoint: {endpoint_name}\")\n\n    # check model deployment status\n    status = _check_endpoint_status(endpoint_name)\n    logger.info(f\"Endpoint status: {status}\")\n\n    if status == 'InService':\n        logger.info(\"endpoint is in service\")\n    else:\n        logger.info(\"endpoint is not in service.\")\n\n    return dict(endpoint_name=endpoint_name,\n                experiment_name=experiment_config['name'],\n                instance_type=experiment_config['instance_type'],\n                instance_count=experiment_config['instance_count'],\n                deployed=True)\n\n\n\n================================================"
  },
  {
    "filename": "deploy_w_hf_tgi.py",
    "path": "fmbench/scripts/deploy_w_hf_tgi.py",
    "directory": "fmbench/scripts",
    "extension": "py",
    "content": "================================================\n# Import necessary libraries\nimport os\nimport json\nimport time\nimport boto3\nimport fmbench\nimport logging\nimport sagemaker\nfrom typing import Dict\nfrom pathlib import Path\nfrom fmbench.scripts import constants\nfrom sagemaker.huggingface import HuggingFaceModel\nfrom sagemaker.huggingface import get_huggingface_llm_image_uri\n\n# globals\nHF_TOKEN_FNAME: str = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"hf_token.txt\")\n\n# Initialize the platform where this script deploys the model\nPLATFORM: str = constants.PLATFORM_SAGEMAKER\n\n# set a logger\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Initialize your S3 client for your model uploading\ns3_client = boto3.client('s3')\n\n# Initialize the sagemaker and sagemaker runtime clients \nsm_client = boto3.client(\"sagemaker\")\nsmr_client = boto3.client(\"sagemaker-runtime\")\n\ntag = [\n    {\n        'Key': 'fmbench-version',\n        'Value': fmbench.__version__\n    }\n]\n\n# Function to create the llm hugging face model\ndef create_hugging_face_model(experiment_config: Dict, role_arn: str) -> HuggingFaceModel:\n    # Define Model and Endpoint configuration parameter\n    model_config = {\n    'HF_MODEL_ID': experiment_config['model_id'],\n    'SM_NUM_GPUS': json.dumps(experiment_config['env']['NUMBER_OF_GPU']), # Number of GPU used per replica\n    'MAX_INPUT_LENGTH': json.dumps(4090),  # Max length of input text\n    'MAX_TOTAL_TOKENS': json.dumps(4096),  # Max length of the generation (including input text)\n    'MAX_BATCH_TOTAL_TOKENS': json.dumps(8192),  # Limits the number of tokens that can be processed in parallel during the generation\n    'HUGGING_FACE_HUB_TOKEN': Path(HF_TOKEN_FNAME).read_text().strip()\n    }\n\n    # create HuggingFaceModel with the image uri\n    llm_model = HuggingFaceModel(role=role_arn,\n                                 image_uri=experiment_config['image_uri'],\n                                 env=model_config)\n\n    print(f\"Hugging face model defined using {model_config} -> {llm_model}\")\n    return llm_model\n\n## Function to check the status of the endpoint\ndef check_endpoint_status(endpoint_name: str) -> str:\n    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n    status = resp[\"EndpointStatus\"]\n    while status == \"Creating\":\n        time.sleep(60)\n        resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n        status = resp[\"EndpointStatus\"]\n    return status\n\n# Deploy the hugging face model\ndef deploy_hugging_face_model(experiment_config: Dict, llm_model: HuggingFaceModel) -> str:   \n    tmout: int = experiment_config['env']['HEALTH_CHECK_TIMEOUT']\n    llm = llm_model.deploy(initial_instance_count=experiment_config['env']['INSTANCE_COUNT'],\n                           instance_type=experiment_config['instance_type'],\n                           container_startup_health_check_timeout=tmout,\n                           tags=tag)\n    return llm.endpoint_name\n\n# Function to deploy the model and create the endpoint\ndef deploy(experiment_config: Dict, role_arn: str) -> Dict[str, str]:\n    logger.info(\"deploying the model using the llm_model and the configurations ....\")\n\n\n    print(f\"Setting the model configurations .....\")\n    llm_model = create_hugging_face_model(experiment_config, role_arn)\n    logger.info(f\"the llm_model has been defined .... {llm_model}\")\n\n    llm_endpoint = deploy_hugging_face_model(experiment_config, llm_model)\n    logger.info(\"Deploying the model now ....\")\n\n    status = check_endpoint_status(llm_endpoint)\n    logger.info(f\"Endpoint status: {status}\")\n\n    return dict(endpoint_name=llm_endpoint, \n                experiment_name=experiment_config['name'], \n                instance_type=experiment_config['instance_type'], \n                instance_count=experiment_config['instance_count'], \n                deployed=True)\n\n\n\n================================================"
  },
  {
    "filename": "ec2_deploy.py",
    "path": "fmbench/scripts/ec2_deploy.py",
    "directory": "fmbench/scripts",
    "extension": "py",
    "content": "================================================\n\"\"\"\nDeploys a model from HuggingFace on Amazon EC2\n\n1. Configuration is read from the configured serving.properties file.\n2. A hf_token.txt file is required to download the model from Hugging Face.\n\"\"\"\n# Import necessary libraries\nimport os\nimport re\nimport sys\nimport time\nimport json\nimport stat\nimport docker\nimport inspect\nimport logging\nimport requests\nimport tempfile\nimport subprocess\nfrom pathlib import Path\nfrom typing import Dict, Union\nfrom fmbench.scripts import constants\nfrom ec2_metadata import ec2_metadata\nfrom fmbench.scripts.constants import (IS_NEURON_INSTANCE, LISTEN_PORT)\nfrom fmbench.scripts.prepare_for_multi_model_containers import prepare_docker_compose_yml\nfrom fmbench.scripts.inference_containers import (djl, vllm, vllm_gpu, triton, ollama, sglang)\n\n# set a logger\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Initialize the platform where this script deploys the model\nPLATFORM: str = constants.PLATFORM_EC2\n\n# globals\nHF_TOKEN_FNAME: str = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"hf_token.txt\")\nFMBENCH_MODEL_CONTAINER_NAME: str = \"fmbench_model_container\"\n\ndef _set_up(model_name: str, local_model_path: str):\n    \"\"\"\n    Create the model serving.properties file locally in a model directory\n    \"\"\"\n    # make the model directory with serving.properties\n    directory = os.path.join(local_model_path, model_name)\n    os.makedirs(directory, exist_ok=True)\n    logger.info(f\"Creating the local directory: {directory}\")\n    \n    # return the directory we created\n    return directory\n\ndef _create_deployment_script(image_uri,\n                              container_type,\n                              privileged_mode,\n                              region,\n                              model_name,\n                              model_id,\n                              hf_token,\n                              directory,\n                              model_loading_timeout,\n                              env,\n                              model_copies,\n                              is_neuron_instance,\n                              cli_params):\n    \"\"\"\n    Write a deployment script for model container\n    \"\"\"\n    logger.info(inspect.getargvalues(inspect.currentframe()))\n    privileged_str: str = \"--privileged\" if privileged_mode else \"\"\n    env_str: str = \"\"\n    if env is not None:\n        for k, v in env.items():\n            env_str += f\"-e {k}={v} \"\n\n    match container_type:\n        case constants.CONTAINER_TYPE_DJL:\n            deploy_script_content = djl.create_script(region, image_uri, model_id, model_name, env_str, privileged_str, hf_token, directory)\n        case constants.CONTAINER_TYPE_VLLM:\n            deploy_script_content = vllm.create_script(region, image_uri, model_id, model_name, env_str, privileged_str, hf_token, directory)\n        case constants.CONTAINER_TYPE_VLLM_GPU:\n            deploy_script_content = vllm_gpu.create_script(region, image_uri, model_id, model_name, env_str, privileged_str, hf_token, directory, cli_params)\n        case constants.CONTAINER_TYPE_TRITON:\n            deploy_script_content = triton.create_script(region, image_uri, model_id, model_name, env_str, privileged_str, hf_token, directory)\n        case constants.CONTAINER_TYPE_OLLAMA:\n            deploy_script_content = ollama.create_script(region, image_uri, model_id, model_name, env_str, privileged_str, hf_token, directory)\n        case constants.CONTAINER_TYPE_SGLANG:\n            logger.info(f\"Going to create the deployment script for deploying {model_id} using SGLang\")\n            deploy_script_content = sglang.create_script(region, image_uri, model_id, model_name, env_str, privileged_str, hf_token, directory, cli_params)\n        case _:\n            raise ValueError(f\"dont know how to handle container_type={container_type}\")\n    script_file_path = os.path.join(directory, \"deploy_model.sh\")\n    Path(script_file_path).write_text(deploy_script_content)\n    st = os.stat(script_file_path)\n    os.chmod(script_file_path, st.st_mode | stat.S_IEXEC)\n\n    logger.info(f\"deploy_model.sh content: {deploy_script_content}\")\n    logger.info(f\"The deploy_model.sh file has been created in {script_file_path}\")\n    return script_file_path\n\ndef _run_container(script_file_path):\n    \"\"\"\n    Runs the deploy_model.sh bash script with the provided arguments.\n    \"\"\"\n    logger.info(f\"Running container script at {script_file_path}\")\n    # Create a Docker client\n    client = docker.from_env()\n\n    try:        \n        logger.info(f\"going to run script {script_file_path}\")\n        subprocess.run([\"bash\", script_file_path], check=True)\n        logger.info(f\"done running bash script\")\n        return True    \n    except Exception as e:\n        logger.error(f\"An unexpected error occurred: {e}\")\n    return False\n\ndef _check_model_deployment(endpoint, model_id, container_type, model_loading_timeout, model_copies, is_neuron_instance):\n    \"\"\"\n    Check the model deployment status and wait for the model to be ready.\n    \"\"\"\n    start_time = time.time()\n    logger.info(f\"Checking deployment status at {endpoint}\")\n    if container_type == constants.CONTAINER_TYPE_DJL:\n        data = {\"inputs\": \"tell me a story of the little red riding hood\"}\n    elif container_type == constants.CONTAINER_TYPE_VLLM \\\n    or container_type == constants.CONTAINER_TYPE_OLLAMA \\\n    or container_type == constants.CONTAINER_TYPE_VLLM_GPU:\n        data = {\"model\": model_id,  # Specify the model to use\n                \"prompt\": \"tell me a story of the little red riding hood\",}\n    elif container_type == constants.CONTAINER_TYPE_TRITON:\n        if is_neuron_instance:\n            data = {\n                    \"text_input\": \"tell me a story of the little red riding hood\",\n                    \"sampling_parameters\": json.dumps({\"top_k\": 50})\n                }\n        else:\n            data = {\"text_input\": \"tell me a story of the little red riding hood\", \"max_tokens\": 50}\n    elif container_type == constants.CONTAINER_TYPE_SGLANG:\n        # this is the sglang container format to check for model deployment\n        data = {\n            \"model\": \"default\",\n            \"messages\": [\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                {\"role\": \"user\", \"content\": \"List 3 countries and their capitals.\"}\n            ],\n            \"temperature\": float(0.1),\n            \"max_tokens\": int(100)\n        }\n    headers = {\"content-type\": \"application/json\"}\n    container_check_timeout = 60\n    logger.info(f\"going to check every {container_check_timeout}s for the inference endpoint to be up...\")\n    logger.info(f\"this will take at least 10 minutes or so, please be patient...\")\n    while time.time() - start_time < model_loading_timeout:        \n        try:\n            response = requests.post(endpoint, headers=headers, json=data)\n            logger.info(f\"response is: {response.text}\")\n            if response.status_code == 200:\n                logger.info(\"model deployment successful!\")\n\n                # if model_copies != 1 then wait for some more time to give\n                # all the containers a chance to be up\n                if model_copies > 1:\n                    # this extra wait is not needed if this is a triton server running on a gpu\n                    # because we start multiple model servers in a single container\n                    if is_neuron_instance is False and container_type == constants.CONTAINER_TYPE_TRITON:\n                        logger.info(f\"this is a triton container running on a gpu instance, skipping the additional wait\")\n                    else:\n                        additional_sleep_time = model_copies * container_check_timeout\n                        logger.info(f\"since model_copies={model_copies}, waiting for an addition {additional_sleep_time}s \"\n                                    f\"to allow all model endpoints to come up\")\n                        time.sleep(additional_sleep_time)\n                total_wait_time = time.time() - start_time\n                logger.info(\"marking endpoint in service, total_wait_time={total_wait_time}s\")\n                return \"InService\"\n            else:\n                logger.info(f\"model deployment is not ready yet. Return code: {response.status_code}\")\n                logger.info(f\"waiting for {container_check_timeout}s before retrying\")\n                time.sleep(container_check_timeout)\n        except Exception as e:\n            logger.error(f\"error occurred while deploying the endpoint: {e}\")\n            logger.info(f\"waiting for {container_check_timeout}s before retrying\")\n            time.sleep(container_check_timeout)\n    return \"Failed\"\n\ndef deploy(experiment_config: Dict, role_arn: str) -> Dict:\n    \"\"\"\n    Function to deploy the model and create the endpoint\n    \"\"\"\n    image_uri: str = experiment_config['image_uri']\n    ep_name: str = experiment_config['ep_name']\n    model_id: str = experiment_config['model_id']\n    model_name: str = Path(model_id).name\n    logger.info(f\"Going to deploy model: {model_name}\")\n    # extract port number from the endpoint name\n    # 8080 from \"http://localhost:8080/v2/models/ensemble/generate\"\n    # this is the port on which either the inference container (if there\n    # is a single container) or the load balancer in case of multiple containers\n    # will listen for new connnections\n    match = re.search(r\"http://[^:/]+:(\\d+)\", ep_name)\n    listen_port = int(match.group(1)) if match else constants.LISTEN_PORT\n    logger.info(f\"ep_name={ep_name}, listen_port={listen_port}\")\n    region: str = experiment_config.get('region')\n    privileged_mode: str = experiment_config['ec2'].get('privileged_mode', False)\n    container_type: str = experiment_config['inference_spec'].get('container_type', constants.CONTAINER_TYPE_DJL)\n    env = experiment_config.get('env')\n    container_params: str = experiment_config['inference_spec'].get('container_params', None)\n    if container_params is None:\n        serving_properties = experiment_config.get('serving.properties', None)\n        logger.info(f\"Serving properties found in experiment config: {serving_properties}\")\n    else: \n        serving_properties = container_params.get('serving.properties', None)\n        logger.info(f\"Serving properties found in container params in the config: {serving_properties}\")\n    logger.info(f\"Serving properties provided to deploy on container type={container_type} using image_uri={image_uri}: {serving_properties}\")\n    model_loading_timeout: int = experiment_config['ec2']['model_loading_timeout']\n    \n    HF_TOKEN: str = Path(HF_TOKEN_FNAME).read_text().strip()\n    dir_path = home_dir = os.getenv(\"HOME\", str(Path.home()))\n    logger.info(f\"Home directory: {dir_path}\")\n    \n    # print the ec2 instance details for it to get logged\n    logger.info(f\"EC2 instance type: {ec2_metadata.instance_type}, AMI ID: {ec2_metadata.ami_id}\")\n    \n    # HF token is mandatory even if no env vars were specific in the config file    \n    if env is None:\n        env = {\"HF_TOKEN\": HF_TOKEN}\n    else:\n        env[\"HF_TOKEN\"] = HF_TOKEN\n    logger.info(f\"env={env}\")    \n\n    model_directory = _set_up(model_name, dir_path)\n    is_neuron_instance = IS_NEURON_INSTANCE(experiment_config['instance_type'])\n    model_copies = experiment_config['inference_spec'].get('model_copies', '1')\n    cli_params = experiment_config['inference_spec'].get('cli_params', '')\n\n\n    # if this is a neuron instance and we are using the djl serving inference container\n    # then use the docker compose approach so we first create the docker compose file\n    # and then create the deployment script, otherwise we create the deployment script\n    # directly (see call to _create_deployment_script below)\n    model_copies_actual = 1\n    if (container_type == constants.CONTAINER_TYPE_DJL) or (container_type == constants.CONTAINER_TYPE_TRITON):  \n        logger.info(f\"container_type={container_type}, is_neuron_instance={is_neuron_instance}, going to create docker compose yml\")\n        model_copies_actual = prepare_docker_compose_yml(model_id=model_id,\n                                                         model_copies=model_copies,\n                                                         inference_params=experiment_config['inference_spec'],\n                                                         image=image_uri,\n                                                         user=container_type,\n                                                         shm_size=experiment_config['inference_spec']['shm_size'],\n                                                         env=env,\n                                                         serving_properties=serving_properties,\n                                                         dir_path=dir_path,\n                                                         listen_port=listen_port)   \n    logger.info(\"Creating the deployment script in model directory\")\n    deployment_script_path = _create_deployment_script(image_uri,\n                                                       container_type,\n                                                       privileged_mode,\n                                                       region,\n                                                       model_name,\n                                                       model_id,\n                                                       HF_TOKEN,\n                                                       model_directory,\n                                                       model_loading_timeout,\n                                                       env,\n                                                       model_copies,\n                                                       is_neuron_instance,\n                                                       cli_params)\n\n    logger.info(\"Running the deployment script\")\n    ran_container = _run_container(deployment_script_path)\n\n    # initialize with None values for error case\n    deployment_result: Dict = dict(endpoint_name=None, \n                                   experiment_name=None,\n                                   instance_type=None,\n                                   instance_count=None, \n                                   deployed=False)\n    if ran_container:\n        logger.info(\"Container ran successfully\")\n        ep_status = _check_model_deployment(ep_name, model_id, container_type, model_loading_timeout, model_copies_actual, is_neuron_instance)\n        logger.info(f\"Endpoint status: {ep_status}\")\n        if ep_status == \"InService\":\n            logger.info(\"Model endpoint running!\")\n            deployment_result['endpoint_name'] = ep_name\n            deployment_result['experiment_name'] = experiment_config['name']\n            deployment_result['instance_type'] = experiment_config['instance_type']\n            deployment_result['instance_count'] = experiment_config['instance_count']\n            deployment_result['deployed'] = True\n            return deployment_result\n        elif ep_status == \"Failed\":\n            logger.error(\"Model endpoint not running!\")\n            return deployment_result\n    else:\n        logger.error(\"Container did not run successfully\")\n        return deployment_result\n\n\n\n================================================"
  },
  {
    "filename": "ec2_metrics.py",
    "path": "fmbench/scripts/ec2_metrics.py",
    "directory": "fmbench/scripts",
    "extension": "py",
    "content": "================================================\n\"\"\"\nCollects and logs GPU and CPU metrics to an EC2 metrics CSV file. This file is populated during the\nduration of the inferences against the model deployed on the EC2 instance.\n\"\"\"\nimport csv\nimport time\nimport psutil\nimport logging\nfrom fmbench.scripts import constants\nfrom nvitop import Device, ResourceMetricCollector\n\n# Setup logging\nlogging.basicConfig(\n    format=\"[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s\",\n    level=logging.INFO,\n)\nlogger = logging.getLogger(__name__)\n\n# Global flag to control data collection for now, change in future.\ncollecting = True\n\n\ndef stop_collect(collector=None):\n    \"\"\"\n    Stops the data collection process by setting the global flag 'collecting' to False.\n    \"\"\"\n    global collecting\n    collecting = False\n    logger.info(\"Stopped collection\")\n\n\ndef _collect_ec2_utilization_metrics():\n    \"\"\"\n    Starts the data collection process by initializing the ResourceMetricCollector and collecting metrics at regular intervals.\n    \"\"\"\n    global collecting\n    logger.info(\"Starting collection\")\n\n    def on_collect(metrics):\n        \"\"\"\n        Collects GPU and CPU metrics, then appends them to the CSV file.\n\n        Parameters:\n        - metrics: The collected metrics from the ResourceMetricCollector.\n\n        Returns:\n        - bool: Returns False if the collection is stopped, otherwise True.\n        \"\"\"\n        if not collecting:\n            return False\n\n        try:\n            # Open the CSV file in append mode and write the collected metrics\n            with open(constants.EC2_SYSTEM_METRICS_FNAME, mode=\"a\", newline=\"\") as csv_file:\n                csv_writer = csv.writer(csv_file)\n\n                # Collect CPU mean utilization\n                cpu_percent_mean = metrics.get(\n                    \"metrics-daemon/host/cpu_percent (%)/mean\", psutil.cpu_percent()\n                )\n                memory_percent_mean = metrics.get(\n                    \"metrics-daemon/host/memory_percent (%)/mean\",\n                    psutil.virtual_memory().percent,\n                )\n                memory_used_mean = metrics.get(\n                    \"metrics-daemon/host/memory_used (GiB)/mean\",\n                    psutil.virtual_memory().available,\n                )\n\n                # Extract the current timestamp\n                timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n                # Initialize variables to sum GPU metrics\n                total_gpu_utilization = 0\n                total_gpu_memory_used = 0\n                total_gpu_memory_free = 0\n                total_gpu_memory_total = 0\n\n                num_gpus = len(Device.cuda.all())\n                # logger.info(f\"Number of GPUs detected: {num_gpus}\")\n\n                # Iterate over all detected GPUs\n                for gpu_id in range(num_gpus):\n                    gpu_utilization_mean = metrics.get(\n                        f\"metrics-daemon/cuda:{gpu_id} (gpu:{gpu_id})/gpu_utilization (%)/mean\",\n                        None,\n                    )\n                    gpu_memory_used_mean = metrics.get(\n                        f\"metrics-daemon/cuda:{gpu_id} (gpu:{gpu_id})/memory_used (MiB)/mean\",\n                        None,\n                    )\n                    gpu_memory_free_mean = metrics.get(\n                        f\"metrics-daemon/cuda:{gpu_id} (gpu:{gpu_id})/memory_free (MiB)/mean\",\n                        None,\n                    )\n                    gpu_memory_total_mean = metrics.get(\n                        f\"metrics-daemon/cuda:{gpu_id} (gpu:{gpu_id})/memory_total (MiB)/mean\",\n                        None,\n                    )\n\n                    if gpu_utilization_mean is not None:\n                        total_gpu_utilization += gpu_utilization_mean\n                    if gpu_memory_used_mean is not None:\n                        total_gpu_memory_used += gpu_memory_used_mean\n                    if gpu_memory_free_mean is not None:\n                        total_gpu_memory_free += gpu_memory_free_mean\n                    if gpu_memory_total_mean is not None:\n                        total_gpu_memory_total += gpu_memory_total_mean\n\n                # Calculate the mean values across all GPUs\n                gpu_utilization_mean_total = (\n                    total_gpu_utilization / num_gpus if num_gpus > 0 else None\n                )\n                gpu_memory_used_mean_total = (\n                    total_gpu_memory_used / num_gpus if num_gpus > 0 else None\n                )\n                gpu_memory_free_mean_total = (\n                    total_gpu_memory_free / num_gpus if num_gpus > 0 else None\n                )\n                gpu_memory_total_mean_total = (\n                    total_gpu_memory_total / num_gpus if num_gpus > 0 else None\n                )\n\n                # Write the row to the CSV file\n                row = [\n                    timestamp,\n                    cpu_percent_mean,\n                    memory_percent_mean,\n                    memory_used_mean,\n                    gpu_utilization_mean_total,\n                    gpu_memory_used_mean_total,\n                    gpu_memory_free_mean_total,\n                    gpu_memory_total_mean_total,\n                ]\n                # logger.info(f\"Writing row: {row}\")\n                csv_writer.writerow(row)\n\n        except ValueError as e:\n            return False\n\n        return True\n\n    # Start the collector and run in the background\n    collector = ResourceMetricCollector(Device.cuda.all())\n    logger.info(\"Starting Daemon collector to run in background\")\n    collector.daemonize(\n        on_collect,\n        interval=constants.EC2_UTILIZATION_METRICS_INTERVAL,\n        on_stop=stop_collect,\n    )\n\n\ndef collect_ec2_metrics():\n    \"\"\"\n    Initializes the CSV file with headers and starts the metrics collection process.\n    \"\"\"\n    global collecting\n    collecting = True\n    # Initialize the CSV file and write the header once\n    with open(constants.EC2_SYSTEM_METRICS_FNAME, mode=\"w\", newline=\"\") as csv_file:\n        csv_writer = csv.writer(csv_file)\n        header = [\n            \"timestamp\",\n            \"cpu_percent_mean\",\n            \"memory_percent_mean\",\n            \"memory_used_mean\",\n            \"gpu_utilization_mean\",\n            \"gpu_memory_used_mean\",\n            \"gpu_memory_free_mean\",\n            \"gpu_memory_total_mean\",\n        ]\n        logger.info(f\"Writing header: {header}\")\n        csv_writer.writerow(header)\n\n    # Call the function to start collecting metrics\n    _collect_ec2_utilization_metrics()\n\n\n\n================================================"
  },
  {
    "filename": "ec2_predictor.py",
    "path": "fmbench/scripts/ec2_predictor.py",
    "directory": "fmbench/scripts",
    "extension": "py",
    "content": "================================================\nimport os\nimport re\nimport json\nimport copy\nimport time\nimport stat\nimport logging\nimport requests\nimport tempfile\nimport subprocess\nimport pandas as pd\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Dict, Optional\nfrom fmbench.scripts import constants\nfrom fmbench.utils import count_tokens\nfrom fmbench.scripts.ec2_metrics import collect_ec2_metrics, stop_collect\nfrom fmbench.scripts.inference_containers.utils import get_accelerator_type\nfrom fmbench.scripts.fmbench_predictor import (FMBenchPredictor,\n                                               FMBenchPredictionResponse)\nfrom fmbench.scripts.inference_containers.utils import (STOP_AND_RM_CONTAINER,\n                                                        FMBENCH_MODEL_CONTAINER_NAME)\n                                            \n# set a logger\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass EC2Predictor(FMBenchPredictor):\n    # overriding abstract method\n    def __init__(self,\n                 endpoint_name: str,\n                 inference_spec: Optional[Dict], \n                 metadata: Optional[Dict]):\n        try:\n            self._endpoint_name: str = endpoint_name\n            self._inference_spec: Dict = inference_spec\n            self._accelerator = get_accelerator_type()\n            # Start collecting EC2 metrics. This will be called once.\n            collect_ec2_metrics()\n        except Exception as e:\n            logger.error(f\"create_predictor, exception occured while creating predictor \"\n                         f\"for endpoint_name={self._endpoint_name}, exception={e}\")\n        logger.info(f\"_endpoint_name={self._endpoint_name}, _inference_spec={self._inference_spec}\")\n\n    def get_prediction(self, payload: Dict) -> FMBenchPredictionResponse:\n        response_json: Optional[Dict] = None\n        response: Optional[str] = None\n        latency: Optional[float] = None\n        TTFT: Optional[float] = None\n        TPOT: Optional[float] = None\n        TTLT: Optional[float] = None\n        prompt_tokens: Optional[int] = None\n        completion_tokens: Optional[int] = None\n        container_type: Optional[str] = None\n        # get the prompt for the EKS endpoint\n        prompt: str = payload['inputs']\n        # represents the number of tokens in the prompt payload\n        prompt_tokens = count_tokens(payload['inputs'])\n        try:            \n            split_input_and_inference_params: Optional[bool] = None\n            if self._inference_spec is not None:\n                split_input_and_inference_params = self._inference_spec.get(\"split_input_and_parameters\")\n                container_type = self._inference_spec.get(\"container_type\", constants.CONTAINER_TYPE_DJL)\n                logger.debug(f\"split input parameters is: {split_input_and_inference_params}, \"\n                             f\"container_type={container_type}\")\n\n            # make a copy of the original payload because we would be making changes\n            # and dont want to change the original variable since it is also used by the calling function\n            payload2 = copy.deepcopy(payload)\n\n            # delete any fields related to evaluations since we dont want to send them to VLLM\n            if 'question' in payload2:\n                del payload2['question']\n            ground_truth = None\n            if 'ground_truth' in payload2:\n                ground_truth = payload2['ground_truth']\n                del payload2['ground_truth']\n                \n            # this is the POST request to the endpoint url for invocations that \n            # is given to you as you deploy a model on EC2 using the DJL serving stack\n            if container_type == constants.CONTAINER_TYPE_DJL:\n                payload2 = payload2 | dict(parameters=self._inference_spec[\"parameters\"])\n                st = time.perf_counter()\n                response = requests.post(self._endpoint_name, json=payload)\n                # record the latency for the response generated\n                latency = time.perf_counter() - st                \n                #logger.info(f\"full_output={response.text}\")\n                response.raise_for_status()\n                \"\"\"\n                the output is of the form\n                {\"generated_text\": \"\\n\\nSuining had a population of 658,798 in 2002.\"}\n                we only need the generated_text field from this\n                \"\"\"\n                full_output = json.loads(response.text).get(\"generated_text\")\n                if full_output is None:\n                    logger.error(f\"failed to extract output from response text, response text = \\\"{response.text}\\\"\")      \n            elif container_type == constants.CONTAINER_TYPE_TRITON:\n                if self._accelerator == constants.ACCELERATOR_TYPE.NEURON:\n                    triton_payload = dict(text_input=payload2[\"inputs\"],\n                                          sampling_parameters=json.dumps(self._inference_spec[\"parameters\"]))\n                else:\n                    triton_payload = dict(text_input=payload2[\"inputs\"]) | self._inference_spec[\"parameters\"]\n\n                logger.info(f\"Endpoint name is: {self._endpoint_name}, triton payload is: {triton_payload}\")\n                st = time.perf_counter()\n                response = requests.post(self._endpoint_name, json=triton_payload)\n                # record the latency for the response generated\n                latency = time.perf_counter() - st\n                response.raise_for_status()\n                response_json = json.loads(response.text)\n                full_output = response_json['text_output']\n            elif container_type == constants.CONTAINER_TYPE_VLLM or container_type == constants.CONTAINER_TYPE_VLLM_GPU:\n                payload2['prompt'] = payload2.pop('inputs')\n                payload2 = payload2 | self._inference_spec[\"parameters\"]\n                st = time.perf_counter()\n                response = requests.post(self._endpoint_name, json=payload2)\n                # record the latency for the response generated\n                latency = time.perf_counter() - st\n                response.raise_for_status()\n                \"\"\"\n                the output is of the form\n                {\n                    \"id\": \"cmpl-e9d590543c374d828d724a228fae2604\",\n                    \"object\": \"text_completion\",\n                    \"created\": 1731734600,\n                    \"model\": \"meta-llama/Llama-3.2-1b-Instruct\",\n                    \"choices\": [\n                        {\n                            \"index\": 0,\n                            \"text\": \"\\n\\nThey are both tennis players.\",\n                            \"logprobs\": null,\n                            \"finish_reason\": \"stop\",\n                            \"stop_reason\": null,\n                            \"prompt_logprobs\": null\n                        }\n                    ],\n                    \"usage\": {\n                        \"prompt_tokens\": 1392,\n                        \"total_tokens\": 1400,\n                        \"completion_tokens\": 8,\n                        \"prompt_tokens_details\": null\n                    }\n                }\n                we only need the response field from this\n                \"\"\"\n                full_output = None\n                choices = json.loads(response.text).get(\"choices\")\n                if choices is not None:\n                    full_output = choices[0].get(\"text\")\n                if full_output is None:\n                    logger.error(f\"failed to extract output from response text, response text = \\\"{response.text}\\\"\")\n            elif container_type == constants.CONTAINER_TYPE_LLAMA_SERVER:                \n                payload2['prompt'] = payload2.pop('inputs')\n                payload2 = payload2 | self._inference_spec[\"parameters\"]\n                logger.info(f\"payload={payload2}\")\n                st = time.perf_counter()\n                response = requests.post(self._endpoint_name, json=payload2)\n                # record the latency for the response generated\n                latency = time.perf_counter() - st\n                response.raise_for_status()\n                \"\"\"\n                the output is of the form\n                {\n                    \"index\": 0,\n                    \"content\": \" Well, this is a question that has been debated among scientists for many years...\",\n                    \"tokens\": [],\n                    \"id_slot\": 1,\n                    \"stop\": true,\n                    \"model\": \"ds\",\n                    \"tokens_predicted\": 600,\n                    \"tokens_evaluated\": 6,\n                    \"generation_settings\": {\n                        ...\n                    },\n                    \"prompt\": \"<\uff5cbegin\u2581of\u2581sentence\uff5c>Is gravity a particle?\",\n                    \"has_new_line\": true,\n                    \"truncated\": false,\n                    \"stop_type\": \"limit\",\n                    \"stopping_word\": \"\",\n                    \"tokens_cached\": 605,\n                    \"timings\": {\n                        \"prompt_n\": 6,\n                        \"prompt_ms\": 201.665,\n                        \"prompt_per_token_ms\": 33.61083333333333,\n                        \"prompt_per_second\": 29.752312002578535,\n                        \"predicted_n\": 600,\n                        \"predicted_ms\": 47212.818,\n                        \"predicted_per_token_ms\": 78.68803,\n                        \"predicted_per_second\": 12.708413211005539\n                    }\n                }\n                we only need the content field from this\n                \"\"\"\n                full_output = json.loads(response.text).get(\"content\")\n                if full_output is None:\n                    logger.error(f\"failed to extract output from response text, response text = \\\"{response.text}\\\"\") \n                else:\n                    logger.info(f\"full_output={full_output}\")\n            elif container_type == constants.CONTAINER_TYPE_OLLAMA:                \n                payload2['prompt'] = payload2.pop('inputs')\n                payload2 = payload2 | self._inference_spec[\"parameters\"]\n                st = time.perf_counter()\n                response = requests.post(self._endpoint_name, json=payload2)\n                # record the latency for the response generated\n                latency = time.perf_counter() - st\n                response.raise_for_status()\n                \"\"\"\n                the output is of the form\n                {\"model\":\"llama3.1:8b\",\n                 \"created_at\":\"2024-10-27T13:44:12.400070826Z\",\n                 \"response\":\"Once upon a time, in a small village nestled\",\n                 \"done\":true,\n                 \"done_reason\":\"length\",\n                 \"context\":[128006,882,128007,271,73457,757,264,3446,128009,128006,78191,128007,271,12805,5304,264,892,11,304,264,2678,14458,89777],\n                 \"total_duration\":195361027,\n                 \"load_duration\":25814720,\n                 \"prompt_eval_count\":14,\n                 \"prompt_eval_duration\":14601000,\n                 \"eval_count\":10,\n                 \"eval_duration\":96146000}\n                we only need the response field from this\n                \"\"\"\n                full_output = json.loads(response.text).get(\"response\")\n                if full_output is None:\n                    logger.error(f\"failed to extract output from response text, response text = \\\"{response.text}\\\"\")\n            elif container_type == constants.CONTAINER_TYPE_SGLANG:\n                user_input = payload2.pop('inputs')\n                payload2 = {\n                    \"model\": \"default\",\n                    \"messages\": [\n                        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                        {\"role\": \"user\", \"content\": user_input}\n                    ]\n                } | self._inference_spec[\"parameters\"]\n\n                st = time.perf_counter()\n                response = requests.post(self._endpoint_name, json=payload2)\n                latency = time.perf_counter() - st\n                response.raise_for_status()\n\n                response_json_raw = response.json()\n                full_output = response_json_raw.get(\"response\")\n                # 'choices' (OpenAI-style)\n                if full_output is None:\n                    choices = response_json_raw.get(\"choices\", [])\n                    if choices:\n                        full_output = choices[0].get(\"message\", {}).get(\"content\", \"\")\n                if not full_output:\n                    logger.error(f\"Failed to extract output. Response: {response.text}\")\n            else:\n                raise ValueError(\"container_type={container_type}, dont know how to handle this\") \n            \n            answer_only = full_output.replace(prompt, \"\", 1) #.strip('[\"]?\\n')\n            response_json = dict(generated_text=answer_only)\n            # counts the completion tokens for the model using the default/user provided tokenizer\n            completion_tokens = count_tokens(response_json.get(\"generated_text\"))\n        except Exception as e:\n            logger.error(f\"get_prediction, exception occurred while getting prediction for payload={payload} \"\n                         f\"from predictor={self._endpoint_name}, response={response}, exception={e}\")\n\n        return FMBenchPredictionResponse(response_json=response_json,\n                                         latency=latency,\n                                         time_to_first_token=TTFT,\n                                         time_per_output_token=TPOT,\n                                         time_to_last_token=TTLT,\n                                         completion_tokens=completion_tokens,\n                                         prompt_tokens=prompt_tokens)\n\n    def shutdown(self) -> None:\n        \"\"\"Represents the function to shutdown the predictor\n           cleanup the endpooint/container/other resources\n        \"\"\"\n        # Stop collecting EC2 metrics either when the model container is stopped and removed, \n        # or once the benchmarking test has completed \n        stop_collect()\n\n        script = f\"\"\"#!/bin/sh\n\n        {STOP_AND_RM_CONTAINER}\n        \"\"\"\n        tmpdir = tempfile.gettempdir()\n        script_file_path = os.path.join(tmpdir, \"shutdown_container.sh\")\n        Path(script_file_path).write_text(script)\n        st = os.stat(script_file_path)\n        os.chmod(script_file_path, st.st_mode | stat.S_IEXEC)\n\n        logger.info(f\"going to run script {script_file_path}\")\n        subprocess.run([\"bash\", script_file_path], check=True)\n        logger.info(f\"done running bash script\")\n        return None\n    \n    @property\n    def endpoint_name(self) -> str:\n        \"\"\"The endpoint name property.\"\"\"\n        return self._endpoint_name\n\n    # The rest ep is deployed on an instance that incurs hourly cost hence, the calculcate cost function\n    # computes the cost of the experiment on an hourly basis. If your instance has a different pricing structure\n    # modify this function.\n    def calculate_cost(self,\n                       instance_type: str,\n                       instance_count: int,\n                       pricing: Dict,\n                       duration: float,\n                       prompt_tokens: int,\n                       completion_tokens: int) -> float:\n        \"\"\"Calculate the cost of each experiment run.\"\"\"\n        experiment_cost: Optional[float] = None\n        try:\n            instance_based_pricing = pricing['pricing']['instance_based']\n            hourly_rate = instance_based_pricing.get(instance_type, None)\n            logger.info(f\"the hourly rate for running on {instance_type} is {hourly_rate}, instance_count={instance_count}\")\n            # calculating the experiment cost for instance based pricing\n            instance_count = instance_count if instance_count else 1\n            experiment_cost = (hourly_rate / 3600) * duration * instance_count\n        except Exception as e:\n            logger.error(f\"exception occurred during experiment cost calculation, exception={e}\")\n        return experiment_cost\n    \n    def get_metrics(self,\n                start_time: datetime,\n                end_time: datetime,\n                period: int = 60) -> pd.DataFrame:\n        \"\"\"\n        Retrieves EC2 system metrics from the CSV file generated during metric collection.\n\n        Args:\n            start_time (datetime): Start time of metrics collection\n            end_time (datetime): End time of metrics collection\n            period (int): Sampling period in seconds (default 60)\n        \n        Returns:\n            pd.DataFrame: DataFrame containing system metrics\n        \"\"\"\n        try:\n            filtered_metrics: Optional[pd.DataFrame] = None\n            # Read the CSV file generated by collect_ec2_metrics()\n            metrics_df = pd.read_csv(constants.EC2_SYSTEM_METRICS_FNAME, parse_dates=['timestamp'])\n            filtered_metrics = metrics_df[(metrics_df['timestamp'] >= start_time) & \n                                            (metrics_df['timestamp'] <= end_time)]\n        except FileNotFoundError:\n            logger.warning(\"Metrics CSV file containin the EC2 metrics not found.\")\n            filtered_metrics = None\n        except Exception as e:\n            logger.error(f\"Error retrieving metrics: {e}\")\n        return filtered_metrics\n\n    @property\n    def inference_parameters(self) -> Dict:\n        \"\"\"The inference parameters property.\"\"\"\n        return self._inference_spec.get(\"parameters\")\n    \n    @property\n    def platform_type(self) -> Dict:\n        \"\"\"The inference parameters property.\"\"\"\n        return constants.PLATFORM_EC2\n\n\ndef create_predictor(endpoint_name: str, inference_spec: Optional[Dict], metadata: Optional[Dict]):\n    return EC2Predictor(endpoint_name, inference_spec, metadata)\n\n\n\n================================================"
  },
  {
    "filename": "eks_deploy.py",
    "path": "fmbench/scripts/eks_deploy.py",
    "directory": "fmbench/scripts",
    "extension": "py",
    "content": "================================================\n# import the required libraries\nimport os\nimport time\nimport json\nimport logging\nimport sagemaker\nimport subprocess\nfrom pathlib import Path\nfrom typing import Dict, Optional\nfrom fmbench.scripts import constants\n\n# session/account specific variables\nsess = sagemaker.session.Session()\n\n# Initialize the platform where this script deploys the model\nPLATFORM: str = constants.PLATFORM_EKS\n\n# Define the location of your s3 prefix for model artifacts\nregion: str =sess._region_name\nHF_TOKEN_FNAME: str = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"hf_token.txt\")\n\n# set a logger\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\ndef _init_eks_checks(eks_cluster_name: str):\n    \"\"\"\n    This function describes the EKS cluster, and updates the kubeconfig for the cluser \n    before deploying the model\n    \"\"\"\n    try:\n        # Describe the EKS Cluster\n        logger.info(\"Describing EKS Cluster...\")\n        describe_command_args = [\"aws\", \"eks\", \"--region\", region, \"describe-cluster\", \"--name\", eks_cluster_name]\n        describe_result = subprocess.run(describe_command_args, capture_output=True, text=True)\n\n        # Check if the cluster exists in the user account\n        if describe_result.returncode != 0:\n            logger.error(\"Error: EKS cluster does not exists. Please run the Terraform script\")\n            return\n        else:\n            logger.info(\"Describe cluster step done, cluster exists!\")\n\n        # Update the kubeconfig before deploying the model\n        logger.info(\"Updating the kubeconfig...\")\n        update_command_args = [\"aws\", \"eks\", \"--region\", region, \"update-kubeconfig\", \"--name\", eks_cluster_name]\n        update_result = subprocess.run(update_command_args, capture_output=True, text=True)\n        logger.info(update_result.stdout)\n        # check if the kubeconfig has been updated\n        if update_result.returncode != 0:\n            logger.error(\"Error: kubeconfg not updated\")\n            return\n        else:\n            logger.info(\"kubeconfig updated\")\n\n        # Check for the nodes available in the cluster\n        logger.info(\"Showing kubectl\")\n        show_nodes_command_args = [\"kubectl\", \"version\"]\n        show_nodes_result = subprocess.run(show_nodes_command_args, capture_output=True, text=True)\n        if show_nodes_result.returncode != 0:\n            logger.error(\"Error: nodes not shown\")\n            return\n        else:\n            logger.info(f\"available kubectl version: {show_nodes_result.stdout}\")\n\n        # show the nodes\n        logger.info(\"Showing nodes...\")\n        show_nodes_command_args = [\"kubectl\", \"get\", \"nodes\"]\n        show_nodes_result = subprocess.run(show_nodes_command_args, capture_output=True, text=True)\n        if show_nodes_result.returncode != 0:\n            logger.error(\"Error: nodes not shown\")\n            return\n        else:\n            logger.info(f\"Nodes shown: {show_nodes_result.stdout}\")\n    except Exception as e:\n        logger.error(f\"Error occurred while updating the kubeconfig: {e}\")\n\n\ndef _deploy_ray(manifest_file_name: str, manifest_dir_path: str):\n    \"\"\"\n    This function deploys the model using ray with a kubectl apply command\n    \"\"\"\n    try:\n        # check the path to the ray file within the configs/eks_manifests directory\n        # manifest_ray_fpath: str = os.path.join(MANIFESTS_FOLDER, manifest_file_name)\n        current_dir: str = os.path.dirname(os.path.realpath(__file__))\n        parent_dir: str = os.path.abspath(os.path.join(current_dir, os.pardir))\n        manifest_dir_path = os.path.join(parent_dir, manifest_dir_path)\n        # Get the absolute path of the manifest file\n        manifest_ray_fpath = os.path.join(manifest_dir_path, manifest_file_name)\n        logger.info(f\"Manifest file absolute path: {manifest_ray_fpath}\")\n        # HF token required for gated model downloads form HF\n        hf_token_file_path = Path(HF_TOKEN_FNAME)\n        if hf_token_file_path.is_file() is True:\n            logger.info(f\"hf_token file path: {hf_token_file_path} is a file\")\n            hf_token = Path(HF_TOKEN_FNAME).read_text().strip()\n        else:\n            logger.info(f\"hf_token file path: {hf_token_file_path} is not a file\")\n        os.environ[\"AWS_REGION\"] = region\n        os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = hf_token\n        # make sure the file is at the path\n        if not os.path.isfile(manifest_ray_fpath):\n            logger.error(\"Error: Ray file not found\")\n            subprocess.run(\"pwd\")\n            return\n        logger.info(f\"Deploying the model using the {manifest_file_name} manifest file\")\n        deploy_result = subprocess.run(f\"envsubst < {manifest_ray_fpath} | kubectl apply -f -\", shell=True, capture_output=True)\n        logger.info(deploy_result.stdout)\n        if deploy_result.returncode != 0:\n            logger.error(f\"Error: Ray not deployed: {deploy_result.stderr}\")\n        else:\n            logger.info(f\"Ray Serve Initiated: {deploy_result.stdout}\")\n    except Exception as e:\n        logger.error(f\"Error occurred while deploying the model: {e}\")\n\n\ndef _check_ray_service_status(eks_model_namespace: str):\n    \"\"\"\n    After the deployment step begins, this function checks the status of the model deployment\n    every 15 seconds for 30 minutes. If the deployment fails, it errors out.\n    \"\"\"\n    try:\n        # Set time limit\n        start_time: float = time.time()\n        # 30 minutes\n        timeout: int = (30 * 60)\n\n        while (time.time() - start_time) < timeout:\n            logger.info(\"Checking if Ray service is deployed...\")\n            # Check the status of the services\n            check_command_args = [\"kubectl\", \"-n\", eks_model_namespace, \"get\", \"services\", \"-o\", \"json\"]\n            check_result = subprocess.run(check_command_args, capture_output=True, text=True)\n            # get the svc count\n            status_json = json.loads(check_result.stdout)\n            status_list = status_json['items']\n            svc_count = len(status_list)\n            # Check if all services are in the \"Active\" state\n            if (svc_count == 3):\n                logger.info(\"Ray service is deployed\")\n                break\n            else:\n                logger.info(\"Ray service is not fully deployed yet, waiting for 15 seconds...\")\n                logger.info(f\"Current # of svc deployed: {svc_count}\")\n                time.sleep(15)\n        else:\n            logger.error(\"Error: Ray service is not deployed within 20 minutes.\")\n    except Exception as e:\n        logger.error(f\"Error occurred while checking the deployment status of the model: {e}\")\n\n\ndef _print_ingress(eks_model_namespace: str, inference_url_format: str) -> str:\n    \"\"\"\n    This function prints the endpoint url string that is returned once the model gets\n    deployed\n    \"\"\"\n    endpoint_url: Optional[str] = None\n    try:\n        get_command_args = [\"kubectl\", \"-n\", eks_model_namespace, \"get\", \"ingress\"]\n        get_endpoint_result = subprocess.run(get_command_args, capture_output=True, text=True)\n        if get_endpoint_result.returncode != 0:\n            logger.error(\"Error: Ingress not printed\")\n\n        else:\n            logger.info(\"Ingress Info:\")\n        # get the ELB ID\n        get_elb_id_command_args = [\"kubectl\", \"get\", \"-n\", eks_model_namespace, \"ingress\", \"-o\", \"jsonpath='{.items[*].status.loadBalancer.ingress[0].hostname}'\"]\n        get_elb_id_result = subprocess.run(get_elb_id_command_args, capture_output=True, text=True)\n        elb_id: str = get_elb_id_result.stdout\n        ep_api_url: str = 'http://' + elb_id[1:-1]\n        # append the inference format to the endpoint url to run inferences against it\n        endpoint_url = ep_api_url + inference_url_format\n        logger.info(f\"Inference URL: {endpoint_url}\")\n    except Exception as e:\n        logger.error(f\"Error occurred while returning the endpoint url: {e}\")\n        endpoint_url = None\n    return endpoint_url\n\n\ndef deploy(experiment_config: Dict, role_arn: str) -> Dict:\n    \"\"\"\n    This is the deploy function to run all of the deployment steps to deploy the \n    model on EKS and return a dictionary containing the endpoint url, experiment name, \n    instance type and instance count\n    \"\"\"\n    eks_endpoint_info: Optional[Dict] = None\n    try:\n        # first update the kubeconfig\n        eks_cluster_name: str = experiment_config['eks']['eks_cluster_name']\n        _init_eks_checks(eks_cluster_name)\n\n        # deploy the model\n        manifest_file_name: str = experiment_config['eks']['manifest_file']\n        manifest_dir_path: str = experiment_config['eks']['manifest_dir']\n        _deploy_ray(manifest_file_name, manifest_dir_path)\n\n        # check the status every 15 seconds during deployment\n        eks_model_namespace: str = experiment_config['eks']['eks_model_namespace']\n        _check_ray_service_status(eks_model_namespace)\n\n        # fetch the endpoint url once the model is deployed\n        inference_url_format: str = experiment_config['inference_spec']['inference_url_format']\n        endpoint_url: str = _print_ingress(eks_model_namespace, inference_url_format)\n        logger.info(f\"Deployed endpoint URL: {endpoint_url}\")\n        eks_endpoint_info = dict(endpoint_name= endpoint_url,\n                                 experiment_name=experiment_config['name'],\n                                 instance_type=experiment_config['instance_type'],\n                                 instance_count=experiment_config['instance_count'], \n                                 deployed=True)\n    except Exception as e:\n        logger.error(f\"Error occured while deploying the model: {e}\")\n        eks_endpoint_info = None\n    return eks_endpoint_info\n\n\n\n\n\n\n\n================================================"
  },
  {
    "filename": "eks_predictor.py",
    "path": "fmbench/scripts/eks_predictor.py",
    "directory": "fmbench/scripts",
    "extension": "py",
    "content": "================================================\nimport os\nimport json\nimport math\nimport time\nimport boto3\nimport logging\nimport requests\nimport datetime\nimport pandas as pd\nfrom datetime import datetime\nfrom fmbench.scripts import constants\nfrom fmbench.utils import count_tokens\nfrom typing import Dict, Optional, List\nfrom fmbench.scripts.fmbench_predictor import (FMBenchPredictor,\n                                               FMBenchPredictionResponse)\n\n# set a logger\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass EKSPredictor(FMBenchPredictor):\n    # overriding abstract method\n    def __init__(self,\n                 endpoint_name: str,\n                 inference_spec: Optional[Dict],\n                 metadata: Optional[Dict]):\n        try:\n            self._endpoint_name: str = endpoint_name\n            self._inference_spec: Dict = inference_spec \n        except Exception as e:\n            logger.error(f\"create_predictor, exception occured while creating predictor \"\n                         f\"for endpoint_name={self._endpoint_name}, exception={e}\")\n        logger.info(f\"_endpoint_name={self._endpoint_name}, _inference_spec={self._inference_spec}\")\n\n    def get_prediction(self, payload: Dict) -> FMBenchPredictionResponse:\n        response_json: Optional[Dict] = None\n        response: Optional[str] = None\n        latency: Optional[float] = None\n        TTFT: Optional[float] = None\n        TPOT: Optional[float] = None\n        TTLT: Optional[float] = None\n        prompt_tokens: Optional[int] = None\n        completion_tokens: Optional[int] = None\n        timeout: Optional[int] = None\n        auth: Optional[Dict] = None\n        # get the prompt for the EKS endpoint\n        prompt: str = payload['inputs']\n        # represents the number of tokens in the prompt payload\n        prompt_tokens = count_tokens(payload[\"inputs\"])\n        try:\n            st = time.perf_counter()\n            split_input_and_inference_params: Optional[bool] = None\n            if self._inference_spec is not None:\n                split_input_and_inference_params = self._inference_spec.get(\"split_input_and_parameters\")\n                logger.info(f\"split input parameters is: {split_input_and_inference_params}\")\n                timeout = self._inference_spec.get(\"timeout\", 180)\n                auth = self._inference_spec.get(\"auth\", None)\n                logger.info(f\"Initializing the timeout to: {timeout}, auth to configured authentication information\")\n                # Use the parameters that the model needs at inference. \n                # parameters: Optional[Dict] = inference_spec.get('parameters')\n\n            # This endpoint only supports the GET method now, you can add support for POST method if your endpoint supports it.\n            # As an example, the following URL is used with a query added at the end of the URL.\n            # http://<NLB_DNS_NAME>/serve/infer?sentence=<prompt_payload>\n            response = requests.get(f\"{self._endpoint_name}/{prompt}\",timeout=timeout) \n\n            latency = time.perf_counter() - st\n            logger.info(response)\n            # the response from the model on ray serve from the url prompt is given in this format. \n            # For other response types, change the logic below and add the response in the `generated_text` key within the response_json dict\n            response.raise_for_status()\n            full_output = response.text\n            answer_only = full_output.replace(prompt, \"\", 1).strip('[\"]?\\n')\n            response_json = dict(generated_text=answer_only)\n            # counts the completion tokens for the model using the default/user provided tokenizer\n            completion_tokens = count_tokens(response_json.get(\"generated_text\"))\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"get_prediction, exception occurred while getting prediction for payload={payload} \"\n                         f\"from predictor={self._endpoint_name}, response={response}, exception={e}\")\n        return FMBenchPredictionResponse(response_json=response_json,\n                                         latency=latency,\n                                         time_to_first_token=TTFT,\n                                         time_per_output_token=TPOT,\n                                         time_to_last_token=TTLT,\n                                         completion_tokens=completion_tokens,\n                                         prompt_tokens=prompt_tokens)    \n\n    @property\n    def endpoint_name(self) -> str:\n        \"\"\"The endpoint name property.\"\"\"\n        return self._endpoint_name\n\n    def get_metrics(self,\n                    start_time: datetime,\n                    end_time: datetime,\n                    period: int = 60) -> pd.DataFrame:\n        # not implemented\n        return None\n\n    # The rest ep is deployed on an instance that incurs hourly cost hence, the calculcate cost function\n    # computes the cost of the experiment on an hourly basis. If your instance has a different pricing structure\n    # modify this function.\n    def calculate_cost(self,\n                       instance_type: str,\n                       instance_count: int,\n                       pricing: Dict,\n                       duration: float,\n                       prompt_tokens: int,\n                       completion_tokens: int) -> float:\n        \"\"\"Calculate the cost of each experiment run.\"\"\"\n        experiment_cost: Optional[float] = None\n        try:\n            instance_based_pricing = pricing['pricing']['instance_based']\n            hourly_rate = instance_based_pricing.get(instance_type, None)\n            logger.info(f\"the hourly rate for running on {instance_type} is {hourly_rate}, instance_count={instance_count}\")\n            # calculating the experiment cost for instance based pricing\n            instance_count = instance_count if instance_count else 1\n            experiment_cost = (hourly_rate / 3600) * duration * instance_count\n        except Exception as e:\n            logger.error(f\"exception occurred during experiment cost calculation, exception={e}\")\n        return experiment_cost\n\n    def shutdown(self) -> None:\n        \"\"\"Represents the function to shutdown the predictor\n           cleanup the endpooint/container/other resources\n        \"\"\"\n        return None\n    \n    @property\n    def inference_parameters(self) -> Dict:\n        \"\"\"The inference parameters property.\"\"\"\n        return self._inference_spec.get(\"parameters\")\n\n    @property\n    def platform_type(self) -> Dict:\n        \"\"\"The inference parameters property.\"\"\"\n        return constants.PLATFORM_EC2\n    \ndef create_predictor(endpoint_name: str, inference_spec: Optional[Dict], metadata: Optional[Dict]):\n    return EKSPredictor(endpoint_name, inference_spec, metadata)\n\n\n\n================================================"
  },
  {
    "filename": "fmbench_predictor.py",
    "path": "fmbench/scripts/fmbench_predictor.py",
    "directory": "fmbench/scripts",
    "extension": "py",
    "content": "================================================\nimport pandas as pd\nfrom datetime import datetime\nfrom typing import Dict, Optional\nfrom abc import ABC, abstractmethod, abstractproperty\n\n\nclass FMBenchPredictor(ABC):\n\n    @abstractmethod\n    def __init__(self,\n                 endpoint_name: str,\n                 inference_spec: Optional[Dict],\n                 metadata: Optional[Dict]):\n        pass\n\n    @abstractmethod\n    def get_prediction(self, payload: Dict) -> Dict:\n        pass\n\n    @abstractmethod\n    def calculate_cost(self,\n                       instance_type: str,\n                       instance_count: int,\n                       config: Dict,\n                       duration: float,\n                       metrics: Dict) -> float:\n        \"\"\"Represents the function to calculate the\n           cost of each experiment run.\n        \"\"\"\n        pass\n    \n    @abstractmethod\n    def get_metrics(self,\n                    start_time: datetime,\n                    end_time: datetime,\n                    period: int = 60) -> pd.DataFrame:\n        \"\"\"Represents the function to calculate the\n           metrics for each endpoint\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def shutdown(self) -> None:\n        \"\"\"Represents the function to shutdown the predictor\n           cleanup the endpooint/container/other resources\n        \"\"\"\n        return None\n\n    @abstractproperty\n    def endpoint_name(self) -> str:\n        \"\"\"The endpoint name property.\"\"\"\n        pass\n\n    @abstractproperty\n    def inference_parameters(self) -> Dict:\n        \"\"\"The inference parameters property.\"\"\"\n        pass\n\n    @abstractproperty\n    def platform_type(self) -> Dict:\n        \"\"\"The inference parameters property.\"\"\"\n        pass\n\n\nclass FMBenchPredictionResponse(dict):\n\n    def __init__(self, *k, **kwargs):\n        self.__dict__ = self\n        self.__dict__['response_json'] = kwargs['response_json']\n        self.__dict__['latency'] = kwargs['latency']\n        self.__dict__['prompt_tokens'] = kwargs['prompt_tokens']\n        self.__dict__['completion_tokens'] = kwargs['completion_tokens']\n        super().__init__(*k, **kwargs)\n\n\n\n================================================"
  },
  {
    "filename": "jumpstart.py",
    "path": "fmbench/scripts/jumpstart.py",
    "directory": "fmbench/scripts",
    "extension": "py",
    "content": "================================================\nimport time\nimport fmbench\nfrom typing import Dict\nfrom fmbench.scripts import constants\nfrom sagemaker.predictor import Predictor\nfrom sagemaker.jumpstart.model import JumpStartModel\n\n\n# Initialize the platform where this script deploys the model\nPLATFORM: str = constants.PLATFORM_SAGEMAKER\n\ntag = [\n    {\n        'Key': 'fmbench-version',\n        'Value': fmbench.__version__\n    }\n]\n\n\ndef deploy(experiment_config: Dict, role_arn: str) -> Dict:\n    model = JumpStartModel(\n            model_id=experiment_config['model_id'],\n            model_version=experiment_config['model_version'],\n            image_uri=experiment_config['image_uri'],\n            env=experiment_config['env'],\n            role=role_arn,\n            instance_type=experiment_config['instance_type']\n        )\n\n    sec, us = str(time.time()).split(\".\")\n    ep_name = f\"{experiment_config['ep_name']}-{sec}-{us}\"\n    accept_eula = experiment_config.get('accept_eula')\n    if accept_eula is not None:\n        predictor = model.deploy(initial_instance_count=experiment_config['instance_count'],\n                                 accept_eula=accept_eula,\n                                 endpoint_name=ep_name,\n                                 tags=tag)\n    else:\n        predictor = model.deploy(initial_instance_count=experiment_config['instance_count'],\n                                 endpoint_name=ep_name,\n                                 tags=tag)\n\n    return dict(endpoint_name=predictor.endpoint_name, \n                experiment_name=experiment_config['name'], \n                instance_type=experiment_config['instance_type'], \n                instance_count=experiment_config['instance_count'], \n                deployed=True)\n\n\n\n================================================"
  },
  {
    "filename": "neuron_deploy.py",
    "path": "fmbench/scripts/neuron_deploy.py",
    "directory": "fmbench/scripts",
    "extension": "py",
    "content": "================================================\n\"\"\"\nDeploys a model on AWS silicon\n\n1. Configuration is read from the configured serving.properties file.\n2. A hf_token.txt file is required to download the model from Hugging Face.\n\"\"\"\n\n# Import necessary libraries\nimport os\nimport sys\nimport stat\nimport logging\nimport subprocess\nfrom typing import Dict\nfrom pathlib import Path\nfrom fmbench.scripts import constants\n\n# Set up a logger\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Initialize the platform where this script deploys the model\nPLATFORM: str = constants.PLATFORM_SAGEMAKER\n\n# Global constant for the Hugging Face token file\nSCRIPT_DIRECTORY: str = os.path.dirname(os.path.realpath(__file__))\nHF_TOKEN_FNAME: str = os.path.join(SCRIPT_DIRECTORY, \"hf_token.txt\")\nneuron_script_dir: str = os.path.join(SCRIPT_DIRECTORY, \"compile-llm-for-aws-silicon\")\nshell_script_path: str = os.path.join(neuron_script_dir, \"scripts/download_compile_deploy.sh\")\n\n\ndef make_executable(file_path: str) -> None:\n    \"\"\"Make the script executable if it is not already.\"\"\"\n    st = os.stat(file_path)\n    os.chmod(file_path, st.st_mode | stat.S_IEXEC)\n    logger.info(\"Made script executable: %s\", file_path)\n\ndef deploy(experiment_config: Dict, role_arn: str) -> Dict:\n    \"\"\"\n    Function to deploy the model and create the endpoint\n    \"\"\"\n    logger.info(\"Inside neuron deploy function\")\n\n    # Ensure the script is executable\n    make_executable(shell_script_path)\n    \n    requirements_file_path = os.path.join(neuron_script_dir, \"requirements.txt\")\n    command = [sys.executable, '-m', 'pip', 'install', '-r', requirements_file_path]\n\n    try:\n        subprocess.check_call(command)\n        print(\"Requirements installed successfully.\")\n    except subprocess.CalledProcessError as e:\n        print(f\"Error installing requirements: {e}\")\n\n    try:\n        model_id = experiment_config['model_id']\n        region = experiment_config['region']\n        ml_instance_type = experiment_config['instance_type']\n        batch_size = experiment_config['ec2']['batch_size']\n        image_uri = experiment_config['image_uri']\n        num_neuron_cores = experiment_config['ec2']['num_neuron_cores']\n        neuron_version = experiment_config['ec2']['neuron_version']\n        model_loading_timeout = experiment_config['ec2']['model_loading_timeout']\n        prefix = experiment_config['prefix']\n        serving_properties = experiment_config['serving.properties']\n        s3_bucket = experiment_config['bucket']\n        role = experiment_config['sagemaker_execution_role']\n        instance_count = experiment_config['ec2']['instance_count']\n\n        logger.info(\"Model ID: %s\", model_id)\n        logger.info(\"Region: %s\", region)\n        logger.info(\"Instance Type: %s\", ml_instance_type)\n        logger.info(\"Batch Size: %s\", batch_size)\n        logger.info(\"Number of Neuron Cores: %s\", num_neuron_cores)\n        logger.info(\"Neuron Version: %s\", neuron_version)\n        logger.info(\"S3 Bucket: %s\", s3_bucket)\n        logger.info(\"Role ARN: %s\", role)\n        logger.info(\"Script Path: %s\", neuron_script_dir)\n        logger.info(\"Model Loading Timeout: %s\", model_loading_timeout)\n        logger.info(\"Initial Instance Count: %s\", instance_count)\n        hf_token_file_path = Path(HF_TOKEN_FNAME)\n        if hf_token_file_path.is_file() is True:\n            logger.info(f\"hf_token file path: {hf_token_file_path} is a file\")\n            HF_TOKEN = Path(HF_TOKEN_FNAME).read_text().strip()\n        else:\n            logger.info(f\"hf_token file path: {hf_token_file_path} is not a file\")\n        logger.info(\"HF Token is: %s\", HF_TOKEN)\n\n    except KeyError as e:\n        logger.error(\"Missing key in experiment_config: %s\", e)\n        raise\n    except FileNotFoundError as e:\n        logger.error(\"File not found: %s\", e)\n        raise\n    except Exception as e:\n        logger.error(\"Error reading configuration: %s\", e)\n        raise\n\n    logger.info(f\"Image uri that is being used: {image_uri}\")\n    command = [\n        shell_script_path,\n        HF_TOKEN,\n        model_id,\n        neuron_version,\n        \"model_store\",\n        s3_bucket,\n        prefix,\n        region,\n        role,\n        batch_size,\n        num_neuron_cores,\n        ml_instance_type,\n        model_loading_timeout,\n        serving_properties,\n        neuron_script_dir,\n        instance_count, \n        image_uri\n    ]\n    \n    logger.info(\"Constructed command: %s\", command)\n\n    deployment_result = {\n        'endpoint_name': None,\n        'experiment_name': None,\n        'instance_type': None,\n        'instance_count': None,\n        'deployed': False\n    }\n\n    try:\n        with open('scripts.log', 'a') as log_file:\n            process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, bufsize=1, universal_newlines=True)\n    \n            for line in process.stdout:\n                log_file.write(f\"Standard Output: {line}\")\n                log_file.flush()\n                logger.info(\"Script output: %s\", line.strip())\n\n            for line in process.stderr:\n                log_file.write(f\"Standard Error: {line}\")\n                log_file.flush()\n                logger.error(\"Script error: %s\", line.strip())\n            return_code = process.wait()\n            \n            if return_code == 0:\n                logger.info(\"Script executed successfully\")\n            else:\n                logger.error(\"Script failed with return code: %d\", return_code)\n            \n            logger.info(\"Going to read endpoint from endpoint.txt\")\n            try:\n                # Read the endpoint name from the endpoint.txt file\n                endpoint_file_path = os.path.join(neuron_script_dir, \"endpoint.txt\")\n                ep_name = Path(endpoint_file_path).read_text().strip()\n                logger.info(\"Endpoint is: %s\", ep_name)\n                if ep_name:\n                    logger.info(\"Model endpoint running with name: %s\", ep_name)\n                    deployment_result.update({\n                        'endpoint_name': ep_name,\n                        'experiment_name': experiment_config['name'],\n                        'instance_type': experiment_config['instance_type'],\n                        'instance_count': experiment_config['instance_count'],\n                        'deployed': True\n                    })\n                    logger.info(\"Deployment result: %s\", deployment_result)\n                else:\n                    logger.error(\"Model endpoint not running!\")\n            except FileNotFoundError:\n                logger.error(\"File endpoint.txt not found!\")\n            except Exception as e:\n                logger.error(\"Error reading endpoint.txt: %s\", str(e))\n                raise\n\n    except subprocess.CalledProcessError as e:\n        with open('scripts.log', 'a') as log_file:\n            log_file.write(\"Standard Error:\\n\")\n            log_file.write(e.stderr)\n        \n        logger.error(\"Error executing script: %s\", e.stderr)\n        raise\n    except Exception as e:\n        logger.error(\"Unexpected error: %s\", str(e))\n        raise\n    logger.info(\"Deployment result: %s\", deployment_result)\n    return deployment_result\n\n\n\n================================================"
  },
  {
    "filename": "neuron_metrics.py",
    "path": "fmbench/scripts/neuron_metrics.py",
    "directory": "fmbench/scripts",
    "extension": "py",
    "content": "================================================\nimport json\nimport time\nimport logging\nimport subprocess\nimport pandas as pd\nfrom threading import Thread\n\n# Setup logging\nlogging.basicConfig(\n    format=\"[%(asctime)s] %(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s\",\n    level=logging.INFO,\n)\nlogger = logging.getLogger(__name__)\n\n# Global variables to manage state\ndata_collection = []\nstop_collecting = False\ncollection_thread = None\n\n\ndef _collect_data(shell_command):\n    global data_collection, stop_collecting\n    logger.info(f\"Starting data collection using command: {shell_command}\")\n\n    with subprocess.Popen(\n        shell_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n    ) as proc:\n        try:\n            while not stop_collecting:\n                output = proc.stdout.readline()\n                if output == b\"\" and proc.poll() is not None:\n                    break\n                if output:\n                    json_data = json.loads(output.decode(\"utf-8\"))\n\n                    # Extract system data\n                    system_data = json_data.get(\"system_data\", {})\n                    vcpu_usage = system_data.get(\"vcpu_usage\", {}).get(\n                        \"average_usage\", {}\n                    )\n                    memory_info = system_data.get(\"memory_info\", {})\n\n                    memory_total_gb = memory_info.get(\"memory_total_bytes\", 0) / (\n                        1024**3\n                    )\n                    memory_used_gb = memory_info.get(\"memory_used_bytes\", 0) / (1024**3)\n                    swap_total_gb = memory_info.get(\"swap_total_bytes\", 0) / (1024**3)\n                    swap_used_gb = memory_info.get(\"swap_used_bytes\", 0) / (1024**3)\n\n                    # Extract neuron_runtime_data\n                    neuron_runtime_data = json_data.get(\"neuron_runtime_data\", [])\n                    for runtime_entry in neuron_runtime_data:\n                        report = runtime_entry.get(\"report\", {})\n                        neuroncore_counters = report.get(\"neuroncore_counters\", {})\n                        neuron_cores_in_use = neuroncore_counters.get(\n                            \"neuroncores_in_use\", {}\n                        )\n\n                        neuron_utilization = {}\n                        total_utilization = 0\n                        neuroncore_count = 0\n\n                        # Calculate the total utilization and count the number of NeuronCores\n                        for core_index, core_data in neuron_cores_in_use.items():\n                            utilization = core_data.get(\"neuroncore_utilization\", 0)\n                            neuron_utilization[\n                                f\"neuroncore_{core_index}_utilization\"\n                            ] = utilization\n                            neuron_utilization[f\"neuroncore_{core_index}_flops\"] = (\n                                core_data.get(\"flops\", None)\n                            )\n\n                            if utilization is not None:\n                                total_utilization += utilization\n                                neuroncore_count += 1\n\n                        # Calculate mean utilization as a percentage of total possible utilization\n                        if neuroncore_count > 0:\n                            mean_utilization = total_utilization / neuroncore_count\n                        else:\n                            mean_utilization = 0\n\n                        flattened_data = {\n                            \"vcpu_user\": vcpu_usage.get(\"user\"),\n                            \"vcpu_system\": vcpu_usage.get(\"system\"),\n                            \"vcpu_idle\": vcpu_usage.get(\"idle\"),\n                            \"memory_total_gb\": memory_total_gb,\n                            \"memory_used_gb\": memory_used_gb,\n                            \"swap_total_gb\": swap_total_gb,\n                            \"swap_used_gb\": swap_used_gb,\n                            \"neuroncore_count\": neuroncore_count,\n                            \"total_neuroncore_utilization\": total_utilization,\n                            \"mean_neuroncore_utilization_percentage\": mean_utilization,\n                            \"mean_neuroncore_utilization_total_possible\": mean_utilization\n                            * neuroncore_count,  # out of 100% * number of cores\n                        }\n\n                        # Merge neuron utilization data into the flattened_data\n                        flattened_data.update(neuron_utilization)\n\n                        # Append the flattened data to the collection\n                        data_collection.append(flattened_data)\n        except Exception as e:\n            logger.error(f\"Error occurred during data collection: {e}\")\n        finally:\n            proc.terminate()\n            logger.info(\"Data collection process terminated.\")\n\n\ndef start_collection(shell_command):\n    \"\"\"Starts the data collection in a separate thread.\"\"\"\n    global stop_collecting, collection_thread\n    stop_collecting = False\n    collection_thread = Thread(target=_collect_data, args=(shell_command,))\n    collection_thread.start()\n    logger.info(\"Data collection started in the background.\")\n\n\ndef stop_collection():\n    \"\"\"Stops the data collection.\"\"\"\n    global stop_collecting, collection_thread\n    stop_collecting = True\n    if collection_thread:\n        collection_thread.join()\n    logger.info(\"Data collection stopped.\")\n\n\ndef get_collected_data():\n    \"\"\"Returns the collected data as a Pandas DataFrame.\"\"\"\n    global data_collection\n    logger.info(f\"Returning collected data with {len(data_collection)} records.\")\n    return pd.DataFrame(data_collection)\n\n\ndef reset_collection():\n    \"\"\"Resets the collected data and state.\"\"\"\n    global data_collection, stop_collecting, collection_thread\n    data_collection = []\n    stop_collecting = False\n    collection_thread = None\n    logger.info(\"Data collection has been reset.\")\n\n\n\n================================================"
  },
  {
    "filename": "ollama_predictor.py",
    "path": "fmbench/scripts/ollama_predictor.py",
    "directory": "fmbench/scripts",
    "extension": "py",
    "content": "================================================\nimport os\nimport json\nimport copy\nimport time\nimport stat\nimport logging\nimport requests\nimport tempfile\nimport subprocess\nimport pandas as pd\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Dict, Optional\nfrom fmbench.scripts import constants\nfrom fmbench.utils import count_tokens\nfrom fmbench.scripts.inference_containers.utils import get_accelerator_type\nfrom fmbench.scripts.fmbench_predictor import (FMBenchPredictor,\n                                               FMBenchPredictionResponse)\nfrom fmbench.scripts.inference_containers.utils import (STOP_AND_RM_CONTAINER,\n                                                        FMBENCH_MODEL_CONTAINER_NAME)\n                                            \n# set a logger\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass ollamaPredictor(FMBenchPredictor):\n    # overriding abstract method\n    def __init__(self,\n                 endpoint_name: str,\n                 inference_spec: Optional[Dict], \n                 metadata: Optional[Dict]):\n        try:\n            self._endpoint_name: str = endpoint_name\n            self._inference_spec: Dict = inference_spec\n            self._accelerator = get_accelerator_type()  # Not needed for Ollama, but keeping in case it's useful\n        except Exception as e:\n            logger.error(f\"create_predictor, exception occurred while creating predictor \"\n                         f\"for endpoint_name={self._endpoint_name}, exception={e}\")\n        logger.info(f\"_endpoint_name={self._endpoint_name}, _inference_spec={self._inference_spec}\")\n\n    def get_prediction(self, payload: Dict) -> FMBenchPredictionResponse:\n        response_json: Optional[Dict] = None\n        response: Optional[str] = None\n        latency: Optional[float] = None\n        TTFT: Optional[float] = None\n        TPOT: Optional[float] = None\n        TTLT: Optional[float] = None\n        prompt_tokens: Optional[int] = None\n        completion_tokens: Optional[int] = None\n\n        # Get the prompt for the Ollama endpoint\n        prompt: str = payload['inputs']\n        # Calculate the number of tokens in the prompt\n        prompt_tokens = count_tokens(payload['inputs'])\n\n        try:\n            # Adjust the payload for Ollama\n            payload2 = copy.deepcopy(payload)\n            payload2['prompt'] = payload2.pop('inputs')  # Replace 'inputs' with 'prompt'\n            payload2 = payload2 | self._inference_spec.get(\"parameters\", {})  # Add inference parameters if present\n\n            # Send POST request to the Ollama endpoint\n            st = time.perf_counter()  # Start latency timer\n            response = requests.post(self._endpoint_name, json=payload2)\n            latency = time.perf_counter() - st  # Record the latency\n\n            # Raise exception if the response failed\n            response.raise_for_status()\n            full_output = response.text\n\n            # Extract the generated response (removing the original prompt if necessary)\n            answer_only = full_output.replace(prompt, \"\", 1).strip('[\"]?\\n')\n            response_json = dict(generated_text=answer_only)\n\n            # Count the completion tokens for the generated text\n            completion_tokens = count_tokens(response_json.get(\"generated_text\"))\n\n        except Exception as e:\n            logger.error(f\"get_prediction, exception occurred while getting prediction for payload={payload} \"\n                         f\"from predictor={self._endpoint_name}, response={response}, exception={e}\")\n\n        return FMBenchPredictionResponse(response_json=response_json,\n                                         latency=latency,\n                                         time_to_first_token=TTFT,\n                                         time_per_output_token=TPOT,\n                                         time_to_last_token=TTLT,\n                                         completion_tokens=completion_tokens,\n                                         prompt_tokens=prompt_tokens)\n    \n    \n    def shutdown(self) -> None:\n        \"\"\"Represents the function to shutdown the predictor\n           cleanup the endpooint/container/other resources\n        \"\"\"\n        script = f\"\"\"#!/bin/sh\n\n        {STOP_AND_RM_CONTAINER}\n        \"\"\"\n        tmpdir = tempfile.gettempdir()\n        script_file_path = os.path.join(tmpdir, \"shutdown_container.sh\")\n        Path(script_file_path).write_text(script)\n        st = os.stat(script_file_path)\n        os.chmod(script_file_path, st.st_mode | stat.S_IEXEC)\n\n        logger.info(f\"going to run script {script_file_path}\")\n        subprocess.run([\"bash\", script_file_path], check=True)\n        logger.info(f\"done running bash script\")\n        return None\n    \n    @property\n    def endpoint_name(self) -> str:\n        \"\"\"The endpoint name property.\"\"\"\n        return self._endpoint_name\n\n    # The rest ep is deployed on an instance that incurs hourly cost hence, the calculcate cost function\n    # computes the cost of the experiment on an hourly basis. If your instance has a different pricing structure\n    # modify this function.\n    def calculate_cost(self,\n                    instance_type: str,\n                    instance_count: int,\n                    pricing: Dict,\n                    duration: float,\n                    prompt_tokens: int,\n                    completion_tokens: int) -> float:\n        \"\"\"Calculate the cost of each experiment run.\"\"\"\n        experiment_cost: Optional[float] = None\n        try:\n            instance_based_pricing = pricing['pricing']['instance_based']\n            hourly_rate = instance_based_pricing.get(instance_type, None)\n            logger.info(f\"the hourly rate for running on {instance_type} is {hourly_rate}, instance_count={instance_count}\")\n            # calculating the experiment cost for instance based pricing\n            instance_count = instance_count if instance_count else 1\n            experiment_cost = (hourly_rate / 3600) * duration * instance_count\n        except Exception as e:\n            logger.error(f\"exception occurred during experiment cost calculation, exception={e}\")\n        return experiment_cost\n    \n    def get_metrics(self,\n                    start_time: datetime,\n                    end_time: datetime,\n                    period: int = 60) -> pd.DataFrame:\n        # not implemented\n        return None\n\n    @property\n    def inference_parameters(self) -> Dict:\n        \"\"\"The inference parameters property.\"\"\"\n        return self._inference_spec.get(\"parameters\")\n    \n    @property\n    def platform_type(self) -> Dict:\n        \"\"\"The inference parameters property.\"\"\"\n        return constants.PLATFORM_EC2\n\n\ndef create_predictor(endpoint_name: str, inference_spec: Optional[Dict], metadata: Optional[Dict]):\n    return ollamaPredictor(endpoint_name, inference_spec, metadata)\n\n\n\n================================================"
  },
  {
    "filename": "prepare_for_multi_model_containers.py",
    "path": "fmbench/scripts/prepare_for_multi_model_containers.py",
    "directory": "fmbench/scripts",
    "extension": "py",
    "content": "================================================\n# script that prepares a directory structure to run multiple copies of a model\n# using the DJL container. This code is written for Neuron for now but would be modified\n# to run on GPUs as well and then maybe other inference containers as well.\n# It creates a docker_compose.yml file that creates multiple containers and a load balancer.\n# which provides a single endpoint to external applications that want to use these containers.\n\nimport os\nimport json\nimport yaml\nimport shutil\nimport docker\nimport logging\nimport subprocess\nfrom os import listdir\nfrom pathlib import Path\nfrom os.path import isfile, join\nimport fmbench.scripts.constants as constants\nfrom typing import Dict, List, Optional, Tuple, Union\nimport fmbench.scripts.inference_containers.djl as djl\nimport fmbench.scripts.inference_containers.vllm as vllm\nimport fmbench.scripts.inference_containers.triton as triton\nimport fmbench.scripts.inference_containers.utils as ic_utils\n\nlogging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef _create_config_files(model_id: str, \n                        num_model_copies: int, \n                        devices_per_model: int, \n                        image: str, \n                        tp_degree: int,\n                        batch_size: int,\n                        user: str, \n                        shm_size: str, \n                        env: List, \n                        accelerator: constants.ACCELERATOR_TYPE,\n                        dir_path: str,\n                        listen_port: int, \n                        backend: constants.BACKEND) -> Tuple:\n    \"\"\"\n    Creates the docker compose yml file, nginx config file, these are common to all containers\n    and then creates individual config.properties files\n\n    version: '3.8'\n\n    services:\n      fmbench_model_container_1:\n        image: 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n        # deepjavalibrary/djl-serving:0.29.0-pytorch-inf2 #\n        container_name: fmbench_model_container_1\n        user: djl\n        shm_size: 12GB\n        devices:\n        - \"/dev/neuron0:/dev/neuron0\"\n        - \"/dev/neuron1:/dev/neuron1\"\n        - \"/dev/neuron2:/dev/neuron2\"\n        - \"/dev/neuron3:/dev/neuron3\"\n        - \"/dev/neuron4:/dev/neuron4\"\n        - \"/dev/neuron5:/dev/neuron5\"\n        environment:\n        - MODEL_LOADING_TIMEOUT=2400\n        - HF_TOKEN=<hf_token>\n        volumes:\n        - /home/ubuntu/Mistral-7B-Instruct-v0.2-i1:/opt/ml/model:ro\n        - /home/ubuntu/Mistral-7B-Instruct-v0.2-i1/conf:/opt/djl/conf:ro\n        - /home/ubuntu/Mistral-7B-Instruct-v0.2-i1/model_server_logs:/opt/djl/logs\n        ports:\n        - \"8081:8081\"\n        deploy:\n        restart_policy:\n            condition: on-failure\n\n      fmbench_model_container_2:\n        image: 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1    \n        container_name: fmbench_model_container_2\n        user: djl\n        shm_size: 12GB\n        devices:\n        - \"/dev/neuron6:/dev/neuron0\"\n        - \"/dev/neuron7:/dev/neuron1\"\n        - \"/dev/neuron8:/dev/neuron2\"\n        - \"/dev/neuron9:/dev/neuron3\"\n        - \"/dev/neuron10:/dev/neuron4\"\n        - \"/dev/neuron11:/dev/neuron5\"\n        environment:\n        - MODEL_LOADING_TIMEOUT=2400\n        - HF_TOKEN=<hf_token>\n        volumes:\n        - /home/ubuntu/Mistral-7B-Instruct-v0.2-i2:/opt/ml/model:ro\n        - /home/ubuntu/Mistral-7B-Instruct-v0.2-i2/conf:/opt/djl/conf:ro\n        - /home/ubuntu/Mistral-7B-Instruct-v0.2-i2/model_server_logs:/opt/djl/logs\n        ports:\n        - \"8082:8082\"\n        deploy:\n        restart_policy:\n            condition: on-failure\n\n      loadbalancer:\n        image: nginx:alpine\n        container_name: fmbench_model_container_loadbalancer\n        ports:\n        - \"8080:80\"\n        volumes:\n        - ./nginx.conf:/etc/nginx/nginx.conf:ro\n        depends_on:\n        - fmbench_model_container_1\n        - fmbench_model_container_2\n        deploy:\n        placement:\n            constraints: [node.role == manager]\n    \"\"\"\n\n    # do we need to create a load balancer?\n    # if the model copies or the model containers are > 1\n    # then yes..note that we could have a situation where multiple\n    # model servers are created within a single container so go with\n    # model copies as the guide rather than containers\n    lb_needed = True if num_model_copies > 1 else False\n    try:\n        if lb_needed is False:\n            base_port = listen_port\n            logger.info(f\"lb_needed={lb_needed}, going to make the container listen on {listen_port} instead of \"\n                        f\"{base_port}\")\n        else:\n            base_port = constants.BASE_PORT_FOR_CONTAINERS\n        if user == constants.CONTAINER_TYPE_TRITON:            \n            logger.info(f\"container type is {constants.CONTAINER_TYPE_TRITON}, accelerator={accelerator}. \"\n                        f\"preparing the service for docker-compose\")\n            services, per_container_info_list, nginx_command = triton.create_triton_service(model_id, \n                                                                    num_model_copies, \n                                                                    devices_per_model, \n                                                                    image, \n                                                                    user, \n                                                                    shm_size,\n                                                                    env, \n                                                                    base_port,\n                                                                    accelerator,\n                                                                    tp_degree,\n                                                                    batch_size, \n                                                                    backend)\n        else:\n            logger.info(f\"Container type is {user}. Preparing the service for docker-compose\")\n            services, per_container_info_list, nginx_command = djl.create_djl_service(model_id, \n                                                                    num_model_copies, \n                                                                    devices_per_model, \n                                                                    image, \n                                                                    user, \n                                                                    shm_size, \n                                                                    env, \n                                                                    base_port,\n                                                                    accelerator)\n        # Load balancer setup\n        if lb_needed is True:\n            fmbench_container_names = [ci['container_name'] for ci in per_container_info_list]\n            # if this is a triton container and \n            lb = {\n                \"image\": \"nginx:alpine\",\n                \"container_name\": \"fmbench_model_container_load_balancer\",\n                \"command\": nginx_command,\n                \"restart\": \"on-failure\",\n                \"ports\": [f\"{listen_port}:80\"],\n                \"volumes\": [\"./nginx.conf:/etc/nginx/nginx.conf:ro\"],\n                \"depends_on\": fmbench_container_names,\n                \"deploy\": {\"placement\": {\"constraints\": ['node.role == manager']}}\n            }\n            if nginx_command is None:\n                lb.pop(\"command\")\n            services.update({\"loadbalancer\": lb})\n        else:\n            logger.info(f\"not creating load balancer\")\n        docker_compose = {\"services\": services}\n\n        # nginx.conf generation\n        if lb_needed is True:\n            # the container info list contains the nginx.conf \"server fmbench_model_Container_1:8000\" type lines\n            # as well ..but..not all inference containers are the same..for DJL we would have one model server\n            # per container (or even if we have multiple they listen on the same port (?) so we dont have to care\n            # about it in the nginx ln) but in Triton the model servers listen on different ports and so even if\n            # there is a single triton container it could have say 4 model servers on different ports..all this \n            # to say that the nginx_conf_lines is a list and so we have a list of list situation happening here and \n            # we need to flatten this list of list\n            \n            nginx_server_lines = [l for ci in per_container_info_list for l in ci['nginx_server_lines']]\n            # make it into a string\n            nginx_server_lines = \"\\n\".join(nginx_server_lines)\n\n            nginx_config = f\"\"\"\n### Nginx Load Balancer\nevents {{}}\nhttp {{\n    upstream fmcluster {{\n{nginx_server_lines}\n    }}\n    server {{\n        listen 80;\n        location / {{\n            proxy_pass http://fmcluster;\n        }}\n    }}\n}}\n\"\"\"\n        else:\n            nginx_config = None\n    except Exception as e:\n        logger.error(f\"Error occurred while generating config files: {e}\")\n        docker_compose, nginx_config, per_container_info_list = None, None, None\n    return docker_compose, nginx_config, per_container_info_list\n\n\ndef prepare_docker_compose_yml(model_id: str,\n                               model_copies: str,\n                               inference_params: Dict,\n                               image: str,\n                               user: str,\n                               shm_size: str,\n                               env: Dict,\n                               serving_properties: Optional[str],\n                               dir_path: str,\n                               listen_port: int) -> int:\n\n    # convert the env dict to a list of k=v pairs\n    env_as_list = []\n    if env is not None:\n        for k,v in env.items():\n            env_as_list.append(f\"{k}={v}\")\n\n    # this is specific to the triton container\n    # then default it to None and let the inference\n    # container the best default values for parameters\n    container_params: Optional[Dict] = inference_params.get('container_params', dict())\n    logger.info(f\"container_params that will be used for {image} image: {container_params}\")  \n\n    # the following code handles backwards compatibility where batch_Size and tp_degree are now specified\n    # as part of container params but if not found then it falls back to the original scheme where\n    # they were specified as part of the inference params\n    batch_size: int = container_params.get('batch_size', inference_params.get('batch_size', 4))\n    tp_degree: int = container_params.get('tp_degree', inference_params.get('tp_degree', 1))\n\n    backend: Optional[str] = inference_params.get('backend')\n    triton_content: Optional[str] = None\n    if backend is not None:\n        if backend == constants.BACKEND.VLLM_BACKEND:\n            triton_content = constants.TRITON_CONTENT_DIR_NAME_VLLM\n        elif backend == constants.BACKEND.DJL_BACKEND:\n            triton_content = constants.TRITON_CONTENT_DIR_NAME_DJL\n        else:\n            logger.error(f\"No backend={backend} provided\")\n    else:\n        logger.error(f\"No backend={backend} provided\")\n\n    # first check if this is an NVIDIA instance or a AWS Chips instance    \n    accelerator = ic_utils.get_accelerator_type()\n\n    if accelerator == constants.ACCELERATOR_TYPE.NVIDIA:\n        model_copies_as_int, devices_per_model = ic_utils.get_model_copies_to_start_nvidia(tp_degree, model_copies)\n    else:\n        model_copies_as_int, devices_per_model = ic_utils.get_model_copies_to_start_neuron(tp_degree, model_copies)\n    \n    # special handling for Triton && Neuron\n    if user == constants.CONTAINER_TYPE_TRITON and accelerator == constants.ACCELERATOR_TYPE.NEURON:\n        logger.info(f\"running Triton container on neuron, creating specific config files for this combination\")\n        current_dir: str = os.path.dirname(os.path.realpath(__file__))\n        triton_content: str = os.path.join(current_dir, triton_content)\n        # handles custom tensor pd, batch size into the model repository files\n        triton.handle_triton_serving_properties_and_inf_params(triton_content, \n                                                               tp_degree, \n                                                               container_params,\n                                                               model_id,\n                                                               backend)\n\n    # create the docker compose and nginx.conf file in the top level\n    # directory path for this model\n    dir_path = os.path.join(dir_path, Path(model_id).name)\n    os.makedirs(dir_path, exist_ok=True)\n    docker_compose, nginx_config, per_container_info_list = _create_config_files(model_id,\n                                                                                 model_copies_as_int,\n                                                                                 devices_per_model,\n                                                                                 image,\n                                                                                 tp_degree,\n                                                                                 batch_size,\n                                                                                 user,\n                                                                                 shm_size,\n                                                                                 env_as_list,\n                                                                                 accelerator,\n                                                                                 dir_path,\n                                                                                 listen_port, \n                                                                                 backend)\n\n    yaml.Dumper.ignore_aliases = lambda self, data: True\n    docker_compose_yaml = yaml.dump(docker_compose)\n\n\n    # ready to create the common files: docker_compose.yml and nginx.conf\n    # any inference server specific files are created by the _create_config_files function\n    # which in turn calls inference server specific functions\n    dc_path = os.path.join(dir_path, \"docker-compose.yml\")\n    logger.info(f\"writing docker compose to {dc_path}, contents --->\\n{docker_compose_yaml}\")    \n    Path(dc_path).write_text(docker_compose_yaml)\n\n    # create nginx.conf file\n    if nginx_config is not None:\n        ngc_path = os.path.join(dir_path, \"nginx.conf\")\n        logger.info(f\"writing nginx conf to {ngc_path}, contents --->\\n{nginx_config}\")    \n        Path(ngc_path).write_text(nginx_config)\n    else:\n        logger.info(f\"model_copies={model_copies}, nginx_config={nginx_config}, \"\n                    f\"not creating nginx.conf\")\n\n    # create sub directories for each model instance\n    for idx, pc in enumerate(per_container_info_list):\n        logger.info(f\"creating files for container {idx+1} of {len(per_container_info_list)}...\")\n        # create serving.properties\n        if serving_properties is not None:\n            os.makedirs(pc[\"dir_path_on_host\"], exist_ok=True)\n            sp_fpath = os.path.join(pc[\"dir_path_on_host\"], \"serving.properties\")\n            logger.info(f\"writing {serving_properties} to {sp_fpath}\")\n            Path(sp_fpath).write_text(serving_properties)\n        else:\n            logger.info(f\"serving_properties={serving_properties}, skipping\")\n\n        # write config.properties, only applicable to DJL\n        if pc['config_properties'] is not None:\n            conf_dir = os.path.join(pc[\"dir_path_on_host\"], \"conf\")\n            os.makedirs(conf_dir, exist_ok=True)\n            cp_fpath = os.path.join(conf_dir, \"config.properties\")\n            logger.info(f\"writing {pc['config_properties']} to {cp_fpath}\")\n            Path(cp_fpath).write_text(pc['config_properties'])\n        else:\n            logger.info(f\"config_properties={pc['config_properties']}, skipping\")\n\n    return model_copies_as_int\n\n\n\n================================================"
  },
  {
    "filename": "pricing.py",
    "path": "fmbench/scripts/pricing.py",
    "directory": "fmbench/scripts",
    "extension": "py",
    "content": "================================================\nimport json\nimport yaml\nimport boto3\nimport logging\nfrom pathlib import Path\nfrom fmbench.utils import load_config\nfrom typing import Dict, Union, List\n\nlogger = logging.getLogger(__name__)\n\n# YAML map dictionary for region codes to human-readable location names\nREGION_MAP = {\n    \"us-east-1\": \"US East (N. Virginia)\",\n    \"us-west-1\": \"US West (N. California)\",\n    \"us-west-2\": \"US West (Oregon)\",\n    \"eu-west-1\": \"EU (Ireland)\",\n    \"ap-southeast-1\": \"Asia Pacific (Singapore)\",\n    \"ap-southeast-2\": \"Asia Pacific (Sydney)\",\n    \"ap-northeast-1\": \"Asia Pacific (Tokyo)\",\n    \"ap-northeast-2\": \"Asia Pacific (Seoul)\",\n    \"sa-east-1\": \"South America (Sao Paulo)\",\n}\n\n\ndef get_ec2_pricing(\n    instance_type: str,\n    region_code: str,\n    operating_system: str = \"Linux\",\n    tenancy: str = \"Shared\",\n) -> float:\n    \"\"\"\n    Retrieve on-demand pricing for a specified EC2 instance type and region.\n\n    Parameters:\n    - instance_type: e.g., \"t3.micro\"\n    - region_code: e.g., \"us-east-1\", \"us-west-2\"\n    - operating_system: e.g., \"Linux\" (default)\n    - tenancy: e.g., \"Shared\" (default) or \"Dedicated\"\n\n    Returns:\n    - Price in USD (float) for the specified instance type and region.\n    \"\"\"\n    # Convert region_code to human-readable region name\n    region_name = REGION_MAP.get(region_code)\n    if not region_name:\n        raise ValueError(f\"Unsupported region code: {region_code}\")\n\n    # Filters for the product\n    filters = [\n        {\"Type\": \"TERM_MATCH\", \"Field\": \"instanceType\", \"Value\": instance_type},\n        {\"Type\": \"TERM_MATCH\", \"Field\": \"location\", \"Value\": region_name},\n        {\"Type\": \"TERM_MATCH\", \"Field\": \"operatingSystem\", \"Value\": operating_system},\n        {\"Type\": \"TERM_MATCH\", \"Field\": \"tenancy\", \"Value\": tenancy},\n        {\"Type\": \"TERM_MATCH\", \"Field\": \"preInstalledSw\", \"Value\": \"NA\"},\n        {\"Type\": \"TERM_MATCH\", \"Field\": \"capacitystatus\", \"Value\": \"Used\"},\n    ]\n\n    # Create a Boto3 client for the Pricing service\n    client = boto3.client(\"pricing\", region_name=\"us-east-1\")\n\n    try:\n        # Call AWS Pricing API\n        response = client.get_products(\n            ServiceCode=\"AmazonEC2\", Filters=filters, FormatVersion=\"aws_v1\"\n        )\n\n        # Parse response to extract pricing\n        for price_item in response[\"PriceList\"]:\n            price_data = json.loads(price_item)\n            for term in price_data[\"terms\"][\"OnDemand\"].values():\n                for price_dimension in term[\"priceDimensions\"].values():\n                    price_per_hour = float(price_dimension[\"pricePerUnit\"][\"USD\"])\n                    return price_per_hour\n\n        # Raise an error if no pricing data is found\n        raise ValueError(f\"No pricing data found for {instance_type} in {region_code}\")\n\n    except boto3.exceptions.Boto3Error as e:\n        logger.error(\n            f\"Boto3 client error while fetching pricing for {instance_type}: {e}\"\n        )\n        raise\n    except Exception as e:\n        logger.error(\n            f\"Unexpected error while fetching pricing for {instance_type}: {e}\"\n        )\n        raise\n\n\ndef load_and_update_pricing(\n    PRICING_YAML_PATH: Union[Path, str],\n    PRICING_FALLBACK_YAML_PATH: Union[Path, str],\n    instances: List,\n    region_code: str = 'us-east-1',\n) -> dict:\n    \"\"\"\n    EC2 pricing function which checks if the pricing for a given instance type exists in pricing.yml.\n    If it does, it skips fetching. If not, it fetches the pricing using `get_ec2_pricing` and updates pricing.yml.\n    If fetching fails, it falls back to the fallback pricing file.\n\n    Args:\n        PRICING_YAML_PATH (Union[Path, str]): Path to the pricing YAML file or S3 URI.\n        PRICING_FALLBACK_YAML_PATH (Union[Path, str]): Path to the fallback YAML file or S3 URI.\n        instances (List): List of instances in experiments to fetch pricing for (e.g., \"t2.micro\").\n        region_code (str): The AWS region code (default is us-east-1).\n        \n    Returns:\n        dict: A dictionary containing updated EC2 pricing data.\n    \"\"\"\n    try:\n        # Load existing pricing data using the `load_config` function\n        pricing_data = load_config(PRICING_YAML_PATH)\n        logger.info(f\"Loaded pricing data from {PRICING_YAML_PATH}\")\n    except Exception as e:\n        logger.error(f\"Error loading pricing YAML from {PRICING_YAML_PATH}: {e}\")\n        logger.warning(\"Attempting to load fallback pricing YAML.\")\n        try:\n            pricing_data = load_config(PRICING_FALLBACK_YAML_PATH)\n            logger.warning(f\"Falling back to {PRICING_FALLBACK_YAML_PATH}\")\n        except Exception as fallback_error:\n            logger.critical(f\"Failed to load fallback pricing YAML: {fallback_error}\")\n            raise FileNotFoundError(\n                \"Pricing data could not be loaded from any source.\"\n            ) from fallback_error\n\n    for instance_type in instances:\n        # Check if the instance type already exists under 'pricing > instance_based'\n        if \"pricing\" in pricing_data and \"instance_based\" in pricing_data[\"pricing\"]:\n            if instance_type in pricing_data[\"pricing\"][\"instance_based\"]:\n                logger.info(f\"Pricing for {instance_type} already exists. Skipping fetch.\")\n                continue\n        if \"token_based\" in pricing_data[\"pricing\"] and instance_type in pricing_data[\"pricing\"][\"token_based\"]:\n            logger.info(f\"Token-based pricing for {instance_type} already exists. Skipping fetch.\")\n            continue  # Skip fetching for this instance type\n\n        # Fetch pricing using `get_ec2_pricing`\n        logger.info(\n            f\"Fetching pricing for instance type: {instance_type} in region: {region_code}\"\n        )\n        \n        try:\n            price = get_ec2_pricing(instance_type, region_code)\n            if price:\n                pricing_data[\"pricing\"][\"instance_based\"][instance_type] = price\n                logger.info(f\"Fetched pricing for {instance_type}: {price} USD\")\n            else:\n                raise ValueError(\n                    f\"No pricing data found for {instance_type} in {region_code}\"\n                )\n        except Exception as e:\n            logger.error(f\"Error fetching pricing: {e}\")\n            logger.warning(\n                \"Fetching pricing failed. Falling back to fallback pricing data.\"\n            )\n            try:\n                pricing_data = load_config(PRICING_FALLBACK_YAML_PATH)\n                logger.warning(\n                    f\"Fallback pricing data loaded from {PRICING_FALLBACK_YAML_PATH}\"\n                )\n            except Exception as fallback_error:\n                logger.critical(\n                    f\"Failed to load fallback pricing YAML after fetch error: {fallback_error}\"\n                )\n                raise FileNotFoundError(\n                    \"Failed to retrieve pricing and fallback pricing data.\"\n                ) from fallback_error\n\n    return pricing_data\n\n\n\n================================================"
  },
  {
    "filename": "rest_predictor.py",
    "path": "fmbench/scripts/rest_predictor.py",
    "directory": "fmbench/scripts",
    "extension": "py",
    "content": "================================================\nimport os\nimport json\nimport math\nimport time\nimport boto3\nimport logging\nimport requests\nimport pandas as pd\nfrom datetime import datetime\nfrom fmbench.scripts import constants\nfrom fmbench.utils import count_tokens\nfrom typing import Dict, Optional, List\nfrom fmbench.scripts.fmbench_predictor import (FMBenchPredictor,\n                                               FMBenchPredictionResponse)\n\n# set a logger\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass RESTPredictor(FMBenchPredictor):\n    # overriding abstract method\n    def __init__(self,\n                 endpoint_name: str,\n                 inference_spec: Optional[Dict],\n                 metadata: Optional[Dict]):\n        try:\n            self._endpoint_name: str = endpoint_name\n            self._inference_spec: Dict = inference_spec \n        except Exception as e:\n            logger.error(f\"create_predictor, exception occured while creating predictor \"\n                         f\"for endpoint_name={self._endpoint_name}, exception={e}\")\n        logger.info(f\"_endpoint_name={self._endpoint_name}, _inference_spec={self._inference_spec}\")\n\n    def get_prediction(self, payload: Dict) -> FMBenchPredictionResponse:\n        response_json: Optional[Dict] = None\n        response: Optional[str] = None\n        latency: Optional[float] = None\n        TTFT: Optional[float] = None\n        TPOT: Optional[float] = None\n        TTLT: Optional[float] = None\n        prompt_tokens: Optional[int] = None\n        completion_tokens: Optional[int] = None\n        timeout: Optional[int] = None\n        auth: Optional[Dict] = None\n        # get the prompt for the EKS endpoint\n        prompt: str = payload['inputs']\n        # represents the number of tokens in the prompt payload\n        prompt_tokens = count_tokens(payload[\"inputs\"])\n        try:\n            st = time.perf_counter()\n            split_input_and_inference_params: Optional[bool] = None\n            if self._inference_spec is not None:\n                split_input_and_inference_params = self._inference_spec.get(\"split_input_and_parameters\")\n                logger.info(f\"split input parameters is: {split_input_and_inference_params}\")\n                timeout = self._inference_spec.get(\"timeout\", 180)\n                auth = self._inference_spec.get(\"auth\", None)\n                logger.info(f\"Initializing the timeout to: {timeout}, auth to configured authentication information\")\n                # Use the parameters that the model needs at inference. In this case, the model does not require inference\n                # parameters and it is handled in the ray serve script that is used to deploy this model 'ray_serve_llama2.py'\n                # parameters: Optional[Dict] = inference_spec.get('parameters')\n\n            # the self._endpoint_name will contain the endpoint url that is used to invoke the model and get the response\n            # In this case, we use ray serve with `NousResearch/Llama-2-13b-chat-hf` model deployed on an EKS cluster.\n            # the endpoint url format used in this example is \"http://<NLB_DNS_NAME>/serve/infer?sentence=<PROMPT_PAYLOAD>\"\n\n            # This endpoint only supports the GET method now, you can add support for POST method if your endpoint supports it.\n            # As an example, the following URL is used with a query added at the end of the URL.\n            # http://<NLB_DNS_NAME>/serve/infer?sentence=what is data parallelism and tensor parallelism and the differences\n            response = requests.get(self._endpoint_name, params={\"sentence\": prompt,\n                                                                    timeout: timeout,\n                                                                    auth: auth}) # the auth dictionary contains\n                                                                                 # authentication parameters. \n                                                                                 # You can do any custom auth handling that your endpoint supports.\n\n            # the response from the model on ray serve from the url prompt is given in this format. \n            # For other response types, change the logic below and add the response in the `generated_text` key within the response_json dict\n            response.raise_for_status()\n            full_output = response.text\n            answer_only = full_output.replace(prompt, \"\", 1).strip('[\"]?\\n')\n            response_json = dict(generated_text=answer_only)\n            # counts the completion tokens for the model using the default/user provided tokenizer\n            completion_tokens = count_tokens(response_json.get(\"generated_text\"))\n        except requests.exceptions.RequestException as e:\n            logger.error(f\"get_prediction, exception occurred while getting prediction for payload={payload} \"\n                         f\"from predictor={self._endpoint_name}, response={response}, exception={e}\")\n        return FMBenchPredictionResponse(response_json=response_json,\n                                         latency=latency,\n                                         time_to_first_token=TTFT,\n                                         time_per_output_token=TPOT,\n                                         time_to_last_token=TTLT,\n                                         completion_tokens=completion_tokens,\n                                         prompt_tokens=prompt_tokens)\n\n    @property\n    def endpoint_name(self) -> str:\n        \"\"\"The endpoint name property.\"\"\"\n        return self._endpoint_name\n\n    # The rest ep is deployed on an instance that incurs hourly cost hence, the calculcate cost function\n    # computes the cost of the experiment on an hourly basis. If your instance has a different pricing structure\n    # modify this function.\n    def calculate_cost(self,\n                       instance_type: str,\n                       instance_count: int,\n                       pricing: Dict,\n                       duration: float,\n                       prompt_tokens: int,\n                       completion_tokens: int) -> float:\n        \"\"\"Calculate the cost of each experiment run.\"\"\"\n        experiment_cost: Optional[float] = None\n        try:\n            instance_based_pricing = pricing['pricing']['instance_based']\n            hourly_rate = instance_based_pricing.get(instance_type, None)\n            logger.info(f\"the hourly rate for running on {instance_type} is {hourly_rate}, instance_count={instance_count}\")\n            # calculating the experiment cost for instance based pricing\n            instance_count = instance_count if instance_count else 1\n            experiment_cost = (hourly_rate / 3600) * duration * instance_count\n        except Exception as e:\n            logger.error(f\"exception occurred during experiment cost calculation, exception={e}\")\n        return experiment_cost\n    \n    def get_metrics(self,\n                    start_time: datetime,\n                    end_time: datetime,\n                    period: int = 60) -> pd.DataFrame:\n        # not implemented\n        return None\n\n    def shutdown(self) -> None:\n        \"\"\"Represents the function to shutdown the predictor\n           cleanup the endpooint/container/other resources\n        \"\"\"\n        return None\n    \n    @property\n    def inference_parameters(self) -> Dict:\n        \"\"\"The inference parameters property.\"\"\"\n        return self._inference_spec.get(\"parameters\")\n\n    @property\n    def platform_type(self) -> Dict:\n        \"\"\"The inference parameters property.\"\"\"\n        return constants.PLATFORM_EXTERNAL\n    \ndef create_predictor(endpoint_name: str, inference_spec: Optional[Dict], metadata: Optional[Dict]):\n    return RESTPredictor(endpoint_name, inference_spec, metadata)\n\n\n\n================================================"
  },
  {
    "filename": "sagemaker_deploy.py",
    "path": "fmbench/scripts/sagemaker_deploy.py",
    "directory": "fmbench/scripts",
    "extension": "py",
    "content": "================================================\nimport boto3\nimport fmbench\nimport logging\nimport sagemaker\nfrom typing import Dict\nfrom sagemaker import Model\nfrom fmbench.scripts import constants\nfrom sagemaker.utils import name_from_base\n\nlogging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\n# Initialize the platform where this script deploys the model\nPLATFORM: str = constants.PLATFORM_SAGEMAKER\n\ntag = [\n    {\n        'Key': 'fmbench-version',\n        'Value': fmbench.__version__\n    }\n]\n\n\ndef deploy(experiment_config: Dict, role_arn: str) -> Dict:\n    role = role_arn\n    endpoint_name = name_from_base(experiment_config['ep_name'])\n\n    boto3_session=boto3.session.Session()\n    smr = boto3.client('sagemaker-runtime')\n    sm = boto3.client('sagemaker')\n    # sagemaker session for interacting with different AWS APIs\n    sess = sagemaker.session.Session(boto3_session, \n                                     sagemaker_client=sm, \n                                     sagemaker_runtime_client=smr)\n    logger.info(\"======================================\")\n    logger.info(f\"Will load artifacts from {experiment_config['model_data']}\")\n    logger.info(\"======================================\")\n\n    logger.info(\"======================================\")\n    logger.info(f\"Using Container image {experiment_config['image_uri']}\")\n    logger.info(\"======================================\")\n    model = Model(name=endpoint_name,\n                  # Enable SageMaker uncompressed model artifacts\n                  model_data=experiment_config['model_data'],\n                  image_uri=experiment_config['image_uri'],\n                  role=role,\n                  env=experiment_config['env'],\n                  sagemaker_session=sess)\n    logger.info(model)\n\n    logger.info(f'\\nModel deployment initiated Endpoint Name: {endpoint_name}\\n')\n    if experiment_config.get('accept_eula') is not None:\n        model.deploy(\n            initial_instance_count=experiment_config['instance_count'],\n            instance_type=experiment_config['instance_type'],\n            endpoint_name=endpoint_name,\n            #volume_size=512, # not allowed for the selected Instance type ml.g5.12xlarge\n            model_data_download_timeout=1200, # increase the timeout to download large model\n            container_startup_health_check_timeout=1200, # increase the timeout to load large model,\n            wait=True,\n            tags=tag, \n            accept_eula=experiment_config.get('accept_eula')\n        )\n    else:\n        model.deploy(\n            initial_instance_count=experiment_config['instance_count'],\n            instance_type=experiment_config['instance_type'],\n            endpoint_name=endpoint_name,\n            #volume_size=512, # not allowed for the selected Instance type ml.g5.12xlarge\n            model_data_download_timeout=1200, # increase the timeout to download large model\n            container_startup_health_check_timeout=1200, # increase the timeout to load large model,\n            wait=True,\n            tags=tag \n        )\n    logger.info(f'Model deployment on Endpoint Name: {endpoint_name} finished\\n')\n    return dict(endpoint_name=endpoint_name,\n                experiment_name=experiment_config['name'],\n                instance_type=experiment_config['instance_type'],\n                instance_count=experiment_config['instance_count'], \n                deployed=True)\n\n\n\n================================================"
  },
  {
    "filename": "sagemaker_metrics.py",
    "path": "fmbench/scripts/sagemaker_metrics.py",
    "directory": "fmbench/scripts",
    "extension": "py",
    "content": "================================================\n\"\"\"\nRetrieves metrics for SageMaker Endpoints from CloudWatch.\nSee https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-cloudwatch.html for\nfull list of metrics.\n\"\"\"\nimport json\nimport boto3\nimport logging\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\n# Setup logging\nlogging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef _get_endpoint_utilization_metrics(endpoint_name: str,\n                                      variant_name: str,\n                                      start_time: datetime,\n                                      end_time: datetime,\n                                      period : int = 60) -> pd.DataFrame:\n    \"\"\"\n    Retrieves utilization metrics for a specified SageMaker endpoint within a given time range.\n\n    Parameters:\n    - endpoint_name (str): The name of the SageMaker endpoint.\n    - start_time (datetime): The start time for the metrics data.\n    - end_time (datetime): The end time for the metrics data.\n    - period (int): The granularity, in seconds, of the returned data points. Default is 60 seconds.\n\n    Returns:\n    - Dataframe: A Dataframe containing metric values for utilization metrics like CPU and GPU Usage.\n    \"\"\"\n    \n    metrics = [\"CPUUtilization\",\n               \"MemoryUtilization\",\n               \"DiskUtilization\",\n               \"InferenceLatency\",\n               \"GPUUtilization\",\n               \"GPUMemoryUtilization\"]\n    \n    client = boto3.client('cloudwatch')\n    data = []\n    namespace = \"/aws/sagemaker/Endpoints\"\n    \n    for metric_name in metrics:\n        logger.debug(f\"_get_endpoint_utilization_metrics, endpoint_name={endpoint_name}, variant_name={variant_name}, \"\n                     f\"metric_name={metric_name}, start_time={start_time}, end_time={end_time}\")\n        response = client.get_metric_statistics(\n            Namespace=namespace,\n            MetricName=metric_name,\n            Dimensions=[\n                {\n                    'Name': 'EndpointName',\n                    'Value': endpoint_name\n                },\n                {\n                    'Name': 'VariantName',\n                    'Value': variant_name\n                }\n            ],\n            StartTime=start_time,\n            EndTime=end_time,\n            Period=period,\n            Statistics=['Average']  # You can also use 'Sum', 'Minimum', 'Maximum', 'SampleCount'\n        )\n        logger.debug(response)\n        for datapoint in response['Datapoints']:\n            data.append({\n                'EndpointName': endpoint_name, \n                'Timestamp': datapoint['Timestamp'],\n                'MetricName': metric_name,\n                'Average': datapoint['Average']\n            })\n\n    # Create a DataFrame from the collected data\n    df = pd.DataFrame(data)\n\n    # Pivot the DataFrame to have metrics as columns\n    df_pivot = df.pivot_table(index=['Timestamp', 'EndpointName'], columns='MetricName', values='Average').reset_index()\n    \n    # Remove the index column heading\n    sm_utilization_metrics_df = df_pivot.rename_axis(None, axis=1)\n    \n    return sm_utilization_metrics_df\n\n\ndef _get_endpoint_invocation_metrics(endpoint_name: str,\n                                     variant_name: str,\n                                     start_time: datetime,\n                                     end_time: datetime,\n                                     period : int = 60):\n    \"\"\"\n    Retrieves Invocation metrics for a specified SageMaker endpoint within a given time range.\n\n    Parameters:\n    - endpoint_name (str): The name of the SageMaker endpoint.\n    - start_time (datetime): The start time for the metrics data.\n    - end_time (datetime): The end time for the metrics data.\n    - period (int): The granularity, in seconds, of the returned data points. Default is 60 seconds.\n\n    Returns:\n    - Dataframe: A Dataframe containing metric values for Invocation metrics like Invocations and Model Latency.\n    \"\"\"\n    metric_names = [\"Invocations\",\n                    \"Invocation4XXErrors\",\n                    \"Invocation5XXErrors\",\n                    \"ModelLatency\",\n                    \"InvocationsPerInstance\"]\n    \n    # Initialize a session using Amazon CloudWatch\n    client = boto3.client('cloudwatch')\n\n    namespace = \"AWS/SageMaker\"\n    data = []\n    \n    for metric_name in metric_names:\n        if metric_name == 'ModelLatency':\n            stat = 'Average'\n        else:\n            stat = 'Sum'\n        logger.debug(f\"_get_endpoint_invocation_metrics, endpoint_name={endpoint_name}, variant_name={variant_name}, \"\n                     f\"metric_name={metric_name}, start_time={start_time}, end_time={end_time}\")\n        # Get metric data for the specified metric\n        response = client.get_metric_data(\n            MetricDataQueries=[\n                {\n                    'Id': f'metric_{metric_name}',\n                    'MetricStat': {\n                        'Metric': {\n                            'Namespace': namespace,\n                            'MetricName': metric_name,\n                            'Dimensions': [\n                                {\n                                    'Name': 'EndpointName',\n                                    'Value': endpoint_name\n                                },\n                                {\n                                    'Name': 'VariantName',\n                                    'Value': variant_name\n                                }\n                            ]\n                        },\n                        'Period': period,  # Period in seconds\n                        'Stat': stat  # Statistic to retrieve\n                    },\n                    'ReturnData': True,\n                },\n            ],\n            StartTime=start_time,\n            EndTime=end_time\n        )\n        logger.debug(response)\n        # Extract the data points from the response\n        timestamps = response['MetricDataResults'][0]['Timestamps']\n        values = response['MetricDataResults'][0]['Values']\n        \n        for timestamp, value in zip(timestamps, values):\n            data.append({\n                'EndpointName': endpoint_name, \n                'Timestamp': timestamp,\n                'MetricName': metric_name,\n                'Value': value\n            })\n\n    # Create a DataFrame from the collected data\n    df = pd.DataFrame(data)\n    \n    # Pivot the DataFrame to have metrics as columns\n    df_pivot = df.pivot_table(index=['Timestamp', 'EndpointName'], columns='MetricName', values='Value').reset_index()\n    \n    # Remove the index column heading\n    sm_invocation_metrics_df = df_pivot.rename_axis(None, axis=1)\n    \n    return sm_invocation_metrics_df\n\n\ndef get_endpoint_metrics(endpoint_name: str,\n                         variant_name: str,\n                         start_time: datetime,\n                         end_time: datetime,\n                         period: int = 60):\n    \"\"\"\n    Retrieves Invocation and Utilization metrics for a specified SageMaker endpoint within a given time range.\n\n    Parameters:\n    - endpoint_name (str): The name of the SageMaker endpoint.\n    - start_time (datetime): The start time for the metrics data.\n    - end_time (datetime): The end time for the metrics data.\n    - period (int): The granularity, in seconds, of the returned data points. Default is 60 seconds.\n\n    Returns:\n    - Dataframe: A Dataframe containing metric values for Utilization and Invocation metrics.\n    \"\"\"\n    \n    endpoint_metrics_df: Optional[pd.DataFrame] = None\n    try:\n        logger.info(f\"get_endpoint_metrics, going to retrieve endpoint utlization metrics for \"\n                    f\"endpoint={endpoint_name}, variant_name={variant_name}, start_time={start_time}, \"\n                    f\"end_time={end_time}, period={period}\")\n        utilization_metrics_df = _get_endpoint_utilization_metrics(endpoint_name=endpoint_name,\n                                                                   variant_name=variant_name,\n                                                                   start_time=start_time,\n                                                                   end_time=end_time,\n                                                                   period=period)\n        logger.info(f\"get_endpoint_metrics, going to retrieve endpoint invocation metrics for \"\n                    f\"endpoint={endpoint_name}, variant_name={variant_name}, start_time={start_time}, \"\n                    f\"end_time={end_time}, period={period}\")\n        invocation_metrics_df = _get_endpoint_invocation_metrics(endpoint_name=endpoint_name,\n                                                                 variant_name=variant_name,\n                                                                 start_time=start_time,\n                                                                 end_time=end_time,\n                                                                 period=period)\n\n        endpoint_metrics_df = pd.merge(utilization_metrics_df,\n                                       invocation_metrics_df,\n                                       on=['Timestamp', 'EndpointName'],\n                                       how='outer')\n        logger.info(f\"get_endpoint_metrics, shape of invocation and utilization metrics for \"\n                    f\"endpoint={endpoint_name} is {endpoint_metrics_df.shape}\")\n        logger.info(f\"get_endpoint_metrics, endpoint_metrics_df={endpoint_metrics_df.head()}\")\n    except Exception as e:\n        logger.error(f\"get_endpoint_metrics, exception occured while retrieving metrics for {endpoint_name}, \"\n                     f\"exception={e}\")\n\n    return endpoint_metrics_df\n\n\n================================================"
  },
  {
    "filename": "sagemaker_predictor.py",
    "path": "fmbench/scripts/sagemaker_predictor.py",
    "directory": "fmbench/scripts",
    "extension": "py",
    "content": "================================================\nimport time\nimport json\nimport copy\nimport boto3\nimport logging\nimport sagemaker\nimport pandas as pd\nfrom datetime import datetime\nfrom typing import Dict, Optional\nfrom fmbench.scripts import constants\nfrom fmbench.utils import count_tokens\nfrom sagemaker.predictor import Predictor\nfrom sagemaker.serializers import JSONSerializer\nfrom fmbench.scripts.stream_responses import get_response_stream\nfrom fmbench.scripts.sagemaker_metrics import get_endpoint_metrics\nfrom fmbench.scripts.fmbench_predictor import (FMBenchPredictor,\n                                               FMBenchPredictionResponse)\n\n# set a logger\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Initialize the SageMaker runtime to support getting response streams\nsagemaker_runtime = boto3.client('sagemaker-runtime')\nsm_client = boto3.client(\"sagemaker\")\n\n# Initialize the platform where this script deploys the model\nPLATFORM: str = constants.PLATFORM_SAGEMAKER\n\nclass SageMakerPredictor(FMBenchPredictor):\n    # overriding abstract method\n    def __init__(self,\n                 endpoint_name: str,\n                 inference_spec: Optional[Dict],\n                 metadata: Optional[Dict]):\n        self._predictor: Optional[sagemaker.base_predictor.Predictor] = None\n        self._endpoint_name: str = endpoint_name\n        self._inference_spec: Dict = inference_spec\n        self._variant_name: Optional[str] = None\n        self._use_messages_api_format: Optional[bool] = None\n        if metadata is not None:\n            self._variant_name = metadata.get(\"variant_name\")\n            self._use_messages_api_format = metadata.get(\"use_messages_api_format\")\n\n        try:\n            # Create a SageMaker Predictor object\n            self._predictor = Predictor(\n                endpoint_name=self._endpoint_name,\n                sagemaker_session=sagemaker.Session(),\n                serializer=JSONSerializer()\n            )\n        except Exception as e:\n            logger.error(f\"create_predictor, exception occured while creating predictor \"\n                         f\"for endpoint_name={self._endpoint_name}, exception={e}\")\n        logger.info(f\"__init__ _predictor={self._predictor}, \"\n                    f\"_variant_name={self._variant_name}_\"\n                    f\"inference_spec={self._inference_spec}, \"\n                    f\"_use_messages_api_format={self._use_messages_api_format}\")\n\n    def get_prediction(self, payload: Dict) -> FMBenchPredictionResponse:\n        response_json: Optional[Dict] = None\n        response: Optional[str] = None\n        latency: Optional[float] = None\n        TTFT: Optional[float] = None\n        TPOT: Optional[float] = None\n        TTLT: Optional[float] = None\n        prompt_tokens: Optional[int] = None\n        completion_tokens: Optional[int] = None\n        streaming: Optional[bool] = None\n        stop_token: Optional[str] = None\n        \n        \n        \n        # represents the number of tokens in the prompt payload\n        prompt_tokens = count_tokens(payload[\"inputs\"])\n\n        try:\n            container_type = self._inference_spec.get(\"container_type\")\n            st = time.perf_counter()\n            split_input_and_inference_params = None\n            if self._inference_spec is not None:\n                split_input_and_inference_params = self._inference_spec.get(\"split_input_and_parameters\")\n            response = None\n            streaming = self._inference_spec.get(\"stream\", False)\n            if split_input_and_inference_params is True:\n                response = self._predictor.predict(payload[\"inputs\"],\n                                                   self._inference_spec[\"parameters\"])\n            else:\n                if self._use_messages_api_format is True:\n                    # if needed in future add support for system prompt as well\n                    # and multiple system/user prompts but all that is not needed for now\n                    payload = {\"messages\": [{\"role\": \"user\",\n                                             \"content\": payload[\"inputs\"]}]}\n                    # the final payload should look like this:\n                    # {\n                    #   \"top_p\": 0.9,\n                    #   \"max_tokens\": 100,\n                    #   \"messages\": [\n                    #     {\n                    #       \"role\": \"user\",\n                    #       \"content\": \"this is the prompt\"\n                    #     }\n                    #   ]\n                    # }\n                    payload = payload | dict(self._inference_spec[\"parameters\"])\n                else:\n                    # the final payload should look like this:\n                    # {\n                    #   \"parameters\": { \n                    #     \"top_p\": 0.9,\n                    #     \"max_tokens\": 100\n                    #    },\n                    #   \"inputs\": \"this is the prompt\"\n                    # }\n                    if container_type == constants.CONTAINER_TYPE_HUGGINGFACE:\n                        payload2 = copy.deepcopy(payload)\n                        payload2['text_inputs'] = payload2.pop('inputs')\n                        payload2['mode'] = \"embedding\"\n                    else:\n                        payload = payload | dict(parameters=self._inference_spec[\"parameters\"])\n\n            # if the response streaming is step, call the get_response stream on the \n            # sagemaker endpoint, else use the simple predict call\n            if streaming is True:\n                start_token = self._inference_spec.get(\"start_token\", None)\n                stop_token = self._inference_spec.get(\"stop_token\", None)\n                payload[\"stream\"] = streaming\n                logger.info(f\"streaming={streaming}, calling invoke_endpoint_with_response_stream\")\n                response_stream = sagemaker_runtime.invoke_endpoint_with_response_stream(\n                                                    EndpointName=self._endpoint_name,\n                                                    Body=json.dumps(payload),\n                                                    ContentType=\"application/json\")\n                response_dict = get_response_stream(response_stream['Body'],\n                                                    st,\n                                                    start_token,\n                                                    stop_token,\n                                                    is_sagemaker=True)\n                TTFT = response_dict.get('TTFT')\n                TPOT = response_dict.get('TPOT')\n                TTLT = response_dict.get('TTLT')\n                response = response_dict.get('response')\n            else:\n                logger.info(f\"streaming={streaming}, calling predict\")\n                if container_type:\n                    response = self._predictor.predict(payload2)\n                    logger.info(\"Running predictor with HuggingFace Container for Embedding model\")\n                else:\n                    response = self._predictor.predict(payload)\n                    logger.info(\"Running predictor for Foundation model\")\n\n            latency = time.perf_counter() - st\n            if isinstance(response, bytes):\n                response = response.decode('utf-8')\n            response_json = json.loads(response)\n\n            # we want to set the \"generated_text\" key in the response\n            if isinstance(response_json, list):\n                response_json = response_json[0]\n                # add a key called completion, if not there\n                if response_json.get(\"generated_text\") is None:\n                    # look for predicted label and set that as generated text\n                    if response_json.get(\"predicted_label\") is not None:\n                        response_json[\"generated_text\"] = response_json.get(\"predicted_label\")\n                    else:\n                        logger.error(\"response_json is list but did not contain generated_text or predicted_label, dont know how to handle this\")\n            elif isinstance(response_json, dict):\n                choices = response_json.get(\"choices\")\n                if choices is not None:\n                    if isinstance(choices, list):\n                        response_json = response_json[\"choices\"][0][\"message\"]\n                        if response_json.get(\"generated_text\") is None:\n                            if response_json.get(\"content\") is not None:\n                                response_json[\"generated_text\"] = response_json.get(\"content\")\n                            else:\n                                logger.error(f\"response_json is a dict, choices is a list, but response_json does not contain generated_text, dont know how to handle this\")\n                        else:\n                            logger.error(f\"response_json is a dict, choices is a list, but generated_text ia None, dont know how to handle this\")\n                    else:\n                        logger.error(f\"response_json is a dict, but choices is not a list but rather it is {type(choices)}, dont know how to handle this\")\n                else:\n                    if container_type == constants.CONTAINER_TYPE_HUGGINGFACE:\n                        response_json[\"generated_text\"] = response_json.get(\"embedding\")\n                        completion_tokens = len(response_json.get(\"generated_text\"))\n                    # logger.error(f\"response_json is a dict, but does not contain choices, dont know how to handle this\")\n            else:\n                logger.error(f\"response_json data type is {type(response_json)}, dont know how to handle this\")\n            # counts the completion tokens for the model using the default/user provided tokenizer\n            if not completion_tokens:\n                completion_tokens = count_tokens(response_json.get(\"generated_text\"))\n\n        except Exception as e:\n            logger.error(f\"get_prediction, exception occurred while getting prediction for payload={payload} \"\n                         f\"from predictor={self._endpoint_name}, response={response}, exception={e}\")\n        return FMBenchPredictionResponse(response_json=response_json,\n                                         latency=latency,\n                                         time_to_first_token=TTFT,\n                                         time_per_output_token=TPOT,\n                                         time_to_last_token=TTLT,\n                                         completion_tokens=completion_tokens,\n                                         prompt_tokens=prompt_tokens)\n\n    @property\n    def endpoint_name(self) -> str:\n        \"\"\"The endpoint name property.\"\"\"\n        return self._endpoint_name\n\n    def calculate_cost(self,\n                       instance_type: str,\n                       instance_count: int,\n                       pricing: Dict,\n                       duration: float,\n                       prompt_tokens: int,\n                       completion_tokens: int) -> float:\n        \"\"\"Calculate the cost of each experiment run.\"\"\"\n        experiment_cost: Optional[float] = None\n        try:\n            instance_based_pricing = pricing['pricing']['instance_based']\n            hourly_rate = instance_based_pricing.get(instance_type, None)\n            logger.info(f\"the hourly rate for running on {instance_type} is {hourly_rate}, \"\n                        f\"instance_count={instance_count}\")\n            # calculating the experiment cost for instance based pricing\n            instance_count = instance_count if instance_count else 1\n            experiment_cost = (hourly_rate / 3600) * duration * instance_count\n        except Exception as e:\n            logger.error(f\"exception occurred during experiment cost calculation, exception={e}\")\n        return experiment_cost\n\n    def get_metrics(self,\n                    start_time: datetime,\n                    end_time: datetime,\n                    period: int = 60) -> pd.DataFrame:\n        return get_endpoint_metrics(self._endpoint_name, self._variant_name, start_time, end_time)\n        \n    def shutdown(self) -> None:\n        \"\"\"Represents the function to shutdown the predictor\n           cleanup the endpooint/container/other resources\n        \"\"\"\n        try:\n            ep_name = self.endpoint_name\n            ## Describe the model endpoint \n            logger.info(f\"Going to describe the endpoint -> {ep_name}\")\n            resp = sm_client.describe_endpoint(EndpointName=ep_name)\n\n            ## If the given model endpoint is in service, delete it \n            if resp['EndpointStatus'] == 'InService':\n                logger.info(f\"going to delete {ep_name}\")\n                ## deleting the model endpoint\n                sm_client.delete_endpoint(EndpointName=ep_name)\n                logger.info(f\"deleted {ep_name}\")\n                return True\n        except Exception as e:\n            logger.error(f\"error deleting endpoint={ep_name}, exception={e}\")\n            return False\n    \n    @property\n    def inference_parameters(self) -> Dict:\n        \"\"\"The inference parameters property.\"\"\"\n        return self._inference_spec.get(\"parameters\")\n\n\n    @property\n    def platform_type(self) -> Dict:\n        \"\"\"The inference parameters property.\"\"\"\n        return constants.PLATFORM_SAGEMAKER\n    \ndef create_predictor(endpoint_name: str, inference_spec: Optional[Dict], metadata: Optional[Dict]):\n    return SageMakerPredictor(endpoint_name, inference_spec, metadata)\n\n\n\n================================================"
  },
  {
    "filename": "stream_responses.py",
    "path": "fmbench/scripts/stream_responses.py",
    "directory": "fmbench/scripts",
    "extension": "py",
    "content": "================================================\nimport io\nimport time\nimport json\nimport boto3\nimport litellm\nimport logging\nimport botocore\nfrom typing import Dict, Optional, List, Union\n\n# Set up logger\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Initialize the SageMaker runtime to invoke the endpoint\nsagemaker_runtime = boto3.client('sagemaker-runtime')\n\n\nclass LineIterator:\n\n    \"\"\"\n    A helper class for parsing the byte stream input.\n\n    The output of the model will be in the following format:\n    ```\n    b'{\"outputs\": [\" a\"]}\\n'\n    b'{\"outputs\": [\" challenging\"]}\\n'\n    b'{\"outputs\": [\" problem\"]}\\n'\n    ...\n    ```\n\n    While usually each PayloadPart event from the event stream will contain a byte array \n    with a full json, this is not guaranteed and some of the json objects may be split across\n    PayloadPart events. For example:\n    ```\n    {'PayloadPart': {'Bytes': b'{\"outputs\": '}}\n    {'PayloadPart': {'Bytes': b'[\" problem\"]}\\n'}}\n    ```\n\n    This class accounts for this by concatenating bytes written via the 'write' function\n    and then exposing a method which will return lines (ending with a '\\n' character) within\n    the buffer via the 'scan_lines' function. It maintains the position of the last read \n    position to ensure that previous bytes are not exposed again. \n    \"\"\"\n\n    def __init__(self, stream):\n        self.byte_iterator = iter(stream)\n        self.buffer = io.BytesIO()\n        self.read_pos = 0\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        while True:\n            self.buffer.seek(self.read_pos)\n            line = self.buffer.readline()\n            if line and line[-1] == ord('\\n'):\n                self.read_pos += len(line)\n                return line[:-1]\n            try:\n                chunk = next(self.byte_iterator)\n            except StopIteration:\n                if self.read_pos < self.buffer.getbuffer().nbytes:\n                    continue\n                raise\n            if 'PayloadPart' not in chunk:\n                logger.warning('Unknown event type:' + str(chunk))\n                continue\n            self.buffer.seek(0, io.SEEK_END)\n            self.buffer.write(chunk['PayloadPart']['Bytes'])\n\n\ndef get_response_stream(response_stream: Union[litellm.utils.CustomStreamWrapper, botocore.eventstream.EventStream],\n                        start_time: float,\n                        start_token: str,\n                        stop_token: str,\n                        is_sagemaker: bool = False) -> Dict:\n    \"\"\"\n    Helper function to get the response streams from bedrock or sagemaker invocations\n    and parse each as appropriate to calculate the Time To First Token (TTFT), Time Per Output Token (TPOT), \n    and Time To Last Token (TTLT)\n\n    return: This function returns a dictionary containing the entire response, and the TTFT, TTLT, TPOT metrics\n    \"\"\"\n    logger.info(f\"get_response_stream, type(response_stream)={type(response_stream)}\")\n    sm_start_token = b\"{\"\n    first_token_time: Optional[float] = None\n    token_times: List[float] = []\n    last_token_time = start_time\n    response_text: str = \"\"\n    TTFT: Optional[float] = None\n    TPOT: Optional[float] = None\n    TTLT: Optional[float] = None\n    result: Optional[Dict] = None\n\n    try:\n        # get the event from the sagemaker or bedrock response streams\n        event_iterator = LineIterator(response_stream) if is_sagemaker else response_stream\n\n        for event in event_iterator:\n            # if the response stream is from a sagemaker call, then use the \n            # line iterator to get the first token from the streaming response\n            token_id: Optional[int] = None\n            if is_sagemaker:\n                if event != b'' and sm_start_token in event:\n                    data = json.loads(event[event.find(sm_start_token):].decode('utf-8'))\n                    #logger.info(f\"data={data}\")\n                    token_id = data['token']['id']\n                    if token_id != [-1]:\n                        token_text = data['token']['text']\n                    else:\n                        token_text = None\n                else:\n                    continue\n            else:\n                # if the response stream is from a bedrock call, then get the chunks from the response\n                # and the first token\n                if hasattr(event, 'choices') and hasattr(event.choices[0], 'delta'):\n                    token_text = event.choices[0].delta.get('content', '')\n                else:\n                    continue\n            # record the current time\n            current_time = time.perf_counter()\n            if token_text and token_text != stop_token:\n                if first_token_time is None:\n                    first_token_time = current_time\n                    # get the time to first token latency\n                    TTFT = first_token_time - start_time\n                    logger.info(f\"Time to First Token: {TTFT:.6f} seconds\")\n                else:\n                    # if the token is not the first token, then get the inter-token\n                    # latency\n                    token_time = current_time - last_token_time\n                    # append all token times to get the time per output token\n                    token_times.append(token_time)\n                last_token_time = current_time\n                response_text += token_text\n\n            if stop_token and stop_token in response_text:\n                logger.info(f\"got the last token: {stop_token}\")\n                break\n            elif token_id == [-1]:\n                logger.info(f\"end of stream because token id is {token_id}\")\n                break\n\n        # Calculate TTLT at the reception of the last token\n        current_time = time.perf_counter()\n        TTLT = current_time - start_time\n        logger.info(f\"Time to Last Token (TTLT): {TTLT:.6f} seconds, total tokens received {len(token_times)}\")\n\n        if token_times:\n            # get the average of all token times to compute the time per output token\n            TPOT = sum(token_times) / len(token_times)\n            logger.info(f\"Time Per Output Token (TPOT): {TPOT:.6f} seconds\")\n\n        response_data = [{\"generated_text\": response_text}]\n        response_json_str = json.dumps(response_data)\n        result = {\n            \"TTFT\": TTFT,\n            \"TPOT\": TPOT,\n            \"TTLT\": TTLT,\n            \"response\": response_json_str\n        }\n    except Exception as e:\n        logger.error(f\"Error occurred while generating and computing metrics \"\n                     f\"associated with the streaming response: {e}\", exc_info=True)\n        result = None\n\n    logger.info(f\"Final result: {result}\")\n    return result\n\n\n\n\n================================================"
  },
  {
    "filename": "requirements.txt",
    "path": "fmbench/scripts/compile-llm-for-aws-silicon/requirements.txt",
    "directory": "fmbench/scripts/compile-llm-for-aws-silicon",
    "extension": "txt",
    "content": "================================================\n## packages for neuron sdk 2.18.1\n--extra-index-url https://pip.repos.neuron.amazonaws.com\ntransformers-neuronx==0.11.351.0\nneuronx-cc==2.14.213.0\ntorch-neuronx==2.1.2.2.2.0\n## needed to package the model artifacts\ntorchserve==0.11.0\n## needed for LLamaTokenizer\n## sentencepiece==0.1.99\ntiktoken==0.7.0\n\n\n================================================"
  },
  {
    "filename": "compile.py",
    "path": "fmbench/scripts/compile-llm-for-aws-silicon/scripts/compile.py",
    "directory": "fmbench/scripts/compile-llm-for-aws-silicon/scripts",
    "extension": "py",
    "content": "================================================\nimport os\nimport sys\nimport time\nimport torch\nimport logging\nimport argparse\nimport torch_neuronx\nfrom transformers import AutoTokenizer\nfrom transformers_neuronx.config import GenerationConfig\nfrom transformers_neuronx.llama.model import LlamaForSampling\nfrom transformers_neuronx import LlamaForSampling, NeuronConfig, GQA, QuantizationConfig\n\nroot = logging.getLogger()\nif root.handlers:\n    for handler in root.handlers:\n        root.removeHandler(handler)\nlogging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nif __name__ == \"__main__\":\n    \n    # Define and parse command-line arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--action\",\n        type=str,\n        help=\"What do you want the script do: \\\"compile\\\" or \\\"infer\\\"?\"\n    )\n    parser.add_argument(\n        \"--batch-size\",\n        type=int,\n        default=4,\n        help=\"Batch size with which to compile the model, default=4\",\n    )\n\n    parser.add_argument(\n        \"--num-neuron-cores\",\n        type=int,\n        default=8,\n        help=\"Number of Neuron cores in the instance in which the model would be run, default=8\",\n    )\n\n    parser.add_argument(\n        \"--model-dir\",\n        type=str,\n        help=\"Directory from where to read the model binaries\",\n    )\n    args = parser.parse_args()\n    logger.info(f\"args={args}\")\n\n    # we will pin cores to 8 for inf2.24xlarge \n    os.environ['NEURON_RT_NUM_CORES'] = str(args.num_neuron_cores)\n    os.environ[\"NEURON_CC_FLAGS\"] = \"-O3\"  ## for best perf\n    BATCH_SIZE = args.batch_size\n    CONTEXT_LENGTH = 44 # hard coded for sample prompt\n    model_dir = args.model_dir\n    model_compiled_dir = os.path.join(os.path.dirname(os.path.dirname(model_dir)), \"neuronx_artifacts\")\n    neuron_config = NeuronConfig(on_device_embedding=False,\n                                attention_layout='BSH',\n                                fuse_qkv=True,\n                                group_query_attention=GQA.REPLICATED_HEADS,\n                                quant=QuantizationConfig(),\n                                on_device_generation=GenerationConfig(do_sample=True))\n\n    if args.action == \"compile\":\n        start = time.perf_counter()\n        model = LlamaForSampling.from_pretrained(\n                model_dir,\n                batch_size=args.batch_size,\n                tp_degree=args.num_neuron_cores,\n                amp='f16',\n                neuron_config=neuron_config,\n                n_positions=4096,\n                )\n        model.to_neuron()\n        # save model to the disk\n        model.save(model_compiled_dir)\n        elapsed = time.perf_counter() - start\n        logger.info(f'\\nCompilation and loading took {elapsed:.2f} seconds\\n')\n\n\n================================================"
  },
  {
    "filename": "download_compile_deploy.sh",
    "path": "fmbench/scripts/compile-llm-for-aws-silicon/scripts/download_compile_deploy.sh",
    "directory": "fmbench/scripts/compile-llm-for-aws-silicon/scripts",
    "extension": "sh",
    "content": "================================================\n#!/bin/sh\nset -e\n# download model from HuggingFace -> compile it for Neuron -> deploy on SageMaker\n# param 1: HuggingFace token\n# param 2: HuggingFace model id, for example meta-llama/Meta-Llama-3-8B-Instruct\n# param 3: local directory path to save the model\n\n## split and save the model \ntoken=$1\nmodel_id=$2\nneuron_version=$3\nmodel_store=$4\ns3_bucket=$5\nprefix=$6\nregion=$7\nrole=$8\nbatch_size=$9\nnum_neuron_cores=${10}\nml_instance_type=${11}\nmodel_loading_timeout=${12}\nserving_properties=${13}\nscript_path=${14}\ninstance_count=${15}\nml_image_uri=${16}\nmodel_id_wo_repo=`basename $2`\nmodel_id_wo_repo_split=$model_id_wo_repo-split\nlocal_dir=neuron_version/$neuron_version/$model_store/$model_id_wo_repo/$model_id_wo_repo_split\nexport HF_TOKEN=$token\n\necho model_id_wo_repo=$model_id_wo_repo, model_id_wo_repo_split=$model_id_wo_repo_split\necho model_id=$model_id, local_dir=$local_dir, neuron_version=$neuron_version, model_store=$model_store\necho s3_bucket=$s3_bucket, prefix=$prefix, region=$region, role=$role\necho batch_size=$batch_size, num_neuron_cores=$num_neuron_cores, ml_instance_type=$ml_instance_type\necho HF_TOKEN=$token\necho script_path=$script_path\n\n# download the model\necho going to download model_id=$model_id, local_dir=$local_dir\necho Going into split and save with HF_token=$token\npython $script_path/scripts/split_and_save.py --model-name $model_id --save-path $local_dir \necho model download step completed\n\n#LLama3 tokenizer fix\ntokenizer_config_json=`find . -name tokenizer_config.json`\nsed -i 's/end_of_text/eot_id/g' $tokenizer_config_json\n\n#\"../2.18/model_store/Meta-Llama-3-8B-Instruct/Meta-Llama-3-8B-Instruct-split/\"\n# compile the model\necho starting model compilation...\npython $script_path/scripts/compile.py --action compile --batch-size $batch_size --num-neuron-cores $num_neuron_cores --model-dir $local_dir \necho done with model compilation\n\n# now upload the model binaries to the s3 bucket\necho going to upload from neuron_version/$neuron_version/$4/ to s3://$s3_bucket/$prefix/\naws s3 cp --recursive neuron_version/$neuron_version/$model_store/ s3://$s3_bucket/$prefix/\necho done with s3 upload\n\n# dir for storing model artifacts\nmodel_dir=smep-with-lmi/models/$model_id\nmkdir -p $model_dir\n# prepare serving.properties\nserving_prop_fpath=$model_dir/serving-inf2.properties\ncat << EOF > $serving_prop_fpath\n$serving_properties\nEOF\n\n# prepare model packaging script\nmodel_packaging_script_fpath=$model_dir/package-inf2.sh\ncat << EOF > $model_packaging_script_fpath\nmkdir mymodel\ncp serving-inf2.properties  mymodel/serving.properties\ntar czvf mymodel-inf2.tar.gz mymodel/\nrm -rf mymodel\naws s3 cp mymodel-inf2.tar.gz s3://${s3_bucket}/${prefix}/${model_id_wo_repo}/${model_id_wo_repo_split}/code/\nEOF\nchmod +x $model_packaging_script_fpath\n\n# now change director to the model dir we just created and run\n# the above model packaging script which creates a model.tar.gz that has\n# the serving.properties which in turn contains the model path in s3 and \n# other model parameters.\ncd $model_dir\necho now in `pwd`\n./package-inf2.sh\ncd -\necho now back in `pwd`\n\necho near the end of the script, we will deploy the model\npython $script_path/smep-with-lmi/deploy.py --device inf2 \\\n  --aws-region $region \\\n  --role-arn $role \\\n  --bucket $s3_bucket \\\n  --model-id $model_id \\\n  --prefix $prefix \\\n  --inf2-instance-type $ml_instance_type \\\n  --inf2-image-uri $ml_image_uri \\\n  --model-s3-uri s3://${s3_bucket}/${prefix}/${model_id_wo_repo}/${model_id_wo_repo_split}/code/mymodel-inf2.tar.gz \\\n  --neuronx-artifacts-s3-uri s3://${s3_bucket}/${prefix}/${model_id_wo_repo}/neuronx_artifacts \\\n  --script-path $script_path \\\n  --model-data-timeout $model_loading_timeout \\\n  --initial-instance-count $instance_count\n\necho all done\n\n\n\n\n================================================"
  },
  {
    "filename": "split_and_save.py",
    "path": "fmbench/scripts/compile-llm-for-aws-silicon/scripts/split_and_save.py",
    "directory": "fmbench/scripts/compile-llm-for-aws-silicon/scripts",
    "extension": "py",
    "content": "================================================\nimport os\nimport torch\nimport logging\nimport argparse\nfrom transformers.models.opt import OPTForCausalLM\nfrom transformers_neuronx.module import save_pretrained_split\nfrom transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n\nMODEL_REPO: str = \"meta-llama\"\nMODEL_ID: str = \"Meta-Llama-3-1-70bB-Instruct\"\nNEURON_VER: str = \"2.18.2\"\n\nroot = logging.getLogger()\nif root.handlers:\n    for handler in root.handlers:\n        root.removeHandler(handler)\nlogging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nif __name__ == \"__main__\":\n    \n    if 'HF_TOKEN' not in os.environ:\n        logger.info('Hugging Face Hub token is missing')\n        exit(-1)\n\n    # Define and parse command-line arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model-name\", \"-m\", \n        type=str, \n        default=f\"{MODEL_REPO}/{MODEL_ID}\",\n        help=\"HuggingFace model name\"\n    )\n    parser.add_argument(\n        \"--save-path\", \"-s\",\n        type=str,\n        default=f\"../{NEURON_VER}/model_store/{MODEL_ID}/{MODEL_ID}-split/\",\n        help=\"Output directory for downloaded model files\",\n    )\n    args = parser.parse_args()\n    logger.info(f\"args={args}\")\n\n    save_path = os.makedirs(args.save_path, exist_ok=True)\n    logger.info(f\"Save path defined for the model: {save_path}\")\n\n    # Load HuggingFace model\n    logger.info(f\"Going to load the hugging face model {args.model_name}\")\n    hf_model = AutoModelForCausalLM.from_pretrained(args.model_name, \n                                                    low_cpu_mem_usage=True)\n    logger.info(f\"Successfully loaded the model {args.model_name}\")\n\n\n    # Save the model\n    logger.info('Going to split and save the model')\n    save_pretrained_split(hf_model, args.save_path)\n    logger.info('Model splitted and saved locally')\n\n    # Load and save tokenizer for the model\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n    tokenizer.save_pretrained(args.save_path)\n    logger.info('Tokenizer saved locally')\n\n\n\n================================================"
  },
  {
    "filename": "deploy.py",
    "path": "fmbench/scripts/compile-llm-for-aws-silicon/smep-with-lmi/deploy.py",
    "directory": "fmbench/scripts/compile-llm-for-aws-silicon/smep-with-lmi",
    "extension": "py",
    "content": "================================================\nimport os\nimport sys\nimport boto3\nimport logging\nimport argparse\nimport sagemaker\nfrom sagemaker import Model\nfrom sagemaker.utils import name_from_base\nfrom pathlib import Path\n\nroot = logging.getLogger()\nif root.handlers:\n    for handler in root.handlers:\n        root.removeHandler(handler)\nlogging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nif __name__ == \"__main__\":\n    \n    # Define and parse command-line arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        help=\"Device name, allowed values \\\"inf2\\\" or \\\"gpu\\\"\"\n    )\n    parser.add_argument(\n        \"--aws-region\",\n        type=str,\n        default=\"us-east-1\",\n        help=\"AWS region, default=\\\"us-east-1\\\"\",\n    )\n\n    parser.add_argument(\n        \"--role-arn\",\n        type=str,\n        help=\"ARN for the role to be used to deploy the model\",\n    )\n\n    parser.add_argument(\n        \"--bucket\",\n        type=str,\n        help=\"S3 bucket name (without s3://) that contains the model binaries\",\n    )\n\n    parser.add_argument(\n        \"--model-id\",\n        type=str,\n        help=\"Model id\",\n    )\n\n    parser.add_argument(\n        \"--prefix\",\n        type=str,\n        default=\"lmi\",\n        help=\"S3 bucket prefix in which model binaries are kept\",\n    )\n\n    parser.add_argument(\n        \"--inf2-instance-type\",\n        type=str,\n        default=\"ml.inf2.24xlarge\",\n        help=\"Inf2 instance type for deploying the model\",\n    )\n\n    parser.add_argument(\n        \"--inf2-image-uri\",\n        type=str,\n        default=\"763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.27.0-neuronx-sdk2.18.1\",\n        help=\"Image URI for the Inf2 inference container\",\n    )\n\n    parser.add_argument(\n        \"--model-s3-uri\",\n        type=str,\n        help=\"S3 URI for model.tar.gzr\",\n    )\n\n    parser.add_argument(\n        \"--neuronx-artifacts-s3-uri\",\n        type=str,\n        help=\"S3 URI for Neuronx artifacts\",\n    )\n    parser.add_argument(\n        \"--script-path\",\n        type=str,\n        help=\"path to this script directory\",\n    )\n    parser.add_argument(\n        \"--initial-instance-count\",\n        type=int,\n        help=\"number of instances to start with\",\n    )\n    parser.add_argument(\n        \"--model-data-timeout\",\n        type=int,\n        help=\"increase the timeout to download large model\",\n    )\n\n    args = parser.parse_args()\n    logger.info(f\"args={args}\")\n\n\n    dev = args.device\n    aws_region = args.aws_region\n    os.environ['AWS_DEFAULT_REGION'] = aws_region\n    role = args.role_arn\n    bucket_name = args.bucket\n    model_id = args.model_id\n    prefix = args.prefix\n    s3_uri = args.model_s3_uri\n    neuronx_artifacts = args.neuronx_artifacts_s3_uri\n    script_path = args.script_path\n    if dev == 'gpu':        \n        instance_type = args.gpu_instance_type\n        image_uri = args.gpu_image_uri\n    elif dev == 'inf2':\n        instance_type = args.inf2_instance_type\n        image_uri = args.inf2_image_uri\n    else:\n        logger.error('Invalid device type')\n        sys.exit(-1)\n\n    model_name = f\"{model_id}-{dev}\".replace(\"/\", \"-\")\n    endpoint_name = sagemaker.utils.name_from_base(model_name).replace(\".\", \"-\")\n    logger.info(f\"going to deploy model_id={model_id}, endpoint={endpoint_name},\\\n                  s3_uri={s3_uri}\\\n                  aws_region={aws_region}\\\n                  role={role},\\\n                  bucket_name={bucket_name},\\\n                  prefix={prefix},\\\n                  instance_type={instance_type},\\\n                  image_uri={image_uri}\")\n\n    boto3_session=boto3.session.Session(region_name=aws_region)\n    smr = boto3.client('sagemaker-runtime')\n    sm = boto3.client('sagemaker')\n    # sagemaker session for interacting with different AWS APIs\n    sess = sagemaker.session.Session(boto3_session, \n                                    sagemaker_client=sm, \n                                    sagemaker_runtime_client=smr)\n        \n    logger.info(f'Deploying on {dev}')\n    logger.info(\"======================================\")\n    logger.info(f'Will load artifacts from {s3_uri}')\n    logger.info(\"======================================\")\n\n    logger.info(\"======================================\")\n    logger.info(f'Using Container image {image_uri}')\n    logger.info(\"======================================\")\n    model = Model(\n        name=endpoint_name,\n        # Enable SageMaker uncompressed model artifacts\n        model_data={\n            \"S3DataSource\": {\n                    \"S3Uri\": s3_uri,\n                    \"S3DataType\": \"S3Prefix\",\n                    \"CompressionType\": \"Gzip\",\n            }\n        },\n        image_uri=image_uri,\n        role=role,\n        env = {\n            \"NEURON_COMPILE_CACHE_URL\": neuronx_artifacts\n        },\n        sagemaker_session=sess\n        #env - set TS_INSTALL_PY_DEP_PER_MODEL to true, if you are using Pytorch serving\n        #this will tell server to run requirements.txt to deploy any additional packages\n    )\n    logger.info(model)\n\n    # Path to the endpoint.txt file in the higher directory\n    endpoint_file_path = os.path.join(script_path, \"endpoint.txt\")\n    logger.info(f'endpoint_file_path={endpoint_file_path}')\n\n    logger.info(f'\\nModel deployment initiated on {dev}\\nEndpoint Name: {endpoint_name}\\n')\n    if \"trn1\" in instance_type:\n        model.deploy(\n            initial_instance_count=1,\n            instance_type=instance_type,\n            endpoint_name=endpoint_name,\n            model_data_download_timeout=2400,  # increase the timeout to download large model\n            container_startup_health_check_timeout=2400,  # increase the timeout to load large model,\n            wait=True,\n        )\n    else:\n        model.deploy(\n            initial_instance_count=1,\n            instance_type=instance_type,\n            endpoint_name=endpoint_name,\n            volume_size=512, # not allowed for the selected Instance type ml.g5.12xlarge and ml.trn1.32xlarge\n            model_data_download_timeout=2400,  # increase the timeout to download large model\n            container_startup_health_check_timeout=2400,  # increase the timeout to load large model,\n            wait=True,\n        )   \n    logger.info(f'Model deployment on {dev}\\nEndpoint Name: {endpoint_name} finished\\n')\n    logger.info(f'Now writing the endpoint and end of DEPLOY')\n    # Write the endpoint name to the endpoint.txt file in the higher directory\n    Path(endpoint_file_path).write_text(endpoint_name)\n    \n \n\n\n================================================"
  },
  {
    "filename": "djl.py",
    "path": "fmbench/scripts/inference_containers/djl.py",
    "directory": "fmbench/scripts/inference_containers",
    "extension": "py",
    "content": "================================================\n\"\"\"\nDJL specific code\n\"\"\"\nimport os\nfrom pathlib import Path\nimport fmbench.scripts.constants as constants\nfrom typing import Dict, List, Optional, Tuple, Union\nfrom fmbench.scripts.inference_containers.utils import (STOP_AND_RM_CONTAINER,\n                                                        FMBENCH_MODEL_CONTAINER_NAME)\n\nCONFIG_PROPERTIES: str = \"\"\"\n            inference_address=http://0.0.0.0:{port}\n            management_address=http://0.0.0.0:{port}\n            cluster_address=http://0.0.0.0:8888\n            model_store=/opt/ml/model\n            load_models=ALL\n            \"\"\"\n\ndef create_script(region, image_uri, model_id, model_name, env_str, privileged_str, hf_token, directory):\n    \"\"\"\n    Script for running the docker container for the inference server\n    \"\"\"\n    script = f\"\"\"#!/bin/sh\n        echo \"Going to download model container\"\n        echo \"Content in docker command: {region}, {image_uri}, {model_name}\"\n\n        # Login to AWS ECR and pull the Docker image\n        aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin {image_uri}\n        docker pull {image_uri}       \n\n        # Run the new Docker container with specified settings\n        cd {directory}\n        # shutdown existing docker compose\n        docker compose down\n        {STOP_AND_RM_CONTAINER}\n        docker compose up -d\n        cd -\n        echo \"started docker compose in daemon mode\"\n    \"\"\"\n    return script\n\ndef create_djl_service(model_id: str, \n                        num_model_copies: int, \n                        devices_per_model: int, \n                        image: str, \n                        user: str, \n                        shm_size: str, \n                        env: List, \n                        base_port: int, \n                        accelerator: constants.ACCELERATOR_TYPE) -> Tuple:\n    \"\"\"\n    Creates the service for DJL, setting up devices and other configurations.\n    \"\"\"\n    try:\n        cnames: List = []\n        services: Dict = {}\n        per_container_info_list: List = []\n        home = str(Path.home())\n        dir_path_on_host: str = os.path.join(home, Path(model_id).name)\n        \n        # Iterate through the number of model copies and prepare the docker service for djl\n        for i in range(num_model_copies):\n            cname = f\"fmbench_model_container_{i+1}\"\n            cnames.append(cname)\n\n            device_offset = devices_per_model * i\n\n            if accelerator == constants.ACCELERATOR_TYPE.NEURON:\n                devices = [f\"/dev/neuron{j + device_offset}:/dev/neuron{j}\" for j in range(devices_per_model)]\n                extra_env = []\n            else:\n                devices = None\n                gpus = \",\".join([str(j + device_offset) for j in range(devices_per_model)])\n                extra_env = [f\"NVIDIA_VISIBLE_DEVICES={gpus}\"]\n                env = env + extra_env\n\n            volumes = [f\"{dir_path_on_host}/i{i+1}:/opt/ml/model:ro\",\n                       f\"{dir_path_on_host}/i{i+1}/conf:/opt/djl/conf:ro\",\n                       f\"{dir_path_on_host}/i{i+1}/model_server_logs:/opt/djl/logs\"]\n            # compute the port\n            port = base_port + i\n\n            service = {\n                cname: {\n                    \"image\": image,\n                    \"container_name\": cname,\n                    \"user\": user,\n                    \"shm_size\": shm_size,\n                    \"devices\": devices,\n                    \"environment\": env,\n                    \"volumes\": volumes,\n                    \"ports\": [f\"{port}:{port}\"],\n                    \"deploy\": {\"restart_policy\": {\"condition\": \"on-failure\"}}\n                }\n            }\n            if accelerator == constants.ACCELERATOR_TYPE.NVIDIA:\n                service[cname]['runtime'] = constants.ACCELERATOR_TYPE.NVIDIA.value\n                service[cname].pop(\"devices\")\n            services.update(service)\n            config_properties = CONFIG_PROPERTIES.format(port=port)\n\n\n            nginx_server_lines = [f\"        server {cname}:{port};\"]\n    \n            per_container_info_list.append(dict(dir_path_on_host=f\"{dir_path_on_host}/i{i+1}\",\n                                                config_properties=config_properties,\n                                                container_name=cname,\n                                                nginx_server_lines=nginx_server_lines))\n    except Exception as e:\n        logger.error(f\"Error occurred while generating configuration files for djl/vllm: {e}\")\n        services, per_container_info_list=None, None\n    # since the djl container comes up immediately and the nginx lb does not have to wait\n    # so the nginx command is a noop\n    nginx_command = None\n    return services, per_container_info_list, nginx_command\n\n\n\n================================================"
  },
  {
    "filename": "ollama.py",
    "path": "fmbench/scripts/inference_containers/ollama.py",
    "directory": "fmbench/scripts/inference_containers",
    "extension": "py",
    "content": "================================================\n\"\"\"\nollama specific code\n\"\"\"\nimport logging\nfrom fmbench.scripts.inference_containers.utils import (STOP_AND_RM_CONTAINER,\n                                                        FMBENCH_MODEL_CONTAINER_NAME)\n\nlogging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef create_script(region, image_uri, model_id, model_name, env_str, privileged_str, hf_token, directory):\n    \"\"\"\n    Script for running the docker container for the inference server\n    \"\"\"\n    script = f\"\"\"#!/bin/sh\n\n        {STOP_AND_RM_CONTAINER}\n\n        # Check for CUDA devices and enable them\n        echo \"Checking for CUDA devices...\"\n        if command -v nvidia-smi &> /dev/null; then\n            gpu_count=`nvidia-smi --list-gpus | wc -l`\n            gpu_count=$((gpu_count - 1))\n            gpus_to_enable=`seq -s, 0 $gpu_count`\n            export CUDA_VISIBLE_DEVICES=$gpus_to_enable\n            echo \"CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES\"            \n        else\n            echo \"nvidia-smi not found. Running in CPU mode\"\n        fi\n    \n        # check if ollama is already installed\n        ollama -v\n        if [ $? -ne 0 ]; then\n            echo \"Ollama is not installed, going to install it now\"\n            # Install ollama\n            curl -fsSL https://ollama.com/install.sh | sh            \n        fi        \n        \n        # Stop ollama\n        systemctl stop ollama.service\n\n        # Pull the specified model using Ollama\n        echo \"Pulling model: {model_id}...\"\n        ollama pull {model_id}\n        if [ $? -ne 0 ]; then\n            echo \"Error: Failed to pull model {model_id}.\"\n            exit 1\n        fi\n\n        # Serve the specified model using Ollama\n        echo \"Starting to serve model: {model_id}...\"\n        ollama serve {model_id} &\n        if [ $? -ne 0 ]; then\n            echo \"Error: Failed to serve model {model_id}.\"\n            exit 1\n        fi\n\n        echo \"Successfully serving model_id = {model_id} with Ollama.\"\n    \"\"\"\n    return script\n\n\n\n================================================"
  },
  {
    "filename": "sglang.py",
    "path": "fmbench/scripts/inference_containers/sglang.py",
    "directory": "fmbench/scripts/inference_containers",
    "extension": "py",
    "content": "================================================\n#!/usr/bin/env python3\n\"\"\"\nsglang.py\n\nThis module provides a helper function to create a shell script for deploying\nthe SGLang server with the DeepSeek-R1 models on sglang.\n\nThe generated script will:\n  - Stop and remove any existing container with the same name.\n  - Pull the SGLang Docker image.\n  - Launch the container with the hardcoded model and runtime parameters.\n\"\"\"\n\nimport logging\nfrom fmbench.scripts.inference_containers.utils import (STOP_AND_RM_CONTAINER,\n                                                        FMBENCH_MODEL_CONTAINER_NAME)\n\nlogging.basicConfig(\n    format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s',\n    level=logging.INFO\n)\nlogger = logging.getLogger(__name__)\n\ndef create_script(region, image_uri, model_id, model_name, env_str, privileged_str, hf_token, directory, cli_params=\"\"):\n    \"\"\"\n    Create a shell script for deploying the SGLang server.\n\n    Parameters:\n        region (str): Deployment region (not used directly here).\n        image_uri (str): Docker image URI for SGLang (e.g. \"lmsysorg/sglang:latest\").\n        model_id (str): The Hugging Face model identifier \n                        (e.g. \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\").\n        model_name (str): A name for the model (used in naming the container).\n        env_str (str): Additional environment variables (not used here).\n        privileged_str (str): Docker flag for privileged mode (e.g. \"--privileged\").\n        hf_token (str): Hugging Face token (not required for SGLang).\n        directory (str): Directory where the deployment script will reside.\n        cli_params (str): Extra CLI parameters to pass to the SGLang server command.\n\n    Returns:\n        str: The content of the shell deployment script.\n    \"\"\"\n    server_port = \"30000\"\n    # The model_id and image_uri are passed in, but here we assume they are hardcoded values.\n    script = f\"\"\"#!/bin/sh\n# Stop and remove any existing container with the same name\n{STOP_AND_RM_CONTAINER}\n\necho \"Pulling SGLang Docker image: {image_uri}\"\ndocker pull {image_uri}\n\necho \"Launching SGLang container with model: {model_id} on port: {server_port}\"\ndocker run -d --rm --gpus all --shm-size 32g \\\\\n    -p {server_port}:{server_port} \\\\\n    -v $HOME/.cache/huggingface:/root/.cache/huggingface \\\\\n    --ipc host --network host {privileged_str} \\\\\n    --name {FMBENCH_MODEL_CONTAINER_NAME} {image_uri} \\\\\n    python3 -m sglang.launch_server \\\\\n    --model {model_id} --trust-remote-code --port {server_port} {cli_params}\n\n\"\"\"\n    logger.info(\"SGLang deployment script created.\")\n    return script\n\n\n\n================================================"
  },
  {
    "filename": "triton.py",
    "path": "fmbench/scripts/inference_containers/triton.py",
    "directory": "fmbench/scripts/inference_containers",
    "extension": "py",
    "content": "================================================\n\"\"\"\nTriton specific code\n\"\"\"\nimport os\nimport json\nimport stat\nimport shutil\nimport logging\nfrom pathlib import Path\nfrom typing import Tuple, List\nimport fmbench.scripts.constants as constants\nfrom typing import Dict, List, Optional, Tuple, Union\nfrom fmbench.scripts.inference_containers.utils import (STOP_AND_RM_CONTAINER,\n                                                        FMBENCH_MODEL_CONTAINER_NAME)\n\nlogging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nCONFIG_PROPERTIES: str = \"\"\"\n            inference_address=http://0.0.0.0:{port}\n            management_address=http://0.0.0.0:{port}\n            cluster_address=http://0.0.0.0:8888\n            model_store=/opt/ml/model\n            load_models=ALL\n            \"\"\"\n\ndef create_script(region, image_uri, model_id, model_name, env_str, privileged_str, hf_token, directory):\n    \"\"\"\n    Script for running the docker container for the inference server\n    \"\"\"\n    script = f\"\"\"#!/bin/sh\n        echo \"Going to download model container\"\n        echo \"Content in docker command: {region}, {image_uri}, {model_name}\"\n        \n        # using the locally built triton image\n\n        # Run the new Docker container with specified settings\n        cd {directory}\n        # shutdown existing docker compose        \n        docker compose down\n        {STOP_AND_RM_CONTAINER}\n\n        # download the model\n        export HF_TOKEN={hf_token}\n        pip freeze | grep huggingface-hub\n        RESULT=$?\n        if [ $RESULT -eq 0 ]; then\n          echo huggingface-hub is already installed..\n        else\n          echo huggingface-hub is not installed, going to install it now\n          pip install -U \"huggingface_hub[cli]\"\n        fi\n\n        huggingface-cli download {model_id} --local-dir $HOME/{model_id}\n\n        # bring up the inference container and load balancer\n        docker compose up -d\n        cd -\n        echo \"started docker compose in daemon mode\"\n    \"\"\"\n    return script\n\n\ndef handle_triton_serving_properties_and_inf_params(triton_dir: str,\n                                                    tp_degree: int,\n                                                    container_params: Dict,\n                                                    hf_model_id: str, \n                                                    backend: constants.BACKEND):\n    \"\"\"\n    Substitutes parameters within the triton model repository files for different container types\n    For both containers (djl and vllm), the only file that is created based on custom user provided\n    metrics is the \"model.json\" file. Other files (config.pbtxt and model.py) remain unchanged. The\n    model.json file contains the model id, tp degree, and another set of container parameters provided\n    in the configuration file specific to the djl/vllm containers\n    \"\"\"\n    try:\n        model_json_file: str = \"model.json\"\n        model_json_content: Dict = {}\n        model_json_path: str = os.path.join(triton_dir, model_json_file)\n        if container_params is None or container_params == {}:\n            logger.error(f\"container_params is not specified, fix the config file to proceed\")\n            ValueError(\"container_params is not specified, fix the config file to proceed\")\n        # if there is a tp degree in inference container parameters, delete it since those will already be added here. TP\n        # degree is required in the container_params within the configuration file        \n        del container_params['tp_degree']\n        # if serving.properties are provided in the inference container parameters, then pop \n        # it out, since it is not needed for deploying the model using triton on neuron\n        if 'serving.properties' in container_params:\n            del container_params['serving.properties']\n            logger.info(f\"Removed the serving properties for deploying the model on triton on neuron: {container_params}\")\n        else:\n            logger.info(f\"No serving properties provided, using the inference model parameters directly in model deployment\")\n        if backend == constants.BACKEND.VLLM_BACKEND:\n            logger.info(f\"Handling parameter subsitution for {backend}\")\n            with open(model_json_path, \"w\") as f:\n                model_json_content['model'] = hf_model_id\n                model_json_content[\"tensor_parallel_size\"] = tp_degree\n                # update the model.json to contain additional variables, such as\n                # max_num_seqs, max_model_len, batch_size and more\n                model_json_content.update(container_params)\n                json.dump(model_json_content, f, indent=2)\n                logger.info(f\"Updated {model_json_path}, model.json={model_json_content}\")\n        elif backend == constants.BACKEND.DJL_BACKEND:\n            logger.info(f\"Handling parameter subsitution for {backend}\")\n            with open(model_json_path, \"w\") as f:\n                model_json_content['model_id'] = hf_model_id\n                model_json_content[\"tensor_parallel_degree\"] = tp_degree\n                # update the model.json to contain additional variables, such as\n                # max_num_seqs, max_model_len, batch_size and more\n                model_json_content.update(container_params)\n                json.dump(model_json_content, f, indent=2)\n                logger.info(f\"Updated {model_json_path}, model.json={model_json_content}\")\n        else:\n            # If there are no backend container options for deploying on triton, throw an exception\n            logger.info(f\"No backend option provided for triton. Please use a valid backend type (vllm/djl) for the model.json file to be created. Backend provided={backend}\")\n            return\n    except Exception as e:\n        raise Exception(f\"Error occurred while preparing files for the triton model container: {e}\")\n\n\ndef _create_triton_service_neuron(model_id: str, \n                          num_model_copies: int, \n                          devices_per_model: int, \n                          image: str,\n                          user: str, \n                          shm_size: str, \n                          env: List,\n                          base_port: int, \n                          accelerator: constants.ACCELERATOR_TYPE, \n                          backend: constants.BACKEND) -> Tuple:\n    \"\"\"\n    Creates the Triton service-specific part of the docker compose file. This function is responsible for handling the volume\n    mounting, and other aspects to the docker compose file, such as the entrypoint command, port mapping, and more.\n    \"\"\"\n    try:\n        # initialize the cnames, services dictionary for triton on neuron, \n        # and more \n        cnames: List = []\n        services: Dict = {}\n        per_container_info_list: List = []\n        ports_per_model_server: int = 3 # http, grps, metrics\n        home = str(Path.home())\n        triton_content_dir: Optional[str] = None\n        triton_inference_script: Optional[str] = None\n        dir_path_on_host: str = os.path.join(home, Path(model_id).name)\n\n        if backend == constants.BACKEND.VLLM_BACKEND:\n            logger.info(f\"Setting triton_content_dir to {constants.TRITON_CONTENT_DIR_NAME_VLLM}\")\n            triton_content_dir = constants.TRITON_CONTENT_DIR_NAME_VLLM\n            triton_inference_script = constants.TRITON_INFERENCE_SCRIPT_VLLM\n        elif backend == constants.BACKEND.DJL_BACKEND:\n            logger.info(f\"Setting triton_content_dir to {constants.TRITON_CONTENT_DIR_NAME_DJL}\")\n            triton_content_dir = constants.TRITON_CONTENT_DIR_NAME_DJL\n            triton_inference_script = constants.TRITON_INFERENCE_SCRIPT_DJL\n        else: \n            logger.error(f\"Triton content directory for {backend} not provided.\")\n            return\n        \n        # Iterate through the number of model copies and prepare the docker service for triton\n        for i in range(num_model_copies):\n            cname: str = f\"fmbench_model_container_{i+1}\"\n            cnames.append(cname)\n\n            device_offset = devices_per_model * i\n\n            if accelerator == constants.ACCELERATOR_TYPE.NEURON:\n                devices = [f\"/dev/neuron{j + device_offset}:/dev/neuron{j}\" for j in range(devices_per_model)]\n                extra_env = []\n            else:\n                devices = None\n                gpus = \",\".join([str(j + device_offset) for j in range(devices_per_model)])\n                extra_env = [f\"NVIDIA_VISIBLE_DEVICES={gpus}\"]\n                env = env + extra_env\n\n            # Setup Triton model content for each instance\n            instance_dir: str = os.path.join(dir_path_on_host, f\"i{i+1}\")\n            triton_instance_dir: str = os.path.join(instance_dir, triton_content_dir)\n            os.makedirs(triton_instance_dir, exist_ok=True)\n            current_dir: str = os.path.dirname(os.path.realpath(__file__))\n            parent_dir: str = os.path.abspath(os.path.join(current_dir, os.pardir))\n            triton_content: str = os.path.join(parent_dir, triton_content_dir)\n\n            # Copy all files from triton_content to the instance's triton directory\n            for item in os.listdir(triton_content):\n                s = os.path.join(triton_content, item)\n                d = os.path.join(triton_instance_dir, item)\n                if os.path.isfile(s):\n                    shutil.copy2(s, d)\n                elif os.path.isdir(s):\n                    shutil.copytree(s, d, dirs_exist_ok=True)\n            # Set execute permissions for the script. The triton volumes contain the model repostory that\n            # are mapped into the container and used during deployment\n            # os.chmod((triton_instance_dir), 0o755)\n            os.chmod(os.path.join(triton_instance_dir, os.path.basename(triton_inference_script)), 0o755)\n            volumes = [f\"{triton_instance_dir}:/scripts/triton:rw\",\n                       f\"{triton_instance_dir}:/triton:rw\"]\n            # Create the Triton service\n            total_neuron_cores = num_model_copies * devices_per_model * 2\n            service = {\n                cname: {\n                    \"image\": image,\n                    \"container_name\": cname,\n                    \"shm_size\": shm_size,\n                    \"devices\": devices,\n                    \"environment\": env,\n                    \"volumes\": volumes,\n                    \"ports\": [f\"{base_port + i*ports_per_model_server}:{base_port + i*ports_per_model_server}\"],\n                    \"deploy\": {\"restart_policy\": {\"condition\": \"on-failure\"}},\n                    \"command\": f\"{triton_inference_script} {model_id} {Path(model_id).name} {base_port + i*ports_per_model_server} {total_neuron_cores}\"\n                }\n            }\n            services.update(service)\n            # for the triton container we could have multiple model servers within the same container\n            nginx_server_lines = [f\"        server {cname}:{base_port + i*ports_per_model_server};\"]\n            config_properties = CONFIG_PROPERTIES.format(port=(base_port + i*ports_per_model_server))\n            per_container_info_list.append(dict(dir_path_on_host=instance_dir, \n                                                config_properties=config_properties, \n                                                container_name=cname,\n                                                nginx_server_lines=nginx_server_lines))\n    except Exception as e:\n        logger.error(f\"Error occurred while creating configuration files for triton: {e}\")\n        services, per_container_info_list = None, None\n    nginx_command = \"sh -c \\\"echo going to sleep for 240s && sleep 240 && echo after sleep && nginx -g \\'daemon off;\\' && echo started nginx\\\"\"\n    return services, per_container_info_list, nginx_command\n\n\ndef _create_triton_service_gpu(model_id: str, \n                          num_model_copies: int, \n                          devices_per_model: int, \n                          image: str, \n                          user: str, \n                          shm_size: str, \n                          env: List,\n                          base_port: int, \n                          accelerator: constants.ACCELERATOR_TYPE,\n                          tp_degree: int,\n                          batch_size: int) -> Tuple:\n    \"\"\"\n    Creates the Triton service-specific part of the docker compose file. This function is responsible for handling the volume\n    mounting, and other aspects to the docker compose file, such as the entrypoint command, port mapping, and more.\n    \"\"\"\n    try:\n        \"\"\"\n        the gpu version of services looks like this\n        services:\n    fmbench_model_container:\n        image: nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3\n        container_name: fmbench_model_container  # Name of the container\n        runtime: nvidia  # Enables GPU support for the container\n        shm_size: 12g  # Shared memory size\n        ulimits:\n          memlock: -1  # Remove memory locking limits\n          stack: 67108864  # Set stack size\n        ports:\n        # sufficient for upto 4 instances of the model\n        # add more if needed\n        - 8000:8000\n        - 8003:8003\n        - 8006:8006\n        - 8009:8009\n        deploy:\n          resources:\n            reservations:\n              devices:\n                - driver: nvidia\n                  count: all  # Use all available GPUs\n                  capabilities: [gpu]\n        volumes:\n        - ${HOME}/tensorrtllm_backend:/tensorrtllm_backend\n        - ${HOME}/${MODEL_ID}:/${MODEL_ID}\n        - ${HOME}/engines:/engines\n        - ${HOME}/deploy_on_triton/scripts:/scripts\n        #network_mode: host  # Use the host's network stack\n        tty: true  # Allocate a pseudo-TTY (interactive terminal)\n        command: bash -c \"/scripts/serve_model.sh ${MODEL_ID} ${TP_DEGREE} ${BATCH_SIZE} ${MODEL_COPIES} && bash\"  # Run script and keep the container alive with bash\n        restart: on-failure  # Ensure container restarts if it stops unexpectedly\n        \"\"\"\n        cnames: List = []\n        services: Dict = {}\n        per_container_info_list: List = []\n        home = str(Path.home())\n        dir_path_on_host: str = os.path.join(home, Path(model_id).name)\n        \n        cname: str = FMBENCH_MODEL_CONTAINER_NAME\n        ports_per_model_server: int = 3 # http, grps, metrics\n        logger.info(f\"_create_triton_service_gpu, model_id=\\\"{model_id}\\\", home=\\\"{home}\\\", dir_path_on_host=\\\"{dir_path_on_host}\\\", \"\n                    f\"tp_degree=\\\"{tp_degree}\\\", batch_size=\\\"{batch_size}\\\", num_model_copies=\\\"{num_model_copies}\\\"\")\n        volumes = [f\"{home}/tensorrtllm_backend:/tensorrtllm_backend\",\n                   f\"{home}/{model_id}:/{model_id}\",\n                   f\"{dir_path_on_host}/engines:/engines\",\n                   f\"{dir_path_on_host}/scripts:/scripts\"]\n        \n        # copy trtiton serving script\n        triton_scripts_dir = os.path.join(dir_path_on_host, \"scripts\")\n        os.makedirs(triton_scripts_dir, exist_ok=True)\n        triton_serve_model_script_dst_path = os.path.join(triton_scripts_dir, constants.TRITON_SERVE_SCRIPT)\n        script_dir_path = Path( __file__ ).parent.absolute()\n        triton_serve_model_script_src_path = os.path.join(script_dir_path, constants.TRITON_SERVE_SCRIPT)\n        logger.info(f\"going to copy {triton_serve_model_script_src_path} to {triton_serve_model_script_dst_path}\")\n        shutil.copyfile(triton_serve_model_script_src_path, triton_serve_model_script_dst_path)\n        st = os.stat(triton_serve_model_script_dst_path)\n        os.chmod(triton_serve_model_script_dst_path, st.st_mode | stat.S_IEXEC)\n\n        # setup the services section\n        service = {\n                cname: {\n                    \"image\": image,\n                    \"container_name\": cname,\n                    \"shm_size\": shm_size,\n                    \"ulimits\": {\"memlock\": -1, \"stack\": 67108864},\n                    \"volumes\": volumes,\n                    \"ports\": [f\"{base_port + i*ports_per_model_server}:{base_port + i*ports_per_model_server}\" for i in range(num_model_copies)],\n                    \"deploy\": {\"resources\": {\"reservations\": {\"devices\": [{\"driver\": \"nvidia\", \"count\": \"all\", \"capabilities\": ['gpu']}]}}},\n                    \"tty\": True,                    \n                    \"command\": f\"bash -c \\\"/scripts/{constants.TRITON_SERVE_SCRIPT} {model_id} {tp_degree} {batch_size} {num_model_copies} {base_port} && bash\\\"\",\n                    \"restart\": \"on-failure\"\n                }\n            }\n        services.update(service)\n        # for the triton container we could have multiple model servers within the same container\n        nginx_server_lines = [f\"        server {cname}:{base_port + i*ports_per_model_server};\" for i in range(num_model_copies)]\n        per_container_info_list.append(dict(dir_path_on_host=dir_path_on_host,\n                                            config_properties=None,\n                                            container_name=cname,\n                                            nginx_server_lines=nginx_server_lines))\n    except Exception as e:\n        logger.error(f\"Error occurred while creating configuration files for triton: {e}\")\n        services, per_container_info_list = None, None\n    # ask the nginx lb to wait for a few minutes for the triton container to come up\n    nginx_command = \"sh -c \\\"echo going to sleep for 240s && sleep 240 && echo after sleep && nginx -g \\'daemon off;\\' && echo started nginx\\\"\"\n    return services, per_container_info_list, nginx_command\n\ndef create_triton_service(model_id: str, \n                          num_model_copies: int, \n                          devices_per_model: int, \n                          image: str, \n                          user: str, \n                          shm_size: str, \n                          env: List,\n                          base_port: int, \n                          accelerator: constants.ACCELERATOR_TYPE,\n                          tp_degree: int,\n                          batch_size: int,\n                          backend: constants.BACKEND) -> Tuple:\n    \"\"\"\n    Creates the Triton service-specific part of the docker compose file. This function is responsible for handling the volume\n    mounting, and other aspects to the docker compose file, such as the entrypoint command, port mapping, and more.\n    \"\"\"\n\n    if accelerator == constants.ACCELERATOR_TYPE.NEURON:\n        logger.info(f\"accelerator={accelerator}, calling the neuron version of this function\")\n        return _create_triton_service_neuron(model_id, \n                          num_model_copies, \n                          devices_per_model, \n                          image, \n                          user, \n                          shm_size, \n                          env,\n                          base_port, \n                          accelerator,\n                          backend) \n    else:\n        logger.info(f\"accelerator={accelerator}, calling the gpu version of this function\")\n        return _create_triton_service_gpu(model_id, \n                          num_model_copies, \n                          devices_per_model, \n                          image, \n                          user, \n                          shm_size, \n                          env,\n                          base_port, \n                          accelerator,\n                          tp_degree,\n                          batch_size) \n\n\n\n\n\n================================================"
  },
  {
    "filename": "triton_serve_model.sh",
    "path": "fmbench/scripts/inference_containers/triton_serve_model.sh",
    "directory": "fmbench/scripts/inference_containers",
    "extension": "sh",
    "content": "================================================\nHF_MODEL=/$1\nUNIFIED_CKPT_PATH=/tmp/ckpt/$HF_MODEL\nENGINE_DIR=/engines\nCONVERT_CHKPT_SCRIPT=/tensorrtllm_backend/tensorrt_llm/examples/llama/convert_checkpoint.py\nTP_DEGREE=$2\nWORKERS=$TP_DEGREE\nBATCH_SIZE=$3\nWORLD_SIZE=$TP_DEGREE\nMODEL_COPIES=$4\nHTTP_PORT=$5\nGRPC_PORT=$((HTTP_PORT + 1))\nMETRICS_PORT=$((HTTP_PORT + 2))\n\n\necho HF_MODEL=$HF_MODEL, UNIFIED_CKPT_PATH=$UNIFIED_CKPT_PATH, ENGINE_DIR=$ENGINE_DIR, CONVERT_CHKPT_SCRIPT=$CONVERT_CHKPT_SCRIPT\necho TP_DEGREE=$TP_DEGREE, WORKERS=$WORKERS, BATCH_SIZE=$BATCH_SIZE, WORLD_SIZE=$WORLD_SIZE, HTTP_PORT=$HTTP_PORT, GRPC_PORT=$GRPC_PORT, METRICS_PORT=$METRICS_PORT\n\ncmd=\"python3 ${CONVERT_CHKPT_SCRIPT} --model_dir ${HF_MODEL} --output_dir ${UNIFIED_CKPT_PATH} --dtype float16 --tp_size $TP_DEGREE --workers $WORKERS\"\necho going to run $cmd\n$cmd\n\ncmd=\"trtllm-build --checkpoint_dir ${UNIFIED_CKPT_PATH} --remove_input_padding enable --gpt_attention_plugin float16 --context_fmha enable --gemm_plugin float16 --output_dir ${ENGINE_DIR} --max_batch_size $BATCH_SIZE --workers $WORKERS\"\necho going to run $cmd\n$cmd\n\ninflight_batcher_llm=\"/tensorrtllm_backend/all_models/inflight_batcher_llm\"\nTRITON_SERVER_DIR=\"/opt/tritonserver/.\"\necho going to copy $inflight_batcher_llm to $inflight_batcher_llm\ncp -R $inflight_batcher_llm $TRITON_SERVER_DIR\n\n# preprocessing\nTOKENIZER_DIR=$HF_MODEL/\nTOKENIZER_TYPE=auto\nENGINE_DIR=/engines\nDECOUPLED_MODE=false\nMODEL_FOLDER=/opt/tritonserver/inflight_batcher_llm\nMAX_BATCH_SIZE=$BATCH_SIZE\nINSTANCE_COUNT=1\nMAX_QUEUE_DELAY_MS=10000\nTRITON_BACKEND=tensorrtllm\nFILL_TEMPLATE_SCRIPT=/tensorrtllm_backend/tools/fill_template.py\necho \"going to run a whole bunch of pre and post processing commands\"\npython3 ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/preprocessing/config.pbtxt tokenizer_dir:${TOKENIZER_DIR},tokenizer_type:${TOKENIZER_TYPE},triton_max_batch_size:${MAX_BATCH_SIZE},preprocessing_instance_count:${INSTANCE_COUNT}\npython3 ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/postprocessing/config.pbtxt tokenizer_dir:${TOKENIZER_DIR},tokenizer_type:${TOKENIZER_TYPE},triton_max_batch_size:${MAX_BATCH_SIZE},postprocessing_instance_count:${INSTANCE_COUNT}\npython3 ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/tensorrt_llm_bls/config.pbtxt triton_max_batch_size:${MAX_BATCH_SIZE},decoupled_mode:${DECOUPLED_MODE},bls_instance_count:${INSTANCE_COUNT}\npython3 ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/ensemble/config.pbtxt triton_max_batch_size:${MAX_BATCH_SIZE}\npython3 ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/tensorrt_llm/config.pbtxt triton_backend:${TRITON_BACKEND},triton_max_batch_size:${MAX_BATCH_SIZE},decoupled_mode:${DECOUPLED_MODE},engine_dir:${ENGINE_DIR},max_queue_delay_microseconds:${MAX_QUEUE_DELAY_MS},batching_strategy:inflight_fused_batching,exclude_input_in_output:True\n# from this blog https://developer.nvidia.com/blog/turbocharging-meta-llama-3-performance-with-nvidia-tensorrt-llm-and-nvidia-triton-inference-server/\n#python3 ${FILL_TEMPLATE_SCRIPT} -i ${MODEL_FOLDER}/tensorrt_llm/config.pbtxt triton_backend:${TRITON_BACKEND},triton_max_batch_size:${MAX_BATCH_SIZE},decoupled_mode:${DECOUPLED_MODE},engine_dir:${ENGINE_DIR},max_queue_delay_microseconds:${MAX_QUEUE_DELAY_MS},batching_strategy:inflight_fused_batching,max_beam_width:1,max_tokens_in_paged_kv_cache:2560,max_attention_window_size:2560,kv_cache_free_gpu_mem_fraction:0.5,exclude_input_in_output:True,enable_kv_cache_reuse:False\n\necho \"after running a whole bunch of pre and post processing commands\"\n\n# start triton servers\nfor ((i=0; i<$MODEL_COPIES; i++)); do\n  # Calculate the start and end GPU IDs based on tp_degree\n  start_id=$((i * TP_DEGREE))\n  end_id=$((start_id + TP_DEGREE - 1))\n  \n  # Create the CUDA_VISIBLE_DEVICES string as a comma-separated list\n  cvd=$(seq -s, $start_id $end_id)\n\n  # Adjust port numbers for each iteration (increment by 3)\n  http_port=$((HTTP_PORT + i * 3))\n  grpc_port=$((GRPC_PORT + i * 3))\n  metrics_port=$((METRICS_PORT + i * 3))\n\n  echo \"Starting server $((i + 1))\"\n  echo \"CUDA_VISIBLE_DEVICES=$cvd\"\n\n  # Construct the command with updated CUDA_VISIBLE_DEVICES and port numbers\n  export CUDA_VISIBLE_DEVICES=$cvd\n  cmd=\"python3 /tensorrtllm_backend/scripts/launch_triton_server.py \\\n    --world_size=$WORLD_SIZE --model_repo=/opt/tritonserver/inflight_batcher_llm \\\n    --http_port $http_port --grpc_port $grpc_port --metrics_port $metrics_port\"\n  \n  echo \"Going to start the Triton server with the following command: $cmd\"\n  \n  # Execute the command\n  $cmd\n\n  # wait for a few seconds so that the previous instance is able to bind to the GPUs\n  sleep 10\ndone\n\n\n\n================================================"
  },
  {
    "filename": "utils.py",
    "path": "fmbench/scripts/inference_containers/utils.py",
    "directory": "fmbench/scripts/inference_containers",
    "extension": "py",
    "content": "================================================\n\"\"\"\nUtility functions common across inference containers\n\"\"\"\nimport json\nimport logging\nimport subprocess\nimport fmbench.scripts.constants as constants\nfrom typing import Dict, List, Optional, Tuple, Union\n\nlogging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nFMBENCH_MODEL_CONTAINER_NAME: str = \"fmbench_model_container\"\n\nSTOP_AND_RM_CONTAINER = f\"\"\"\n    # Attempt to stop and remove the container up to 3 times if container exists\n        if [ -n \"$(docker ps -aq --filter \"name={FMBENCH_MODEL_CONTAINER_NAME}\")\" ]; then\n            for i in {{1..3}}; do\n                echo \"Attempt $i to stop and remove the container: {FMBENCH_MODEL_CONTAINER_NAME}\"\n                \n                # Stop the container\n                docker ps -q --filter \"name={FMBENCH_MODEL_CONTAINER_NAME}\" | xargs -r docker stop\n                \n                # Wait for 5 seconds\n                sleep 5\n                \n                # Remove the container\n                docker ps -aq --filter \"name={FMBENCH_MODEL_CONTAINER_NAME}\" | xargs -r docker rm\n                \n                # Wait for 5 seconds\n                sleep 5\n                \n                # Check if the container is removed\n                if [ -z \"$(docker ps -aq --filter \"name={FMBENCH_MODEL_CONTAINER_NAME}\")\" ]; then\n                    echo \"Container {FMBENCH_MODEL_CONTAINER_NAME} successfully stopped and removed.\"\n                    break\n                else\n                    echo \"Container {FMBENCH_MODEL_CONTAINER_NAME} still exists, retrying...\"\n                fi\n            done\n        else\n            echo \"Container {FMBENCH_MODEL_CONTAINER_NAME} does not exist. No action taken.\"\n        fi\n    \"\"\"\n\n\ndef get_accelerator_type() -> constants.ACCELERATOR_TYPE:\n    # first check if this is an NVIDIA instance or a AWS Chips instance\n    # we do this by checking for nvidia-smi and neuron-ls, they should be\n    # present on NVIDIA and AWS Chips instances respectively\n    NVIDIA_SMI: str = \"nvidia-smi\"\n    NEURON_LS: str = \"neuron-ls\"\n    # if the utility is not present the return from the whereis command is like\n    # 'neuron-ls:\\n' or 'nvidia-smi:\\n' otherwise it is like \n    # 'nvidia-smi: /usr/bin/nvidia-smi /usr/share/man/man1/nvidia-smi.1.gz\\n'\n    utility_present = lambda x: subprocess.check_output([\"whereis\", x]).decode() != f\"{x}:\\n\"\n    is_nvidia = utility_present(NVIDIA_SMI)\n    is_neuron = utility_present(NEURON_LS)\n    logger.info(f\"is_nvidia={is_nvidia}, is_neuron={is_neuron}\")\n    if is_nvidia is True:\n        return constants.ACCELERATOR_TYPE.NVIDIA\n    else:\n        return constants.ACCELERATOR_TYPE.NEURON\n\n\ndef get_model_copies_to_start_neuron(tp_degree: int, model_copies: str) -> Tuple[int, int]:\n    logger.info(f\"get_model_copies_to_start_neuron, tp_degree={tp_degree}, model_copies={model_copies}\")\n    # get the number of neuron cores\n    cmd = [\"neuron-ls -j\"]\n    process = subprocess.Popen(cmd,\n                               stdout=subprocess.PIPE, \n                               stderr=subprocess.PIPE,\n                               text=True,\n                               shell=True)\n    std_out, std_err = process.communicate()\n    logger.info(std_out.strip())\n    logger.info(std_err)\n\n    if std_err != '':\n        logger.error(\"error determining neuron info, exiting\")\n        return None\n\n    # convert output to a dict for easy parsing\n    neuron_info = json.loads(std_out)\n    num_neuron_devices = len(neuron_info)\n    num_neuron_cores = 2 * num_neuron_devices\n\n    if model_copies == constants.MODEL_COPIES.AUTO:\n        model_copies_as_int = 1\n        devices_per_model = num_neuron_devices\n        logger.info(f\"model copies is set to {model_copies}, model_copies_as_int={model_copies_as_int}, \"\n                    f\"devices_per_model={devices_per_model}\")\n        return model_copies_as_int, devices_per_model\n              \n    # tensor parallelism requires as many neuron cores as tp degree\n    # and the number of devices required is half of number of cores\n    neuron_devices_needed_per_model_copy = int(tp_degree / 2)\n    model_copies_possible = int(num_neuron_devices/neuron_devices_needed_per_model_copy)\n    # if model_copies is max then load as many copies as possible\n    if isinstance(model_copies, str):\n        if model_copies == constants.MODEL_COPIES.MAX:\n            model_copies = model_copies_possible\n            logger.info(f\"model_copies was set to \\\"{constants.MODEL_COPIES.MAX}\\\", \"\n                        f\"this instance can support a max of {model_copies} model copies, \"\n                        f\"going to load {model_copies} copies\")\n        else:            \n            logger.info(f\"model_copies set to a str value={model_copies}, \"\n                        f\"will see if we can load these many models\")\n            model_copies = int(model_copies)\n    else:\n        logger.info(f\"model_copies set to a numerical value={model_copies}, \"\n                    f\"will see if we can load these many models\")\n\n    num_devices_needed = model_copies * neuron_devices_needed_per_model_copy\n    \n    logger.info(f\"model_copies={model_copies}, tp_degree={tp_degree},\\n\"\n                f\"num_neuron_devices={num_neuron_devices}, num_neuron_cores={num_neuron_cores},\\n\"\n                f\"neuron_devices_needed_per_model_copy={neuron_devices_needed_per_model_copy}, num_devices_needed={num_devices_needed},\\n\"\n                f\"model_copies_possible={model_copies_possible}\")\n\n    if model_copies_possible < model_copies:\n        logger.error(f\"model_copies={model_copies} but model_copies_possible={model_copies_possible}, \"\n                     f\"setting model_copies to max possible for this instance which is {model_copies_possible}\")\n        model_copies = model_copies_possible\n    else:\n        logger.error(f\"model_copies={model_copies} and model_copies_possible={model_copies_possible}, \"\n                     f\"it is possible to run {model_copies} models, going with that\")\n\n    return model_copies, neuron_devices_needed_per_model_copy\n\ndef get_model_copies_to_start_nvidia(tp_degree: int, model_copies: str) -> Tuple[int, int]:\n    logger.info(f\"get_model_copies_to_start_nvidia, tp_degree={tp_degree}, model_copies={model_copies}\")\n    # get the number of neuron cores\n    cmd = [\"nvidia-smi --query-gpu=name --format=csv,noheader | wc -l\"]\n    process = subprocess.Popen(cmd,\n                               stdout=subprocess.PIPE, \n                               stderr=subprocess.PIPE,\n                               text=True,\n                               shell=True)\n    std_out, std_err = process.communicate()\n    logger.info(std_out.strip())\n    logger.info(std_err)\n\n    if std_err != '':\n        logger.error(\"error determining neuron info, exiting\")\n        return None\n\n    num_gpus = int(std_out.strip())\n\n    if model_copies == constants.MODEL_COPIES.AUTO:\n        model_copies_as_int = 1\n        devices_per_model = num_gpus\n        logger.info(f\"model copies is set to {model_copies}, model_copies_as_int={model_copies_as_int}, \"\n                    f\"devices_per_model={devices_per_model}\")\n        return model_copies_as_int, devices_per_model\n    \n    # tensor parallelism requires as many gpus as tp degree\n    gpus_needed_per_model_copy = tp_degree\n    model_copies_possible = int(num_gpus/gpus_needed_per_model_copy)\n    # if model_copies is max then load as many copies as possible\n    if isinstance(model_copies, str):\n        if model_copies == constants.MODEL_COPIES.MAX:\n            model_copies = model_copies_possible\n            logger.info(f\"model_copies was set to \\\"{constants.MODEL_COPIES.MAX}\\\", \"\n                        f\"this instance can support a max of {model_copies} model copies, \"\n                        f\"going to load {model_copies} copies\")\n        else:            \n            logger.info(f\"model_copies set to a str value={model_copies}, \"\n                        f\"will see if we can load these many models\")\n            model_copies = int(model_copies)\n    else:\n        logger.info(f\"model_copies set to a numerical value={model_copies}, \"\n                    f\"will see if we can load these many models\")\n\n    num_gpus_needed = model_copies * gpus_needed_per_model_copy\n    \n    logger.info(f\"model_copies={model_copies}, tp_degree={tp_degree},\\n\"\n                f\"num_gpus={num_gpus},\\n\"\n                f\"gpus_needed_per_model_copy={gpus_needed_per_model_copy}, num_gpus_needed={num_gpus_needed},\\n\"\n                f\"model_copies_possible={model_copies_possible}\")\n\n    if model_copies_possible < model_copies:\n        logger.error(f\"model_copies={model_copies} but model_copies_possible={model_copies_possible}, \"\n                     f\"setting model_copies to max possible for this instance which is {model_copies_possible}\")\n        model_copies = model_copies_possible\n    else:\n        logger.info(f\"model_copies={model_copies} and model_copies_possible={model_copies_possible}, \"\n                     f\"it is possible to run {model_copies} models, going with that\")\n\n    return model_copies, gpus_needed_per_model_copy\n\n\n\n\n================================================"
  },
  {
    "filename": "vllm.py",
    "path": "fmbench/scripts/inference_containers/vllm.py",
    "directory": "fmbench/scripts/inference_containers",
    "extension": "py",
    "content": "================================================\n\"\"\"\nvllm specific code\n\"\"\"\nimport logging\nfrom fmbench.scripts.inference_containers.utils import (STOP_AND_RM_CONTAINER,\n                                                        FMBENCH_MODEL_CONTAINER_NAME)\n\nlogging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef create_script(region, image_uri, model_id, model_name, env_str, privileged_str, hf_token, directory):\n    \"\"\"\n    Script for running the docker container for the inference server\n    \"\"\"\n    script = f\"\"\"#!/bin/sh\n\n        {STOP_AND_RM_CONTAINER}\n\n        # Run the new Docker container with specified settings\n        docker run -d {privileged_str} --rm --name={FMBENCH_MODEL_CONTAINER_NAME} --env \"HF_TOKEN={hf_token}\" --ipc=host -p 8000:8000 {env_str} {image_uri} --model {model_id}\n\n        echo \"started docker run in daemon mode\"\n    \"\"\"\n    return script\n\n\n\n================================================"
  },
  {
    "filename": "vllm_gpu.py",
    "path": "fmbench/scripts/inference_containers/vllm_gpu.py",
    "directory": "fmbench/scripts/inference_containers",
    "extension": "py",
    "content": "================================================\n\"\"\"\nvllm specific code\n\"\"\"\nimport logging\nfrom fmbench.scripts.inference_containers.utils import (STOP_AND_RM_CONTAINER,\n                                                        FMBENCH_MODEL_CONTAINER_NAME)\n\nlogging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef create_script(region, image_uri, model_id, model_name, env_str, privileged_str, hf_token, directory, cli_params):\n    \"\"\"\n    Script for running the docker container for the inference server\n    \"\"\"\n    script = f\"\"\"#!/bin/sh\n\n        {STOP_AND_RM_CONTAINER}\n\n        # Run the new Docker container with specified settings\n        docker run -d {privileged_str} --rm --name={FMBENCH_MODEL_CONTAINER_NAME} --runtime nvidia --gpus all --env \"HF_TOKEN={hf_token}\" --ipc=host -p 8000:8000 {env_str} {image_uri} --model {model_id} {cli_params}\n\n        echo \"started docker run in daemon mode\"\n    \"\"\"\n    return script\n\n\n================================================"
  },
  {
    "filename": "build_and_push_triton.sh",
    "path": "fmbench/scripts/triton/build_and_push_triton.sh",
    "directory": "fmbench/scripts/triton",
    "extension": "sh",
    "content": "================================================\n    #!/usr/bin/env bash\n\n    # This script builds a Docker image and saves it locally in the home directory.\n\n    # Set the image name and tag\n    export IMAGE_NAME=tritonserver-neuronx\n    export IMAGE_TAG=fmbench\n\n    # Get the directory of the current script\n    DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" && pwd )\"\n\n    # Build the Docker image locally with the image name\n    docker build -f ${DIR}/Dockerfile_triton -t ${IMAGE_NAME}:${IMAGE_TAG} ${DIR}/..\n\n    if [ $? -ne 0 ]; then\n        echo \"Error: Docker image build failed\"\n        exit 1\n    fi\n\n\n\n\n================================================"
  },
  {
    "filename": "Dockerfile_triton",
    "path": "fmbench/scripts/triton/Dockerfile_triton",
    "directory": "fmbench/scripts/triton",
    "extension": "",
    "content": "================================================\nFROM public.ecr.aws/lts/ubuntu:22.04_stable\n\nENV DEBIAN_FRONTEND=noninteractive\nENV DEBCONF_NONINTERACTIVE_SEEN=true\n\nRUN apt-get update \\\n      && apt-get install -y --no-install-recommends \\\n            ca-certificates \\\n            autoconf \\\n            automake \\\n            build-essential \\\n            git \\\n            gperf \\\n            libre2-dev \\\n            libssl-dev \\\n            libtool \\\n            libcurl4-openssl-dev \\\n            libb64-dev \\\n            libgoogle-perftools-dev \\\n            patchelf \\\n            python3-dev \\\n            python3-pip \\\n            python3-setuptools \\\n            rapidjson-dev \\\n            scons \\\n            software-properties-common \\\n            pkg-config \\\n            unzip \\\n            wget \\\n            zlib1g-dev \\\n            libarchive-dev \\\n            libxml2-dev \\\n            libnuma-dev \\\n            wget \\\n      && rm -rf /var/lib/apt/lists/*\n\n      RUN pip3 install --upgrade pip \\\n      && pip3 install --upgrade \\\n          wheel \\\n          setuptools \\\n          docker \\\n          virtualenv\n\nRUN wget -O /tmp/boost.tar.gz \\\n          https://archives.boost.io/release/1.80.0/source/boost_1_80_0.tar.gz \\\n      && (cd /tmp && tar xzf boost.tar.gz) \\\n      && mv /tmp/boost_1_80_0/boost /usr/include/boost\n\nRUN apt update -q=2 \\\n      && apt install -y gpg wget \\\n      && wget -O - https://apt.kitware.com/keys/kitware-archive-latest.asc 2>/dev/null | gpg --dearmor - |  tee /usr/share/keyrings/kitware-archive-keyring.gpg >/dev/null \\\n      && . /etc/os-release \\\n      && echo \"deb [signed-by=/usr/share/keyrings/kitware-archive-keyring.gpg] https://apt.kitware.com/ubuntu/ $UBUNTU_CODENAME main\" | tee /etc/apt/sources.list.d/kitware.list >/dev/null \\\n      && apt-get update -q=2 \\\n      && apt-get install -y --no-install-recommends cmake=3.27.7* cmake-data=3.27.7*\n\nRUN git clone https://github.com/triton-inference-server/server.git /server\nRUN cd /server && git fetch origin bf86c27169ad4027202c76cf609ce42a2ec7b533\nRUN cd /server && git reset --hard bf86c27169ad4027202c76cf609ce42a2ec7b533\nRUN cd /server && ./build.py -v --no-container-build --build-dir=/server/build --backend=python --enable-metrics --enable-logging --enable-stats --endpoint=\"http\" --endpoint=\"grpc\" --filesystem=\"s3\"\n\nRUN apt-get -y install gnupg2\nRUN wget -qO - https://apt.repos.neuron.amazonaws.com/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB > ./GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB\nRUN gpg --no-default-keyring --keyring ./aws_neuron_keyring.gpg --import  ./GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB\nRUN gpg --no-default-keyring --keyring ./aws_neuron_keyring.gpg  --export >  ./aws_neuron.gpg\nRUN mv ./aws_neuron.gpg /etc/apt/trusted.gpg.d/\nRUN rm ./GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB\n\nRUN add-apt-repository -y  \"deb https://apt.repos.neuron.amazonaws.com jammy main\"\nRUN apt-get -y update\n\nRUN apt-get -y install aws-neuronx-collectives=2.*\nRUN apt-get -y install aws-neuronx-runtime-lib=2.*\nRUN apt-get -y install aws-neuronx-tools=2.*\n\nRUN pip3 config set global.extra-index-url https://pip.repos.neuron.amazonaws.com\nRUN pip3 install awscli\nRUN pip3 install --upgrade torchvision\nRUN pip3 install neuronx-cc==2.14.227.0+2d4f85be torch-neuronx==2.1.2.2.2.0 transformers-neuronx==0.11.351 libneuronxla==2.0.2335 aws-neuronx-runtime-discovery==2.9\n\nRUN pip3 install ray\n\nRUN cp -r /server/build/opt/* /opt/\nRUN rm -rf /server\n\nENV PATH=/opt/aws/neuron/bin:/opt/tritonserver/bin:$PATH\nENV PYTHONDONTWRITEBYTECODE=1\nENV PYTHONUNBUFFERED=1\nENV PJRT_DEVICE=NEURON\n\n\n\n================================================"
  },
  {
    "filename": "config.pbtxt",
    "path": "fmbench/scripts/triton/djl/config.pbtxt",
    "directory": "fmbench/scripts/triton/djl",
    "extension": "pbtxt",
    "content": "================================================\nbackend: \"python\"\nmax_batch_size: 0\nmodel_transaction_policy {\n    decoupled: true\n}\n\ninput [ \n    {\n        name: \"text_input\"\n        data_type: TYPE_STRING\n        dims: [1]\n    },\n    {\n        name: \"sampling_parameters\"\n        data_type: TYPE_STRING\n        dims: [1]\n        optional: true\n    }\n] \noutput [\n    {\n    name: \"text_output\"\n    data_type: TYPE_STRING\n    dims: [-1]\n    }\n]\n\ninstance_group [\n    {\n    count: 1\n    kind: KIND_MODEL\n    }\n]\n\n\n================================================"
  },
  {
    "filename": "model.py",
    "path": "fmbench/scripts/triton/djl/model.py",
    "directory": "fmbench/scripts/triton/djl",
    "extension": "py",
    "content": "================================================\nimport json\nimport os\n\nimport torch\nfrom djl_python import Input, Output, PairList\nfrom djl_python.transformers_neuronx import TransformersNeuronXService\nfrom djl_python.test_model import decode_encoded_output_binary, create_concurrent_batch_request\nimport numpy as np\nimport time\nimport asyncio, threading, copy\n\nimport triton_python_backend_utils as pb_utils\n\n_MODEL_ARGS_FILENAME: str = \"model.json\"\n\nclass TritonPythonModel:\n\n  def initialize(self, args):\n    self.logger = pb_utils.Logger\n    self.model_config = json.loads(args[\"model_config\"])\n    text_output_config = pb_utils.get_output_config_by_name(self.model_config, \"text_output\")\n    self.text_output_dtype = pb_utils.triton_string_to_numpy(text_output_config[\"data_type\"])\n    self.__tasks = set()\n    self._init_service()\n    self.__tasks_inited = False\n      \n\n  @staticmethod\n  def auto_complete_config(auto_complete_model_config):\n      \n    inputs = [\n    {\"name\": \"text_input\", \"data_type\": \"TYPE_STRING\", \"dims\": [1]},\n    {\n        \"name\": \"sampling_parameters\",\n        \"data_type\": \"TYPE_STRING\",\n        \"dims\": [1],\n        \"optional\": True,\n    }\n    ]\n    outputs = [{\"name\": \"text_output\", \"data_type\": \"TYPE_STRING\", \"dims\": [-1]}]\n\n    config = auto_complete_model_config.as_dict()\n    input_names = []\n    output_names = []\n    for input in config['input']:\n      input_names.append(input['name'])\n    for output in config['output']:\n      output_names.append(output['name'])\n\n    for input in inputs:\n      if input['name'] not in input_names:\n          auto_complete_model_config.add_input(input)\n    for output in outputs:\n      if output['name'] not in output_names:\n          auto_complete_model_config.add_output(output)\n\n    auto_complete_model_config.set_model_transaction_policy(dict(decoupled=True))\n    auto_complete_model_config.set_max_batch_size(0)\n\n    return auto_complete_model_config\n\n  def _init_service(self):\n    self.logger.log_info(\"Enter: _init_service\")\n\n    max_batch_size = int(self.model_config.get('max_batch_size', 0))\n    assert (max_batch_size == 0), \"max_batch_size must be 0\"\n    self.using_decoupled = pb_utils.using_decoupled_model_transaction_policy(self.model_config) \n    assert (\n        self.using_decoupled \n    ), \"Python backend must be configured to use decoupled model transaction policy\"\n\n    model_args_filepath = os.path.join( \n        pb_utils.get_model_dir(), _MODEL_ARGS_FILENAME\n    )\n    assert os.path.isfile(\n        model_args_filepath\n    ), f\"'{_MODEL_ARGS_FILENAME}' containing model args must be provided in '{pb_utils.get_model_dir()}'\"\n    with open(model_args_filepath) as file:\n      self.model_args = json.load(file)\n\n    self.batch_size = self.model_args.get(\"batch_size\", None)\n    if self.batch_size is None:\n      self.batch_size = self.model_args.get(\"max_rolling_batch_size\", 1)\n\n    self.logger.log_info(f\"initialize service: {self.model_args}\")\n    self.__service = TransformersNeuronXService()\n    self.__service.initialize(self.model_args)\n    self.logger.log_info(\"service initialized.\")\n\n    self.logger.log_info(\"Create request asyncio queue: maxsize {self.batch_size}\")\n    self.__request_queue = asyncio.Queue(maxsize=self.batch_size)\n\n    self.logger.log_info(\"Create response asyncio queue: maxsize {self.batch_size}\")\n    self.__response_queue = asyncio.Queue(maxsize=self.batch_size)\n\n    self.logger.log_info(\"Exit: _init_service\")\n\n  def get_sampling_params_dict(self, params_json):              \n    params_dict = json.loads(params_json) if params_json else {}\n\n    float_keys = [\n        \"temperature\",\n        \"top_p\"\n    ]\n    for k in float_keys:\n        if k in params_dict:\n            params_dict[k] = float(params_dict[k])\n        \n    int_keys = [\"sequence_length\", \"top_k\"]\n    for k in int_keys:\n        if k in params_dict:\n            params_dict[k] = int(params_dict[k])\n\n    if not params_dict:\n        params_dict[\"max_new_tokens\"] = 8192\n        params_dict[\"top_k\"] = 50\n    else:\n        if \"max_new_tokens\" not in params_dict:\n          params_dict[\"max_new_tokens\"] = 8192\n\n    return params_dict\n\n  async def __init_tasks(self):\n    self.logger.log_info(\"Start respond loop\")\n    task = asyncio.create_task(self.__respond_loop())\n    self.__tasks.add(task)\n    task.add_done_callback(self.__tasks.discard)\n    \n    self.logger.log_info(\"Start generate loop\")\n    task = asyncio.create_task(self.__generate_loop())\n    self.__tasks.add(task)\n    task.add_done_callback(self.__tasks.discard)\n\n    self.__tasks_inited = True\n\n  async def execute(self, requests):\n    if not self.__tasks_inited:\n      try:\n        await self.__init_tasks()\n      except KeyError:\n        print(\"Future not found or has already completed.\")\n\n    for request in requests:\n      try:\n        await self.__request_queue.put(request)\n      except KeyError:\n        print(\"Future not found or has already completed.\")\n\n  async def __check_new_requests(self):\n    self.__new_requests = []\n    if len(self.__requests) == 0:\n      new_request = await self.__request_queue.get()\n      self.__request_queue.task_done()\n      self.__new_requests.append(new_request)\n      self.__requests.append(new_request)\n\n    while len(self.__requests) < self.batch_size:\n      try:\n        await asyncio.sleep(.001)\n        new_request = self.__request_queue.get_nowait()\n        self.__request_queue.task_done()\n        self.__requests.append(new_request)\n        self.__new_requests.append(new_request)\n      except asyncio.QueueEmpty:\n        break\n\n  def __inference(self):\n\n    for request in self.__new_requests:\n      prompts = pb_utils.get_input_tensor_by_name(request, \"text_input\").as_numpy().flatten().tolist()\n      inputs = [ p.decode(\"utf-8\") if isinstance(p, bytes) else p for p in prompts]\n      inputs = inputs[0] if len(inputs) == 1 else inputs\n\n      parameters_input_tensor = pb_utils.get_input_tensor_by_name(request, \"sampling_parameters\")\n      if parameters_input_tensor:\n        parameters = parameters_input_tensor.as_numpy().flatten()\n        parameters = parameters.tolist()[0] # assume uniform sampling parameters in batch\n        parameters = parameters.decode('utf-8') if isinstance(parameters, bytes) else parameters\n      else:\n        parameters = request.parameters()\n      params = self.get_sampling_params_dict(parameters)\n      \n      self.__input_list.append({\"inputs\": inputs,\"parameters\": params})\n\n    n_new_requests = len(self.__new_requests)\n    if n_new_requests > 0:\n      properties = [{\"eula\": \"true\", \"Content-type\": \"application/json\"}]*len(self.__input_list)\n      input = create_concurrent_batch_request(inputs=self.__input_list,\n                                    properties=properties,\n                                    serving_properties=copy.deepcopy(self.model_args))\n      output = self.__service.inference(input)\n      for i in range(output.content.size()):\n        result = decode_encoded_output_binary(output.content.value_at(i))\n        if i < len(self.__results):\n          self.__results[i] = result\n        else:\n          self.__results.append(result)\n    elif self.__service.rolling_batch: \n      self.__results = self.__service.rolling_batch.inference([])\n    \n  async def __generate_loop(self):\n    \n  \n    while True:\n      try:\n        await self.__check_new_requests()\n\n        unique_reqs = { f\"{x}\" for x in self.__requests }\n        assert len(unique_reqs) == len(self.__requests), \\\n          f\"requests are not unique, {self.__requests}\"\n\n        self.__inference()\n\n        assert len(self.__requests) == len(self.__input_list), \\\n          f\"requests: {len(self.__requests)} != input_list: {len(self.__input_list)}\"\n\n        assert len(self.__requests) == len(self.__results), \\\n          f\"requests: {len(self.__requests)} != results: {len(self.__results)}\"\n\n        finished = []\n        for i in range(len(self.__requests)):\n          res = self.__results[i]\n          if str(res['last']).lower() == 'true':\n            req = self.__requests[i]\n            input = self.__input_list[i]\n            try:\n              self.__response_queue.put_nowait((req, res))\n            except asyncio.QueueFull:\n              self.logger.log_info(\"response queue is full; await put\")\n              await self.__response_queue.put((req, res))\n            finished.append((req, res, input))\n          \n        for item in finished:\n          req, res, input = item\n          self.__requests.remove(req)\n          self.__results.remove(res)\n          self.__input_list.remove(input)\n\n        assert len(self.__requests) == len(self.__input_list), \\\n          f\"requests: {len(self.__requests)} != input_list: {len(self.__input_list)}\"\n\n        assert len(self.__requests) == len(self.__results), \\\n          f\"requests: {len(self.__requests)} != results: {len(self.__results)}\"\n\n      except Exception as e:\n        self.logger.log_error(f\"Unpexpected error: {e}. Inflight requests discarded. Reset engine.\")\n        self.reset()\n\n  def reset(self):\n    self.__requests = []\n    self.__results = []\n    self.__input_list = []\n\n    if self.__service.rolling_batch:\n      self.__service.rolling_batch.reset()\n\n  async def __respond_loop(self):\n    self.reset()\n\n    while True:\n      try:\n        req, res = await self.__response_queue.get()\n        self.__response_queue.task_done()\n        \n        t = threading.Thread(target=self.__send_response, \n          kwargs={\"request\": req, \"response\": res})\n        t.start()\n      except Exception as e:\n        print(f\"Error: respond loop exception {e}\")\n\n  def __send_response(self, request, response: dict):\n    try:\n      response_sender = request.get_response_sender()\n      text_output = response['data']\n    \n      out_tensor = pb_utils.Tensor(\"text_output\", np.array(text_output).astype(self.text_output_dtype))\n      inference_response = pb_utils.InferenceResponse(output_tensors=[out_tensor])\n\n      response_sender.send(inference_response, flags=pb_utils.TRITONSERVER_RESPONSE_COMPLETE_FINAL)\n    except Exception as e:\n      print(f\"send error: {request}\", flush=True)\n      print(e, flush=True)\n\n  def finalize(self):\n    self.logger.log_info(\"Cleaning up...\")\n\n\n================================================"
  },
  {
    "filename": "triton-djl-python-neuronx.sh",
    "path": "fmbench/scripts/triton/djl/triton-djl-python-neuronx.sh",
    "directory": "fmbench/scripts/triton/djl",
    "extension": "sh",
    "content": "================================================\n#!/bin/bash\n[ ! -d /triton ] && echo \"/triton dir must exist\" && exit 1\n[ $# -ne 4 ] && echo \"usage: $0 hf-model-id model-name http-port opm-num-threads\" && exit 1\n\n# The HF model id, model name and http port\n# are parsed from the triton multi model preparation\n# function here\nHF_MODEL_ID=$1\nMODEL_NAME=$2\nHTTP_PORT=$3\nOPM_NUM_THREADS=$4\n\nLOG_ROOT=/triton/logs\nMODEL_REPO=/triton/model_repository\nCACHE_DIR=/cache\n\npip3 install --extra-index-url https://pip.repos.neuron.amazonaws.com optimum[neuronx]\nGIT_CLONE_DIR=/tmp/djl-serving\ngit clone https://github.com/deepjavalibrary/djl-serving.git $GIT_CLONE_DIR\ncd $GIT_CLONE_DIR\ngit fetch origin c343d60b35f0d42f96f678570a553953f055ab32\ngit reset --hard c343d60b35f0d42f96f678570a553953f055ab32\ncd $GIT_CLONE_DIR/engines/python/setup \npip3 install .\n\nGIT_CLONE_DIR=/tmp/vllm\ngit clone https://github.com/vllm-project/vllm.git $GIT_CLONE_DIR\ncd $GIT_CLONE_DIR\ngit fetch origin 38c4b7e863570a045308af814c72f4504297222e\ngit reset --hard 38c4b7e863570a045308af814c72f4504297222e\npip3 install -r requirements-neuron.txt\npip3 install .\npip3 install pynvml==11.5.3 transformers==4.44.2\n\nmkdir -p $LOG_ROOT\nOUTPUT_LOG=\"$LOG_ROOT/triton-server.log\"\nrm -rf $MODEL_REPO\nmkdir -p $MODEL_REPO\nVERSION=1\nMODEL_NAME=$MODEL_NAME\nmkdir -p $MODEL_REPO/$MODEL_NAME/$VERSION\ncp /triton/model.py $MODEL_REPO/$MODEL_NAME/$VERSION/model.py\ncp /triton/model.json $MODEL_REPO/$MODEL_NAME/$VERSION/model.json\ncp /triton/config.pbtxt $MODEL_REPO/$MODEL_NAME/config.pbtxt\nexport NEURON_CC_FLAGS=\"--model-type transformer\"\nexport NEURON_COMPILE_CACHE_URL=\"$CACHE_DIR\"\nexport OMP_NUM_THREADS=$OPM_NUM_THREADS\ntritonserver \\\n--model-repository=${MODEL_REPO} \\\n--http-port=$HTTP_PORT \\\n--disable-auto-complete-config \\\n--log-file=$OUTPUT_LOG \\\n&& /bin/bash -c \"trap : TERM INT; sleep infinity & wait\"\n\n\n================================================"
  },
  {
    "filename": "config.pbtxt",
    "path": "fmbench/scripts/triton/vllm/config.pbtxt",
    "directory": "fmbench/scripts/triton/vllm",
    "extension": "pbtxt",
    "content": "================================================\nbackend: \"vllm\"\nmax_batch_size: 0\nmodel_transaction_policy {\n  decoupled: true\n}\n\ninput [ \n  {\n    name: \"text_input\"\n    data_type: TYPE_STRING\n    dims: [1]\n  },\n  {\n      name: \"stream\"\n      data_type: TYPE_BOOL\n      dims: [1]\n      optional: true\n  },\n  {\n      name: \"sampling_parameters\"\n      data_type: TYPE_STRING\n      dims: [1]\n      optional: true\n  },\n  {\n      name: \"exclude_input_in_output\"\n      data_type: TYPE_BOOL\n      dims: [1]\n      optional: true\n  }\n] \noutput [\n  {\n    name: \"text_output\"\n    data_type: TYPE_STRING\n    dims: [-1]\n  }\n]\n\ninstance_group [\n  {\n    count: 1\n    kind: KIND_MODEL\n  }\n]\n\n\n================================================"
  },
  {
    "filename": "triton-vllm-neuronx.sh",
    "path": "fmbench/scripts/triton/vllm/triton-vllm-neuronx.sh",
    "directory": "fmbench/scripts/triton/vllm",
    "extension": "sh",
    "content": "================================================\n#!/bin/bash\n[ ! -d /triton ] && echo \"/triton dir must exist\" && exit 1\n[ $# -ne 4 ] && echo \"usage: $0 hf-model-id model-name http-port opm-num-threads\" && exit 1\n\n# The HF model id, model name and http port\n# are parsed from the triton multi model preparation\n# function here\nHF_MODEL_ID=$1\nMODEL_NAME=$2\nHTTP_PORT=$3\nOPM_NUM_THREADS=$4\n\nLOG_ROOT=/triton/logs\nMODEL_REPO=/triton/model_repository\nCACHE_DIR=/cache\n\nGIT_CLONE_DIR=/tmp/vllm\ngit clone https://github.com/vllm-project/vllm.git $GIT_CLONE_DIR\ncd $GIT_CLONE_DIR\ngit checkout main\ngit fetch origin 5b734fb7edfdf3f8a836a3ddee81eba506230fdd\ngit reset --hard 5b734fb7edfdf3f8a836a3ddee81eba506230fdd\ngit apply --ignore-whitespace /triton/vllm-neuron-issue-1.patch\n\nmkdir -p $LOG_ROOT\nOUTPUT_LOG=\"$LOG_ROOT/triton_server.log\"\nrm -rf $MODEL_REPO\nmkdir -p $MODEL_REPO\nVERSION=1\nMODEL_NAME=$MODEL_NAME\nmkdir -p $MODEL_REPO/$MODEL_NAME/$VERSION\ncp /triton/model.json $MODEL_REPO/$MODEL_NAME/$VERSION/model.json\ncp /triton/config.pbtxt $MODEL_REPO/$MODEL_NAME/config.pbtxt\ncd $GIT_CLONE_DIR\npip3 install -r requirements-neuron.txt\npip3 install .\npip3 install triton==2.2.0\npip3 install pynvml==11.5.3\ngit clone https://github.com/triton-inference-server/vllm_backend.git /tmp/vllm_backend\ncd /tmp/vllm_backend\ngit fetch origin 507e4dccabf85c3b7821843261bcea7ea5828802\ngit reset --hard 507e4dccabf85c3b7821843261bcea7ea5828802\n\nmkdir -p /opt/tritonserver/backends/vllm\ncp -r /tmp/vllm_backend/src/* /opt/tritonserver/backends/vllm/\ncd $GIT_CLONE_DIR\nexport NEURON_CC_FLAGS=\"--model-type transformer\"\nexport NEURON_COMPILE_CACHE_URL=\"$CACHE_DIR\"\nexport OMP_NUM_THREADS=$OPM_NUM_THREADS\ntritonserver \\\n--model-repository=${MODEL_REPO} \\\n--http-port=$HTTP_PORT \\\n--disable-auto-complete-config \\\n--log-file=$OUTPUT_LOG \\\n&& /bin/bash -c \"trap : TERM INT; sleep infinity & wait\"\n\n\n================================================"
  },
  {
    "filename": "vllm-neuron-issue-1.patch",
    "path": "fmbench/scripts/triton/vllm/vllm-neuron-issue-1.patch",
    "directory": "fmbench/scripts/triton/vllm",
    "extension": "patch",
    "content": "================================================\ndiff --git a/vllm/worker/neuron_worker.py b/vllm/worker/neuron_worker.py\nindex 9b4367d7..11dbc0f4 100644\n--- a/vllm/worker/neuron_worker.py\n+++ b/vllm/worker/neuron_worker.py\n@@ -68,7 +68,7 @@ class NeuronWorker(LoraNotSupportedWorkerBase, LocalOrDistributedWorkerBase):\n         # Set the number of GPU blocks to be the same as the maximum number of\n         # sequences that can be processed in a single batch. This is equivalent\n         # to schedule without PagedAttention.\n-        num_gpu_blocks = self.scheduler_config.max_num_seqs\n+        num_gpu_blocks = self.scheduler_config.max_num_seqs + 1\n \n         # Swap not yet supported with Neuron backend.\n         num_cpu_blocks = 0\n@@ -82,7 +82,7 @@ class NeuronWorker(LoraNotSupportedWorkerBase, LocalOrDistributedWorkerBase):\n \n         # Different values are not tested.\n         assert num_cpu_blocks == 0\n-        assert num_gpu_blocks == self.scheduler_config.max_num_seqs\n+        assert num_gpu_blocks == self.scheduler_config.max_num_seqs + 1\n \n         self.cache_config.num_gpu_blocks = num_gpu_blocks\n         self.cache_config.num_cpu_blocks = num_cpu_blocks\n\n\n================================================"
  },
  {
    "filename": "config.json",
    "path": "fmbench/tokenizer/config.json",
    "directory": "fmbench/tokenizer",
    "extension": "json",
    "content": "================================================\n{\n  \"architectures\": [\n    \"MistralForCausalLM\"\n  ],\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 32768,\n  \"model_type\": \"mistral\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_theta\": 10000.0,\n  \"sliding_window\": 4096,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.34.0.dev0\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\n\n\n================================================"
  },
  {
    "filename": ".keep",
    "path": "fmbench/tokenizer/.keep",
    "directory": "fmbench/tokenizer",
    "extension": "",
    "content": "================================================\n\n\n\n\n================================================"
  },
  {
    "filename": "ec2_instance_creation_steps.md",
    "path": "misc/ec2_instance_creation_steps.md",
    "directory": "misc",
    "extension": "md",
    "content": "================================================\n# Create an EC2 instance suitable for an LMI (Large Model Inference)\n\nFollow the steps below to create an EC2 instance for hosting a model in an LMI.\n\n1. On the homepage of AWS Console go to \u2018EC2\u2019 - it is likely in recently visited:\n   ![](../img/ec2connect1.png)\n\n1. If not found, go to the search bar on the top of the page. Type `ec2` into the search box and click the entry that pops up with name `EC2` :\n   ![](../img/ec2connect2.png)\n\n1. Click \u201cInstances\u201d:\n   ![](../img/ec2connect3.png)\n\n1. Click \"Launch Instances\":\n   ![](../img/ec2connect4.png)\n\n1. Type in a name for your instance (recommended to include your alias in the name), and then scroll down. Search for \u2018deep learning ami\u2019 in the box. Select the one that says \u201cDeep Learning OSS Nvidia Driver AMI GPU PyTorch\u201d. **Your version number might be different**. \n    ![](../img/ec2connect5a.png)\n\n1. Name your instance _FMBenchInstance_.\n   \n1. Add a _fmbench-version_ tag to your instance.\n   ![](../img/ec2tag.png)\n   \n1. Scroll down to _Instance Type_. For large model inference, the g5.12xlarge is recommended.\n\n   ![](../img/ec2connect6.png)\n\n1. Make a key pair by clicking _Create new key pair_. Give it a name, keep all settings as is, and then click \u201cCreate key pair\u201d.\n   ![](../img/ec2connect7.png)\n   \n1. Skip over _Network settings_ (leave it as it is), going straight to _Configure storage_. 45 GB, the suggested amount, is not nearly enough, and using that will cause the LMI docker container to download for an arbitrarily long time and then error out. Change it to 100 GB or more:\n    ![](../img/ec2connect8.png)\n\n1. Create an IAM role to your instance called _FMBenchEC2Role_. Attach the following permission policies: `AmazonSageMakerFullAccess`, `AmazonBedrockFullAccess`.\n\n    Edit the trust policy to be the following:\n    ```\n    {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Effect\": \"Allow\",\n                \"Principal\": {\n                    \"Service\": \"ec2.amazonaws.com\"\n                },\n                \"Action\": \"sts:AssumeRole\"\n            },\n            {\n                \"Effect\": \"Allow\",\n                \"Principal\": {\n                    \"Service\": \"sagemaker.amazonaws.com\"\n                },\n                \"Action\": \"sts:AssumeRole\"\n            },\n            {\n                \"Effect\": \"Allow\",\n                \"Principal\": {\n                    \"Service\": \"bedrock.amazonaws.com\"\n                },\n                \"Action\": \"sts:AssumeRole\"\n            }\n        ]\n    }\n    ```\n    Select this role in the _IAM instance profile_ setting of your instance.\n    ![](../img/ec2-iam.png)\n    \n1. Then, we\u2019re done with the settings of the instance. Click _Launch Instance_ to finish. You can connect to your EC2 instance using any of these option\n    ![](../img/ec2connect10.png)\n\n\n\n================================================"
  },
  {
    "filename": "eks_cluster-creation_steps.md",
    "path": "misc/eks_cluster-creation_steps.md",
    "directory": "misc",
    "extension": "md",
    "content": "================================================\n# EKS cluster creation steps\n\nThe steps below create an EKS cluster called `trainium-inferentia`.\n\n1. Before we begin, ensure you have all the prerequisites in place to make the deployment process smooth and hassle-free. Ensure that you have installed the following tools on your machine: [aws-cli](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html), [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/) and [terraform](https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli). We use the [`DoEKS`](https://github.com/awslabs/data-on-eks/tree/main) repository as a guide to deploy the cluster infrastructure in an AWS account.\n\n1. Ensue that your account has enough `Inf2` on-demand VCPUs as most of the DoEKS blueprints utilize this specific instance. To increase service quota navigate to the service quota page for the region you are in [service quota](https://us-east-1.console.aws.amazon.com/servicequotas/home?region=us-east-1). Then select **services** under the left side menu and search for **Amazon Elastic Compute Cloud (Amazon EC2)**. This will bring up the service quota page, here search for `inf` and there should be an option for **Running On-Demand Inf instances**. Increase this quota to 300. \n\n1. Clone the [`DoEKS`](https://github.com/awslabs/data-on-eks) repository\n\n    ``` {.bash}\n    git clone https://github.com/awslabs/data-on-eks.git\n    ```\n\n1. Ensure that the region names are correct in [`variables.tf`](https://github.com/awslabs/data-on-eks/blob/d532720d0746959daa6d3a3f5925fc8be114ccc4/ai-ml/trainium-inferentia/variables.tf#L12) file before running the cluster creation script.\n\n1. Ensure that the ELB to be created would be external facing. Change the helm value from `internal` to `internet-facing` [here](https://github.com/awslabs/data-on-eks/blob/3ef55e21cf30b54341bb771a2bb2dbd1280c3edd/ai-ml/trainium-inferentia/helm-values/ingress-nginx-values.yaml#L8).\n\n1. Ensure that the IAM role you are using has the permissions needed to create the cluster. **While we expect the following set of permissions to work but the current recommendation is to also add the `AdminstratorAccess` permission to the IAM role. At a later date you could remove the  `AdminstratorAccess` and experiment with cluster creation without it.**\n\n    1. Attach the following managed policies: `AmazonEKSClusterPolicy`, `AmazonEKS_CNI_Policy`, and `AmazonEKSWorkerNodePolicy`.\n    1. In addition to the managed policies add the following as inline policy. Replace _your-account-id_ with the actual value of the AWS account id you are using.\n    \n    \n        ```{.bash}\n        {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Sid\": \"VisualEditor0\",\n                \"Effect\": \"Allow\",\n                \"Action\": [\n                    \"ec2:CreateVpc\",\n                    \"ec2:DeleteVpc\"\n                ],\n                \"Resource\": [\n                    \"arn:aws:ec2:*:your-account-id:ipv6pool-ec2/*\",\n                    \"arn:aws:ec2::your-account-id:ipam-pool/*\",\n                    \"arn:aws:ec2:*:your-account-id:vpc/*\"\n                ]\n            },\n            {\n                \"Sid\": \"VisualEditor1\",\n                \"Effect\": \"Allow\",\n                \"Action\": [\n                    \"ec2:ModifyVpcAttribute\",\n                    \"ec2:DescribeVpcAttribute\"\n                ],\n                \"Resource\": \"arn:aws:ec2:*:<your-account-id>:vpc/*\"\n            },\n            {\n                \"Sid\": \"VisualEditor2\",\n                \"Effect\": \"Allow\",\n                \"Action\": \"ec2:AssociateVpcCidrBlock\",\n                \"Resource\": [\n                    \"arn:aws:ec2:*:your-account-id:ipv6pool-ec2/*\",\n                    \"arn:aws:ec2::your-account-id:ipam-pool/*\",\n                    \"arn:aws:ec2:*:your-account-id:vpc/*\"\n                ]\n            },\n            {\n                \"Sid\": \"VisualEditor3\",\n                \"Effect\": \"Allow\",\n                \"Action\": [\n                    \"ec2:DescribeSecurityGroupRules\",\n                    \"ec2:DescribeNatGateways\",\n                    \"ec2:DescribeAddressesAttribute\"\n                ],\n                \"Resource\": \"*\"\n            },\n            {\n                \"Sid\": \"VisualEditor4\",\n                \"Effect\": \"Allow\",\n                \"Action\": [\n                    \"ec2:CreateInternetGateway\",\n                    \"ec2:RevokeSecurityGroupEgress\",\n                    \"ec2:CreateRouteTable\",\n                    \"ec2:CreateSubnet\"\n                ],\n                \"Resource\": [\n                    \"arn:aws:ec2:*:your-account-id:security-group/*\",\n                    \"arn:aws:ec2:*:your-account-id:internet-gateway/*\",\n                    \"arn:aws:ec2:*:your-account-id:subnet/*\",\n                    \"arn:aws:ec2:*:your-account-id:route-table/*\",\n                    \"arn:aws:ec2::your-account-id:ipam-pool/*\",\n                    \"arn:aws:ec2:*:your-account-id:vpc/*\"\n                ]\n            },\n            {\n                \"Sid\": \"VisualEditor5\",\n                \"Effect\": \"Allow\",\n                \"Action\": [\n                    \"ec2:AttachInternetGateway\",\n                    \"ec2:AssociateRouteTable\"\n                ],\n                \"Resource\": [\n                    \"arn:aws:ec2:*:your-account-id:vpn-gateway/*\",\n                    \"arn:aws:ec2:*:your-account-id:internet-gateway/*\",\n                    \"arn:aws:ec2:*:your-account-id:subnet/*\",\n                    \"arn:aws:ec2:*:your-account-id:route-table/*\",\n                    \"arn:aws:ec2:*:your-account-id:vpc/*\"\n                ]\n            },\n            {\n                \"Sid\": \"VisualEditor6\",\n                \"Effect\": \"Allow\",\n                \"Action\": \"ec2:AllocateAddress\",\n                \"Resource\": [\n                    \"arn:aws:ec2:*:your-account-id:ipv4pool-ec2/*\",\n                    \"arn:aws:ec2:*:your-account-id:elastic-ip/*\"\n                ]\n            },\n            {\n                \"Sid\": \"VisualEditor7\",\n                \"Effect\": \"Allow\",\n                \"Action\": \"ec2:ReleaseAddress\",\n                \"Resource\": \"arn:aws:ec2:*:your-account-id:elastic-ip/*\"\n            },\n            {\n                \"Sid\": \"VisualEditor8\",\n                \"Effect\": \"Allow\",\n                \"Action\": \"ec2:CreateNatGateway\",\n                \"Resource\": [\n                    \"arn:aws:ec2:*:your-account-id:subnet/*\",\n                    \"arn:aws:ec2:*:your-account-id:natgateway/*\",\n                    \"arn:aws:ec2:*:your-account-id:elastic-ip/*\"\n                ]\n            }\n        ]\n        }\n        ```\n1. Add the Role ARN and name here in the `variables.tf` file by updating [these lines](https://github.com/awslabs/data-on-eks/blob/d532720d0746959daa6d3a3f5925fc8be114ccc4/ai-ml/trainium-inferentia/variables.tf#L126). Move the structure inside the `defaut` list and replace the role ARN and name values with the values for the role you are using.\n\n1. Navigate into the `ai-ml/trainium-inferentia/` directory and run install.sh script.\n\n    ``` {.bash}\n    cd data-on-eks/ai-ml/trainium-inferentia/\n    ./install.sh\n    ```\n\n    Note: This step takes about 12-15 minutes to deploy the EKS infrastructure and cluster in the AWS account. To view more details on cluster creation, view an example here: [Deploy Llama3 on EKS](https://awslabs.github.io/data-on-eks/docs/gen-ai/inference/llama3-inf2) in the _prerequisites_ section.\n\n1. After the cluster is created, navigate to the **Karpenter EC2 node IAM role** called `karpenter-trainium-inferentia-XXXXXXXXXXXXXXXXXXXXXXXXX`. Attach the following inline policy to the role:\n\n    ``` {.bash}\n    {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Sid\": \"Statement1\",\n                \"Effect\": \"Allow\",\n                \"Action\": [\n                    \"iam:CreateServiceLinkedRole\"\n                ],\n                \"Resource\": \"*\"\n            }\n        ]\n    }\n    ```\n\n\n\n================================================"
  },
  {
    "filename": "the-diy-version-w-gory-details.md",
    "path": "misc/the-diy-version-w-gory-details.md",
    "directory": "misc",
    "extension": "md",
    "content": "================================================\n### The DIY version (with gory details)\n\nFollow the prerequisites below to set up your environment before running the code:\n\n1. **Python 3.11**: Setup a Python 3.11 virtual environment and install `FMBench`.\n\n    ```{.bash}\n    python -m venv .fmbench\n    pip install fmbench\n    ```\n\n1. **S3 buckets for test data, scripts, and results**: Create two buckets within your AWS account:\n\n    * _Read bucket_: This bucket contains `tokenizer files`, `prompt template`, `source data` and `deployment scripts` stored in a directory structure as shown below. `FMBench` needs to have read access to this bucket.\n    \n        ```{.bash}\n        s3://<read-bucket-name>\n            \u251c\u2500\u2500 source_data/\n            \u251c\u2500\u2500 source_data/<source-data-file-name>.json\n            \u251c\u2500\u2500 prompt_template/\n            \u251c\u2500\u2500 prompt_template/prompt_template.txt\n            \u251c\u2500\u2500 scripts/\n            \u251c\u2500\u2500 scripts/<deployment-script-name>.py\n            \u251c\u2500\u2500 tokenizer/\n            \u251c\u2500\u2500 tokenizer/tokenizer.json\n            \u251c\u2500\u2500 tokenizer/config.json\n        ```\n\n        * The details of the bucket structure is as follows:\n\n            1. **Source Data Directory**: Create a `source_data` directory that stores the dataset you want to benchmark with. `FMBench` uses `Q&A` datasets from the [`LongBench dataset`](https://github.com/THUDM/LongBench) or alternatively from [this link](https://huggingface.co/datasets/THUDM/LongBench/resolve/main/data.zip). _Support for bring your own dataset will be added soon_.\n\n                * Download the different files specified in the [LongBench dataset](https://github.com/THUDM/LongBench) into the `source_data` directory. Following is a good list to get started with:\n\n                    * `2wikimqa`\n                    * `hotpotqa`\n                    * `narrativeqa`\n                    * `triviaqa`\n                \n                    Store these files in the `source_data` directory.\n\n            1. **Prompt Template Directory**: Create a `prompt_template` directory that contains a `prompt_template.txt` file. This `.txt` file contains the prompt template that your specific model supports. `FMBench` already supports the [prompt template](src/fmbench/prompt_template/prompt_template.txt) compatible with `Llama` models.\n\n            1. **Scripts Directory**: `FMBench` also supports a `bring your own script (BYOS)` mode for deploying models that are not natively available via SageMaker JumpStart i.e. anything not included in [this](https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html) list. Here are the steps to use BYOS.\n\n                1. Create a Python script to deploy your model on a SageMaker endpoint. This script needs to have a `deploy` function that [`2_deploy_model.ipynb`](./src/fmbench/2_deploy_model.ipynb) can invoke. See [`p4d_hf_tgi.py`](./scripts/p4d_hf_tgi.py) for reference.\n\n                1. Place your deployment script in the `scripts` directory in your ***read bucket***. If your script deploys a model directly from HuggingFace and needs to have access to a HuggingFace auth token, then create a file called `hf_token.txt` and put the auth token in that file. The [`.gitignore`](.gitgnore) file in this repo has rules to not commit the `hf_token.txt` to the repo. Today, `FMBench` provides inference scripts for:\n\n                    * [All SageMaker Jumpstart Models](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models.html)\n                    * [Text-Generation-Inference (TGI) container supported models](https://huggingface.co/text-generation-inference)\n                    * [Deep Java Library DeepSpeed container supported models](https://docs.djl.ai/docs/serving/serving/docs/lmi/configurations_large_model_inference_containers.html)\n\n\n                    Deployment scripts for the options above are available in the [scripts](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/s3_metrics/scripts) directory, you can use these as reference for creating your own deployment scripts as well.\n\n            1. **Tokenizer Directory**: Place the `tokenizer.json`, `config.json` and any other files required for your model's tokenizer in the `tokenizer` directory. The tokenizer for your model should be compatible with the [`tokenizers`](https://pypi.org/project/tokenizers/) package. `FMBench` uses `AutoTokenizer.from_pretrained` to load the tokenizer.\n                >As an example, to use the `Llama 2 Tokenizer` for counting prompt and generation tokens for the `Llama 2` family of models: Accept the License here: [meta approval form](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and download the `tokenizer.json` and `config.json` files from [Hugging Face website](https://huggingface.co/meta-llama/Llama-2-7b/tree/main) and place them in the `tokenizer` directory.\n\n    * _Write bucket_: All prompt payloads, model endpoint and metrics generated by `FMBench` are stored in this bucket. `FMBench` requires write permissions to store the results in this bucket. No directory structure needs to be pre-created in this bucket, everything is created by `FMBench` at runtime.\n\n        ```{.bash}\n        s3://<write-bucket-name>\n            \u251c\u2500\u2500 <test-name>\n            \u251c\u2500\u2500 <test-name>/data\n            \u251c\u2500\u2500 <test-name>/data/metrics\n            \u251c\u2500\u2500 <test-name>/data/models\n            \u251c\u2500\u2500 <test-name>/data/prompts\n        ````\n\n\n\n================================================"
  },
  {
    "filename": "__init__.py",
    "path": "tests/__init__.py",
    "directory": "tests",
    "extension": "py",
    "content": "================================================\n\n\n\n================================================"
  },
  {
    "filename": "test_pricing.py",
    "path": "tests/test_pricing.py",
    "directory": "tests",
    "extension": "py",
    "content": "================================================\nimport json\nimport pytest\nfrom pathlib import Path\nfrom unittest.mock import patch, MagicMock\nfrom src.fmbench.scripts.pricing import get_ec2_pricing, load_and_update_pricing\n\n\n@pytest.fixture\ndef mock_pricing_data():\n    return {\n        \"pricing\": {\n            \"instance_based\": {\n                \"ml.c5.xlarge\": 0.204,\n            },\n            \"token_based\": {\n                \"ai21.j2-mid-v1\": {\n                    \"input-per-1k-tokens\": 0.0125,\n                    \"output-per-1k-tokens\": 0.0125,\n                }\n            },\n        }\n    }\n\n\n@patch(\"boto3.client\")\ndef test_get_ec2_pricing_success(mock_boto3_client):\n    # Mock the Pricing API response\n    mock_client = MagicMock()\n    mock_client.get_products.return_value = {\n        \"PriceList\": [\n            json.dumps(\n                {\n                    \"terms\": {\n                        \"OnDemand\": {\n                            \"id\": {\n                                \"priceDimensions\": {\n                                    \"id\": {\"pricePerUnit\": {\"USD\": \"0.25\"}}\n                                }\n                            }\n                        }\n                    }\n                }\n            )\n        ]\n    }\n    mock_boto3_client.return_value = mock_client\n\n    price = get_ec2_pricing(\"ml.c5.xlarge\", \"us-east-1\")\n    assert price == 0.25\n    mock_client.get_products.assert_called_once()\n\n\n@patch(\"boto3.client\")\ndef test_get_ec2_pricing_unsupported_region(mock_boto3_client):\n    with pytest.raises(ValueError, match=\"Unsupported region code\"):\n        get_ec2_pricing(\"ml.c5.xlarge\", \"invalid-region\")\n\n\n@patch(\"boto3.client\")\ndef test_get_ec2_pricing_no_pricing_found(mock_boto3_client):\n    mock_client = MagicMock()\n    mock_client.get_products.return_value = {\"PriceList\": []}\n    mock_boto3_client.return_value = mock_client\n\n    with pytest.raises(ValueError, match=\"No pricing data found\"):\n        get_ec2_pricing(\"ml.c5.xlarge\", \"us-east-1\")\n\n\n@patch(\"src.fmbench.utils.load_config\")\n@patch(\"src.fmbench.utils.save_config\")\n@patch(\"src.fmbench.scripts.pricing.get_ec2_pricing\")\ndef test_load_and_update_pricing_existing_pricing(\n    mock_get_ec2_pricing, mock_save_config, mock_load_config, mock_pricing_data\n):\n    # Mock the load_config to return existing pricing data\n    mock_load_config.return_value = mock_pricing_data\n\n    # Run the function\n    updated_pricing = load_and_update_pricing(\n        PRICING_YAML_PATH=Path(\"src/fmbench/configs/pricing.yml\"),\n        PRICING_FALLBACK_YAML_PATH=Path(\"src/fmbench/configs/pricing_fallback.yml\"),\n        instances=[\"ml.c5.xlarge\"],\n        region_code=\"us-east-1\",\n    )\n\n    # Assert pricing was not fetched again\n    mock_get_ec2_pricing.assert_not_called()\n    assert updated_pricing == mock_pricing_data\n\n\n@patch(\"src.fmbench.utils.load_config\")\n@patch(\"src.fmbench.utils.save_config\")\n@patch(\"src.fmbench.scripts.pricing.get_ec2_pricing\")\ndef test_load_and_update_pricing_fetch_new_pricing(\n    mock_get_ec2_pricing, mock_save_config, mock_load_config, mock_pricing_data\n):\n    # Mock the load_config to return existing pricing data\n    mock_load_config.return_value = mock_pricing_data\n\n    # Mock get_ec2_pricing to return a new price\n    mock_get_ec2_pricing.return_value = 0.5\n\n    # Run the function\n    updated_pricing = load_and_update_pricing(\n        PRICING_YAML_PATH=Path(\"src/fmbench/configs/pricing.yml\"),\n        PRICING_FALLBACK_YAML_PATH=Path(\"src/fmbench/configs/pricing_fallback.yml\"),\n        instances=[\"ml.g5.xlarge\"],\n        region_code=\"us-east-1\",\n    )\n\n    # Assert pricing was fetched for the new instance\n    mock_get_ec2_pricing.assert_called_once_with(\"ml.g5.xlarge\", \"us-east-1\")\n    assert updated_pricing[\"pricing\"][\"instance_based\"][\"ml.g5.xlarge\"] == 0.5\n    mock_save_config.assert_called_once()\n\n\n@patch(\"src.fmbench.utils.load_config\")\n@patch(\"src.fmbench.utils.save_config\")\n@patch(\"src.fmbench.scripts.pricing.get_ec2_pricing\")\ndef test_load_and_update_pricing_fallback(\n    mock_get_ec2_pricing, mock_save_config, mock_load_config\n):\n    # Mock load_config to raise an exception for the main pricing file\n    mock_load_config.side_effect = [\n        FileNotFoundError(\"Main pricing file not found\"),\n        {\"pricing\": {\"instance_based\": {}}},  # Return fallback data\n    ]\n\n    # Run the function\n    updated_pricing = load_and_update_pricing(\n        PRICING_YAML_PATH=Path(\"src/fmbench/configs/pricing.yml\"),\n        PRICING_FALLBACK_YAML_PATH=Path(\"src/fmbench/configs/pricing_fallback.yml\"),\n        instances=[\"ml.g5.xlarge\"],\n        region_code=\"us-east-1\",\n    )\n\n    # Assert fallback pricing was loaded\n    assert \"instance_based\" in updated_pricing[\"pricing\"]\n    mock_get_ec2_pricing.assert_called_once_with(\"ml.g5.xlarge\", \"us-east-1\")\n    mock_save_config.assert_called_once()\n\n\n\n================================================"
  },
  {
    "filename": "create_fmbench_website.py",
    "path": "website/create_fmbench_website.py",
    "directory": "website",
    "extension": "py",
    "content": "================================================\n\"\"\"\nCreate a mkdocs.yml file to render all the available reports as a MKdocs website.\n\"\"\"\n\nimport os\nimport json\nimport glob\nimport yaml\nimport copy\nimport logging\nimport argparse\nfrom pathlib import Path\n\nlogging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nHOME_DIR = str(Path.home())\nDEFAULT_RESULTS_ROOT = os.path.join(HOME_DIR, \"fmbench_data\")\nRESULTS_FOLDER_PREFIX: str = \"results-\"\n\nSCRIPT_DIR_PATH = os.path.dirname(os.path.realpath(__file__))\n\nMKDOCS_YML_FNAME: str = os.path.join(SCRIPT_DIR_PATH, 'mkdocs.yml')\nMKDOCS_TEMPLATE_YML_FNAME: str = os.path.join(SCRIPT_DIR_PATH, 'mkdocs_template.yml')\n\ndef main():\n    parser = argparse.ArgumentParser(description='Analyze multiple FMBench runs')\n    parser.add_argument('--results-root-dir',\n                        type=str,\n                        default=DEFAULT_RESULTS_ROOT,\n                        help=f'Root directory containing results-* folders, default={DEFAULT_RESULTS_ROOT}',\n                        required=False)\n\n    args = parser.parse_args()\n    logger.info(f\"main, {args} = args\")\n\n    # find out the results folders\n    result_folders = glob.glob(os.path.join(args.results_root_dir, RESULTS_FOLDER_PREFIX + \"*\"))\n    logger.info(f\"there are {len(result_folders)} results folders\\n {result_folders}\")\n\n    # read the mkdocs template to add the folders to the content\n    content = Path(MKDOCS_TEMPLATE_YML_FNAME).read_text()\n    mkdocs_template = yaml.safe_load(content)\n    logger.info(f\"read mkdocs_template=\\n{json.dumps(mkdocs_template, indent=2)})\")\n    mkdocs = copy.deepcopy(mkdocs_template)\n    for f in result_folders:\n        f = Path(f).name\n        logger.info(f\"folder={f}\")\n        # folder is of the form results-llama3-1-8b-inf2.48xl-tp=8-bs=4-mc=1-ec2\n        label = f.replace(RESULTS_FOLDER_PREFIX, \"\")\n        tokens = label.split(\"-\")\n        model_id = tokens[0]\n        found = False\n        for e in mkdocs['nav']:\n            keys = list(e.keys())\n            logger.info(f\"keys={keys}\")\n\n            if model_id in keys:\n                logger.info(f\"{model_id} key already exists in element = {json.dumps(e, indent=2)}\")\n                e[model_id].append({label: os.path.join(f, \"report.md\")})\n                found = True\n                break\n            else:\n                logger.info(f\"model_id={model_id} not in keys={keys}\")\n        if found is False:\n            logger.info(f\"model_id={model_id} not found in any existing keys, going to create a new key\")\n            mkdocs['nav'].append({model_id: [{label: os.path.join(f, \"report.md\")}]})\n            logger.info(f\"after adding model_id={model_id}, mkdocs=\\n{json.dumps(mkdocs, indent=2)})\")\n\n    logger.info(f\"final mkdocs yml =\\n{mkdocs}\")\n    # write it to the mkdocs.yml file\n    with open(MKDOCS_YML_FNAME, 'w') as outfile:\n        yaml.dump(mkdocs, outfile, default_flow_style=False)\n\n\nif __name__ == \"__main__\":\n    main()\n\n\n\n\n================================================"
  },
  {
    "filename": "index.md",
    "path": "website/index.md",
    "directory": "website",
    "extension": "md",
    "content": "================================================\n# Foundation Model Benchmarking Tool Website\n\nHere you can find results from multiple benchmarking runs. The benchmarking results data is being periodically synched from your data source (typically Amazon S3).\n\n\n\n================================================"
  },
  {
    "filename": "mkdocs_template.yml",
    "path": "website/mkdocs_template.yml",
    "directory": "website",
    "extension": "yml",
    "content": "================================================\nsite_name: Foundation Model Benchmarking Tool (FMBench)\nrepo_name: aws-samples/foundation-model-benchmarking-tool\nrepo_url: https://github.com/aws-samples/foundation-model-benchmarking-tool\nedit_uri: https://github.com/aws-samples/foundation-model-benchmarking-tool/edit/master/\nsite_url: https://foundation-model-benchmarking-tool.github.io\nuse_directory_urls: false\ndocs_dir: /home/ubuntu/fmbench_data\n\nmarkdown_extensions:\n  - toc:\n      permalink: true\n  - pymdownx.highlight:\n      anchor_linenums: true\n      line_spans: __span\n      pygments_lang_class: true\n  - pymdownx.inlinehilite\n  - pymdownx.snippets\n  - pymdownx.superfences\n\ntheme:\n  name: material\n  logo: ./img/fmbt-small.png\n  favicon: website/img/favicon.png\n  features:\n    - navigation\n    - content.code.copy\n  palette:\n      - scheme: default\n        primary: light-blue\n        accent: light-blue\n        toggle:\n            icon: material/weather-sunny\n            name: Switch to dark mode\n      - scheme: slate\n        primary: black\n        accent: deep-orange\n        toggle:\n            icon: material/weather-night\n            name: Switch to light mode\n  edit_uri: ''\n\nextra:\n\n  social:\n    - icon: fontawesome/brands/github-alt\n      link: https://github.com/aws-samples/foundation-model-benchmarking-tool\n    - icon: fontawesome/brands/slack\n      link: https://github.com/aws-samples/foundation-model-benchmarking-tool\nplugins:\n  - search\n  - mknotebooks\n\nnav: \n  - Home: index.md\n\n\n  \n\n\n================================================"
  },
  {
    "filename": "nginx.conf.template",
    "path": "website/nginx.conf.template",
    "directory": "website",
    "extension": "template",
    "content": "================================================\n# /etc/nginx/nginx.conf\n\nuser  nginx;\nworker_processes  auto;\npid /var/run/nginx.pid;\ninclude /etc/nginx/modules-enabled/*.conf;\n\nevents {\n    worker_connections 1024;\n}\n\nhttp {\n    include       /etc/nginx/mime.types;\n    default_type  application/octet-stream;\n\n    # Logging settings\n    access_log /var/log/nginx/access.log;\n    error_log /var/log/nginx/error.log;\n\n    # Include additional config files from conf.d directory\n    include /etc/nginx/conf.d/*.conf;\n\n    # Gzip settings\n    gzip on;\n    gzip_disable \"msie6\";\n\n    server_names_hash_bucket_size 128;\n\n    # Your server block should be inside the http block\n    server {\n        listen 80;\n        server_name __HOSTNAME__;        \n        return 301 https://$host$request_uri;  # Redirect HTTP to HTTPS\n    }\n\n    server {\n        listen 443 ssl;\n        server_name __HOSTNAME__;\n\n        ssl_certificate /etc/nginx/ssl/nginx-selfsigned.crt;\n        ssl_certificate_key /etc/nginx/ssl/nginx-selfsigned.key;\n\n        ssl_protocols TLSv1.2 TLSv1.3;\n        ssl_prefer_server_ciphers on;\n\n        root /usr/share/nginx/html;\n        index index.html;\n\n        # Enable basic authentication\n        auth_basic \"Restricted Content\";\n        auth_basic_user_file /etc/nginx/.htpasswd;\n\n        location / {\n            try_files $uri $uri/ =404;\n        }\n    }\n}"
  }
]
