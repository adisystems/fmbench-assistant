[
  {
    "markdown": "[Skip to content](https://aws-samples.github.io/foundation-model-benchmarking-tool/#benchmark-foundation-models-on-aws)\n\n# Benchmark foundation models on AWS [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/\\#benchmark-foundation-models-on-aws \"Permanent link\")\n\n`FMBench` is a Python package for running performance benchmarks for **any Foundation Model (FM)** deployed on **any AWS Generative AI service**, be it **Amazon SageMaker**, **Amazon Bedrock**, **Amazon EKS**, or **Amazon EC2**. The FMs could be deployed on these platforms either directly through `FMbench`, or, if they are already deployed then also they could be benchmarked through the **Bring your own endpoint** mode supported by `FMBench`.\n\nHere are some salient features of `FMBench`:\n\n1. **Highly flexible**: in that it allows for using any combinations of instance types ( `g5`, `p4d`, `p5`, `Inf2`), inference containers ( `DeepSpeed`, `TensorRT`, `HuggingFace TGI` and others) and parameters such as tensor parallelism, rolling batch etc. as long as those are supported by the underlying platform.\n\n2. **Benchmark any model**: it can be used to be benchmark _open-source models_, _third party models_, and _proprietary models_ trained by enterprises on their own data.\n\n3. **Run anywhere**: it can be run on any AWS platform where we can run Python, such as Amazon EC2, Amazon SageMaker, or even the AWS CloudShell. _It is important to run this tool on an AWS platform so that internet round trip time does not get included in the end-to-end response time latency_.\n\n\n## The need for benchmarking [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/\\#the-need-for-benchmarking \"Permanent link\")\n\nCustomers often wonder what is the best AWS service to run FMs for _my specific use-case_ and _my specific price performance requirements_. While model evaluation metrics are available on several leaderboards ( [`HELM`](https://crfm.stanford.edu/helm/lite/latest/#/leaderboard), [`LMSys`](https://chat.lmsys.org/?leaderboard)), but the price performance comparison can be notoriously hard to find and even more harder to trust. In such a scenario, we think it is best to be able to run performance benchmarking yourself on either on your own dataset or on a similar (task wise, prompt size wise) open-source datasets such as ( [`LongBench`](https://huggingface.co/datasets/THUDM/LongBench), [`QMSum`](https://paperswithcode.com/dataset/qmsum)). This is the problem that [`FMBench`](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/main) solves.\n\n## [`FMBench`](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/main): an open-source Python package for FM benchmarking on AWS [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/\\#fmbench-an-open-source-python-package-for-fm-benchmarking-on-aws \"Permanent link\")\n\n`FMBench` runs inference requests against endpoints that are either deployed through `FMBench` itself (as in the case of SageMaker) or are available either as a fully-managed endpoint (as in the case of Bedrock) or as bring your own endpoint. The metrics such as inference latency, transactions per-minute, error rates and cost per transactions are captured and presented in the form of a Markdown report containing explanatory text, tables and figures. The figures and tables in the report provide insights into what might be the best serving stack (instance type, inference container and configuration parameters) for a given FM for a given use-case.\n\nThe following figure gives an example of the price performance numbers that include inference latency, transactions per-minute and concurrency level for running the `Llama2-13b` model on different instance types available on SageMaker using prompts for Q&A task created from the [`LongBench`](https://huggingface.co/datasets/THUDM/LongBench) dataset, these prompts are between 3000 to 3840 tokens in length. **_Note that the numbers are hidden in this figure but you would be able to see them when you run `FMBench` yourself_**.\n\n![Llama2-13b on different instance types ](https://aws-samples.github.io/foundation-model-benchmarking-tool/img/business_summary.png)\n\nThe following table (also included in the report) provides information about the best available instance type for that experiment1.\n\n| Information | Value |\n| --- | --- |\n| experiment\\_name | llama2-13b-inf2.24xlarge |\n| payload\\_file | payload\\_en\\_3000-3840.jsonl |\n| instance\\_type | ml.inf2.24xlarge |\n| concurrency | \\*\\* |\n| error\\_rate | \\*\\* |\n| prompt\\_token\\_count\\_mean | 3394 |\n| prompt\\_token\\_throughput | 2400 |\n| completion\\_token\\_count\\_mean | 31 |\n| completion\\_token\\_throughput | 15 |\n| latency\\_mean | \\*\\* |\n| latency\\_p50 | \\*\\* |\n| latency\\_p95 | \\*\\* |\n| latency\\_p99 | \\*\\* |\n| transactions\\_per\\_minute | \\*\\* |\n| price\\_per\\_txn | \\*\\* |\n\n1 \\\\*\\\\* values hidden on purpose, these are available when you run the tool yourself.\n\nThe report also includes latency Vs prompt size charts for different concurrency levels. As expected, inference latency increases as prompt size increases but what is interesting to note is that the increase is much more at higher concurrency levels (and this behavior varies with instance types).\n\n![Effect of prompt size on inference latency for different concurrency levels](https://aws-samples.github.io/foundation-model-benchmarking-tool/img/latency_vs_tokens.png)\n\n## Determine the optimal model for your generative AI workload [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/\\#determine-the-optimal-model-for-your-generative-ai-workload \"Permanent link\")\n\nUse `FMBench` to determine model accuracy using a panel of LLM evaluators (PoLL [\\[1\\]](https://aws-samples.github.io/foundation-model-benchmarking-tool/#1)). Here is one of the plots generated by `FMBench` to help answer the accuracy question for various FMs on Amazon Bedrock (the model ids in the charts have been blurred out on purpose, you can find them in the actual plot generated on running FMBench).\n\n![Accuracy trajectory with prompt size](https://aws-samples.github.io/foundation-model-benchmarking-tool/img/accuracy_trajectory_per_payload.png)\n\n![Overall accuracy](https://aws-samples.github.io/foundation-model-benchmarking-tool/img/overall_candidate_model_majority_voting_accuracy.png)\n\n## References [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/\\#references \"Permanent link\")\n\n\\[1\\] [Pat Verga et al., \"Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models\", arXiv:2404.18796, 2024.](https://arxiv.org/abs/2404.18796)",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/",
      "title": "Benchmark foundation models on AWS - Foundation Model Benchmarking Tool (FMBench)",
      "favicon": {},
      "language": "en",
      "scrapeId": "bbb13085-ffd6-4337-b373-fdaf792370c3",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/",
      "statusCode": 200,
      "theme-color": "#00000000",
      "color-scheme": "normal"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "[Skip to content](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html#releases)\n\n# Releases [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#releases \"Permanent link\")\n\n## 2.1.4 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#214 \"Permanent link\")\n\n1. `Llama3.1-8b` config file for `p5en` instance type.\n2. Remove `vllm` from `pyproject.toml`.\n\n## 2.1.3 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#213 \"Permanent link\")\n\n1. SGLang support.\n\n## 2.1.1 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#211 \"Permanent link\")\n\n1. Optimized prompt templates for DeepSeek-R1 and Amazon Nova for `ConvFinQA` and `LongBench` datasets.\n\n## 2.1.0 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#210 \"Permanent link\")\n\n1. Deepseek-R1 distilled model support using [`vllm`](https://github.com/vllm-project/vllm).\n2. Evaluate Deepseek performance with `LongBench`, `OpenOrca`, `Dolly` and [`ConvFinQA`](https://huggingface.co/datasets/AdaptLLM/ConvFinQA) datasets.\n3. Replace `conda` with [`uv`](https://docs.astral.sh/uv/) for faster installs.\n\n## 2.0.27 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#2027 \"Permanent link\")\n\n1. Ollama end to end support\n\n## 2.0.26 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#2026 \"Permanent link\")\n\n1. Bug fix for missing HuggingFace token file.\n2. Config file enhancements\n\n## 2.0.25 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#2025 \"Permanent link\")\n\n1. Fix bug with an alternate VariantName for SageMaker BYOE.\n\n## 2.0.24 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#2024 \"Permanent link\")\n\n1. ARM benchmarking support (AWS Graviton 4).\n2. Relax IAM permission requirements for Amazon SageMaker bring your own endpoint.\n\n## 2.0.23 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#2023 \"Permanent link\")\n\n1. Bug fixes for Amazon SageMaker BYOE.\n2. Additional config files.\n\n## 2.0.22 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#2022 \"Permanent link\")\n\n1. Benchmarks for the [Amazon Nova](https://docs.aws.amazon.com/nova/latest/userguide/what-is-nova.html) family of models.\n2. Benchmarks for multi-modal models: LLama3.2-11B, Claude 3 Sonnet and Claude 3.5 Sonnet using the [ScienceQA](https://huggingface.co/datasets/derek-thomas/ScienceQA) dataset.\n\n## 2.0.21 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#2021 \"Permanent link\")\n\n1. Dynamically get EC2 pricing from Boto3 API.\n2. Update pricing information and model id for Amazon Bedrock models.\n\n## 2.0.20 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#2020 \"Permanent link\")\n\n1. Add `hf_tokenizer_model_id` parameter to automatically download tokenizers from Hugging Face.\n\n## 2.0.19 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#2019 \"Permanent link\")\n\n1. Config files for `Llama3.1-1b` on AMD/Intel CPU instance types.\n2. Bug fixes for token counting for vLLM.\n\n# 2.0.18 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#2018 \"Permanent link\")\n\n1. Delete SageMaker endpoint as soon as the run finishes.\n\n## 2.0.17 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#2017 \"Permanent link\")\n\n1. Add support for embedding models through SageMaker jumpstart\n2. Add support for LLama 3.2 11b Vision Instruct benchmarking through FMBench\n3. Fix DJL Inference while deploying djl on EC2(424 Inference bug)\n\n## 2.0.16 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#2016 \"Permanent link\")\n\n1. Update to torch 2.4 for compatibility with SageMaker Notebooks.\n\n## 2.0.15 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#2015 \"Permanent link\")\n\n1. Support for [Ollama](https://github.com/ollama/ollama), see more details [here](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#benchmarking-models-on-ollama).\n2. Fix bugs with token counting.\n\n## 2.0.14 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#2014 \"Permanent link\")\n\n1. `Llama3.1-70b` config files and more.\n2. Support for [`fmbench-orchestrator`](https://github.com/awslabs/fmbench-orchestrator).\n\n## 2.0.13 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#2013 \"Permanent link\")\n\n1. Update `pricing.yml` additional config files.\n\n## 2.0.11 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#2011 \"Permanent link\")\n\n1. `Llama3.2-1b` and `Llama3.2-3b` support on EC2 g5.\n2. `Llama3-8b` on EC2 `g6e` instances.\n\n## 2.0.9 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#209 \"Permanent link\")\n\n1. Triton-djl support for AWS Chips.\n2. Tokenizer files are now downloaded directly from Hugging Face (unless provided manually as before)\n\n## 2.0.7 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#207 \"Permanent link\")\n\n1. Support Triton-TensorRT for GPU instances and Triton-vllm for AWS Chips.\n2. Misc. bug fixes.\n\n## 2.0.6 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#206 \"Permanent link\")\n\n1. Run multiple model copies with the DJL serving container and an Nginx load balancer on Amazon EC2.\n2. Config files for `Llama3.1-8b` on `g5`, `p4de` and `p5` Amazon EC2 instance types.\n3. Better analytics for creating internal leaderboards.\n\n## 2.0.5 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#205 \"Permanent link\")\n\n1. Support for Intel CPU based instances such as `c5.18xlarge` and `m5.16xlarge`.\n\n## 2.0.4 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#204 \"Permanent link\")\n\n1. Support for AMD CPU based instances such as `m7a`.\n\n## 2.0.3 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#203 \"Permanent link\")\n\n1. Support for a EFA directory for benchmarking on EC2.\n\n## 2.0.2 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#202 \"Permanent link\")\n\n1. Code cleanup, minor bug fixes and report improvements.\n\n## 2.0.0 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#200 \"Permanent link\")\n\n1. 🚨 Model evaluations done by a **Panel of LLM Evaluators** 🚨\n\n## v1.0.52 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#v1052 \"Permanent link\")\n\n1. Compile for AWS Chips (Trainium, Inferentia) and deploy to SageMaker directly through `FMBench`.\n2. `Llama3.1-8b` and `Llama3.1-70b` config files for AWS Chips (Trainium, Inferentia).\n3. Misc. bug fixes.\n\n## v1.0.51 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#v1051 \"Permanent link\")\n\n1. `FMBench` has a [website](https://aws-samples.github.io/foundation-model-benchmarking-tool/index.html) now. Rework the README file to make it lightweight.\n2. `Llama3.1` config files for Bedrock.\n\n## v1.0.50 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#v1050 \"Permanent link\")\n\n1. `Llama3-8b` on Amazon EC2 `inf2.48xlarge` config file.\n2. Update to new version of DJL LMI (0.28.0).\n\n## v1.0.49 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#v1049 \"Permanent link\")\n\n1. Streaming support for Amazon SageMaker and Amazon Bedrock.\n2. Per-token latency metrics such as time to first token (TTFT) and mean time per-output token (TPOT).\n3. Misc. bug fixes.\n\n## v1.0.48 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#v1048 \"Permanent link\")\n\n1. Faster result file download at the end of a test run.\n2. `Phi-3-mini-4k-instruct` configuration file.\n3. Tokenizer and misc. bug fixes.\n\n## v1.0.47 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#v1047 \"Permanent link\")\n\n1. Run `FMBench` as a Docker container.\n2. Bug fixes for GovCloud support.\n3. Updated README for EKS cluster creation.\n\n## v1.0.46 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#v1046 \"Permanent link\")\n\n1. Native model deployment support for EC2 and EKS (i.e. you can now deploy and benchmark models on EC2 and EKS).\n2. FMBench is now available in GovCloud.\n3. Update to latest version of several packages.\n\n## v1.0.45 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#v1045 \"Permanent link\")\n\n1. Analytics for results across multiple runs.\n2. `Llama3-70b` config files for `g5.48xlarge` instances.\n\n## v1.0.44 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#v1044 \"Permanent link\")\n\n1. Endpoint metrics (CPU/GPU utilization, memory utiliztion, model latency) and invocation metrics (including errors) for SageMaker Endpoints.\n2. `Llama3-8b` config files for `g6` instances.\n\n## v1.0.42 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#v1042 \"Permanent link\")\n\n1. Config file for running `Llama3-8b` on all instance types except `p5`.\n2. Fix bug with business summary chart.\n3. Fix bug with deploying model using a DJL DeepSpeed container in the no S3 dependency mode.\n\n## v1.0.40 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#v1040 \"Permanent link\")\n\n1. Make it easy to run in the Amazon EC2 without any dependency on Amazon S3 dependency mode.\n\n## v1.0.39 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#v1039 \"Permanent link\")\n\n1. Add an internal `FMBench` website.\n\n## v1.0.38 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#v1038 \"Permanent link\")\n\n1. Support for running `FMBench` on Amazon EC2 without any dependency on Amazon S3.\n2. [`Llama3-8b-Instruct`](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) config file for `ml.p5.48xlarge`.\n\n## v1.0.37 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#v1037 \"Permanent link\")\n\n1. `g5`/ `p4d`/ `inf2`/ `trn1` specific config files for [`Llama3-8b-Instruct`](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct).\n1. `p4d` config file for both `vllm` and `lmi-dist`.\n\n## v1.0.36 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#v1036 \"Permanent link\")\n\n1. Fix bug at higher concurrency levels (20 and above).\n2. Support for instance count > 1.\n\n## v1.0.35 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#v1035 \"Permanent link\")\n\n1. Support for [Open-Orca](https://huggingface.co/datasets/Open-Orca/OpenOrca) dataset and corresponding prompts for Llama3, Llama2 and Mistral.\n\n## v1.0.34 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#v1034 \"Permanent link\")\n\n1. Don't delete endpoints for the bring your own endpoint case.\n2. Fix bug with business summary chart.\n\n## v1.0.32 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html\\#v1032 \"Permanent link\")\n\n1. Report enhancements: New business summary chart, config file embedded in the report, version numbering and others.\n\n2. Additional config files: Meta Llama3 on Inf2, Mistral instruct with `lmi-dist` on `p4d` and `p5` instances.",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html",
      "title": "Releases - Foundation Model Benchmarking Tool (FMBench)",
      "favicon": {},
      "language": "en",
      "scrapeId": "07f2d3c5-394e-44ad-9c3a-560f008a68d0",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/releases.html",
      "statusCode": 200,
      "theme-color": "#00000000",
      "color-scheme": "normal"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "[Skip to content](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking.html#benchmark-models-deployed-on-different-aws-generative-ai-services)\n\n# Benchmark models deployed on different AWS Generative AI services [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking.html\\#benchmark-models-deployed-on-different-aws-generative-ai-services \"Permanent link\")\n\n`FMBench` comes packaged with configuration files for benchmarking models on different AWS Generative AI services.\n\n## Full list of benchmarked models [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking.html\\#full-list-of-benchmarked-models \"Permanent link\")\n\n| Model | Amazon EC2 | Amazon SageMaker | Amazon Bedrock |\n| --- | --- | --- | --- |\n| **Deepseek-R1 distilled** | g6e | g6e |  |\n| **Llama3.3-70b instruct** |  |  | On-demand |\n| **Qwen2.5-72b** | g5, g6e |  |  |\n| **Amazon Nova** |  |  | On-demand |\n| **Anthropic Claude-3 Sonnet** |  |  | On-demand, provisioned |\n| **Anthropic Claude-3 Haiku** |  |  | On-demand |\n| **Mistral-7b-instruct** | inf2, trn1 | g4dn, g5, p3, p4d, p5 | On-demand |\n| **Mistral-7b-AWQ** |  | p5 |  |\n| **Mixtral-8x7b-instruct** |  |  | On-demand |\n| **Llama3.2-1b instruct** | g5 |  |  |\n| **Llama3.2-3b instruct** | g5 |  |  |\n| **Llama3.1-8b instruct** | g5, p4d, p4de, p5, p5e, g6e, g6, inf2, trn1 | g4dn, g5, p3, inf2, trn1 | On-demand |\n| **Llama3.1-70b instruct** | p4d, p4de, p5, p5e, g6e, g5, inf2, trn1 | inf2, trn1 | On-demand |\n| **Llama3-8b instruct** | g5, g6e, inf2, trn1, c8g | g4dn, g5, p3, inf2, trn1, p4d, p5e | On-demand |\n| **Llama3-70b instruct** | g5 | g4dn, g5, p3, inf2, trn1, p4d | On-demand |\n| **Llama2-13b chat** |  | g4dn, g5, p3, inf2, trn1, p4d | On-demand |\n| **Llama2-70b chat** |  | g4dn, g5, p3, inf2, trn1, p4d | On-demand |\n| **NousResearch-Hermes-70b** |  | g5, inf2, trn1 | On-demand |\n| **Amazon Titan text lite** |  |  | On-demand |\n| **Amazon Titan text express** |  |  | On-demand |\n| **Cohere Command text** |  |  | On-demand |\n| **Cohere Command light text** |  |  | On-demand |\n| **AI21 J2 Mid** |  |  | On-demand |\n| **AI21 J2 Ultra** |  |  | On-demand |\n| **Gemma-2b** |  | g4dn, g5, p3 |  |\n| **Phi-3-mini-4k-instruct** |  | g4dn, g5, p3 |  |\n| **distilbert-base-uncased** |  | g4dn, g5, p3 |  |",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking.html",
      "title": "Main - Foundation Model Benchmarking Tool (FMBench)",
      "favicon": {},
      "language": "en",
      "scrapeId": "0c6118e6-80ba-48c0-9898-193b116c0539",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking.html",
      "statusCode": 200,
      "theme-color": "#00000000",
      "color-scheme": "normal"
    }
  },
  {
    "markdown": "[Skip to content](https://aws-samples.github.io/foundation-model-benchmarking-tool/announcement.html#release-21-announcement)\n\n# Release 2.1 announcement [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/announcement.html\\#release-21-announcement \"Permanent link\")\n\nWe are excited to announce some major new enhancements for `FMBench`.\n\n**Deepseek-R1 support**: The distilled version of Deepseek-R1 models are now supported for both performance benchmarking and model evaluations 🎉. You can use built in support for 4 different datasets: [`LongBench`](https://huggingface.co/datasets/THUDM/LongBench), [`Dolly`](https://huggingface.co/datasets/databricks/databricks-dolly-15k), [`OpenOrca`](https://huggingface.co/datasets/Open-Orca/OpenOrca) and [`ConvFinQA`](https://huggingface.co/datasets/AdaptLLM/finance-tasks/tree/refs%2Fconvert%2Fparquet/ConvFinQA). You can deploy the Deepseek-R1 distilled models on Amazon EC2, Amazon Bedrock or Amazon SageMaker.\n\n**Faster installs with `uv`**: We now use `uv` instead of `conda` for creating a Python environment and installing dependencies for `FMBench`.\n\n# Release 2.0 announcement [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/announcement.html\\#release-20-announcement \"Permanent link\")\n\nWe are excited to share news about a major FMBench release, we now have release 2.0 for FMBench that supports model evaluations through a panel of LLM evaluators🎉. With the recent feature additions to FMBench we are already seeing increased interest from customers and hope to reach even more customers and have an even greater impact. Check out all the latest and greatest features from FMBench on the FMBench website.\n\n**Support for Model Evaluations**: FMBench now adds support for evaluating candidate models using Majority Voting with a [Panel of LLM Evaluators](https://arxiv.org/abs/2404.18796). Customers can now use FMBench to evaluate model accuracy across open-source and custom datasets, thus FMBench now enables customers to not only measure performance (inference latency, cost, throughput) but also model accuracy.\n\n**Native support for LLM compilation and deployment on AWS Silicon**: FMBench now supports end-to-end compilation and model deployment on AWS Silicon. Customers no longer have to wait for models to be available for AWS Chips via SageMaker JumpStart and neither do they have to go through the process of compiling the model to Neuron themselves, FMBench does it all for them. We can simply put the relevant configuration options in the FMBench config file and it will compile and deploy the model on SageMaker ( [config](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-inf2.yml)) or EC2 ( [config](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-inf2-48xl-deploy-ec2.yml)).\n\n**Website for better user experience**: FMBench has a [website](https://aws-samples.github.io/foundation-model-benchmarking-tool/) now along with an [introduction video](https://youtu.be/yvRCyS0J90c). The website is fully searchable to ease common tasks such as installation, finding the right config file, benchmarking on various hosting platforms (EC2, EKS, Bedrock, Neuron, Docker), model evaluation, etc. This website was created based on feedback from several internal teams and external customers.\n\n**Native support for all AWS generative AI services**: FMBench now benchmarks and evaluates any Foundation Model (FM) deployed on any AWS Generative AI service, be it Amazon SageMaker, Amazon Bedrock, Amazon EKS, or Amazon EC2. We initially built FMBench for SageMaker, and later extended it to Bedrock and then based on customer requests extended it to support models on EKS and EC2 as well. See [list of config files](https://aws-samples.github.io/foundation-model-benchmarking-tool/manifest.html) supported out of the box, you can use these config files either as is or as templates for creating your own custom config.",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/announcement.html",
      "title": "Announcement - Foundation Model Benchmarking Tool (FMBench)",
      "favicon": {},
      "language": "en",
      "scrapeId": "d175caec-f195-4501-bb37-50defa13ce10",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/announcement.html",
      "statusCode": 200,
      "theme-color": "#00000000",
      "color-scheme": "normal"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "[Skip to content](https://aws-samples.github.io/foundation-model-benchmarking-tool/accuracy.html#model-evaluations-using-panel-of-llm-evaluators)\n\n# Model evaluations using panel of LLM evaluators [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/accuracy.html\\#model-evaluations-using-panel-of-llm-evaluators \"Permanent link\")\n\n`FMBench` release 2.0.0 adds support for evaluating candidate models using Majority Voting with a Panel of LLM Evaluators (PoLL). It gathers quantitative metrics such as [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) and overall majority voting accuracy metrics to measure the similarity and accuracy of model responses compared to the ground truth.\n\nAccuracy is defined as percentage of responses generated by the LLM that **_match_** the ground truth included in the dataset (as a separate column). In order to determine if an LLM generated response _matches_ the ground truth we ask other LLMs called the evaluator LLMs to compare the LLM output and the ground truth and provide a verdict if the LLM generated ground truth is correct or not given the ground truth. Here is the [link](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/src/fmbench/prompt_template/eval_criteria/claude_eval_prompt_templates/claude_eval_majority_vote.txt) to the Anthropic Claude 3 Sonnet model prompt being used as an evaluator (or a judge model). A combination of the cosine similarity and the LLM evaluator verdict decides if the LLM generated response is correct or incorrect. Finally, one LLM evaluator could be biased, could have inaccuracies so instead of relying on the judgement of a single evaluator, we rely on the majority vote of 3 different LLM evaluators. By default we use the Anthropic Claude 3 Sonnet, Meta Llama3-70b and the Cohere Command R plus model as LLM evaluators. See\n[Pat Verga et al., \"Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models\", arXiv:2404.18796, 2024.](https://arxiv.org/abs/2404.18796) for more details on using a Panel of LLM Evaluators (PoLL).\n\n## Evaluation Flow [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/accuracy.html\\#evaluation-flow \"Permanent link\")\n\n01. Provide a dataset that includes ground truth responses for each sample. `FMBench` uses the [LongBench](https://huggingface.co/datasets/THUDM/LongBench) dataset by default.\n\n02. Configure the candidate models to be evaluated in the `FMBench` config file. See [this](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/src/fmbench/configs/bedrock/config-bedrock.yml) config file for an example that runs evaluations for multiple models available via Amazon Bedrock. Running evaluations only requires the following two changes to the config file:\n    - Set the `4_get_evaluations.ipynb: yes`, see [this](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/7bb00debea4ffacecf7acaeaf54c9039a42022f7/src/fmbench/configs/bedrock/config-bedrock.yml#L77) line.\n    - Set the `ground_truth_col_key: answers` and `question_col_key: input` parameters, see [this](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/7bb00debea4ffacecf7acaeaf54c9039a42022f7/src/fmbench/configs/bedrock/config-bedrock.yml#L90) line. The value of `ground_truth_col_key` and the `question_col_key` is set to the name of the column in the dataset that contains the ground truth and question respectively.\n03. Run FMBench, which will:\n\n04. Fetch the inference results containing the model responses\n\n05. Calculate quantitative metrics (Cosine Similarity)\n\n06. Use a Panel of LLM Evaluators to compare each model response to the ground truth\n\n07. Each LLM evaluator will provide a binary verdict (correct/incorrect) and an explanation\n\n08. Validate the LLM evaluations using Cosine Similarity thresholds\n\n09. Categorize the final evaluation for each response as correctly correct, correctly incorrect, or needs further evaluation\n\n10. Review the `FMBench` report to analyze the evaluation results and compare the performance of the candidate models. The report contains tables and charts that provide insights into model accuracy.\n\n\nBy leveraging ground truth data and a Panel of LLM Evaluators, FMBench provides a comprehensive and efficient way to assess the quality of generative AI models. The majority voting approach, combined with quantitative metrics, enables a robust evaluation that reduces bias and latency while maintaining consistency across responses.",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/accuracy.html",
      "title": "Model evaluations - Foundation Model Benchmarking Tool (FMBench)",
      "favicon": {},
      "language": "en",
      "scrapeId": "32588fab-438d-47af-b6a9-0e64cf58602c",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/accuracy.html",
      "statusCode": 200,
      "theme-color": "#00000000",
      "color-scheme": "normal"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "[Skip to content](https://aws-samples.github.io/foundation-model-benchmarking-tool/workflow.html#workflow-for-fmbench)\n\n# Workflow for `FMBench` [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/workflow.html\\#workflow-for-fmbench \"Permanent link\")\n\nThe workflow for `FMBench` is as follows:\n\n```md-code__content\nCreate configuration file\n        |\n        |-----> Deploy model on SageMaker/Use models on Bedrock/Bring your own endpoint\n                    |\n                    |-----> Run inference against deployed endpoint(s)\n                                     |\n                                     |------> Create a benchmarking report\n\n```\n\n1. Create a dataset of different prompt sizes and select one or more such datasets for running the tests.\n1. Currently `FMBench` supports datasets from [LongBench](https://github.com/THUDM/LongBench) and filter out individual items from the dataset based on their size in tokens (for example, prompts less than 500 tokens, between 500 to 1000 tokens and so on and so forth). Alternatively, you can download the folder from [this link](https://huggingface.co/datasets/THUDM/LongBench/resolve/main/data.zip) to load the data.\n2. Deploy **any model** that is deployable on SageMaker on **any supported instance type** ( `g5`, `p4d`, `Inf2`).\n1. Models could be either available via SageMaker JumpStart (list available [here](https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html)) as well as models not available via JumpStart but still deployable on SageMaker through the low level boto3 (Python) SDK (Bring Your Own Script).\n2. Model deployment is completely configurable in terms of the inference container to use, environment variable to set, `setting.properties` file to provide (for inference containers such as DJL that use it) and instance type to use.\n3. Benchmark FM performance in terms of inference latency, transactions per minute and dollar cost per transaction for any FM that can be deployed on SageMaker.\n1. Tests are run for each combination of the configured concurrency levels i.e. transactions (inference requests) sent to the endpoint in parallel and dataset. For example, run multiple datasets of say prompt sizes between 3000 to 4000 tokens at concurrency levels of 1, 2, 4, 6, 8 etc. so as to test how many transactions of what token length can the endpoint handle while still maintaining an acceptable level of inference latency.\n4. Generate a report that compares and contrasts the performance of the model over different test configurations and stores the reports in an Amazon S3 bucket.\n1. The report is generated in the [Markdown](https://en.wikipedia.org/wiki/Markdown) format and consists of plots, tables and text that highlight the key results and provide an overall recommendation on what is the best combination of instance type and serving stack to use for the model under stack for a dataset of interest.\n2. The report is created as an artifact of reproducible research so that anyone having access to the model, instance type and serving stack can run the code and recreate the same results and report.\n5. Multiple [configuration files](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/main/src/fmbench/configs) that can be used as reference for benchmarking new models and instance types.",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/workflow.html",
      "title": "Workflow - Foundation Model Benchmarking Tool (FMBench)",
      "favicon": {},
      "language": "en",
      "scrapeId": "24ec68c5-d2ff-42ca-9a80-ac67bb1a4889",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/workflow.html",
      "statusCode": 200,
      "theme-color": "#00000000",
      "color-scheme": "normal"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "[Skip to content](https://aws-samples.github.io/foundation-model-benchmarking-tool/results.html#results)\n\n# Results [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/results.html\\#results \"Permanent link\")\n\nDepending upon the experiments in the config file, the `FMBench` run may take a few minutes to several hours. Once the run completes, you can find the report and metrics in the local `results-*` folder in the directory from where `FMBench` was run. The rpeort and metrics are also written to the write S3 bucket set in the [config file](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/src/fmbench/configs/config-mistral-7b-tgi-g5.yml#L12).\n\nHere is a screenshot of the `report.md` file generated by `FMBench`.\n![Report](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/img/results.gif?raw=true)",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/results.html",
      "title": "Report - Foundation Model Benchmarking Tool (FMBench)",
      "favicon": {},
      "language": "en",
      "scrapeId": "2e2537d3-a651-4ab9-ac79-a30994195fd5",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/results.html",
      "statusCode": 200,
      "theme-color": "#00000000",
      "color-scheme": "normal"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "[Skip to content](https://aws-samples.github.io/foundation-model-benchmarking-tool/cli.html#fmbench-cli)\n\n# `FMBench` CLI [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/cli.html\\#fmbench-cli \"Permanent link\")\n\nHere are the command line options currently supported by the `fmbench` CLI.\n\n```md-code__content\nusage: fmbench [-h] --config-file CONFIG_FILE [--role-arn ROLE_ARN] [--local-mode {yes,no}] [--tmp-dir TMP_DIR] [--write-bucket WRITE_BUCKET] -A [key=value]\n\nRun FMBench with a specified config file.\n\n```\n\noptions:\n\n`-h`, `--help` show this help message and exit\n\n`--config-file` CONFIG\\_FILE\nThe S3 URI of your Config File\n\n`--role-arn` ROLE\\_ARN ( _Optional_) The ARN of the role to be used for FMBench. If an Amazon SageMaker endpoint is being deployed through FMBench then this role would also be used by that endpoint\n\n`--local-mode` {yes,no} Specify if running in local mode or not. Options: yes, no. Default is no.\n\n`--tmp-dir` TMP\\_DIR ( _Optional_) An optional tmp directory if fmbench is running in local mode.\n\n`--write-bucket` WRITE\\_BUCKET Write bucket that is used for sagemaker endpoints in local mode and storing metrics in s3 mode.\n\n`-A` key=value ( _Optional_) Specify a key value pair which will be used to replace the `{key}` in the given config file. The key could be anything that you have templatized in the config file as `{key}` and it will be replaced with `value`. This comes in handy when using a generic configuration file and replace keys such `model_id`, `tp_degree` etc. Note that you are not limited to pre-defined set of keys, you can put any key in the config file as `{key}` and it will get replaced with its value. If there are multiple key value pairs, then simply specify the `-A` option multiple times in the command line.",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/cli.html",
      "title": "CLI - Foundation Model Benchmarking Tool (FMBench)",
      "favicon": {},
      "language": "en",
      "scrapeId": "4f51f51b-5906-49bb-9f41-f84a664d5886",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/cli.html",
      "statusCode": 200,
      "theme-color": "#00000000",
      "color-scheme": "normal"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "[Skip to content](https://aws-samples.github.io/foundation-model-benchmarking-tool/byoe.html#bring-your-own-endpoint-aka-support-for-external-endpoints)\n\n# Bring your own endpoint (a.k.a. support for external endpoints) [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/byoe.html\\#bring-your-own-endpoint-aka-support-for-external-endpoints \"Permanent link\")\n\nIf you have an endpoint deployed on say `Amazon EKS` or `Amazon EC2` or have your models hosted on a fully-managed service such as `Amazon Bedrock`, you can still bring your endpoint to `FMBench` and run tests against your endpoint. To do this you need to do the following:\n\n1. Create a derived class from [`FMBenchPredictor`](https://aws-samples.github.io/foundation-model-benchmarking-tool/src/fmbench/scripts/fmbench_predictor.py) abstract class and provide implementation for the constructor, the `get_predictions` method and the `endpoint_name` property. See [`SageMakerPredictor`](https://aws-samples.github.io/foundation-model-benchmarking-tool/src/fmbench/scripts/sagemaker_predictor.py) for an example. Save this file locally as say `my_custom_predictor.py`.\n\n2. Upload your new Python file ( `my_custom_predictor.py`) for your custom FMBench predictor to your `FMBench` read bucket and the scripts prefix specified in the `s3_read_data` section ( `read_bucket` and `scripts_prefix`).\n\n3. Edit the configuration file you are using for your `FMBench` for the following:\n   - Skip the deployment step by setting the `2_deploy_model.ipynb` step under `run_steps` to `no`.\n   - Set the `inference_script` under any experiment in the `experiments` section for which you want to use your new custom inference script to point to your new Python file ( `my_custom_predictor.py`) that contains your custom predictor.",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/byoe.html",
      "title": "Bring your own endpoint (a.k.a. support for external endpoints) - Foundation Model Benchmarking Tool (FMBench)",
      "favicon": {},
      "language": "en",
      "scrapeId": "04281047-0891-4014-9609-42c022cc4481",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/byoe.html",
      "statusCode": 200,
      "theme-color": "#00000000",
      "color-scheme": "normal"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "[Skip to content](https://aws-samples.github.io/foundation-model-benchmarking-tool/customize_config_files.html#customize-config-files-for-specific-use-cases)\n\n# Customize config files for specific use cases [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/customize_config_files.html\\#customize-config-files-for-specific-use-cases \"Permanent link\")\n\n## Overview [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/customize_config_files.html\\#overview \"Permanent link\")\n\nTo run `FMBench`, you have to provide a configuration file. A configuration file is simple `yml` file that contains the information about the models to benchmark, dataset information, prompt templates, custom thresholds for latency, cost and accuracy and other important metrics. View an annotated config file [here](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/fmbench/configs/llama2/7b/config-llama2-7b-g5-quick.yml). A typical `FMBench` workflow involves either directly using an already provided config file from the `configs` folder provided in the `FMBench` [website](https://aws-samples.github.io/foundation-model-benchmarking-tool/manifest.html) or [Github repo](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/main/src/fmbench/configs) or editing an already provided config file as per your own requirements (for example, benchmarking on a different instance type, or a different inference container, or even with custom datasets and various models).\n\nIn this documentation, we will do a walkthrough of the different sections that you can change within the config file based on your specific use case and requirements. **We will take an example of a user who wants to create a config file for `NousResearch/Hermes-3-Llama-3.1-70B` model on a `trn1.32xlarge` EC2 instance.**\n\n**_Note: This lab is not a hands-on lab. It is a walk through of a sample configuration file that `FMBench` uses to benchmark any Foundation Model (FM) on any AWS generative AI service and description of sections that users can tweak for their own use case._**\n\nLet's get started:\n\n### FMBench Configuration Walkthrough [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/customize_config_files.html\\#fmbench-configuration-walkthrough \"Permanent link\")\n\nLet's take an example to walk through a sample config file. Say a user is interested in using `llama3-70b` for their `question-answering` and `doc-summarization` use cases. A couple of questions they would ask themselves before beginning the benchmarking process is: **_Which model should I use? Should it be open-source/closed-source or proprietary fine-tuned models?, What instance should I host this model on so I can get my minimum requirements for latency, cost and accuracy satisfied?, Which dataset should I use - is there an open source data that I can use as a representation of my own dataset, or can I benchmark using my custom enterprise data? How do I compute pricing? What are the ways I can evaluate my models on accuracy?_** and so on.\n\nThe `FMBench` configuration file takes away the cognitive burden to figure out the answer to these questions and organizing them into parameters for `model id`, `instance types`, `inference containers`, datasets to use, and various other metrics that play a role in model performance and accuracy. The `FMBench` config file is broadly divided in the following:\n\n### Model Information [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/customize_config_files.html\\#model-information \"Permanent link\")\n\nTo decide on which model to use, on a given instance type and container, fill out the information in the `experiments` section of the configuration file. This `experiments` section contains configuration about experiments to be run. The `experiments` section is an array so more than one experiments can be added, these could belong to the **same model but different instance types, or different models, or even different hosting options**. Each experiment represents model under test and the specific information associated to that model. View an example below.\n\n```md-code__content\nexperiments:\n- name: \"Hermes-3-Llama-3.1-70B\"\n    region: {region}\n    model_id: NousResearch/Hermes-3-Llama-3.1-70B #model id, version and image uri not needed for byo endpoint\n    hf_tokenizer_model_id: meta-llama/Llama-3.1-70B\n    model_version:\n    model_name: \"Hermes-3-Llama-3.1-70B\"\n    ep_name: 'http://localhost:8080/v2/models/Hermes-3-Llama-3.1-70B/generate'\n    instance_type: \"trn1.32xlarge\"\n    image_uri: tritonserver-neuronx:fmbench\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n    model_loading_timeout: 10000\n    inference_spec:\n    parameter_set: ec2_djl\n    container_type: triton\n    # For deploying a model using the triton inference container:\n    # mention the backend type, if any. Options for triton are: [djl, vllm or tensorrt]\n    backend: djl\n    # how many copies of the model, 1, 2,..max\n    # set to 1 in the code if not configured,\n    # max: FMBench figures out the max number of model containers to be run\n    #      based on TP degree configured and number of neuron cores/GPUs available.\n    #      For example, if TP=2, GPUs=8 then FMBench will start 4 containers and 1 load balancer,\n    # auto: only supported if the underlying inference container would automatically\n    #       start multiple copies of the model internally based on TP degree and neuron cores/GPUs\n    #       available. In this case only a single container is created, no load balancer is created.\n    #       The DJL serving containers supports auto.\n    model_copies: max\n    shm_size: 12g\n    # The model.json parameters are replaced within the model.json file\n    # for the triton on vllm/djl/tensorrt options. The model.json already contains\n    # the tp degree and model id from above in this config file. This is a dictionary\n    # that contains other variables, such as max_num_seqs, dtype, max_model_len, block_size and more\n    # For tensorrt, the tp degree, batch size and other relevant parameters are\n    # extracted directly from the inference spec.\n    container_params:\n        # tp degree is a mandatory parameter\n        tp_degree: 32\n        amp: \"f16\" # and so on\n        # modify the serving properties to match your model and requirements\n        serving.properties:\n\n```\n\nHere are the list of parameters that can be changed based on your use case and models you would like to benchmark:\n\n- **Model Id ( `model_id`)**: This is the `model id` of the model that you would like to benchmark. This can be any open source model on `HuggingFace`, a `SageMaker Jumpstart` model, a `Bedrock` model, or any Foundation Model that you would like to benchmark on **any AWS generative AI service**. For this specific example, the user was interested in benchmarking the fine-tuned version of `Llama-3.1-70B`, so they pointed the `model_id` to the HF model: `NousResearch/Hermes-3-Llama-3.1-70B`. You can change the `name` and the `model_name` parameter to any custom name that you would like to based on the `model_id` that you are using in the config file.\n\n- **Tokenizer ( `hf_tokenizer_model_id`)**: If your model is a Hugging Face model, and if you would like to use that model's tokenizer, then point the `hf_tokenizer_model_id` parameter to the `model_id` on hugging face and that specific model's tokenizer will be used in the benchmarking test.\n\n- **Instance Type ( `instance_type`)**: This is the instance type/hardware on which the model is deployed and hosted. In this case, the user was interested to deploy the model on a `trn1.32xlarge` instance, so they pointed the `instance_type` parameter to `trn1.32xlarge`. You can point this parameter to any `instance_type` that you want to deploy the model on. This can either be a `GPU`/ `CPU`/ `AWS Silicon (i.e. inf2/trn1/trn2)` instance. View the list of models that have been benchmarked on various instances using `FMBench` [here](https://github.com/aws-samples/foundation-model-benchmarking-tool?tab=readme-ov-file#full-list-of-benchmarked-models)\n\n- **Inference Container ( `image_uri`)**: If the user is interested in using a specific container of choice, they can point the `image_uri` parameter to that inference container. `FMBench` supports the `HF TGI`, `Triton`, `Deep Java Library`, `vLLM` and `Ollama` containers. This means that the user would not have to write any custom code to deploy the model or benchmark it using any of these containers that `FMBench` provides built in support for. In this case, the user was interested in benchmarking `NousResearch/Hermes-3-Llama-3.1-70B` on the `triton` inference server, so they pointed the `image_uri` to `tritonserver-neuronx:fmbench`. Users can bring their own containers and point to that within the configuration file (this would require the user to provide a custom deployment and inference script that supports the deployment and prediction format that the specific inference container supports if it is not already supported on `FMBench`).\n\n- **Inference/Deployment Scripts ( `deployment_script`, `inference_script`)**: `FMBench` comes packaged with multiple inference and deployment scripts. These scripts will deploy models on `SageMaker`, `Bedrock`, `EC2`, `EKS`, and also support inference on those models based on their respective inference scripts. If users deploy and make inferences from a model using a format that is not already supported on `FMBench`, users can **bring in custom deployment and predictor scripts**. Given above is an example for a model deployed on an Amazon `EC2` instance using the `ec2_deploy.py` deployment script and make inferences on the model using the `ec2_predictor.py inference script.` To view how you can bring your own custom deployment and inference files to `FMBench` to benchmark your custom models, view [here](https://aws-samples.github.io/foundation-model-benchmarking-tool/byo_rest_predictor.html). An example custom inference script [here](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/fmbench/scripts/rest_predictor.py) that is specified in [this](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/82fc6f08f168b9c1c2ff1d72a01b1004525708d6/fmbench/configs/llama2/13b/config-byo-rest-ep-llama2-13b.yml#L184) configuration file.\n\n- **Endpoint Name ( `ep_name`)**: This parameter specifies the endpoint URL where the model will be accessible. In the example, it's set to `'http://localhost:8080/v2/models/Hermes-3-Llama-3.1-70B/generate'` since the model is hosted on an EC2 instance. For models deployed on `Bedrock`, the `ep_name` is the `Bedrock model_id` since that is what is used while running inferences against the model. If your model is deployed on `SageMaker`, then the endpoint name is dynamically created based on what you provide as the `ep_name` in the configuration file. If you already have a model deployed and want to use your own endpoint, you can:\n\n\n  - Set `deploy: no` in the experiment configuration\n  - Provide your existing `EC2` endpoint URL/SageMaker endpoint in the `ep_name` field\n  - Skip the deployment-specific parameters as they won't be needed\n\nFor more information on bringing your own endpoint, view the documentation on it [here](https://aws-samples.github.io/foundation-model-benchmarking-tool/byoe.html).\n\n- **Container Parameters ( `container_params`/ `serving.properties`)**: This section allows you to configure model-specific serving parameters such as:\n  - `tp_degree`: Tensor parallelism degree for distributed inference\n  - `amp`: Automatic mixed precision settings (e.g., \"f16\", \"bf16\")\n  - `serving.properties`: Additional serving configuration parameters specific to your inference container such as `max_rolling_batch_size`, `n_positions`, etc.\n    These parameters are not limited and can be changed/extended based on the parameters supported by your inference container.\n\n### Inference Parameters [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/customize_config_files.html\\#inference-parameters \"Permanent link\")\n\nAfter configuring the model deployment settings, the next step is to specify how you want the model to generate responses. The inference parameters section allows you to customize the generation behavior based on your use case:\n\n\n```md-code__content\ninference_parameters:\n  ec2_djl:\n    top_k: 50\n    max_new_tokens: 100\n\n```\n\nThese parameters directly affect the model's output and performance characteristics. For example:\n\n- For a summarization use case, a user might want to set `max_new_tokens` to a higher value like `512` or `1024` to allow for comprehensive summaries of longer documents.\n- For a quick Q&A application, you might keep `max_new_tokens` lower at `100-200` to get concise responses.\n- The `top_k` parameter controls response diversity by limiting the token selection to the k most likely next tokens.\n\nYou can add any parameter that your **inference container supports**. The parameters are organized by deployment type (ec2\\_djl, SageMaker, bedrock, any custom parameters that you would want to set etc.) to match the `parameter_set` specified in your experiment configuration. For example, if using Bedrock's model, you would specify `bedrock` or any custom parameter set name:\n\n```md-code__content\ninference_parameters:\n  bedrock:\n    temperature: 0.7\n    max_new_tokens: 512\n    top_p: 0.9\n\n```\n\nOnce you have defined your inference parameters, you can point to that inference parameter spec in the experiment section as given below:\n\n`yaml\n# Model configurations for llama-2 7b for deploying on g5 x and 2x large instances\nexperiments:\n  - name: <model_name>\n    model_id: # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: <your-model-name>\n    ep_name: \"<your-endpoint-name>\"\n    .\n    .\n    .\n    inference_spec:\n      parameter_set: bedrock # you can have a different inference parameter set for each experiment depending on the\n                             # model inference parameters`\n\n#### Use custom datasets & prompts within `FMBench` [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/customize_config_files.html\\#use-custom-datasets-prompts-within-fmbench \"Permanent link\")\n\n`FMBench` now supports benchmarking models using datasets from Hugging Face with a simplified prefixing method. To specify a Hugging Face dataset and its split, use the `hf:` prefix followed by the `dataset identifier`, `subset name`, and `split name`. If a `subset name` is not provided, it defaults to `default`. If a `split name` is not provided, `FMBench` automatically selects the next available split at runtime.\n\n- To configure your dataset in `FMBench`, add entries to `source_data_files` in your configuration file:\n\n\n\n\n```md-code__content\nsource_data_files:\n# Format: hf:dataset-id/subset-name/split-name\n# If no subset name is provided, use \"default\".\n- hf:THUDM/LongBench/2wikimqa_e/test\n- hf:THUDM/LongBench/2wikimqa/test\n- hf:THUDM/LongBench/hotpotqa_e/test\n- hf:THUDM/LongBench/hotpotqa/test\n- hf:THUDM/LongBench/narrativeqa/test\n- hf:THUDM/LongBench/triviaqa_e/test\n- hf:THUDM/LongBench/triviaqa/test\n\n```\n\nYou can follow this format for any `text` or `image-based` dataset from Hugging Face. Alternatively, you can use custom datasets in `JSONL` format.\n\n- For domain-specific or personalized benchmarking, you can use custom datasets. These datasets can be:\n\n- **Synthetic/Open source datasets (available on Hugging Face)**\n- **Proprietary data (not publicly available)**\n\n- To use custom data, convert it into JSONL format. We provide a sample notebook to help convert Hugging Face or custom datasets into JSONL and upload them to an S3 bucket used by FMBench. Follow the steps in the [bring\\_your\\_own\\_dataset](https://aws-samples.github.io/foundation-model-benchmarking-tool/(https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/src/fmbench/bring_your_own_dataset.ipynb)) notebook to integrate your own dataset into `FMBench`. Place these `JSONL` files in the `source_data` directory within `/tmp/fmbench-read/source_data` in your local instance. .\n\n- **Use specific keys from the dataset in your prompts**: Since `FMBench` uses `LongBench` as the dataset under test by default, it requires specific keys that contain `user queries`, `context`, or other necessary fields. To specify dataset keys, add them under `prompt_template_keys` in the `datasets` section of your configuration file:\n\n\n```md-code__content\ndatasets:\n    prompt_template_keys:\n      - input\n      - context\n\n```\n\nThese keys correspond to fields in the Hugging Face dataset, as shown in the example below:\n\n![hf_ds_keys](https://aws-samples.github.io/foundation-model-benchmarking-tool/img/hf_ds_keys.png)\n\n- **Using a Custom Prompt Template**: The specified dataset keys can be used in a custom prompt template for generating input payloads. Below is an example of a prompt template utilizing these keys:\n\n```md-code__content\n<think>\nThere can be multiple question answer pairs in the context.\nAs soon as you find the first question in the text below immediately stop reading any further and just answer the question.\nAlways start your response with \"<think>\" at the beginning of every output and think step by step.\nKeep your thinking process short and your answers concise, do not overthink.\nMake sure to always provide an answer, if you do not know the answer then say I do not known but never leave the answer field empty in your response.\n</think>\n\n<answer>\nPut your final answer in one line starting with the word Answer:\n</answer>\n\nHere is the text for you to work on:\n\n<text>\n{input}\n\n{context}\n</text>\n\n```\n\n- **Adding the Prompt Template to FMBench**: To use the custom prompt template, place it in the `/tmp/fmbench-read/prompt_templates` directory. `FMBench` will download and apply it during benchmarking.\n\n```md-code__content\n# prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n# the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\nprompt_template_file: prompt_template_llama3.txt\n\n```\n\nBy following these steps, you can seamlessly integrate Hugging Face datasets or custom data into FMBench, enabling tailored benchmarking for your use case.\n\n**Filtering Options**: If your dataset contains multiple languages and includes a language field, you can filter it to retain only prompts in a specific language. Additionally, you can filter prompts based on token length, which is determined using the tokenizer specified in the tokenizer\\_prefix in the S3 bucket. The example below filters for English prompts with a token length between `1,000` and `2,000`, saving the results in a designated payload file that `FMBench` then uses in the benchmarking test. You can filter this based on your custom token length filtering preferences.\n\n```md-code__content\ndatasets:\nfilters:\n    - language: en\n      min_length_in_tokens: 1000\n      max_length_in_tokens: 2000\n      payload_file: payload_en_1000-2000.jsonl\n\n```\n\n**Metrics Configuration**: Specify `dataset_of_interest` for focused performance analysis. While the tests would run on all the datasets configured in the experiment entries below but the price\\|performance analysis is only done for 1 dataset which is listed below as the dataset\\_of\\_interest. If a user is interested in seeing model benchmarks for prompt sizes `1000-2000` tokens, then set the `dataset_of_interest` to `en_1000-2000`. If it is a `summarization use case` and your dataset is large enough, you can add a filter to use `payload_en_3000-3840.jsonl` and set the `dataset_of_interest` to `en_3000-3840` tokens. This can be any custom value.\n\n```md-code__content\ndatasets:\n.\n.\n.\nmetrics:\n    dataset_of_interest: en_1000-2000\n\n```\n\n#### Bring your own Endpoint (BYOE Configuration) [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/customize_config_files.html\\#bring-your-own-endpoint-byoe-configuration \"Permanent link\")\n\n- You can customize FMBench to use the BYOE mode when you want to bring an already deployed model either on AWS or your custom infrastructure.\n\n- Point the `ep_name` parameter in your configuration file to the `endpoint URL` so that `FMBench` can use it while making predictions. View [`here`](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/fmbench/configs/byoe/config-byo-custom-rest-predictor.yml#L199).\n\n\n```md-code__content\nep_name: '<your-ep-url>' # public DNS/URL to send your request\n\n```\n\n\\- If you have an endpoint that has a custom inference format, then create a derived class from [FMBenchPredictor](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/fmbench/scripts/fmbench_predictor.py) abstract class and provide implementation for the constructor, the `get_predictions` method and the `endpoint_name` property. Specify the name of that file in the config file next to the `inference_script` parameter [here](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/e28af9bfa524a0e54205fc50e5f717d81c8d75a9/fmbench/configs/byoe/config-byo-custom-rest-predictor.yml#L212). No deployment script is needed since you are bringing your own endpoint.\n\n```md-code__content\n# FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n# and Bedrock. You can also add your own. This is an example of a custom rest predictor\n# that does a POST request on the endpoint URL with custom headers,\n# parameters and authentication information\ninference_script: custom_rest_predictor.py\n\n```\n\n- Place your custom FMBench predictor (custom\\_rest\\_predictor.py) in your EC2 `/tmp/fmbench-read/scripts` directory. View an example of an inference file that you can use here: https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/fmbench/scripts/custom\\_rest\\_predictor.py.\n\n- Set the `deploy` variable in the experiments section of the config file to `no` because the model does not have to be deployed since this is a `byoe` mode. Set the `2_deploy_model.ipynb` notebook in the `run_steps` section to `yes`. Even though the model is not deployed, the notebook will identify that `deploy` from the experiments section is set to `no` and just log the provided endpoint for further use.\n\n\n```md-code__content\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes # Set the deploy notebook to yes. This will not deploy the model, but will identify that the `deploy` variable in the `experiments` section below is set to 'no',\n                              # and will just log the endpoint provided for further use in the benchmarking test\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: no\n.\n.\n.\n.\nexperiments:\n- name: bring-your-own-sm-endpoint\nmodel_id: # model id, version and image uri not needed for byo endpoint\nmodel_version:\ndeploy: no # set deploy to \"no\" in the experiments section because the model does not have to be deployed since this is a byoe mode\nmodel_name: <your-model-name>\nep_name: \"<your-endpoint-name>\"\ninstance_type:  \"<your-instance-type>\"\n\n```\n\n- Build FMBench as per instructions [here](https://aws-samples.github.io/foundation-model-benchmarking-tool/build.html). This will install a developer version of FMBench in your Python venv.\n\nAfter following these steps, you will be able to run `FMBench` with your own endpoint. `FMBench` will utilize the custom `FMBench` predictor and run inferences against the endpoint. All raw inferences are saved in a `per_inference` directory and used in the report generation process. Follow the steps in the next section to bring your own dataset and prompt templates.\n\n#### Pricing Information [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/customize_config_files.html\\#pricing-information \"Permanent link\")\n\n`FMBench` measures model performance, which translates into inference latency, token throughput and cost per transaction. The cost is determined by `FMBench` in two ways: instance based pricing or token based pricing. All pricing information is stored in a [`pricing.yml`](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/fmbench/configs/pricing.yml) which contains [hourly instance based pricing](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/e28af9bfa524a0e54205fc50e5f717d81c8d75a9/fmbench/configs/pricing.yml#L2) (for example Amazon EC2 instances) and [token based pricing](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/e28af9bfa524a0e54205fc50e5f717d81c8d75a9/fmbench/configs/pricing.yml#L36) (for example Amazon Bedrock). The existing file contains several prices for instances on Amazon EC2 and SageMaker. To bring your own pricing, simply specify the name of your instance type followed by the custom hourly/token-based price and FMBench will use that pricing in the benchmarking test.\n\n```md-code__content\npricing: pricing.yml # <your-custom-pricing.yml>\n\n```\n\nAdd your pricing in the `pricing.yml` file:\n\n```md-code__content\ninstance_based:\nyour-custom-instance-type: <your-pricing>\ntoken_based:\n<your-model-id>:\n    input-per-1k-tokens: <custom price per 1k input tokens>\n    output-per-1k-tokens: <custom price per 1k output tokens>\n\n```\n\n**Note**: Make sure the instance type specified in your `FMBench` config file matches the instance type specified in the `pricing.yml` file so that FMBench can correctly map the cost during the test. Place the pricing.yml file in the `/tmp/fmbench-read/configs` directory.\n\n#### Model Evaluations [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/customize_config_files.html\\#model-evaluations \"Permanent link\")\n\nAccuracy is defined as percentage of responses generated by the `LLM` that match the ground truth included in the dataset (as a separate column). In order to determine if an `LLM` generated response matches the ground truth we ask other `LLMs` called the evaluator `LLMs` to compare the `LLM` output and the ground truth and provide a verdict if the `LLM` generated ground truth is correct or not given the ground truth. Here is the [link](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/fmbench/prompt_template/eval_criteria/claude_eval_prompt_templates/claude_eval_majority_vote.txt) to the `Anthropic Claude 3 Sonnet model` prompt being used as an evaluator (or a judge model). A combination of the `cosine similarity` and the LLM evaluator verdict decides if the `LLM` generated response is correct or incorrect. Finally, one `LLM` evaluator could be biased, could have inaccuracies so instead of relying on the judgement of a single evaluator, we rely on the majority vote of 3 different LLM evaluators. By default we use the `Anthropic Claude 3.5 Sonnet V2`, `Meta Llama3.3-70b Instruct` and the `Cohere Command R plus` model as LLM evaluators. See **_Pat Verga et al., \"Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models\", arXiv:2404.18796, 2024._** for more details on using a `Panel of LLM Evaluators (PoLL)`. The following [file](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/fmbench/configs/model_eval_all_info.yml) in the configuration file contains judge model information, prompt templates used for evaluations, inference parameters, etc.\n\n```md-code__content\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n```\n\nFor more information on model evaluations using `FMBench` view this [notebook](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/fmbench/4_get_evaluations.ipynb) and this [documentation](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/docs/accuracy.md).\n\n- This file contains information about metrics and the LLM judges (with their inference parameters) that will be used while evaluating candidate models. To add the evaluation step to `FMBench`, add it as a step under `run_steps` section in the configuration file (view step 4):\n\n```md-code__content\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: no\n    3_run_inference.ipynb: yes\n    4_get_evaluations.ipynb: yes\n    5_model_metric_analysis.ipynb: yes\n    6_cleanup.ipynb: no\n\n```\n\n- `FMBench`'s panel of LLM judges uses the model responses and compares it to the ground truth provided in the dataset. If there is a ground truth column, replace the following parameters with the name of the column. The ground truth and question column keys can be fetched from the Hugging Face dataset or your custom dataset. View an example below:\n\n![ground-truth-col-key](https://aws-samples.github.io/foundation-model-benchmarking-tool/img/ground_truth_info.png)\n\nThen, use the question and ground truth key from the dataset below in the configuration file. This will be used by `FMBench`'s evaluators to evaluate the correctness of models to be benchmarked.\n\n```md-code__content\n# Represents the column with the ground truth\nground_truth_col_key: answers\n# Represents the column with questions/instructions\nquestion_col_key: input\n\n```\n\n#### Benchmarking Thresholds & components [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/customize_config_files.html\\#benchmarking-thresholds-components \"Permanent link\")\n\nThe `report` section allows you to set specific performance thresholds and constraints for your use case. These thresholds help determine whether a model deployment configuration meets your requirements:\n\n```md-code__content\nreport:\n    latency_budget: 3\n    cosine_similarity_budget: 0.3\n    accuracy_budget: 1\n    accuracy_error_rate_budget: 0\n    cost_per_10k_txn_budget: 200\n    error_rate_budget: 0\n\n```\n\nIn this use case, the user was interested in getting responses to questions within `3s` with a cost budget of `$200` per `10k` transactions. If the user has a more real-time application, they can set the `latency_budget` to `1s` or lower to get the most optimal model serving stack that satisfies that requirement. User's can also set accuracy thresholds in their report. If they are evaluating whether model responses are accurate compared to ground truth provided in the dataset, they can set an `accuracy budget` and a `cosine similarity` budget that are paired together to determine the accuracy of a response.\n\n**Run Steps Configuration**: The `FMBench` workflow consists of several sequential notebooks that handle different aspects of the benchmarking process, from setup to cleanup. Each step can be enabled or disabled using the `run_steps` configuration in the YAML file. While typically all steps would run in sequence, you have the flexibility to skip certain steps by setting them to `no` if you've already completed them or want to rerun specific analyses. For example, if you've already deployed your model and generated/collected inference data, you could set `2_deploy_model.ipynb` and `3_run_inference.ipynb` to `no` and only run the analysis notebooks with different parameters - this is particularly useful when you want to experiment with different performance thresholds (like adjusting latency budgets or cost constraints) without having to redeploy models or rerun inferences.\n\n```md-code__content\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: no\n    3_run_inference.ipynb: yes\n    4_get_evaluations.ipynb: yes\n    5_model_metric_analysis.ipynb: yes\n    6_cleanup.ipynb: no\n\n```\n\n### Resources: [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/customize_config_files.html\\#resources \"Permanent link\")\n\n`FMBench` provides several configuration files for benchmarking models on Bedrock, SageMaker, EC2, Bring your own endpoint, EKS, etc. These configuration files can be found on the `FMBench` Github repo [here](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/main/src/fmbench/configs):\n\n![config-structure](https://aws-samples.github.io/foundation-model-benchmarking-tool/img/config-structure.png)",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/customize_config_files.html",
      "title": "Customizations - Foundation Model Benchmarking Tool (FMBench)",
      "favicon": {},
      "language": "en",
      "scrapeId": "40655363-a823-4a57-8761-c990a0bbb74e",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/customize_config_files.html",
      "statusCode": 200,
      "theme-color": "#00000000",
      "color-scheme": "normal"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "[Skip to content](https://aws-samples.github.io/foundation-model-benchmarking-tool/quickstart.html#quickstart-run-fmbench-on-sagemaker-notebook)\n\n# Quickstart - run `FMBench` on SageMaker Notebook [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/quickstart.html\\#quickstart-run-fmbench-on-sagemaker-notebook \"Permanent link\")\n\n1. Each `FMBench` run works with a configuration file that contains the information about the model, the deployment steps, and the tests to run. A typical `FMBench` workflow involves either directly using an already provided config file from the [`configs`](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/main/src/fmbench/configs) folder in the `FMBench` GitHub repo or editing an already provided config file as per your own requirements (say you want to try benchmarking on a different instance type, or a different inference container etc.).\n\n👉 A simple config file with key parameters annotated is included in this repo, see [`config-llama2-7b-g5-quick.yml`](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/main/src/fmbench/configs/llama2/7b/config-llama2-7b-g5-quick.yml). This file benchmarks performance of Llama2-7b on an `ml.g5.xlarge` instance and an `ml.g5.2xlarge` instance. You can use this config file as it is for this Quickstart.\n\n2. Launch the AWS CloudFormation template included in this repository using one of the buttons from the table below. The CloudFormation template creates the following resources within your AWS account: Amazon S3 buckets, Amazon IAM role and an Amazon SageMaker Notebook with this repository cloned. A read S3 bucket is created which contains all the files (configuration files, datasets) required to run `FMBench` and a write S3 bucket is created which will hold the metrics and reports generated by `FMBench`. The CloudFormation stack takes about 5-minutes to create.\n\n\n| AWS Region | Link |\n| --- | --- |\n| us-east-1 (N. Virginia) | [![](https://aws-samples.github.io/foundation-model-benchmarking-tool/img/ML-FMBT-cloudformation-launch-stack.png)](https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=fmbench&templateURL=https://aws-blogs-artifacts-public.s3.amazonaws.com/artifacts/ML-FMBT/template.yml) |\n| us-west-2 (Oregon) | [![](https://aws-samples.github.io/foundation-model-benchmarking-tool/img/ML-FMBT-cloudformation-launch-stack.png)](https://console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/new?stackName=fmbench&templateURL=https://aws-blogs-artifacts-public.s3.amazonaws.com/artifacts/ML-FMBT/template.yml) |\n| us-gov-west-1 (GovCloud West) | [![](https://aws-samples.github.io/foundation-model-benchmarking-tool/img/ML-FMBT-cloudformation-launch-stack.png)](https://us-gov-west-1.console.amazonaws-us-gov.com/cloudformation/home?region=us-gov-west-1#/stacks/new?stackName=fmbench&templateURL=https://aws-blogs-artifacts-public.s3.amazonaws.com/artifacts/ML-FMBT/template.yml) |\n\n1. Once the CloudFormation stack is created, navigate to SageMaker Notebooks and open the `fmbench-notebook`.\n\n2. On the `fmbench-notebook` open a Terminal and run the following commands.\n\n\n\n\n```md-code__content\nconda create --name fmbench_python311 -y python=3.11 ipykernel\nsource activate fmbench_python311;\npip install -U fmbench\n\n```\n\n3. Now you are ready to `fmbench` with the following command line. We will use a sample config file placed in the S3 bucket by the CloudFormation stack for a quick first run.\n1. We benchmark performance for the `Llama2-7b` model on a `ml.g5.xlarge` and a `ml.g5.2xlarge` instance type, using the `huggingface-pytorch-tgi-inference` inference container. This test would take about 30 minutes to complete and cost about $0.20.\n\n2. It uses a simple relationship of 750 words equals 1000 tokens, to get a more accurate representation of token counts use the `Llama2 tokenizer` (instructions are provided in the next section). **_It is strongly recommended that for more accurate results on token throughput you use a tokenizer specific to the model you are testing rather than the default tokenizer. See instructions provided later in this document on how to use a custom tokenizer_**.\n\n\n\n      ```md-code__content\n      account=`aws sts get-caller-identity | jq .Account | tr -d '\"'`\n      region=`aws configure get region`\n      fmbench --config-file s3://sagemaker-fmbench-read-${region}-${account}/configs/llama2/7b/config-llama2-7b-g5-quick.yml > fmbench.log 2>&1\n\n      ```\n\n3. Open another terminal window and do a `tail -f` on the `fmbench.log` file to see all the traces being generated at runtime.\n\n\n\n      ```md-code__content\n      tail -f fmbench.log\n\n      ```\n\n4. 👉 For streaming support on SageMaker and Bedrock checkout these config files:\n      1. [config-llama3-8b-g5-streaming.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/src/configs/llama3/8b/config-llama3-8b-g5-streaming.yml)\n      2. [config-bedrock-llama3-streaming.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/src/configs/bedrock/config-bedrock-llama3-streaming.yml)\n4. The generated reports and metrics are available in the `sagemaker-fmbench-write-<replace_w_your_aws_region>-<replace_w_your_aws_account_id>` bucket. The metrics and report files are also downloaded locally and in the `results` directory (created by `FMBench`) and the benchmarking report is available as a markdown file called `report.md` in the `results` directory. You can view the rendered Markdown report in the SageMaker notebook itself or download the metrics and report files to your machine for offline analysis.\n\n\n## `FMBench` on GovCloud [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/quickstart.html\\#fmbench-on-govcloud \"Permanent link\")\n\nNo special steps are required for running `FMBench` on GovCloud. The CloudFormation link for `us-gov-west-1` has been provided in the section above.\n\n1. Not all models available via Bedrock or other services may be available in GovCloud. The following commands show how to run `FMBench` to benchmark the [Amazon Titan Text Express](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-text-models.html#titantx-express) model in the GovCloud. See the [Amazon Bedrock GovCloud](https://docs.aws.amazon.com/govcloud-us/latest/UserGuide/govcloud-bedrock.html) page for more details.\n\n\n\n```md-code__content\naccount=`aws sts get-caller-identity | jq .Account | tr -d '\"'`\nregion=`aws configure get region`\nfmbench --config-file s3://sagemaker-fmbench-read-${region}-${account}/configs/bedrock/config-bedrock-titan-text-express.yml > fmbench.log 2>&1\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/quickstart.html",
      "title": "SageMaker - Foundation Model Benchmarking Tool (FMBench)",
      "favicon": {},
      "language": "en",
      "scrapeId": "5db06fd3-85d9-452f-a2b6-004dc8410ef3",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/quickstart.html",
      "statusCode": 200,
      "theme-color": "#00000000",
      "color-scheme": "normal"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "[Skip to content](https://aws-samples.github.io/foundation-model-benchmarking-tool/run_as_container.html#run-fmbench-as-a-docker-container)\n\n# Run `FMBench` as a Docker container [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/run_as_container.html\\#run-fmbench-as-a-docker-container \"Permanent link\")\n\nYou can now run `FMBench` on any platform where you can run a Docker container, for example on an EC2 VM, SageMaker Notebook etc. The advantage is that you do not have to install anything locally, so no `conda` installs needed anymore. Here are the steps to do that.\n\n1. Create local directory structure needed for `FMBench` and copy all publicly available dependencies from the AWS S3 bucket for `FMBench`. This is done by running the `copy_s3_content.sh` script available as part of the `FMBench` repo. You can place model specific tokenizers and any new configuration files you create in the `/tmp/fmbench-read` directory that is created after running the following command.\n\n\n\n```md-code__content\ncurl -s https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/copy_s3_content.sh | sh\n\n```\n\n2. That's it! You are now ready to run the container.\n\n\n\n```md-code__content\n# set the config file path to point to the config file of interest\nCONFIG_FILE=https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/fmbench/configs/llama2/7b/config-llama2-7b-g5-quick.yml\ndocker run -v $(pwd)/fmbench:/app \\\n     -v /tmp/fmbench-read:/tmp/fmbench-read \\\n     -v /tmp/fmbench-write:/tmp/fmbench-write \\\n     aarora79/fmbench:v1.0.47 \\\n    \"fmbench --config-file ${CONFIG_FILE} --local-mode yes --write-bucket placeholder > fmbench.log 2>&1\"\n\n```\n\n3. The above command will create a `fmbench` directory inside the current working directory. This directory contains the `fmbench.log` and the `results-*` folder that is created once the run finished.",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/run_as_container.html",
      "title": "Docker - Foundation Model Benchmarking Tool (FMBench)",
      "favicon": {},
      "language": "en",
      "scrapeId": "656e1731-a4c2-489c-8852-37fbde2d0bfb",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/run_as_container.html",
      "statusCode": 200,
      "theme-color": "#00000000",
      "color-scheme": "normal"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "[Skip to content](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#benchmark-models-on-ec2)\n\n# Benchmark models on EC2 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html\\#benchmark-models-on-ec2 \"Permanent link\")\n\nYou can use `FMBench` to benchmark models on hosted on EC2. This can be done in one of two ways:\n\n- Deploy the model on your EC2 instance independently of `FMBench` and then benchmark it through the [Bring your own endpoint](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#bring-your-own-endpoint-aka-support-for-external-endpoints) mode.\n- Deploy the model on your EC2 instance through `FMBench` and then benchmark it.\n\nThe steps for deploying the model on your EC2 instance are described below.\n\n👉 In this configuration both the model being benchmarked and `FMBench` are deployed on the same EC2 instance.\n\nCreate a new EC2 instance suitable for hosting an LMI as per the steps described [here](https://aws-samples.github.io/foundation-model-benchmarking-tool/misc/ec2_instance_creation_steps.html). _Note that you will need to select the correct AMI based on your instance type, this is called out in the instructions_.\n\nThe steps for benchmarking on different types of EC2 instances (GPU/CPU/Neuron) and different inference containers differ slightly. These are all described below.\n\n## Benchmarking options on EC2 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html\\#benchmarking-options-on-ec2 \"Permanent link\")\n\n- [Benchmarking on an instance type with NVIDIA GPUs or AWS Chips](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#benchmarking-on-an-instance-type-with-nvidia-gpus-or-aws-chips)\n- [Benchmarking on an instance type with NVIDIA GPU and the Triton inference server](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#benchmarking-on-an-instance-type-with-nvidia-gpu-and-the-triton-inference-server)\n- [Benchmarking on an instance type with AWS Chips and the Triton inference server](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#benchmarking-on-an-instance-type-with-aws-chips-and-the-triton-inference-server)\n- [Benchmarking on an CPU instance type with AMD processors](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#benchmarking-on-an-cpu-instance-type-with-amd-processors)\n- [Benchmarking on an CPU instance type with Intel processors](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#benchmarking-on-an-cpu-instance-type-with-intel-processors)\n- [Benchmarking on an CPU instance type with ARM processors (Graviton 4)](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#benchmarking-on-an-cpu-instance-type-with-arm-processors)\n\n- [Benchmarking the Triton inference server](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#benchmarking-the-triton-inference-server)\n\n- [Benchmarking models on Ollama](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#benchmarking-models-on-ollama)\n\n## Benchmarking on an instance type with NVIDIA GPUs or AWS Chips [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html\\#benchmarking-on-an-instance-type-with-nvidia-gpus-or-aws-chips \"Permanent link\")\n\n1. Connect to your instance using any of the options in EC2 (SSH/EC2 Connect), run the following in the EC2 terminal. This command installs `uv` on the instance which is then used to create a new virtual environment for `FMBench`.\n\n\n\n```md-code__content\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nexport PATH=\"$HOME/.local/bin:$PATH\"\n\n```\n\n2. Install `docker-compose`.\n\n\n\n```md-code__content\nsudo apt-get update\nsudo apt-get install --reinstall docker.io -y\nsudo apt-get install -y docker-compose\ndocker compose version\n\n```\n\n3. Setup the `.fmbench_python311` Python environment.\n\n\n\n```md-code__content\nuv venv .fmbench_python311 --python 3.11\nsource .fmbench_python311/bin/activate\n# Add the Python environment activation and directory navigation to .bashrc\necho 'source $HOME/.fmbench_python311/bin/activate' >> $HOME/.bashrc\nuv pip install -U fmbench\n\n```\n\n4. Create local directory structure needed for `FMBench` and copy all publicly available dependencies from the AWS S3 bucket for `FMBench`. This is done by running the `copy_s3_content.sh` script available as part of the `FMBench` repo. **Replace `/tmp` in the command below with a different path if you want to store the config files and the `FMBench` generated data in a different directory**.\n\n\n\n```md-code__content\n# Replace \"/tmp\" with \"/path/to/your/custom/tmp\" if you want to use a custom tmp directory\nTMP_DIR=\"/tmp\"\ncurl -s https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/copy_s3_content.sh | sh -s -- \"$TMP_DIR\"\n\n```\n\n5. To download the model files from HuggingFace, create a `hf_token.txt` file in the `/tmp/fmbench-read/scripts/` directory containing the Hugging Face token you would like to use. In the command below replace the `hf_yourtokenstring` with your Hugging Face token. **Replace `/tmp` in the command below if you are using `/path/to/your/custom/tmp` to store the config files and the `FMBench` generated data**.\n\n\n\n```md-code__content\necho hf_yourtokenstring > $TMP_DIR/fmbench-read/scripts/hf_token.txt\n\n```\n\n6. Run `FMBench` with a packaged or a custom config file. **_This step will also deploy the model on the EC2 instance_**. The `--write-bucket` parameter value is just a placeholder and an actual S3 bucket is not required. **_Skip to the next step if benchmarking for AWS Chips_**. You could set the `--tmp-dir` flag to an EFA path instead of `/tmp` if using a shared path for storing config files and reports.\n\n\n\n```md-code__content\nfmbench --config-file $TMP_DIR/fmbench-read/configs/llama3/8b/config-ec2-llama3-8b.yml --local-mode yes --write-bucket placeholder --tmp-dir $TMP_DIR > fmbench.log 2>&1\n\n```\n\n7. For example, to run `FMBench` on a `llama3-8b-Instruct` model on an `inf2.48xlarge` instance, run the command\ncommand below. The config file for this example can be viewed [here](https://aws-samples.github.io/foundation-model-benchmarking-tool/src/fmbench/configs/llama3/8b/config-ec2-llama3-8b-inf2-48xl.yml).\n\n\n\n```md-code__content\nfmbench --config-file $TMP_DIR/fmbench-read/configs/llama3/8b/config-ec2-llama3-8b-inf2-48xl.yml --local-mode yes --write-bucket placeholder --tmp-dir $TMP_DIR > fmbench.log 2>&1\n\n```\n\n8. Open a new Terminal and do a `tail` on `fmbench.log` to see a live log of the run.\n\n\n\n```md-code__content\ntail -f fmbench.log\n\n```\n\n9. All metrics are stored in the `/tmp/fmbench-write` directory created automatically by the `fmbench` package. Once the run completes all files are copied locally in a `results-*` folder as usual.\n\n\n## Benchmarking on an instance type with NVIDIA GPU and the Triton inference server [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html\\#benchmarking-on-an-instance-type-with-nvidia-gpu-and-the-triton-inference-server \"Permanent link\")\n\n1. Follow steps in the [Benchmarking on an instance type with NVIDIA GPUs or AWS Chips](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#benchmarking-on-an-instance-type-with-nvidia-gpus-or-aws-chips) section to install `FMBench` but do not run any benchmarking tests yet.\n\n2. Once `FMBench` is installed then install the following additional dependencies for Triton.\n\n\n\n```md-code__content\ncd ~\ngit clone https://github.com/triton-inference-server/tensorrtllm_backend.git  --branch v0.12.0\n# Update the submodules\ncd tensorrtllm_backend\n# Install git-lfs if needed\nsudo apt --fix-broken install\nsudo apt-get update && sudo apt-get install git-lfs -y --no-install-recommends\ngit lfs install\ngit submodule update --init --recursive\n\n```\n\n3. Now you are ready to run benchmarking with Triton. For example for benchmarking `Llama3-8b` model on a `g5.12xlarge` use the following command:\n\n\n\n```md-code__content\nfmbench --config-file $TMP_DIR/fmbench-read/configs/llama3/8b/config-llama3-8b-g5.12xl-tp-2-mc-max-triton-ec2.yml --local-mode yes --write-bucket placeholder --tmp-dir $TMP_DIR > fmbench.log 2>&1\n\n```\n\n\n## Benchmarking on an instance type with AWS Chips and the Triton inference server [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html\\#benchmarking-on-an-instance-type-with-aws-chips-and-the-triton-inference-server \"Permanent link\")\n\n**_As of 2024-09-26 this has been tested on a `trn1.32xlarge` instance_**\n\n1. Connect to your instance using any of the options in EC2 (SSH/EC2 Connect), run the following in the EC2 terminal. This command installs `uv` on the instance which is then used to create a new Python virtual environment for `FMBench`.(Note: **_Your EC2 instance needs to have at least 200GB of disk space for this test_**)\n\n\n\n```md-code__content\n# Install Docker and Git using the YUM package manager\nsudo yum install docker git -y\n\n# Start the Docker service\nsudo systemctl start docker\n\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nexport PATH=\"$HOME/.local/bin:$PATH\"\n\n```\n\n2. Setup the `.fmbench_python311` Python virtual environment.\n\n\n\n```md-code__content\nuv venv .fmbench_python311 --python 3.11\nsource .fmbench_python311/bin/activate\n# Add the Python environment activation and directory navigation to .bashrc\necho 'source $HOME/.fmbench_python311/bin/activate' >> $HOME/.bashrc\nuv pip install -U fmbench\n\n```\n\n3. First we need to build the required docker image for `triton`, and push it locally. To do this, curl the `Triton Dockerfile` and the script to build and push the triton image locally:\n\n\n\n\n\n```md-code__content\n# curl the docker file for triton\ncurl -o ./Dockerfile_triton https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/fmbench/scripts/triton/Dockerfile_triton\n\n# curl the script that builds and pushes the triton image locally\ncurl -o build_and_push_triton.sh https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/fmbench/scripts/triton/build_and_push_triton.sh\n\n# Make the triton build and push script executable, and run it\nchmod +x build_and_push_triton.sh\n./build_and_push_triton.sh\n\n```\n\n\n\n    \\- Now wait until the docker image is saved locally and then follow the instructions below to start a benchmarking test.\n\n4. Create local directory structure needed for `FMBench` and copy all publicly available dependencies from the AWS S3 bucket for `FMBench`. This is done by running the `copy_s3_content.sh` script available as part of the `FMBench` repo. **Replace `/tmp` in the command below with a different path if you want to store the config files and the `FMBench` generated data in a different directory**.\n\n\n\n```md-code__content\n# Replace \"/tmp\" with \"/path/to/your/custom/tmp\" if you want to use a custom tmp directory\nTMP_DIR=\"/tmp\"\ncurl -s https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/copy_s3_content.sh | sh -s -- \"$TMP_DIR\"\n\n```\n\n5. To download the model files from HuggingFace, create a `hf_token.txt` file in the `/tmp/fmbench-read/scripts/` directory containing the Hugging Face token you would like to use. In the command below replace the `hf_yourtokenstring` with your Hugging Face token. **Replace `/tmp` in the command below if you are using `/path/to/your/custom/tmp` to store the config files and the `FMBench` generated data**.\n\n\n\n```md-code__content\necho hf_yourtokenstring > $TMP_DIR/fmbench-read/scripts/hf_token.txt\n\n```\n\n6. Run `FMBench` with a packaged or a custom config file. **_This step will also deploy the model on the EC2 instance_**. The `--write-bucket` parameter value is just a placeholder and an actual S3 bucket is not required. You could set the `--tmp-dir` flag to an EFA path instead of `/tmp` if using a shared path for storing config files and reports.\n\n\n\n```md-code__content\nfmbench --config-file $TMP_DIR/fmbench-read/configs/llama3/8b/config-llama3-8b-trn1-32xlarge-triton-djl.yml --local-mode yes --write-bucket placeholder --tmp-dir $TMP_DIR > fmbench.log 2>&1\n\n```\n\n7. Open a new Terminal and and do a `tail` on `fmbench.log` to see a live log of the run.\n\n\n\n```md-code__content\ntail -f fmbench.log\n\n```\n\n8. All metrics are stored in the `/tmp/fmbench-write` directory created automatically by the `fmbench` package. Once the run completes all files are copied locally in a `results-*` folder as usual.\n\n9. **Note**: To deploy a model on AWS Chips using Triton with `djl` or `vllm` backend, the configuration file requires the `backend` and `container_params` parameters within the `inference_spec` dictionary. The backend options are `vllm`/ `djl` and the `container_params` contains container specific parameters to deploy the model, for example `tensor parallel degree`, `n positions`, etc. Tensor parallel degree is a necessary field to be added. If no other parameters are provided, the container will choose the default parameters during deployment.\n\n\n\n```md-code__content\n     # Backend options: [djl, vllm]\n     backend: djl\n\n     # Container parameters that are used during model deployment\n     container_params:\n       # tp degree is a mandatory parameter\n       tp_degree: 8\n       amp: \"f16\"\n       attention_layout: 'BSH'\n       collectives_layout: 'BSH'\n       context_length_estimate: 3072, 3584, 4096\n       max_rolling_batch_size: 8\n       model_loader: \"tnx\"\n       model_loading_timeout: 2400\n       n_positions: 4096\n       output_formatter: \"json\"\n       rolling_batch: \"auto\"\n       rolling_batch_strategy: \"continuous_batching\"\n       trust_remote_code: true\n       # modify the serving properties to match your model and requirements\n       serving.properties:\n\n```\n\n\n## Benchmarking on an CPU instance type with AMD processors [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html\\#benchmarking-on-an-cpu-instance-type-with-amd-processors \"Permanent link\")\n\n**_As of 2024-08-27 this has been tested on a `m7a.16xlarge` instance_**\n\n01. Connect to your instance using any of the options in EC2 (SSH/EC2 Connect), run the following in the EC2 terminal. This command installs `uv` on the instance which is then used to create a new Python virtual environment for `FMBench`.\n\n\n\n    ```md-code__content\n    # Install Docker and Git using the YUM package manager\n    sudo yum install docker git -y\n\n    # Start the Docker service\n    sudo systemctl start docker\n\n    curl -LsSf https://astral.sh/uv/install.sh | sh\n    export PATH=\"$HOME/.local/bin:$PATH\"\n\n    ```\n\n02. Setup the `.fmbench_python311` Python virtual environment.\n\n\n\n    ```md-code__content\n    uv venv .fmbench_python311 --python 3.11\n    source .fmbench_python311/bin/activate\n    # Add the Python environment activation and directory navigation to .bashrc\n    echo 'source $HOME/.fmbench_python311/bin/activate' >> $HOME/.bashrc\n    uv pip install -U fmbench\n\n    ```\n\n03. Build the `vllm` container for serving the model.\n    1. 👉 The `vllm` container we are building locally is going to be references in the `FMBench` config file.\n\n    2. The container being build is for CPU only (GPU support might be added in future).\n\n\n\n       ```md-code__content\n       # Clone the vLLM project repository from GitHub\n       git clone https://github.com/vllm-project/vllm.git\n\n       # Change the directory to the cloned vLLM project\n       cd vllm\n\n       # Build a Docker image using the provided Dockerfile for CPU, with a shared memory size of 4GB\n       sudo docker build -f Dockerfile.cpu -t vllm-cpu-env --shm-size=4g .\n\n       ```\n04. Create local directory structure needed for `FMBench` and copy all publicly available dependencies from the AWS S3 bucket for `FMBench`. This is done by running the `copy_s3_content.sh` script available as part of the `FMBench` repo. **Replace `/tmp` in the command below with a different path if you want to store the config files and the `FMBench` generated data in a different directory**.\n\n\n\n    ```md-code__content\n    # Replace \"/tmp\" with \"/path/to/your/custom/tmp\" if you want to use a custom tmp directory\n    TMP_DIR=\"/tmp\"\n    curl -s https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/copy_s3_content.sh | sh -s -- \"$TMP_DIR\"\n\n    ```\n\n05. To download the model files from HuggingFace, create a `hf_token.txt` file in the `/tmp/fmbench-read/scripts/` directory containing the Hugging Face token you would like to use. In the command below replace the `hf_yourtokenstring` with your Hugging Face token. **Replace `/tmp` in the command below if you are using `/path/to/your/custom/tmp` to store the config files and the `FMBench` generated data**.\n\n\n\n    ```md-code__content\n    echo hf_yourtokenstring > $TMP_DIR/fmbench-read/scripts/hf_token.txt\n\n    ```\n\n06. Before running FMBench, add the current user to the docker group. Run the following commands to run Docker without needing to use `sudo` each time.\n\n\n\n    ```md-code__content\n    sudo usermod -a -G docker $USER\n    newgrp docker\n\n    ```\n\n07. Install `docker-compose`.\n\n\n\n    ```md-code__content\n    DOCKER_CONFIG=${DOCKER_CONFIG:-$HOME/.docker}\n    mkdir -p $DOCKER_CONFIG/cli-plugins\n    sudo curl -L https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m) -o $DOCKER_CONFIG/cli-plugins/docker-compose\n    sudo chmod +x $DOCKER_CONFIG/cli-plugins/docker-compose\n    docker compose version\n\n    ```\n\n08. Run `FMBench` with a packaged or a custom config file. **_This step will also deploy the model on the EC2 instance_**. The `--write-bucket` parameter value is just a placeholder and an actual S3 bucket is not required. You could set the `--tmp-dir` flag to an EFA path instead of `/tmp` if using a shared path for storing config files and reports.\n\n\n\n    ```md-code__content\n    fmbench --config-file $TMP_DIR/fmbench-read/configs/llama3/8b/config-ec2-llama3-8b-m7a-16xlarge.yml --local-mode yes --write-bucket placeholder --tmp-dir $TMP_DIR > fmbench.log 2>&1\n\n    ```\n\n09. Open a new Terminal and and do a `tail` on `fmbench.log` to see a live log of the run.\n\n\n\n    ```md-code__content\n    tail -f fmbench.log\n\n    ```\n\n10. All metrics are stored in the `/tmp/fmbench-write` directory created automatically by the `fmbench` package. Once the run completes all files are copied locally in a `results-*` folder as usual.\n\n\n## Benchmarking on an CPU instance type with Intel processors [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html\\#benchmarking-on-an-cpu-instance-type-with-intel-processors \"Permanent link\")\n\n**_As of 2024-08-27 this has been tested on `c5.18xlarge` and `m5.16xlarge` instances_**\n\n01. Connect to your instance using any of the options in EC2 (SSH/EC2 Connect), run the following in the EC2 terminal. This command installs `uv` on the instance which is then used to create a new Python virtual environment for `FMBench`.\n\n\n\n    ```md-code__content\n    # Install Docker and Git using the YUM package manager\n    sudo yum install docker git -y\n\n    # Start the Docker service\n    sudo systemctl start docker\n\n    curl -LsSf https://astral.sh/uv/install.sh | sh\n    export PATH=\"$HOME/.local/bin:$PATH\"\n\n    ```\n\n02. Setup the `.fmbench_python311` Python virtual environment.\n\n\n\n    ```md-code__content\n    uv venv .fmbench_python311 --python 3.11\n    source .fmbench_python311/bin/activate\n    # Add the Python environment activation and directory navigation to .bashrc\n    echo 'source $HOME/.fmbench_python311/bin/activate' >> $HOME/.bashrc\n    uv pip install -U fmbench\n\n    ```\n\n03. Build the `vllm` container for serving the model.\n    1. 👉 The `vllm` container we are building locally is going to be references in the `FMBench` config file.\n\n    2. The container being build is for CPU only (GPU support might be added in future).\n\n\n\n       ```md-code__content\n       # Clone the vLLM project repository from GitHub\n       git clone https://github.com/vllm-project/vllm.git\n\n       # Change the directory to the cloned vLLM project\n       cd vllm\n\n       # Build a Docker image using the provided Dockerfile for CPU, with a shared memory size of 12GB\n       sudo docker build -f Dockerfile.cpu -t vllm-cpu-env --shm-size=12g .\n\n       ```\n04. Create local directory structure needed for `FMBench` and copy all publicly available dependencies from the AWS S3 bucket for `FMBench`. This is done by running the `copy_s3_content.sh` script available as part of the `FMBench` repo. **Replace `/tmp` in the command below with a different path if you want to store the config files and the `FMBench` generated data in a different directory**.\n\n\n\n    ```md-code__content\n    # Replace \"/tmp\" with \"/path/to/your/custom/tmp\" if you want to use a custom tmp directory\n    TMP_DIR=\"/tmp\"\n    curl -s https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/copy_s3_content.sh | sh -s -- \"$TMP_DIR\"\n\n    ```\n\n05. To download the model files from HuggingFace, create a `hf_token.txt` file in the `/tmp/fmbench-read/scripts/` directory containing the Hugging Face token you would like to use. In the command below replace the `hf_yourtokenstring` with your Hugging Face token. **Replace `/tmp` in the command below if you are using `/path/to/your/custom/tmp` to store the config files and the `FMBench` generated data**.\n\n\n\n    ```md-code__content\n    echo hf_yourtokenstring > $TMP_DIR/fmbench-read/scripts/hf_token.txt\n\n    ```\n\n06. Before running FMBench, add the current user to the docker group. Run the following commands to run Docker without needing to use `sudo` each time.\n\n\n\n    ```md-code__content\n    sudo usermod -a -G docker $USER\n    newgrp docker\n\n    ```\n\n07. Install `docker-compose`.\n\n\n\n    ```md-code__content\n    DOCKER_CONFIG=${DOCKER_CONFIG:-$HOME/.docker}\n    mkdir -p $DOCKER_CONFIG/cli-plugins\n    sudo curl -L https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m) -o $DOCKER_CONFIG/cli-plugins/docker-compose\n    sudo chmod +x $DOCKER_CONFIG/cli-plugins/docker-compose\n    docker compose version\n\n    ```\n\n08. Run `FMBench` with a packaged or a custom config file. **_This step will also deploy the model on the EC2 instance_**. The `--write-bucket` parameter value is just a placeholder and an actual S3 bucket is not required. You could set the `--tmp-dir` flag to an EFA path instead of `/tmp` if using a shared path for storing config files and reports.\n\n\n\n    ```md-code__content\n    fmbench --config-file $TMP_DIR/fmbench-read/configs/llama3/8b/config-ec2-llama3-8b-c5-18xlarge.yml --local-mode yes --write-bucket placeholder --tmp-dir $TMP_DIR > fmbench.log 2>&1\n\n    ```\n\n09. Open a new Terminal and and do a `tail` on `fmbench.log` to see a live log of the run.\n\n\n\n    ```md-code__content\n    tail -f fmbench.log\n\n    ```\n\n10. All metrics are stored in the `/tmp/fmbench-write` directory created automatically by the `fmbench` package. Once the run completes all files are copied locally in a `results-*` folder as usual.\n\n\n## Benchmarking models on Ollama [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html\\#benchmarking-models-on-ollama \"Permanent link\")\n\n**_As of 10/24/2024, this has been tested on `g6e.2xlarge` with `llama 3.1 8b`_**\n\n1. Install Ollama.\n\n\n\n```md-code__content\ncurl -fsSL https://ollama.com/install.sh | sh\n\n```\n\n2. Pull the model required.\n\n\n\n```md-code__content\nollama pull llama3.1:8b\n\n```\n\n3. Serve the model. This might produce the following error message: `Error: accepts 0 arg(s), received 1` but you can safely ignore this error.\n\n\n\n```md-code__content\nollama serve llama3.1:8b\n\n```\n\n4. Create local directory structure needed for `FMBench` and copy all publicly available dependencies from the AWS S3 bucket for `FMBench`. This is done by running the `copy_s3_content.sh` script available as part of the `FMBench` repo. **Replace `/tmp` in the command below with a different path if you want to store the config files and the `FMBench` generated data in a different directory**.\n\n\n\n```md-code__content\n# Replace \"/tmp\" with \"/path/to/your/custom/tmp\" if you want to use a custom tmp directory\nTMP_DIR=\"/tmp\"\ncurl -s https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/copy_s3_content.sh | sh -s -- \"$TMP_DIR\"\n\n```\n\n5. Run `FMBench` with a packaged or a custom config file. The `--write-bucket` parameter value is just a placeholder and an actual S3 bucket is not required. You could set the `--tmp-dir` flag to an EFA path instead of `/tmp` if using a shared path for storing config files and reports.\n\n\n\n```md-code__content\nfmbench --config-file $TMP_DIR/fmbench-read/configs/llama3.1/8b/config-ec2-llama3-1-8b-g6e-2xlarge-byoe-ollama.yml --local-mode yes --write-bucket placeholder --tmp-dir $TMP_DIR > fmbench.log 2>&1\n\n```\n\n\n## Benchmarking on an CPU instance type with ARM processors [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html\\#benchmarking-on-an-cpu-instance-type-with-arm-processors \"Permanent link\")\n\n**_As of 12/24/2024, this has been tested on `c8g.24xlarge` with `llama 3 8b Instruct` on Ubuntu Server 24.04 LTS (HVM), SSD Volume Type_**\n\n01. Connect to your instance using any of the options in EC2 (SSH/EC2 Connect), run the following in the EC2 terminal. This command installs `Docker` and `uv` on the instance which is then used to create a new Python virtual environment for `FMBench`.\n\n\n\n    ```md-code__content\n    sudo apt-get update -y\n    sudo apt-get install -y docker.io git\n    sudo systemctl start docker\n    sudo systemctl enable docker\n\n    curl -LsSf https://astral.sh/uv/install.sh | sh\n    export PATH=\"$HOME/.local/bin:$PATH\"\n\n    ```\n\n02. Setup the `.fmbench_python311` Python virtual environment.\n\n\n\n    ```md-code__content\n    uv venv .fmbench_python311 --python 3.11\n    source .fmbench_python311/bin/activate\n    # Add the Python environment activation and directory navigation to .bashrc\n    echo 'source $HOME/.fmbench_python311/bin/activate' >> $HOME/.bashrc\n    uv pip install -U fmbench\n\n    ```\n\n03. Build the `vllm` container for serving the model.\n    1. 👉 The `vllm` container we are building locally is going to be referenced in the `FMBench` config file.\n\n    2. The container being built is for ARM CPUs only.\n\n\n\n       ```md-code__content\n       # Clone the vLLM project repository from GitHub\n       git clone https://github.com/vllm-project/vllm.git\n\n       # Change the directory to the cloned vLLM project\n       cd vllm\n\n       # Build a Docker image using the provided Dockerfile for CPU, with a shared memory size of 12GB\n       sudo docker build -f Dockerfile.arm -t vllm-cpu-env --shm-size=12g .\n\n       ```\n04. Create local directory structure needed for `FMBench` and copy all publicly available dependencies from the AWS S3 bucket for `FMBench`. This is done by running the `copy_s3_content.sh` script available as part of the `FMBench` repo. **Replace `/tmp` in the command below with a different path if you want to store the config files and the `FMBench` generated data in a different directory**.\n\n\n\n    ```md-code__content\n    # Replace \"/tmp\" with \"/path/to/your/custom/tmp\" if you want to use a custom tmp directory\n    TMP_DIR=\"/tmp\"\n    curl -s https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/copy_s3_content.sh | sh -s -- \"$TMP_DIR\"\n\n    ```\n\n05. To download the model files from HuggingFace, create a `hf_token.txt` file in the `/tmp/fmbench-read/scripts/` directory containing the Hugging Face token you would like to use. In the command below replace the `hf_yourtokenstring` with your Hugging Face token. **Replace `/tmp` in the command below if you are using `/path/to/your/custom/tmp` to store the config files and the `FMBench` generated data**.\n\n\n\n    ```md-code__content\n    echo hf_yourtokenstring > $TMP_DIR/fmbench-read/scripts/hf_token.txt\n\n    ```\n\n06. Before running FMBench, add the current user to the docker group. Run the following commands to run Docker without needing to use `sudo` each time.\n\n\n\n    ```md-code__content\n    sudo usermod -a -G docker $USER\n    newgrp docker\n\n    ```\n\n07. Install `docker-compose`.\n\n\n\n    ```md-code__content\n    DOCKER_CONFIG=${DOCKER_CONFIG:-$HOME/.docker}\n    mkdir -p $DOCKER_CONFIG/cli-plugins\n    sudo curl -L https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m) -o $DOCKER_CONFIG/cli-plugins/docker-compose\n    sudo chmod +x $DOCKER_CONFIG/cli-plugins/docker-compose\n    docker compose version\n\n    ```\n\n08. Run `FMBench` with a packaged or a custom config file. **_This step will also deploy the model on the EC2 instance_**. The `--write-bucket` parameter value is just a placeholder and an actual S3 bucket is not required. You could set the `--tmp-dir` flag to an EFA path instead of `/tmp` if using a shared path for storing config files and reports.\n\n\n\n    ```md-code__content\n    fmbench --config-file $TMP_DIR/fmbench-read/configs/llama3/8b/config-ec2-llama3-8b-c8g-24xlarge.yml --local-mode yes --write-bucket placeholder --tmp-dir $TMP_DIR > fmbench.log 2>&1\n\n    ```\n\n09. Open a new Terminal and and do a `tail` on `fmbench.log` to see a live log of the run.\n\n\n\n    ```md-code__content\n    tail -f fmbench.log\n\n    ```\n\n10. All metrics are stored in the `/tmp/fmbench-write` directory created automatically by the `fmbench` package. Once the run completes all files are copied locally in a `results-*` folder as usual.",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html",
      "title": "EC2 - Foundation Model Benchmarking Tool (FMBench)",
      "favicon": {},
      "language": "en",
      "scrapeId": "5c876876-7753-46a4-a81a-53ce6e9e2438",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html",
      "statusCode": 200,
      "theme-color": "#00000000",
      "color-scheme": "normal"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "[Skip to content](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_sagemaker.html#benchmark-models-on-sagemaker)\n\n# Benchmark models on SageMaker [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_sagemaker.html\\#benchmark-models-on-sagemaker \"Permanent link\")\n\nChoose any config file from the model specific folders, for example the [`Llama3`](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/main/src/fmbench/configs/llama3) folder for `Llama3` family of models. These configuration files also include instructions for `FMBench` to first deploy the model on SageMaker using your configured instance type and inference parameters of choice and then run the benchmarking. Here is an example for benchmarking `Llama3-8b` model on an `ml.inf2.24xlarge` and `ml.g5.12xlarge` instance.\n\n```md-code__content\nfmbench --config-file https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/fmbench/configs/llama3/8b/config-llama3-8b-inf2-g5.yml > fmbench.log 2>&1\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_sagemaker.html",
      "title": "Sagemaker - Foundation Model Benchmarking Tool (FMBench)",
      "favicon": {},
      "language": "en",
      "scrapeId": "48f65da4-204c-41fa-8d3a-e3ccf56fa7e1",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_sagemaker.html",
      "statusCode": 200,
      "theme-color": "#00000000",
      "color-scheme": "normal"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "[Skip to content](https://aws-samples.github.io/foundation-model-benchmarking-tool/features.html#fmbench-features)\n\n# `FMBench` features [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/features.html\\#fmbench-features \"Permanent link\")\n\n**Support for Model Evaluations**: FMBench now adds support for evaluating candidate models using Majority Voting with a [Panel of LLM Evaluators](https://arxiv.org/abs/2404.18796). Customers can now use FMBench to evaluate model accuracy across open-source and custom datasets, thus FMBench now enables customers to not only measure performance (inference latency, cost, throughput) but also model accuracy.\n\n**Native support for LLM compilation and deployment on AWS Silicon**: FMBench now supports end-to-end compilation and model deployment on AWS Silicon. Customers no longer have to wait for models to be available for AWS Chips via SageMaker JumpStart and neither do they have to go through the process of compiling the model to Neuron themselves, FMBench does it all for them. We can simply put the relevant configuration options in the FMBench config file and it will compile and deploy the model on SageMaker ( [config](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-inf2.yml)) or EC2 ( [config](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-inf2-48xl-deploy-ec2.yml)).\n\n**Website for better user experience**: FMBench has a [website](https://aws-samples.github.io/foundation-model-benchmarking-tool/) now along with an [introduction video](https://youtu.be/yvRCyS0J90c). The website is fully searchable to ease common tasks such as installation, finding the right config file, benchmarking on various hosting platforms (EC2, EKS, Bedrock, Neuron, Docker), model evaluation, etc. This website was created based on feedback from several internal teams and external customers.\n\n**Native support for all AWS generative AI services**: FMBench now benchmarks and evaluates any Foundation Model (FM) deployed on any AWS Generative AI service, be it Amazon SageMaker, Amazon Bedrock, Amazon EKS, or Amazon EC2. We initially built FMBench for SageMaker, and later extended it to Bedrock and then based on customer requests extended it to support models on EKS and EC2 as well. See [list of config files](https://aws-samples.github.io/foundation-model-benchmarking-tool/manifest.html) supported out of the box, you can use these config files either as is or as templates for creating your own custom config.",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/features.html",
      "title": "Features - Foundation Model Benchmarking Tool (FMBench)",
      "favicon": {},
      "language": "en",
      "scrapeId": "66419977-068f-453f-beb5-5ede227da9e5",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/features.html",
      "statusCode": 200,
      "theme-color": "#00000000",
      "color-scheme": "normal"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "[Skip to content](https://aws-samples.github.io/foundation-model-benchmarking-tool/byo_dataset.html#bring-your-own-dataset)\n\n# Bring Your Own Dataset [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/byo_dataset.html\\#bring-your-own-dataset \"Permanent link\")\n\nBy default `FMBench` uses the [`LongBench dataset`](https://github.com/THUDM/LongBench) dataset for testing the models, but this is not the only dataset you can test with. You may want to test with other datasets available on HuggingFace or use your own datasets for testing.\n\n## Hugging Face Data Preparation is now integrated within FMBench [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/byo_dataset.html\\#hugging-face-data-preparation-is-now-integrated-within-fmbench \"Permanent link\")\n\nFMBench supports direct loading of Hugging Face datasets with a simplified prefixing method. To specify a Hugging Face dataset and its split, include `hf:`, followed by the `dataset identifier`, `subset name`, and `split name`.\n\nIf you only provide the `dataset-id` and not the `subset name` and `split name`, the following defaults will be used:\n\\- Subset name: `default`\n\\- Split name: `train`\n\n**Important**: If your dataset does not have the default `subset name` and `split name` provided above, then provide the dataset information in the config file in the following format: `hf:dataset-id/subset-name/split-name.`\n\nExample formats:\n\n\n```md-code__content\nsource_data_files:\n# Full specification\n- hf:databricks/databricks-dolly-15k/default/train\n\n# Using defaults (subset: default, split: train)\n- hf:databricks/databricks-dolly-15k\n\n```\n\nIn your configuration file, add entries to `source_data_files` using the following format:\n\n1. In your config file, prefix the dataset name with `hf:` in the `source_data_files` section:\n\n\n\n```md-code__content\nsource_data_files:\n# Format: hf:dataset-id/subset-name/split-name.\n- hf:THUDM/LongBench/2wikimqa_e/test\n- hf:THUDM/LongBench/2wikimqa/test\n- hf:THUDM/LongBench/hotpotqa_e/test\n- hf:THUDM/LongBench/hotpotqa/test\n- hf:THUDM/LongBench/narrativeqa/test\n- hf:THUDM/LongBench/triviaqa_e/test\n- hf:THUDM/LongBench/triviaqa/test\n\n```\n\nWhen FMBench encounters a dataset prefixed with `hf:`, it will:\n\n- Automatically download the dataset from Hugging Face\n- Convert it to the required JSON Lines format\n- Handle both text and image datasets dynamically\n- Store the processed data in either:\n- The S3 read bucket for cloud deployments\n- The `/tmp/fmbench-read/source_data/` directory for local runs\n\n> **Note**: This requires a Hugging Face token to be configured in your environment for private or gated datasets.\n\n## Using Custom Datasets [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/byo_dataset.html\\#using-custom-datasets \"Permanent link\")\n\nIf you want to use your own dataset or a pre-processed dataset, you can:\n\n- Provide the dataset path without the `hf:` prefix in the config:\n\n\n\n```md-code__content\nsource_data_files:\n- my-custom-dataset.jsonl\n\n```\n\n- Or, use the `[` bring\\_your\\_own\\_dataset `](./src/fmbench/bring_your_own_dataset.ipynb) notebook` to convert your custom dataset to JSON Lines format and upload it to the appropriate S3 bucket or local directory.\n\n\nFMBench will use these files directly from the specified location without any preprocessing.\n\n## Support for new Image and Text datasets [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/byo_dataset.html\\#support-for-new-image-and-text-datasets \"Permanent link\")\n\nWhile you can use any hugging face dataset without pre processing, FMBench provides configuration files for running `llama3-2-11b-instruct`, `claude-3-sonnet`, `claude-3-5-sonnet` on the following image and text datasets:\n\n1. Databricks dolly dataset: [config-llama-3-2-11b-databricks-dolly-15k.yml](https://github.com/aws-samples/foundation-model-benchmarking-tool/src/fmbench/configs/bedrock/config-llama-3-2-11b-databricks-dolly-15k.yml)\n2. Multimodal ScienceQA dataset: [config-llama-3-2-11b-vision-instruct-scienceqa.yml](https://github.com/aws-samples/foundation-model-benchmarking-tool/src/fmbench/configs/multimodal/bedrock/config-llama-3-2-11b-vision-instruct-scienceqa.yml)\n3. Multimodal marqo-GS-10M dataset: [config-llama-3-2-11b-vision-instruct-marqo-GS-10M.yml](https://github.com/aws-samples/foundation-model-benchmarking-tool/src/fmbench/configs/multimodal/bedrock/config-llama-3-2-11b-vision-instruct-marqo-GS-10M.yml)\n\n## Support for Open-Orca dataset [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/byo_dataset.html\\#support-for-open-orca-dataset \"Permanent link\")\n\nSupport for [Open-Orca](https://huggingface.co/datasets/Open-Orca/OpenOrca) dataset and corresponding prompts for Llama3, Llama2 and Mistral, see:\n\n1. [bring\\_your\\_own\\_dataset.ipynb](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/main/src/fmbench/bring_your_own_dataset.ipynb)\n2. [prompt templates](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/main/src/fmbench/prompt_template)\n3. [Llama3 config file with OpenOrca](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/main/src/fmbench/configs/llama3/8b/config-llama3-8b-inf2-g5-byoe-w-openorca.yml)",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/byo_dataset.html",
      "title": "BYO dataset - Foundation Model Benchmarking Tool (FMBench)",
      "favicon": {},
      "language": "en",
      "scrapeId": "850e9762-033a-404c-a806-2214f136524b",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/byo_dataset.html",
      "statusCode": 200,
      "theme-color": "#00000000",
      "color-scheme": "normal"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "[Skip to content](https://aws-samples.github.io/foundation-model-benchmarking-tool/resources.html#pending-enhancements)\n\n# Resources\n\n## Pending enhancements [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/resources.html\\#pending-enhancements \"Permanent link\")\n\nView the [ISSUES](https://github.com/aws-samples/foundation-model-benchmarking-tool/issues) on GitHub and add any you might think be an beneficial iteration to this benchmarking harness.\n\n## Security [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/resources.html\\#security \"Permanent link\")\n\nSee [CONTRIBUTING](https://aws-samples.github.io/foundation-model-benchmarking-tool/CONTRIBUTING.md#security-issue-notifications) for more information.\n\n## License [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/resources.html\\#license \"Permanent link\")\n\nThis library is licensed under the MIT-0 License. See the [LICENSE](https://aws-samples.github.io/foundation-model-benchmarking-tool/LICENSE) file.",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/resources.html",
      "title": "Resources - Foundation Model Benchmarking Tool (FMBench)",
      "favicon": {},
      "language": "en",
      "scrapeId": "651b4081-38ea-4b45-8928-df3d6c867ef4",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/resources.html",
      "statusCode": 200,
      "theme-color": "#00000000",
      "color-scheme": "normal"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"latest-FMs-fmbench-bedrock\"\n  model_name: \"FMs available in Amazon Bedrock\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket}\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: llama2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama2.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the\n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\nmetrics:\n  dataset_of_interest: en_2000-3000\n\npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n\ninference_parameters:\n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  - name: meta.llama3-1-70b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: meta.llama3-1-70b-instruct-v1:0\n    model_version:\n    model_name: meta.llama3-1-70b-instruct-v1:0\n    ep_name: meta.llama3-1-70b-instruct-v1:0\n    instance_type: meta.llama3-1-70b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the\n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    # - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n\n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  - name: meta.llama3-1-8b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: meta.llama3-1-8b-instruct-v1:0\n    model_version:\n    model_name: meta.llama3-1-8b-instruct-v1:0\n    ep_name: meta.llama3-1-8b-instruct-v1:0\n    instance_type: meta.llama3-1-8b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the\n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    # - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n\n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  - name: amazon.titan-text-express-v1\n    model_id: amazon.titan-text-express-v1\n    model_version: \"*\"\n    model_name: amazon.titan-text-express-v1\n    ep_name: amazon.titan-text-express-v1\n    instance_type: amazon.titan-text-express-v1\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    # - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env:\n  - name: mistral.mistral-7b-instruct-v0:2\n    model_id: mistral.mistral-7b-instruct-v0:2\n    model_version: \"*\"\n    model_name: mistral.mistral-7b-instruct-v0:2\n    ep_name: mistral.mistral-7b-instruct-v0:2\n    instance_type: mistral.mistral-7b-instruct-v0:2\n    image_uri:\n    deploy: no\n    instance_count: 1\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    # - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env:\n  - name: mistral.mixtral-8x7b-instruct-v0:1\n    model_id: mistral.mixtral-8x7b-instruct-v0:1\n    model_version: \"*\"\n    model_name: mistral.mixtral-8x7b-instruct-v0:1\n    ep_name: mistral.mixtral-8x7b-instruct-v0:1\n    instance_type: mistral.mixtral-8x7b-instruct-v0:1\n    image_uri:\n    deploy: no\n    instance_count: 1\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    # - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env:\n  - name: anthropic.claude-3-sonnet-20240229-v1:0\n    model_id: anthropic.claude-3-sonnet-20240229-v1:0\n    model_version: \"*\"\n    model_name: anthropic.claude-3-sonnet-20240229-v1:0\n    ep_name: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_type: anthropic.claude-3-sonnet-20240229-v1:0\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    # - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env:\n  - name: anthropic.claude-3-haiku-20240307-v1:0\n    model_id: anthropic.claude-3-haiku-20240307-v1:0\n    model_version: \"*\"\n    model_name: anthropic.claude-3-haiku-20240307-v1:0\n    ep_name: anthropic.claude-3-haiku-20240307-v1:0\n    instance_type: anthropic.claude-3-haiku-20240307-v1:0\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    # - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env:\n  - name: cohere.command-text-v14\n    model_id: cohere.command-text-v14\n    model_version: \"*\"\n    model_name: cohere.command-text-v14\n    ep_name: cohere.command-text-v14\n    instance_type: cohere.command-text-v14\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    # - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env:\n  - name: cohere.command-light-text-v14\n    model_id: cohere.command-light-text-v14\n    model_version: \"*\"\n    model_name: cohere.command-light-text-v14\n    ep_name: cohere.command-light-text-v14\n    instance_type: cohere.command-light-text-v14\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    # - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env:\n\nreport:\n  latency_budget: 2\n  cosine_similarity_budget: 0.3\n  accuracy_budget: 1\n  accuracy_error_rate_budget: 0\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-evals-only-conc-1.yml",
      "scrapeId": "0294d1aa-7f36-4318-af49-5908a89157b6",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-evals-only-conc-1.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "[Skip to content](https://aws-samples.github.io/foundation-model-benchmarking-tool/website.html#create-a-website-for-fmbench-reports)\n\n# Create a website for `FMBench` reports [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/website.html\\#create-a-website-for-fmbench-reports \"Permanent link\")\n\nWhen you use `FMBench` as a tool for benchmarking your foundation models you would soon want to have an easy way to view all the reports in one place and search through the results, for example, \" `Llama3.1-8b` results on `trn1.32xlarge`\". An `FMBench` website provides a simple way of viewing these results.\n\nHere are the steps to setup a website using `mkdocs` and `nginx`. The steps below generate a self-signed certificate for SSL and use username and password for authentication. **It is strongly recommended that you use a valid SSL cert and a better authentication mechanism than username and password for your `FMBench` website**.\n\n01. Start an Amazon EC2 machine which will host the `FMBench` website. A `t3.xlarge` machine with an Ubuntu AMI say `ubuntu/images/hvm-ssd-gp3/ubuntu-noble-24.04-amd64-server-20240801` and 50GB storage is good enough. **Allow SSH and TCP port 443 traffic from anywhere into that machine**.\n\n02. SSH into that machine and install `conda`.\n\n\n\n    ```md-code__content\n    wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n    bash Miniconda3-latest-Linux-x86_64.sh -b  # Run the Miniconda installer in batch mode (no manual intervention)\n    rm -f Miniconda3-latest-Linux-x86_64.sh    # Remove the installer script after installation\n    eval \"$(/home/$USER/miniconda3/bin/conda shell.bash hook)\" # Initialize conda for bash shell\n    conda init  # Initialize conda, adding it to the shell\n\n    ```\n\n03. Install `docker-compose`.\n\n\n\n    ```md-code__content\n    sudo apt-get update\n    sudo apt-get install --reinstall docker.io -y\n    sudo apt-get install -y docker-compose\n    sudo usermod -a -G docker $USER\n    newgrp docker\n    docker compose version\n\n    ```\n\n04. Setup the `fmbench_python311` conda environment and clone `FMBench` repo.\n\n\n\n    ```md-code__content\n    conda create --name fmbench_python311 -y python=3.11 ipykernel\n    source activate fmbench_python311\n    pip install -U fmbench mkdocs mkdocs-material mknotebooks\n    git clone https://github.com/aws-samples/foundation-model-benchmarking-tool.git\n\n    ```\n\n05. Get the `FMBench` results data from Amazon S3 or whichever storage system you used to store all the results.\n\n\n\n    ```md-code__content\n    curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n    sudo apt-get install unzip -y\n    unzip awscliv2.zip\n    sudo ./aws/install\n    FMBENCH_S3_BUCKET=your-fmbench-s3-bucket-name-here\n    aws s3 sync s3://$FMBENCH_S3_BUCKET $HOME/fmbench_data --exclude \"*.json\"\n\n    ```\n\n06. Create a directory for the `FMBench` website contents.\n\n\n\n\n\n    ```md-code__content\n    mkdir $HOME/fmbench_site\n    mkdir $HOME/fmbench_site/ssl\n\n    ```\n\n\n\n    1\\. Setup SSL certs (we strongly encourage you to not use self-signed certs, this step here is just for demo purposes, get SSL certs the same way you get them for your current production workloads).\n\n\n\n\n    ```md-code__content\n    sudo openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout $HOME/fmbench_site/ssl/nginx-selfsigned.key -out $HOME/fmbench_site/ssl/nginx-selfsigned.crt\n\n    ```\n\n07. Create an `.httpasswd` file. The `FMBench` website will use the `fmbench_admin` as a username and a password that you enter as part of the command below to allow login to the website.\n\n\n\n    ```md-code__content\n    sudo apt-get install apache2-utils -y\n    htpasswd -c $HOME/fmbench_site/.htpasswd fmbench_admin\n\n    ```\n\n08. Create the `mkdocs.yml` file for the website.\n\n\n\n    ```md-code__content\n    cd foundation-model-benchmarking-tool\n    cp website/index.md $HOME/fmbench_data/\n    cp -r img $HOME/fmbench_data/\n    python website/create_fmbench_website.py\n    mkdocs build -f website/mkdocs.yml --site-dir $HOME/fmbench_site/site\n\n    ```\n\n09. Update `nginx.conf` file. **Note the hostname that is printed out below, the `FMBench` website would be served at this address**.\n\n\n\n    ```md-code__content\n    TOKEN=`curl -X PUT \"http://169.254.169.254/latest/api/token\" -H \"X-aws-ec2-metadata-token-ttl-seconds: 21600\"`\n    HOSTNAME=`curl -H \"X-aws-ec2-metadata-token: $TOKEN\" http://169.254.169.254/latest/meta-data/public-hostname`\n    echo \"hostname is: $HOSTNAME\"\n    sed \"s/__HOSTNAME__/$HOSTNAME/g\" website/nginx.conf.template > $HOME/fmbench_site/nginx.conf\n\n    ```\n\n10. Serve the website.\n\n\n\n    ```md-code__content\n    docker run --name fmbench-nginx -d -p 80:80 -p 443:443   -v $HOME/fmbench_site/site:/usr/share/nginx/html   -v $HOME/fmbench_site/nginx.conf:/etc/nginx/nginx.conf   -v $HOME/fmbench_site/ssl:/etc/nginx/ssl   -v $HOME/fmbench_site/.htpasswd:/etc/nginx/.htpasswd   nginx\n\n    ```\n\n11. Open a web browser and navigate to the hostname you noted in the step above, for example `https://<your-ec2-hostname>.us-west-2.compute.amazonaws.com`, ignore the security warnings if you used a self-signed SSL cert (replace this with a cert that you would normally use in your production websites) and then enter the username and password (the username would be `fmbench_admin` and password would be what you had set when running the `htpasswd` command). You should see a website as shown in the screenshot below.\n\n\n![website](https://aws-samples.github.io/foundation-model-benchmarking-tool/img/website.png)",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/website.html",
      "title": "Website - Foundation Model Benchmarking Tool (FMBench)",
      "favicon": {},
      "language": "en",
      "scrapeId": "6b13f9c9-d4f0-45de-b3e2-bd74094878a3",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/website.html",
      "statusCode": 200,
      "theme-color": "#00000000",
      "color-scheme": "normal"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"fmbench-bedrock-anthropic-models\"\n  model_name: \"All anthropic models available in Amazon Bedrock - max voting\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n## S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n  prompt_template_dir: prompt_template\n  prompt_template_file: prompt_template_claude.txt ## add your desired prompt template type\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\nmetrics:\n  dataset_of_interest: en_3000-4000\n\npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n\ninference_parameters:\n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  # Experiment for claude 3 sonnet\n  - name: anthropic.claude-3-sonnet-20240229-v1:0\n    model_name: anthropic.claude-3-sonnet-20240229-v1:0\n    ep_name: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_type: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  # Experiment for claude 2.1\n  - name: anthropic.claude-v2:1\n    model_name: anthropic.claude-v2:1\n    ep_name: anthropic.claude-v2:1\n    instance_type: anthropic.claude-v2:1\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  # Experiment for claude instant\n  - name: anthropic.claude-instant-v1\n    model_name: anthropic.claude-instant-v1\n    ep_name: anthropic.claude-instant-v1\n    instance_type: anthropic.claude-instant-v1\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  # Experiment for claude v2\n  - name: anthropic.claude-v2\n    model_name: anthropic.claude-v2\n    ep_name: anthropic.claude-v2\n    instance_type: anthropic.claude-v2\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env:\n  # Experiment for claude 3 haiku\n  - name: anthropic.claude-3-haiku-20240307-v1:0\n    model_name: anthropic.claude-3-haiku-20240307-v1:0\n    ep_name: anthropic.claude-3-haiku-20240307-v1:0\n    instance_type: anthropic.claude-3-haiku-20240307-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n\nreport:\n  latency_budget: 5\n  cost_per_10k_txn_budget: 350\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-all-anthropic-models-longbench-data.yml",
      "scrapeId": "0166650f-f55f-47c1-9e70-330340fbc4e4",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-all-anthropic-models-longbench-data.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-70b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-70b-inf2.48xl-ec2\"\n  model_name: \"Hermes-3-Llama-3.1-70B\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    top_k: 50\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Hermes-3-Llama-3.1-70B\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: NousResearch/Hermes-3-Llama-3.1-70B #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Hermes-3-Llama-3.1-70B\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8080/v2/models/Hermes-3-Llama-3.1-70B/generate'\n    instance_type: \"inf2.48xlarge\"\n    image_uri: tritonserver-neuronx:fmbench\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-70b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model.\n      model_loading_timeout: 10000\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # For deploying a model using the triton inference container:\n      # mention the backend type, if any. Options for triton are: [djl, vllm or tensorrt]\n      backend: djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured,\n      # max: FMBench figures out the max number of model containers to be run\n      #      based on TP degree configured and number of neuron cores/GPUs available.\n      #      For example, if TP=2, GPUs=8 then FMBench will start 4 containers and 1 load balancer,\n      # auto: only supported if the underlying inference container would automatically\n      #       start multiple copies of the model internally based on TP degree and neuron cores/GPUs\n      #       available. In this case only a single container is created, no load balancer is created.\n      #       The DJL serving containers supports auto.\n      model_copies: max\n      shm_size: 12g\n      # The model.json parameters are replaced within the model.json file\n      # for the triton on vllm/djl/tensorrt options. The model.json already contains\n      # the tp degree and model id from above in this config file. This is a dictionary\n      # that contains other variables, such as max_num_seqs, dtype, max_model_len, block_size and more\n      # For tensorrt, the tp degree, batch size and other relevant parameters are\n      # extracted directly from the inference spec.\n      container_params:\n        # tp degree is a mandatory parameter\n        tp_degree: 24\n        amp: \"f16\"\n        attention_layout: 'BSH'\n        collectives_layout: 'BSH'\n        context_length_estimate: 3072, 3584, 4096\n        max_rolling_batch_size: 8\n        model_loader: \"tnx\"\n        model_loading_timeout: 24000\n        n_positions: 4096\n        output_formatter: \"json\"\n        rolling_batch: \"auto\"\n        rolling_batch_strategy: \"continuous_batching\"\n        neuron_optimize_level: 2\n        trust_remote_code: true\n        # modify the serving properties to match your model and requirements\n        serving.properties:\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O2\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 200\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/NousResearchHermes70B/config-Nous-Hermes3-1-70b-inf2.48xl-triton-tp24.yml",
      "scrapeId": "50a29c4d-74bd-4b62-a638-b59fb8a2e89e",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/NousResearchHermes70B/config-Nous-Hermes3-1-70b-inf2.48xl-triton-tp24.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"fmbench-bedrock-vision-models-realworldqa\"\n  model_name: \"llama3-2 11b & Claude vision models on Amazon Bedrock\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the https://huggingface.co/datasets/derek-thomas/ScienceQA\n  # This dataset contains images containing science questions and associated ground truth responses to the questions.\n  # when this notebook looks for this dataset - if it is prefixed with hf, and if the token is already there\n  # if the token is there and the hf: is there, then download the dataset from hf first and then process it -\n  source_data_files:\n  # If a split is specified in the dataset identifier and exists in the loaded dataset,\n  # it uses that split. If no split is specified (e.g., hf:derek-thomas/ScienceQA), it will default to\n  # the first available split. In the given example below, users can specify the 'train' and 'validation' splits\n  # of the 'derek-thomas/ScienceQA' dataset. If none are provided, then the first available split will be used.\n\n  # Follow this format below: hf:dataset-id/subset-name/split-name.\n  # If there is no specified subset name, use \"default\"\n  - hf:derek-thomas/ScienceQA/default/train\n  - hf:derek-thomas/ScienceQA/default/validation\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3_images_ScienceQA.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: no\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  # This represents the question that is asked as a part of the\n  # image messages API format. If this is not provided, a description\n  # of the image will be provided instead\n  - question\n  # If you want to benchmark a multimodal model on an image dataset,\n  # then it an image_col parameter is required. This parameter refers to the\n  # name of the column in the dataset that contains the images to be used\n  # during the benchmarking process. If this column is not provided, the\n  # standard text generation benchmark process is be used in the FMBench run.\n  image_col: image\n  # This is the number of rows of the dataset that you want to load\n  # if this parameter is not given, it is defaulted to 100 rows\n  ds_N: 150\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1-500\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-2-11b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: us.meta.llama3-2-11b-instruct-v1:0\n    model_version:\n    model_name: us.meta.llama3-2-11b-instruct-v1:0\n    ep_name: us.meta.llama3-2-11b-instruct-v1:0\n    instance_type: us.meta.llama3-2-11b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the\n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n\n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  - name: anthropic.claude-3-sonnet-20240229-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: anthropic.claude-3-sonnet-20240229-v1:0\n    model_version:\n    model_name: anthropic.claude-3-sonnet-20240229-v1:0\n    ep_name: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_type: anthropic.claude-3-sonnet-20240229-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the\n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n\n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  - name: anthropic.claude-3-5-sonnet-20240620-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: anthropic.claude-3-5-sonnet-20240620-v1:0\n    model_version:\n    model_name: anthropic.claude-3-5-sonnet-20240620-v1:0\n    ep_name: anthropic.claude-3-5-sonnet-20240620-v1:0\n    instance_type: anthropic.claude-3-5-sonnet-20240620-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the\n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n\n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 1\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/multimodal/bedrock/config-llama-3-2-claude-models-scienceqa.yml",
      "scrapeId": "0a96020c-8c59-4a17-b3fd-1883d2ce38ac",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/multimodal/bedrock/config-llama-3-2-claude-models-scienceqa.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "[Skip to content](https://aws-samples.github.io/foundation-model-benchmarking-tool/advanced.html#advanced)\n\n# Advanced [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/advanced.html\\#advanced \"Permanent link\")\n\nBeyond running `FMBench` with the configuration files provided, you may want try out bringing your own dataset or endpoint to `FMBench`.",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/advanced.html",
      "title": "Main - Foundation Model Benchmarking Tool (FMBench)",
      "favicon": {},
      "language": "en",
      "scrapeId": "7513432d-7022-4565-8ae8-5126df0eacaa",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/advanced.html",
      "statusCode": 200,
      "theme-color": "#00000000",
      "color-scheme": "normal"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "# 404 - Not found",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/(https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/src/fmbench/bring_your_own_dataset.ipynb)",
      "error": "Not Found",
      "title": "Foundation Model Benchmarking Tool (FMBench)",
      "favicon": {},
      "language": "en",
      "scrapeId": "83503de7-0c64-42ac-bbc1-c0cd2d571c8d",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/(https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/src/fmbench/bring_your_own_dataset.ipynb)",
      "statusCode": 404
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g5.12xl-tp=2-mc=max-triton-ec2\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    temperature: 0.1\n    do_sample: yes\n    top_p: 0.92\n    top_k: 120\n    bad_words: \"\"\n    stop_words: \"<|eot_id|>\"\n    pad_id: 2\n    end_id: 2\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: http://localhost:8080/v2/models/ensemble/generate\n    instance_type: \"g5.12xlarge\"\n    image_uri: nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      batch_size: 64\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g5.12xl-tp-2-mc-max-triton-ec2.yml",
      "scrapeId": "0424709a-597e-43a3-a3c6-21c0e3a3ed6c",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g5.12xl-tp-2-mc-max-triton-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "# Files\n\nHere is a listing of the various configuration files available out-of-the-box with `FMBench`. Click on any link to view a file. You can use these files as-is or use them as templates to create a custom configuration file for your use-case of interest.\n\n**NousResearchHermes70B**\n\n[├── NousResearchHermes70B/config-Nous-Hermes3-1-70b-g5.48xl-tp-8-mc-max-djl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/NousResearchHermes70B/config-Nous-Hermes3-1-70b-g5.48xl-tp-8-mc-max-djl.yml)\n\n[├── NousResearchHermes70B/config-Nous-Hermes3-1-70b-inf2.48xl-triton-tp24.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/NousResearchHermes70B/config-Nous-Hermes3-1-70b-inf2.48xl-triton-tp24.yml)\n\n[└── NousResearchHermes70B/config-Nous-Hermes3-1-70b-trn1.32xl-deploy-ec2-triton.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/NousResearchHermes70B/config-Nous-Hermes3-1-70b-trn1.32xl-deploy-ec2-triton.yml)\n\n**bedrock**\n\n[├── bedrock/config-bedrock-all-anthropic-models-longbench-data.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-all-anthropic-models-longbench-data.yml)\n\n[├── bedrock/config-bedrock-anthropic-models-OpenOrca.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-anthropic-models-OpenOrca.yml)\n\n[├── bedrock/config-bedrock-claude.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-claude.yml)\n\n[├── bedrock/config-bedrock-evals-only-conc-1.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-evals-only-conc-1.yml)\n\n[├── bedrock/config-bedrock-haiku-sonnet-majority-voting.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-haiku-sonnet-majority-voting.yml)\n\n[├── bedrock/config-bedrock-llama3-1-70b-streaming.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-llama3-1-70b-streaming.yml)\n\n[├── bedrock/config-bedrock-llama3-1-8b-streaming.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-llama3-1-8b-streaming.yml)\n\n[├── bedrock/config-bedrock-llama3-1-no-streaming.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-llama3-1-no-streaming.yml)\n\n[├── bedrock/config-bedrock-llama3-1.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-llama3-1.yml)\n\n[├── bedrock/config-bedrock-llama3-streaming.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-llama3-streaming.yml)\n\n[├── bedrock/config-bedrock-models-OpenOrca.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-models-OpenOrca.yml)\n\n[├── bedrock/config-bedrock-titan-text-express.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-titan-text-express.yml)\n\n[├── bedrock/config-bedrock.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock.yml)\n\n[├── bedrock/config-claude-3-5-sonnet-v2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-claude-3-5-sonnet-v2.yml)\n\n[├── bedrock/config-claude-dolly-dataset.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-claude-dolly-dataset.yml)\n\n[├── bedrock/config-llama-3-2-11b-databricks-dolly-15k.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-llama-3-2-11b-databricks-dolly-15k.yml)\n\n[├── bedrock/config-llama-3-2-1b-3b-no-evals.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-llama-3-2-1b-3b-no-evals.yml)\n\n[├── bedrock/config-llama-3-2-1b-3b.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-llama-3-2-1b-3b.yml)\n\n[├── bedrock/config-llama-3-2-all-models-longbench-hf-version.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-llama-3-2-all-models-longbench-hf-version.yml)\n\n[├── bedrock/config-llama-3-2-all-models.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-llama-3-2-all-models.yml)\n\n[├── bedrock/config-llama-3-3-all-models-open-orca.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-llama-3-3-all-models-open-orca.yml)\n\n[├── bedrock/config-llama-3-3-all-models.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-llama-3-3-all-models.yml)\n\n[├── bedrock/config-nova-all-models-convfinqa.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-nova-all-models-convfinqa.yml)\n\n[├── bedrock/config-nova-all-models-dolly-dataset.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-nova-all-models-dolly-dataset.yml)\n\n[├── bedrock/config-nova-all-models-openarca.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-nova-all-models-openarca.yml)\n\n[└── bedrock/config-nova-all-models.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-nova-all-models.yml)\n\n**bert**\n\n[└── bert/config-distilbert-base-uncased.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bert/config-distilbert-base-uncased.yml)\n\n**byoe**\n\n[├── byoe/config-byo-custom-rest-predictor-tinyllama.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/byoe/config-byo-custom-rest-predictor-tinyllama.yml)\n\n[├── byoe/config-byo-custom-rest-predictor.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/byoe/config-byo-custom-rest-predictor.yml)\n\n[└── byoe/config-model-byo-sagemaker-endpoint.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/byoe/config-model-byo-sagemaker-endpoint.yml)\n\n**deepseek**\n\n[├── deepseek/config-deepseek-r1-ollama.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/deepseek/config-deepseek-r1-ollama.yml)\n\n[├── deepseek/config-deepseek-r1-quant1.58-longbench-byoe.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/deepseek/config-deepseek-r1-quant1.58-longbench-byoe.yml)\n\n[├── deepseek/config-deepseek-r1-sglang.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/deepseek/config-deepseek-r1-sglang.yml)\n\n[├── deepseek/config-deepseek-r1-vllm-convfinqa.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/deepseek/config-deepseek-r1-vllm-convfinqa.yml)\n\n[├── deepseek/config-deepseek-r1-vllm-longbench.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/deepseek/config-deepseek-r1-vllm-longbench.yml)\n\n[└── deepseek/config-deepseek-r1-vllm-openorca.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/deepseek/config-deepseek-r1-vllm-openorca.yml)\n\n**eks\\_manifests**\n\n[├── eks\\_manifests/llama3-ray-service.yaml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/eks_manifests/llama3-ray-service.yaml)\n\n[└── eks\\_manifests/mistral-ray-service.yaml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/eks_manifests/mistral-ray-service.yaml)\n\n**embeddings**\n\n[├── embeddings/bge-base-en-v1-5-c5-embeddings.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/embeddings/bge-base-en-v1-5-c5-embeddings.yml)\n\n[├── embeddings/bge-base-en-v1-5-g5-embeddings.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/embeddings/bge-base-en-v1-5-g5-embeddings.yml)\n\n[└── embeddings/bge-base-en-v1-5-g5-g4dn-c7-embeddings.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/embeddings/bge-base-en-v1-5-g5-g4dn-c7-embeddings.yml)\n\n**gemma**\n\n[└── gemma/config-gemma-2b-g5.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/gemma/config-gemma-2b-g5.yml)\n\n**generic**\n\n**└── generic/ec2**\n\n[├── generic/ec2/Qwen2.5\\_djl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/generic/ec2/Qwen2.5_djl.yml)\n\n[├── generic/ec2/djl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/generic/ec2/djl.yml)\n\n[└── generic/ec2/llama3.1\\_djl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/generic/ec2/llama3.1_djl.yml)\n\n**llama2**\n\n**├── llama2/13b**\n\n[│   ├── llama2/13b/config-bedrock-sagemaker-llama2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama2/13b/config-bedrock-sagemaker-llama2.yml)\n\n[│   ├── llama2/13b/config-byo-rest-ep-llama2-13b.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama2/13b/config-byo-rest-ep-llama2-13b.yml)\n\n[│   ├── llama2/13b/config-llama2-13b-inf2-g5-p4d.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama2/13b/config-llama2-13b-inf2-g5-p4d.yml)\n\n[│   └── llama2/13b/config-llama2-13b-inf2-g5.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama2/13b/config-llama2-13b-inf2-g5.yml)\n\n**├── llama2/70b**\n\n[│   ├── llama2/70b/config-ec2-llama2-70b.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama2/70b/config-ec2-llama2-70b.yml)\n\n[│   ├── llama2/70b/config-llama2-70b-g5-p4d-tgi.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama2/70b/config-llama2-70b-g5-p4d-tgi.yml)\n\n[│   ├── llama2/70b/config-llama2-70b-g5-p4d-trt.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama2/70b/config-llama2-70b-g5-p4d-trt.yml)\n\n[│   └── llama2/70b/config-llama2-70b-inf2-g5.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama2/70b/config-llama2-70b-inf2-g5.yml)\n\n**└── llama2/7b**\n\n[├── llama2/7b/config-llama2-7b-byo-sagemaker-endpoint.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama2/7b/config-llama2-7b-byo-sagemaker-endpoint.yml)\n\n[├── llama2/7b/config-llama2-7b-g4dn-g5-trt.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama2/7b/config-llama2-7b-g4dn-g5-trt.yml)\n\n[├── llama2/7b/config-llama2-7b-g5-no-s3-quick.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama2/7b/config-llama2-7b-g5-no-s3-quick.yml)\n\n[├── llama2/7b/config-llama2-7b-g5-quick.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama2/7b/config-llama2-7b-g5-quick.yml)\n\n[└── llama2/7b/config-llama2-7b-inf2-g5.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama2/7b/config-llama2-7b-inf2-g5.yml)\n\n**llama3**\n\n**├── llama3/70b**\n\n[│   ├── llama3/70b/config-bedrock.yml -> ../../bedrock/config-bedrock.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/70b/bedrock/config-bedrock.yml)\n\n[│   ├── llama3/70b/config-ec2-llama3-70b-instruct.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/70b/config-ec2-llama3-70b-instruct.yml)\n\n[│   ├── llama3/70b/config-ec2-neuron-llama3-70b-inf2-48xl-deploy-sm.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/70b/config-ec2-neuron-llama3-70b-inf2-48xl-deploy-sm.yml)\n\n[│   ├── llama3/70b/config-llama3-70b-instruct-g5-48xl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/70b/config-llama3-70b-instruct-g5-48xl.yml)\n\n[│   ├── llama3/70b/config-llama3-70b-instruct-g5-p4d.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/70b/config-llama3-70b-instruct-g5-p4d.yml)\n\n[│   └── llama3/70b/config-llama3-70b-instruct-p4d.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/70b/config-llama3-70b-instruct-p4d.yml)\n\n**└── llama3/8b**\n\n[├── llama3/8b/config-bedrock.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-bedrock.yml)\n\n[├── llama3/8b/config-ec2-llama3-8b-c5-18xlarge.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-c5-18xlarge.yml)\n\n[├── llama3/8b/config-ec2-llama3-8b-c8g-24xlarge.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-c8g-24xlarge.yml)\n\n[├── llama3/8b/config-ec2-llama3-8b-g6e-2xlarge.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-g6e-2xlarge.yml)\n\n[├── llama3/8b/config-ec2-llama3-8b-inf2-48xl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-inf2-48xl.yml)\n\n[├── llama3/8b/config-ec2-llama3-8b-m5-16xlarge.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-m5-16xlarge.yml)\n\n[├── llama3/8b/config-ec2-llama3-8b-m7a-16xlarge.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-m7a-16xlarge.yml)\n\n[├── llama3/8b/config-ec2-llama3-8b-m7a-24xlarge.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-m7a-24xlarge.yml)\n\n[├── llama3/8b/config-ec2-llama3-8b-m7i-12xlarge.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-m7i-12xlarge.yml)\n\n[├── llama3/8b/config-ec2-llama3-8b-m7i-16xlarge.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-m7i-16xlarge.yml)\n\n[├── llama3/8b/config-ec2-llama3-8b-m7i-24xlarge.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-m7i-24xlarge.yml)\n\n[├── llama3/8b/config-ec2-llama3-8b-neuron-trn1-32xl-tp16-sm.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-neuron-trn1-32xl-tp16-sm.yml)\n\n[├── llama3/8b/config-ec2-llama3-8b-p4d-tp-2-mc-max.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-p4d-tp-2-mc-max.yml)\n\n[├── llama3/8b/config-ec2-llama3-8b-p4d-tp-4-mc-max.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-p4d-tp-4-mc-max.yml)\n\n[├── llama3/8b/config-ec2-llama3-8b-p4d-tp-8-mc-max.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-p4d-tp-8-mc-max.yml)\n\n[├── llama3/8b/config-ec2-llama3-8b-p4de-tp-2-mc-max.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-p4de-tp-2-mc-max.yml)\n\n[├── llama3/8b/config-ec2-llama3-8b-p4de-tp-4-mc-max.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-p4de-tp-4-mc-max.yml)\n\n[├── llama3/8b/config-ec2-llama3-8b-p4de-tp-8-mc-max.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-p4de-tp-8-mc-max.yml)\n\n[├── llama3/8b/config-ec2-llama3-8b-p5-tp-2-mc-max.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-p5-tp-2-mc-max.yml)\n\n[├── llama3/8b/config-ec2-llama3-8b-p5-tp-8-mc-auto.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-p5-tp-8-mc-auto.yml)\n\n[├── llama3/8b/config-ec2-llama3-8b.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b.yml)\n\n[├── llama3/8b/config-ec2-neuron-llama3-8b-inf2-24xl-deploy-sm.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-neuron-llama3-8b-inf2-24xl-deploy-sm.yml)\n\n[├── llama3/8b/config-ec2-neuron-llama3-8b-inf2-48xl-deploy-sm.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-neuron-llama3-8b-inf2-48xl-deploy-sm.yml)\n\n[├── llama3/8b/config-llama3-8b-eks-inf2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-eks-inf2.yml)\n\n[├── llama3/8b/config-llama3-8b-g5-streaming.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g5-streaming.yml)\n\n[├── llama3/8b/config-llama3-8b-g5.12xl-tp-2-mc-max-djl-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g5.12xl-tp-2-mc-max-djl-ec2.yml)\n\n[├── llama3/8b/config-llama3-8b-g5.12xl-tp-2-mc-max-triton-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g5.12xl-tp-2-mc-max-triton-ec2.yml)\n\n[├── llama3/8b/config-llama3-8b-g5.12xl-tp-4-mc-max-djl-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g5.12xl-tp-4-mc-max-djl-ec2.yml)\n\n[├── llama3/8b/config-llama3-8b-g5.12xl-tp-4-mc-max-triton-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g5.12xl-tp-4-mc-max-triton-ec2.yml)\n\n[├── llama3/8b/config-llama3-8b-g5.48xl-tp-8-mc-max-triton-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g5.48xl-tp-8-mc-max-triton-ec2.yml)\n\n[├── llama3/8b/config-llama3-8b-g5.4xl-tp-1-mc-max-triton-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g5.4xl-tp-1-mc-max-triton-ec2.yml)\n\n[├── llama3/8b/config-llama3-8b-g6e.12xl-tp-2-mc-max-djl-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.12xl-tp-2-mc-max-djl-ec2.yml)\n\n[├── llama3/8b/config-llama3-8b-g6e.12xl-tp-2-mc-max-triton-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.12xl-tp-2-mc-max-triton-ec2.yml)\n\n[├── llama3/8b/config-llama3-8b-g6e.12xl-tp-4-mc-max-djl-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.12xl-tp-4-mc-max-djl-ec2.yml)\n\n[├── llama3/8b/config-llama3-8b-g6e.12xl-tp-4-mc-max-triton-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.12xl-tp-4-mc-max-triton-ec2.yml)\n\n[├── llama3/8b/config-llama3-8b-g6e.24xl-tp-2-mc-max-djl-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.24xl-tp-2-mc-max-djl-ec2.yml)\n\n[├── llama3/8b/config-llama3-8b-g6e.24xl-tp-2-mc-max-triton-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.24xl-tp-2-mc-max-triton-ec2.yml)\n\n[├── llama3/8b/config-llama3-8b-g6e.24xl-tp-4-mc-max-djl-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.24xl-tp-4-mc-max-djl-ec2.yml)\n\n[├── llama3/8b/config-llama3-8b-g6e.24xl-tp-4-mc-max-triton-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.24xl-tp-4-mc-max-triton-ec2.yml)\n\n[├── llama3/8b/config-llama3-8b-g6e.2xl-tp-1-mc-max-triton-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.2xl-tp-1-mc-max-triton-ec2.yml)\n\n[├── llama3/8b/config-llama3-8b-g6e.48xl-tp-2-mc-max-djl-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.48xl-tp-2-mc-max-djl-ec2.yml)\n\n[├── llama3/8b/config-llama3-8b-g6e.48xl-tp-2-mc-max-triton-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.48xl-tp-2-mc-max-triton-ec2.yml)\n\n[├── llama3/8b/config-llama3-8b-g6e.48xl-tp-4-mc-max-djl-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.48xl-tp-4-mc-max-djl-ec2.yml)\n\n[├── llama3/8b/config-llama3-8b-g6e.48xl-tp-4-mc-max-triton-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.48xl-tp-4-mc-max-triton-ec2.yml)\n\n[├── llama3/8b/config-llama3-8b-g6e.48xl-tp-8-mc-max-djl-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.48xl-tp-8-mc-max-djl-ec2.yml)\n\n[├── llama3/8b/config-llama3-8b-g6e.48xl-tp-8-mc-max-triton-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.48xl-tp-8-mc-max-triton-ec2.yml)\n\n[├── llama3/8b/config-llama3-8b-g6e.4xl-tp-1-mc-max-djl-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.4xl-tp-1-mc-max-djl-ec2.yml)\n\n[├── llama3/8b/config-llama3-8b-g6e.4xl-tp-1-mc-max-triton-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.4xl-tp-1-mc-max-triton-ec2.yml)\n\n[├── llama3/8b/config-llama3-8b-g6e.xl-tp-1-mc-max-triton-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.xl-tp-1-mc-max-triton-ec2.yml)\n\n[├── llama3/8b/config-llama3-8b-inf2-24xl-tp=8-bs=4-byoe.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-inf2-24xl-tp%3D8-bs%3D4-byoe.yml)\n\n[├── llama3/8b/config-llama3-8b-inf2-48xl-tp=8-bs=4-byoe.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-inf2-48xl-tp%3D8-bs%3D4-byoe.yml)\n\n[├── llama3/8b/config-llama3-8b-inf2-48xlarge-triton-djl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-inf2-48xlarge-triton-djl.yml)\n\n[├── llama3/8b/config-llama3-8b-inf2-g5-byoe-w-openorca.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-inf2-g5-byoe-w-openorca.yml)\n\n[├── llama3/8b/config-llama3-8b-inf2-g5.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-inf2-g5.yml)\n\n[├── llama3/8b/config-llama3-8b-instruct-all.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-all.yml)\n\n[├── llama3/8b/config-llama3-8b-instruct-g5-12xl-4-instances.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-g5-12xl-4-instances.yml)\n\n[├── llama3/8b/config-llama3-8b-instruct-g5-12xl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-g5-12xl.yml)\n\n[├── llama3/8b/config-llama3-8b-instruct-g5-24xl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-g5-24xl.yml)\n\n[├── llama3/8b/config-llama3-8b-instruct-g5-2xl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-g5-2xl.yml)\n\n[├── llama3/8b/config-llama3-8b-instruct-g5-48xl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-g5-48xl.yml)\n\n[├── llama3/8b/config-llama3-8b-instruct-g5-p4d.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-g5-p4d.yml)\n\n[├── llama3/8b/config-llama3-8b-instruct-g6-12xl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-g6-12xl.yml)\n\n[├── llama3/8b/config-llama3-8b-instruct-g6-24xl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-g6-24xl.yml)\n\n[├── llama3/8b/config-llama3-8b-instruct-g6-48xl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-g6-48xl.yml)\n\n[├── llama3/8b/config-llama3-8b-instruct-p4d-djl-lmi-dist.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-p4d-djl-lmi-dist.yml)\n\n[├── llama3/8b/config-llama3-8b-instruct-p4d-djl-vllm.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-p4d-djl-vllm.yml)\n\n[├── llama3/8b/config-llama3-8b-instruct-p5-djl-lmi-dist.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-p5-djl-lmi-dist.yml)\n\n[├── llama3/8b/config-llama3-8b-trn1-32xl-tp-16-bs-4-byoe.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-trn1-32xl-tp-16-bs-4-byoe.yml)\n\n[├── llama3/8b/config-llama3-8b-trn1-32xl-tp-8-bs-4-byoe.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-trn1-32xl-tp-8-bs-4-byoe.yml)\n\n[├── llama3/8b/config-llama3-8b-trn1-32xl-tp16-bs-4-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-trn1-32xl-tp16-bs-4-ec2.yml)\n\n[├── llama3/8b/config-llama3-8b-trn1-32xlarge-triton-djl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-trn1-32xlarge-triton-djl.yml)\n\n[├── llama3/8b/config-llama3-8b-trn1-32xlarge-triton-vllm.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-trn1-32xlarge-triton-vllm.yml)\n\n[├── llama3/8b/config-llama3-8b-trn1.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-trn1.yml)\n\n[├── llama3/8b/llama3-8b-inf2-24xl-byoe-g5-12xl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/llama3-8b-inf2-24xl-byoe-g5-12xl.yml)\n\n[├── llama3/8b/llama3-8b-inf2-48xl-byoe-g5-24xl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/llama3-8b-inf2-48xl-byoe-g5-24xl.yml)\n\n[└── llama3/8b/llama3-8b-trn1-32xl-byoe-g5-24xl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/llama3-8b-trn1-32xl-byoe-g5-24xl.yml)\n\n[llama3.1](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1)\n\n**├── llama3.1/70b**\n\n[│   ├── llama3.1/70b/config-ec2-llama3-1-70b-inf2-48xl-deploy-ec2-djl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-ec2-llama3-1-70b-inf2-48xl-deploy-ec2-djl.yml)\n\n[│   ├── llama3.1/70b/config-ec2-llama3-1-70b-inf2-deploy-sm.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-ec2-llama3-1-70b-inf2-deploy-sm.yml)\n\n[│   ├── llama3.1/70b/config-ec2-llama3-1-70b-p4de.24xl-deploy-ec2-large-prompts.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-ec2-llama3-1-70b-p4de.24xl-deploy-ec2-large-prompts.yml)\n\n[│   ├── llama3.1/70b/config-ec2-llama3-1-70b-p4de.24xl-deploy-ec2-longbench.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-ec2-llama3-1-70b-p4de.24xl-deploy-ec2-longbench.yml)\n\n[│   ├── llama3.1/70b/config-ec2-llama3-1-70b-p4de.24xl-deploy-ec2-summarization.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-ec2-llama3-1-70b-p4de.24xl-deploy-ec2-summarization.yml)\n\n[│   ├── llama3.1/70b/config-llama3-1-70b-g5.48xl-tp-8-mc-max-djl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-llama3-1-70b-g5.48xl-tp-8-mc-max-djl.yml)\n\n[│   ├── llama3.1/70b/config-llama3-1-70b-g6.48xl-tp-8-mc-max-djl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-llama3-1-70b-g6.48xl-tp-8-mc-max-djl.yml)\n\n[│   ├── llama3.1/70b/config-llama3-1-70b-g6e.24xl-tp-4-mc-max-djl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-llama3-1-70b-g6e.24xl-tp-4-mc-max-djl.yml)\n\n[│   ├── llama3.1/70b/config-llama3-1-70b-g6e.48xl-tp-8-mc-max-djl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-llama3-1-70b-g6e.48xl-tp-8-mc-max-djl.yml)\n\n[│   ├── llama3.1/70b/config-llama3-1-70b-inf2.48xl-deploy-ec2-triton.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-llama3-1-70b-inf2.48xl-deploy-ec2-triton.yml)\n\n[│   ├── llama3.1/70b/config-llama3-1-70b-inf2.48xl-triton-tp24.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-llama3-1-70b-inf2.48xl-triton-tp24.yml)\n\n[│   ├── llama3.1/70b/config-llama3-1-70b-p5-djl-lmi.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-llama3-1-70b-p5-djl-lmi.yml)\n\n[│   ├── llama3.1/70b/config-llama3-1-70b-trn1.32xl-deploy-ec2-triton.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-llama3-1-70b-trn1.32xl-deploy-ec2-triton.yml)\n\n[│   └── llama3.1/70b/config-llama3-1-7b-inf2.48xl-triton-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-llama3-1-7b-inf2.48xl-triton-ec2.yml)\n\n**└── llama3.1/8b**\n\n[├── llama3.1/8b/client-config-ec2-llama3-1-8b.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/client-config-ec2-llama3-1-8b.yml)\n\n[├── llama3.1/8b/config-ec2-llama3-1-8b-g6e-2xlarge-byoe-ollama.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-g6e-2xlarge-byoe-ollama.yml)\n\n[├── llama3.1/8b/config-ec2-llama3-1-8b-inf2-48xl-deploy-ec2-tp24-bs12.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-inf2-48xl-deploy-ec2-tp24-bs12.yml)\n\n[├── llama3.1/8b/config-ec2-llama3-1-8b-inf2-48xl-deploy-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-inf2-48xl-deploy-ec2.yml)\n\n[├── llama3.1/8b/config-ec2-llama3-1-8b-inf2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-inf2.yml)\n\n[├── llama3.1/8b/config-ec2-llama3-1-8b-p4-tp-2-mc-max.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-p4-tp-2-mc-max.yml)\n\n[├── llama3.1/8b/config-ec2-llama3-1-8b-p4-tp-4-mc-max.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-p4-tp-4-mc-max.yml)\n\n[├── llama3.1/8b/config-ec2-llama3-1-8b-p4-tp-8-mc-max.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-p4-tp-8-mc-max.yml)\n\n[├── llama3.1/8b/config-ec2-llama3-1-8b-p5-tp-2-mc-max.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-p5-tp-2-mc-max.yml)\n\n[├── llama3.1/8b/config-ec2-llama3-1-8b-tp-8-mc-auto-p5.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-tp-8-mc-auto-p5.yml)\n\n[├── llama3.1/8b/config-ec2-llama3-1-8b-trn1-32xl-deploy-ec2-tp32-bs8.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-trn1-32xl-deploy-ec2-tp32-bs8.yml)\n\n[├── llama3.1/8b/config-llama3-1-8b-g5.12xlarge-djl-lmi-sm.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3-1-8b-g5.12xlarge-djl-lmi-sm.yml)\n\n[├── llama3.1/8b/config-llama3-1-8b-p5en-djl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3-1-8b-p5en-djl.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-g5-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5-ec2.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-g5.12xl-tp-2-mc-auto-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.12xl-tp-2-mc-auto-ec2.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-g5.12xl-tp-2-mc-max-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.12xl-tp-2-mc-max-ec2.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-g5.12xl-tp-4-mc-max-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.12xl-tp-4-mc-max-ec2.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-g5.24xl-tp-2-mc-auto-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.24xl-tp-2-mc-auto-ec2.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-g5.24xl-tp-2-mc-max-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.24xl-tp-2-mc-max-ec2.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-g5.24xl-tp-4-mc-max-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.24xl-tp-4-mc-max-ec2.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-g5.2xl-g5.4xl-sm.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.2xl-g5.4xl-sm.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-g5.2xl-tp-1-mc-max-ec2-conc-1-2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.2xl-tp-1-mc-max-ec2-conc-1-2.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-g5.2xl-tp-1-mc-max-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.2xl-tp-1-mc-max-ec2.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-g5.48xl-tp-2-mc-auto-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.48xl-tp-2-mc-auto-ec2.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-g5.48xl-tp-2-mc-max-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.48xl-tp-2-mc-max-ec2.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-g5.48xl-tp-4-mc-max-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.48xl-tp-4-mc-max-ec2.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-g5.48xl-tp-8-mc-max-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.48xl-tp-8-mc-max-ec2.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-g5.xl-tp-1-mc-max-ec2-conc-1-2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.xl-tp-1-mc-max-ec2-conc-1-2.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-g5.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-g6e.12xl-tp-2-mc-max-djl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g6e.12xl-tp-2-mc-max-djl.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-g6e.12xl-tp-4-mc-max-djl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g6e.12xl-tp-4-mc-max-djl.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-g6e.24xl-tp-2-mc-max-djl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g6e.24xl-tp-2-mc-max-djl.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-g6e.24xl-tp-4-mc-max-djl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g6e.24xl-tp-4-mc-max-djl.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-g6e.2xl-ollama.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g6e.2xl-ollama.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-g6e.2xl-tp-1-mc-max-djl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g6e.2xl-tp-1-mc-max-djl.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-g6e.48xl-tp-2-mc-max-djl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g6e.48xl-tp-2-mc-max-djl.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-g6e.48xl-tp-4-mc-max-djl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g6e.48xl-tp-4-mc-max-djl.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-g6e.48xl-tp-8-mc-max-djl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g6e.48xl-tp-8-mc-max-djl.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-g6e.4xl-tp-1-mc-max-djl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g6e.4xl-tp-1-mc-max-djl.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-inf2-48xl-deploy-tp-24-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-inf2-48xl-deploy-tp-24-ec2.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-inf2-48xl-deploy-tp-8-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-inf2-48xl-deploy-tp-8-ec2.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-trn1-32xl-deploy-tp-8-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-trn1-32xl-deploy-tp-8-ec2.yml)\n\n[├── llama3.1/8b/config-llama3.1-8b-trn32xl-triton-vllm.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-trn32xl-triton-vllm.yml)\n\n[└── llama3.1/8b/server-config-ec2-llama3-1-8b-inf2-48xl-deploy-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/server-config-ec2-llama3-1-8b-inf2-48xl-deploy-ec2.yml)\n\n[llama3.2](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.2)\n\n**├── llama3.2/11b**\n\n[│   └── llama3.2/11b/config-llama3.2-11b-g6e-2xl-tp-1-mc-max-djl-vllm-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.2/11b/config-llama3.2-11b-g6e-2xl-tp-1-mc-max-djl-vllm-ec2.yml)\n\n**├── llama3.2/1b**\n\n[│   ├── llama3.2/1b/config-llama3.2-1b-g5.2xl-summarization-500-50.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.2/1b/config-llama3.2-1b-g5.2xl-summarization-500-50.yml)\n\n[│   ├── llama3.2/1b/config-llama3.2-1b-g5.2xl-tp-1-mc-max-djl-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.2/1b/config-llama3.2-1b-g5.2xl-tp-1-mc-max-djl-ec2.yml)\n\n[│   ├── llama3.2/1b/config-llama3.2-1b-g5.4xl-tp-1-mc-max-djl-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.2/1b/config-llama3.2-1b-g5.4xl-tp-1-mc-max-djl-ec2.yml)\n\n[│   ├── llama3.2/1b/config-llama3.2-1b-g6e.2xl-tp-1-mc-max-djl.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.2/1b/config-llama3.2-1b-g6e.2xl-tp-1-mc-max-djl.yml)\n\n[│   ├── llama3.2/1b/config-llama3.2-1b-m5-16xlarge-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.2/1b/config-llama3.2-1b-m5-16xlarge-ec2.yml)\n\n[│   ├── llama3.2/1b/config-llama3.2-1b-m7a-16xlarge-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.2/1b/config-llama3.2-1b-m7a-16xlarge-ec2.yml)\n\n[│   ├── llama3.2/1b/config-llama3.2-1b-m7a-24xlarge-ec2-summarization.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.2/1b/config-llama3.2-1b-m7a-24xlarge-ec2-summarization.yml)\n\n[│   ├── llama3.2/1b/config-llama3.2-1b-m7a-24xlarge-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.2/1b/config-llama3.2-1b-m7a-24xlarge-ec2.yml)\n\n[│   └── llama3.2/1b/config-llama3.2-1b-m7i-12xlarge-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.2/1b/config-llama3.2-1b-m7i-12xlarge-ec2.yml)\n\n**└── llama3.2/3b**\n\n[└── llama3.2/3b/config-llama3.2-3b-g5.4xl-tp-1-mc-max-djl-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.2/3b/config-llama3.2-3b-g5.4xl-tp-1-mc-max-djl-ec2.yml)\n\n**mistral**\n\n[├── mistral/config-mistral-7b-eks-inf2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-7b-eks-inf2.yml)\n\n[├── mistral/config-mistral-7b-tgi-g5.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-7b-tgi-g5.yml)\n\n[├── mistral/config-mistral-7b-trn1-32xl-triton.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-7b-trn1-32xl-triton.yml)\n\n[├── mistral/config-mistral-instruct-AWQ-p4d.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-instruct-AWQ-p4d.yml)\n\n[├── mistral/config-mistral-instruct-AWQ-p5-byo-ep.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-instruct-AWQ-p5-byo-ep.yml)\n\n[├── mistral/config-mistral-instruct-AWQ-p5.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-instruct-AWQ-p5.yml)\n\n[├── mistral/config-mistral-instruct-p4d.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-instruct-p4d.yml)\n\n[├── mistral/config-mistral-instruct-v1-p5-trtllm.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-instruct-v1-p5-trtllm.yml)\n\n[├── mistral/config-mistral-instruct-v2-p4d-lmi-dist.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-instruct-v2-p4d-lmi-dist.yml)\n\n[├── mistral/config-mistral-instruct-v2-p4d-trtllm.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-instruct-v2-p4d-trtllm.yml)\n\n[├── mistral/config-mistral-instruct-v2-p5-lmi-dist.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-instruct-v2-p5-lmi-dist.yml)\n\n[├── mistral/config-mistral-instruct-v2-p5-trtllm.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-instruct-v2-p5-trtllm.yml)\n\n[├── mistral/config-mistral-trn1-32xl-deploy-ec2-tp32.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-trn1-32xl-deploy-ec2-tp32.yml)\n\n[└── mistral/config-mistral-v3-inf2-48xl-deploy-ec2-tp24.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-v3-inf2-48xl-deploy-ec2-tp24.yml)\n\n**mixtral**\n\n[└── mixtral/config-mixtral-8x7b-g6e.48xl-ec2.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mixtral/config-mixtral-8x7b-g6e.48xl-ec2.yml)\n\n[model\\_eval\\_all\\_info.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/model_eval_all_info.yml)\n\n**multimodal**\n\n**└── multimodal/bedrock**\n\n[├── multimodal/bedrock/config-claude-scienceqa.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/multimodal/bedrock/config-claude-scienceqa.yml)\n\n[├── multimodal/bedrock/config-llama-3-2-11b-vision-instruct-marqo-GS-10M.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/multimodal/bedrock/config-llama-3-2-11b-vision-instruct-marqo-GS-10M.yml)\n\n[├── multimodal/bedrock/config-llama-3-2-11b-vision-instruct-scienceqa.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/multimodal/bedrock/config-llama-3-2-11b-vision-instruct-scienceqa.yml)\n\n[└── multimodal/bedrock/config-llama-3-2-claude-models-scienceqa.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/multimodal/bedrock/config-llama-3-2-claude-models-scienceqa.yml)\n\n**phi**\n\n[└── phi/config-phi-3-g5.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/phi/config-phi-3-g5.yml)\n\n[pricing.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/pricing.yml)\n\n[pricing\\_fallback.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/pricing_fallback.yml)",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/manifest.html",
      "title": "Files - Foundation Model Benchmarking Tool (FMBench)",
      "favicon": {},
      "language": "en",
      "scrapeId": "92ae9862-dac1-415f-9acd-4cb26e15b767",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/manifest.html",
      "statusCode": 200,
      "theme-color": "#00000000",
      "color-scheme": "normal"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g6e.48xl-tp=8-mc=max-djl-ec2\"\n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g6e.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    - 14\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g6e.48xl-tp-8-mc-max-djl.yml",
      "scrapeId": "1a30ddd7-e725-4645-8ee7-9b23217dcdf0",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g6e.48xl-tp-8-mc-max-djl.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"llama3-1-8b-p5en-djl-lmi\"\n  model_name: \"llama3-1-8b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\nmetrics:\n  dataset_of_interest: en_3000-3840\n\npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n    return_full_text: False\n\nexperiments:\n  - name: llama-3-1-8b-instruct-p5en-48xlarge\n    model_id: meta-llama/Llama-3.1-8B-Instruct\n    hf_tokenizer_model_id: meta-llama/Llama-3.1-8B-Instruct\n    model_version: \"*\"\n    model_name: Llama-3-1-8B-Instruct\n    ep_name: Llama-3-1-8B-Instruct\n    download_from_hf_place_in_s3: yes\n    model_s3_path: s3://{write_bucket}/meta-llama/Llama-3.1-8B-Instruct\n    instance_type: \"ml.p5en.48xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.31.0-lmi13.0.0-cu124'\n    deploy: yes\n    bucket: {write_bucket}\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=4\n      option.model_id=s3://{write_bucket}/meta-llama/Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      #HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      # SM_NUM_GPUS: \"1\"\n      # SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3-1-8b-p5en-djl.yml",
      "scrapeId": "0a1bd0b8-79d6-4d87-988e-c8b991cd53ca",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3-1-8b-p5en-djl.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-m7a.24xl-ec2\"\n  model_name: \"llama3-8b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_vllm:\n    model: meta-llama/Meta-Llama-3-8B-Instruct\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-8b-instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3-8b-instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8000/v1/completions'\n    instance_type: \"m7a.24xlarge\"\n    image_uri: vllm-cpu-env\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # Privileged Mode makes the docker container run with root.\n      # This basically means that if you are root in a container you have the privileges of root on the host system\n      # Only need this if you need to set VLLM_CPU_OMP_THREADS_BIND env variable.\n      privileged_mode: yes\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting:\n      # This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_vllm\n      # if not set assume djl\n      container_type: vllm\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      MODEL_LOADING_TIMEOUT: 2400\n      VLLM_CPU_KVCACHE_SPACE: 192\n      # This instance is equipped with 96 CPUs, and we are allocating 93 of them to run this container.\n      # For additional details, refer to the following URL:\n      # https://docs.vllm.ai/en/latest/getting_started/cpu-installation.html#related-runtime-environment-variables\n      VLLM_CPU_OMP_THREADS_BIND: 0-92\n\nreport:\n  latency_budget: 35\n  cost_per_10k_txn_budget: 60\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-m7a-24xlarge.yml",
      "scrapeId": "0aafd567-eb51-4718-b12e-a75d09c1c82a",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-m7a-24xlarge.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g6e.12xl-tp=2-mc=max-djl-ec2\"\n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g6e.12xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    - 14\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g6e.12xl-tp-2-mc-max-djl.yml",
      "scrapeId": "18793f05-38d7-488c-9662-2cb2b16e76eb",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g6e.12xl-tp-2-mc-max-djl.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.48xl-tp=2-mc=max-djl-ec2\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g6e.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.48xl-tp-2-mc-max-djl-ec2.yml",
      "scrapeId": "1426ac8f-8dfe-4eb7-9cd6-9fadf92627d6",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.48xl-tp-2-mc-max-djl-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g5.48xl-ec2\"\n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.48xl-tp-8-mc-max-ec2.yml",
      "scrapeId": "00679cdc-01da-4c88-a259-50f081a2b285",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.48xl-tp-8-mc-max-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-8b-trn1-32xl-deploy-ec2-tp32-bs8\"\n  model_name: \"llama3-1-8b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1000-2000\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-1-8b-trn1-32xl-deploy-ec2-tp32-bs8\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"trn1.32xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and Inferentia settings for the instance\n      # '-u djl' tells the container to use the Deep Java Library (DJL) runtime\n      # '--device /dev/neuron0 ... /dev/neuron5' specifies the Inferentia devices to be used\n      gpu_or_neuron_setting: -u djl --device /dev/neuron0 --device /dev/neuron1 --device /dev/neuron2 --device /dev/neuron3 --device /dev/neuron4 --device /dev/neuron5 --device /dev/neuron6 --device /dev/neuron7 --device /dev/neuron8 --device /dev/neuron9 --device /dev/neuron10 --device /dev/neuron11 --device /dev/neuron12 --device /dev/neuron13 --device /dev/neuron14 --device /dev/neuron15\n      #This setting specifies the timeout (in seconds) for loading the model.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 32\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=meta-llama/Meta-Llama-3.1-8b-Instruct\n      option.tensor_parallel_degree=32\n      option.n_positions=4096\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=8\n      option.model_loading_timeout=2400\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O3\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-trn1-32xl-deploy-ec2-tp32-bs8.yml",
      "scrapeId": "060685e7-7521-4ca8-a2cb-39474f48d0e6",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-trn1-32xl-deploy-ec2-tp32-bs8.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"fmbench-bedrock-llama3.2-1b-3b\"\n  model_name: \"llama3-2 Models on Amazon Bedrock\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-2-1b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: us.meta.llama3-2-1b-instruct-v1:0\n    model_version:\n    model_name: us.meta.llama3-2-1b-instruct-v1:0\n    ep_name: us.meta.llama3-2-1b-instruct-v1:0\n    instance_type: us.meta.llama3-2-1b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the\n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n\n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-2-3b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: us.meta.llama3-2-3b-instruct-v1:0\n    model_version:\n    model_name: us.meta.llama3-2-3b-instruct-v1:0\n    ep_name: us.meta.llama3-2-3b-instruct-v1:0\n    instance_type: us.meta.llama3-2-3b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the\n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n\n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 1\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-llama-3-2-1b-3b.yml",
      "scrapeId": "0f6dfbf3-9df3-470b-8db5-bc497ba619b1",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-llama-3-2-1b-3b.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-p5.48xl-ec2\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 924\n    payload_file: payload_en_1-924.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"p5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: auto\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 10\n    - 30\n    - 40\n    - 50\n    - 60\n    - 65\n    - 70\n    - 75\n    - 100\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-p5-tp-8-mc-auto.yml",
      "scrapeId": "06ad5659-013e-408e-8bf3-95c6deff981b",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-p5-tp-8-mc-auto.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.4xl-tp=1-mc=max-triton-ec2\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    temperature: 0.1\n    do_sample: yes\n    top_p: 0.92\n    top_k: 120\n    bad_words: \"\"\n    stop_words: \"<|eot_id|>\"\n    pad_id: 2\n    end_id: 2\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: http://localhost:8080/v2/models/ensemble/generate\n    instance_type: \"g6e.4xlarge\"\n    image_uri: nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 1\n      shm_size: 12g\n      batch_size: 64\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.4xl-tp-1-mc-max-triton-ec2.yml",
      "scrapeId": "07ed7ee3-d8e6-477d-82ee-fca778a5950c",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.4xl-tp-1-mc-max-triton-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"bge-base-en-v1-5\"\n  model_name: \"bge-base-en-v1-5\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - just_text.jsonl\n    - banking77.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_bert.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - text\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 50\n    payload_file: payload_en_1-50.jsonl\n  - language: en\n    min_length_in_tokens: 50\n    max_length_in_tokens: 100\n    payload_file: payload_en_50-100.jsonl\n  - language: en\n    min_length_in_tokens: 100\n    max_length_in_tokens: 150\n    payload_file: payload_en_100-150.jsonl\n\n\nmetrics:\n  dataset_of_interest: en_1-50\n\npricing: pricing.yml\n\ninference_parameters:\n  embedding:\n    top_p: 0.92\n\nexperiments:\n  - name: bge-base-en-v1-5-g5.xl\n    model_id: huggingface-sentencesimilarity-bge-base-en-v1-5\n    model_version: \"*\"\n    model_name: bge-base-en-v1.5\n    ep_name: bge-base-en-v1-5\n    instance_type: \"ml.g5.xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-inference:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: embedding\n      container_type: huggingface\n    payload_files:\n    - payload_en_1-50.jsonl\n    - payload_en_50-100.jsonl\n    - payload_en_100-150.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n    accept_eula: false\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"1200\"\n      SM_NUM_GPUS: \"1\"\n\n  - name: bge-base-en-v1-5-g5.2xl\n    model_id: huggingface-sentencesimilarity-bge-base-en-v1-5\n    model_version: \"*\"\n    model_name: bge-base-en-v1.5\n    ep_name: bge-base-en-v1-5\n    instance_type: \"ml.g4dn.2xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-inference:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: embedding\n      container_type: huggingface\n    payload_files:\n    - payload_en_1-50.jsonl\n    - payload_en_50-100.jsonl\n    - payload_en_100-150.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n    accept_eula: false\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"1200\"\n      SM_NUM_GPUS: \"1\"\n      # MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      # SAGEMAKER_ENV: \"1\"\n      # HF_MODEL_ID: \"BAAI/bge-base-en-v1.5\"\n      # MAX_INPUT_LENGTH: \"4095\"\n      # MAX_TOTAL_TOKENS: \"4096\"\n    #   # SAGEMAKER_MODEL_SERVER_WORKERS: \"2\"\n\n  - name: bge-base-en-v1-5-g5.2xl\n    model_id: huggingface-sentencesimilarity-bge-base-en-v1-5\n    model_version: \"*\"\n    model_name: bge-base-en-v1.5\n    ep_name: bge-base-en-v1-5\n    instance_type: \"ml.c7i.xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-inference:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: embedding\n      container_type: huggingface\n    payload_files:\n    - payload_en_1-50.jsonl\n    - payload_en_50-100.jsonl\n    - payload_en_100-150.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n    accept_eula: false\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"1200\"\n\nreport:\n  latency_budget: 0.1\n  cost_per_10k_txn_budget: 5\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/embeddings/bge-base-en-v1-5-g5-g4dn-c7-embeddings.yml",
      "scrapeId": "08c5821b-6ee1-4f2d-9ac8-71dcec56287d",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/embeddings/bge-base-en-v1-5-g5-g4dn-c7-embeddings.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"fmbench-bedrock-claude-3-5-sonnet-v2\"\n  model_name: \"Claude Sonnet 3.5 V2\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket}\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n\n    configs_prefix: configs\n    config_files:\n    - pricing.yml ## mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # Format: hf:dataset-id/subset-name/split-name. Use   \"default\" if no subset name is provided.\n    - hf:THUDM/LongBench/2wikimqa_e/test\n    - hf:THUDM/LongBench/2wikimqa/test\n    - hf:THUDM/LongBench/hotpotqa_e/test\n    - hf:THUDM/LongBench/hotpotqa/test\n    - hf:THUDM/LongBench/narrativeqa/test\n    - hf:THUDM/LongBench/triviaqa_e/test\n    - hf:THUDM/LongBench/triviaqa/test\n    tokenizer_prefix: llama3_1_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_claude.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: no\n    3_run_inference.ipynb: yes\n    4_get_evaluations.ipynb: yes\n    5_model_metric_analysis.ipynb: yes\n    6_cleanup.ipynb: no\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\nmetrics:\n  dataset_of_interest: en_3000-4000\n\n\npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n\ninference_parameters:\n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  - name: anthropic.claude-3-5-sonnet-20241022-v2:0\n    model_name: anthropic.claude-3-5-sonnet-20241022-v2:0\n    ep_name: anthropic.claude-3-5-sonnet-20241022-v2:0\n    instance_type: anthropic.claude-3-5-sonnet-20241022-v2:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    env:\n\nreport:\n  latency_budget: 5\n  cost_per_10k_txn_budget: 350\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-claude-3-5-sonnet-v2.yml",
      "scrapeId": "0d12142d-fcc6-492e-96f1-14dc0a8f6317",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-claude-3-5-sonnet-v2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: {results_dir}\n  model_name: {model_id}\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  # Format: hf:dataset-id/subset-name/split-name. Use   \"default\" if no subset name is provided.\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_deepseek_longbench.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_sglang:\n    model: default\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_tokens: 2048\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: {model_id}\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    hf_tokenizer_model_id: deepseek-ai/DeepSeek-R1\n    model_id: {model_id} # model id, version and image uri not needed for byo endpoint\n    model_version:\n    deploy: yes\n    model_name: {model_id}\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:30000/v1/chat/completions'\n    instance_type: {instance_type}\n    image_uri: lmsysorg/sglang:latest\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_sglang\n      container_type: sglang\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/deepseek/config-deepseek-r1-sglang.yml",
      "scrapeId": "0a0264d7-838f-44f9-ad9b-31b5cc1dc78b",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/deepseek/config-deepseek-r1-sglang.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"llama3-8b-g5-12xl\"\n  model_name: \"llama3-8b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\nmetrics:\n  dataset_of_interest: en_3000-3840\n\npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\nexperiments:\n  - name: llama-3-8b-instruct-g5-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct-g5.12xl\n    ep_name: llama-3-8b-instruct-g5-12xl\n    instance_type: \"ml.g5.12xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 5\n    - 8\n    - 10\n    - 16\n    - 20\n    - 30\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      # SM_NUM_GPUS: \"8\"\n      # SAGEMAKER_MODEL_SERVER_WORKERS: \"2\"\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-g5-12xl.yml",
      "scrapeId": "0c3a2ea5-a036-42a6-867c-8f773bae78b1",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-g5-12xl.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"latest-FMs-fmbench-bedrock\"\n  model_name: \"FMs available in Amazon Bedrock\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket}\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: llama2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama2.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the\n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 4000\n    max_length_in_tokens: 5000\n    payload_file: payload_en_4000-5000.jsonl\n  - language: en\n    min_length_in_tokens: 5000\n    max_length_in_tokens: 6000\n    payload_file: payload_en_5000-6000.jsonl\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\nmetrics:\n  dataset_of_interest: en_2000-3000\n\npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n\ninference_parameters:\n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  - name: meta.llama3-1-70b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: meta.llama3-1-70b-instruct-v1:0\n    model_version:\n    model_name: meta.llama3-1-70b-instruct-v1:0\n    ep_name: meta.llama3-1-70b-instruct-v1:0\n    instance_type: meta.llama3-1-70b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the\n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    - payload_en_4000-5000.jsonl\n    - payload_en_5000-6000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n\n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  - name: meta.llama3-1-8b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: meta.llama3-1-8b-instruct-v1:0\n    model_version:\n    model_name: meta.llama3-1-8b-instruct-v1:0\n    ep_name: meta.llama3-1-8b-instruct-v1:0\n    instance_type: meta.llama3-1-8b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the\n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    - payload_en_4000-5000.jsonl\n    - payload_en_5000-6000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n\n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  - name: amazon.titan-text-express-v1\n    model_id: amazon.titan-text-express-v1\n    model_version: \"*\"\n    model_name: amazon.titan-text-express-v1\n    ep_name: amazon.titan-text-express-v1\n    instance_type: amazon.titan-text-express-v1\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    - payload_en_4000-5000.jsonl\n    - payload_en_5000-6000.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env:\n  - name: mistral.mistral-7b-instruct-v0:2\n    model_id: mistral.mistral-7b-instruct-v0:2\n    model_version: \"*\"\n    model_name: mistral.mistral-7b-instruct-v0:2\n    ep_name: mistral.mistral-7b-instruct-v0:2\n    instance_type: mistral.mistral-7b-instruct-v0:2\n    image_uri:\n    deploy: no\n    instance_count: 1\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    - payload_en_4000-5000.jsonl\n    - payload_en_5000-6000.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env:\n  - name: mistral.mixtral-8x7b-instruct-v0:1\n    model_id: mistral.mixtral-8x7b-instruct-v0:1\n    model_version: \"*\"\n    model_name: mistral.mixtral-8x7b-instruct-v0:1\n    ep_name: mistral.mixtral-8x7b-instruct-v0:1\n    instance_type: mistral.mixtral-8x7b-instruct-v0:1\n    image_uri:\n    deploy: no\n    instance_count: 1\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    - payload_en_4000-5000.jsonl\n    - payload_en_5000-6000.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env:\n  - name: anthropic.claude-3-sonnet-20240229-v1:0\n    model_id: anthropic.claude-3-sonnet-20240229-v1:0\n    model_version: \"*\"\n    model_name: anthropic.claude-3-sonnet-20240229-v1:0\n    ep_name: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_type: anthropic.claude-3-sonnet-20240229-v1:0\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    - payload_en_4000-5000.jsonl\n    - payload_en_5000-6000.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env:\n  - name: anthropic.claude-3-haiku-20240307-v1:0\n    model_id: anthropic.claude-3-haiku-20240307-v1:0\n    model_version: \"*\"\n    model_name: anthropic.claude-3-haiku-20240307-v1:0\n    ep_name: anthropic.claude-3-haiku-20240307-v1:0\n    instance_type: anthropic.claude-3-haiku-20240307-v1:0\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    - payload_en_4000-5000.jsonl\n    - payload_en_5000-6000.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env:\n  - name: cohere.command-text-v14\n    model_id: cohere.command-text-v14\n    model_version: \"*\"\n    model_name: cohere.command-text-v14\n    ep_name: cohere.command-text-v14\n    instance_type: cohere.command-text-v14\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    - payload_en_4000-5000.jsonl\n    - payload_en_5000-6000.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env:\n  - name: cohere.command-light-text-v14\n    model_id: cohere.command-light-text-v14\n    model_version: \"*\"\n    model_name: cohere.command-light-text-v14\n    ep_name: cohere.command-light-text-v14\n    instance_type: cohere.command-light-text-v14\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    - payload_en_4000-5000.jsonl\n    - payload_en_5000-6000.jsonl\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    # - 10\n    env:\n\nreport:\n  latency_budget: 2\n  cosine_similarity_budget: 0.3\n  accuracy_budget: 1\n  accuracy_error_rate_budget: 0\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock.yml",
      "scrapeId": "111d15c6-b36a-4131-8684-d47c59959abd",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-8b-trn1.32xl-ec2\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n    return_full_text: no\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"trn1.32xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and Inferentia settings for the instance\n      # '-u djl' tells the container to use the Deep Java Library (DJL) runtime\n      # '--device /dev/neuron0 ... /dev/neuron5' specifies the Inferentia devices to be used\n      gpu_or_neuron_setting: -u djl --device /dev/neuron0 --device /dev/neuron1 --device /dev/neuron2 --device /dev/neuron3 --device /dev/neuron4 --device /dev/neuron5 --device /dev/neuron6 --device /dev/neuron7 --device /dev/neuron8 --device /dev/neuron9 --device /dev/neuron10 --device /dev/neuron11 --device /dev/neuron12 --device /dev/neuron13 --device /dev/neuron14 --device /dev/neuron15\n      #This setting specifies the timeout (in seconds) for loading the model.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured,\n      # max: FMBench figures out the max number of model containers to be run\n      #      based on TP degree configured and number of neuron cores/GPUs available.\n      #      For example, if TP=2, GPUs=8 then FMBench will start 4 containers and 1 load balancer,\n      # auto: only supported if the underlying inference container would automatically\n      #       start multiple copies of the model internally based on TP degree and neuron cores/GPUs\n      #       available. In this case only a single container is created, no load balancer is created.\n      #       The DJL serving containers supports auto.\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.tensor_parallel_degree=8\n      option.n_positions=4096\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=4\n      option.model_loading_timeout=2400\n      option.group_query_attention=replicated-heads\n      option.attention_layout=BSH\n      #option.fuse_qkv=True\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n    - 8\n    - 10\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O3\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-trn1-32xl-deploy-tp-8-ec2.yml",
      "scrapeId": "1463248a-0af1-44bb-9995-0b0c7a39e965",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-trn1-32xl-deploy-tp-8-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g5.48xl-tp=8-mc=max-triton-ec2\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    temperature: 0.1\n    do_sample: yes\n    top_p: 0.92\n    top_k: 120\n    bad_words: \"\"\n    stop_words: \"<|eot_id|>\"\n    pad_id: 2\n    end_id: 2\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: http://localhost:8080/v2/models/ensemble/generate\n    instance_type: \"g5.48xlarge\"\n    image_uri: nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n      batch_size: 256\n\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g5.48xl-tp-8-mc-max-triton-ec2.yml",
      "scrapeId": "113c9372-f08f-4adf-b8c3-1fe08a13223b",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g5.48xl-tp-8-mc-max-triton-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.xl-tp=1-mc=max-triton-ec2\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    temperature: 0.1\n    do_sample: yes\n    top_p: 0.92\n    top_k: 120\n    bad_words: \"\"\n    stop_words: \"<|eot_id|>\"\n    pad_id: 2\n    end_id: 2\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: http://localhost:8080/v2/models/ensemble/generate\n    instance_type: \"g6e.xlarge\"\n    image_uri: nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 1\n      shm_size: 12g\n      batch_size: 64\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.xl-tp-1-mc-max-triton-ec2.yml",
      "scrapeId": "1c740ce7-05d8-40d8-9b14-497faa507376",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.xl-tp-1-mc-max-triton-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"fmbench-bedrock-llama3-2-11b-vision-instruct-realworldqa\"\n  model_name: \"llama3-2 11b on Amazon Bedrock\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the https://huggingface.co/datasets/derek-thomas/ScienceQA\n  # This dataset contains images containing science questions and associated ground truth responses to the questions.\n  # when this notebook looks for this dataset - if it is prefixed with hf, and if the token is already there\n  # if the token is there and the hf: is there, then download the dataset from hf first and then process it -\n  source_data_files:\n  # If a split is specified in the dataset identifier and exists in the loaded dataset,\n  # it uses that split. If no split is specified (e.g., hf:derek-thomas/ScienceQA), it will default to\n  # the first available split. In the given example below, users can specify the 'train' and 'validation' splits\n  # of the 'derek-thomas/ScienceQA' dataset. If none are provided, then the first available split will be used.\n\n  # Follow this format below: hf:dataset-id/subset-name/split-name.\n  # If there is no specified subset name, use \"default\"\n  - hf:derek-thomas/ScienceQA/default/train\n  - hf:derek-thomas/ScienceQA/default/validation\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3_images_ScienceQA.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: no\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  # This represents the question that is asked as a part of the\n  # image messages API format. If this is not provided, a description\n  # of the image will be provided instead\n  - question\n  # If you want to benchmark a multimodal model on an image dataset,\n  # then it an image_col parameter is required. This parameter refers to the\n  # name of the column in the dataset that contains the images to be used\n  # during the benchmarking process. If this column is not provided, the\n  # standard text generation benchmark process is be used in the FMBench run.\n  image_col: image\n  # This is the number of rows of the dataset that you want to load\n  # if this parameter is not given, it is defaulted to 100 rows\n  ds_N: 150\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1-500\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-2-11b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: us.meta.llama3-2-11b-instruct-v1:0\n    model_version:\n    model_name: us.meta.llama3-2-11b-instruct-v1:0\n    ep_name: us.meta.llama3-2-11b-instruct-v1:0\n    instance_type: us.meta.llama3-2-11b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the\n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n\n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    - 14\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 1\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/multimodal/bedrock/config-llama-3-2-11b-vision-instruct-scienceqa.yml",
      "scrapeId": "21613b26-86ad-4e15-9e76-9173fc7b40d4",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/multimodal/bedrock/config-llama-3-2-11b-vision-instruct-scienceqa.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g5.48xl-ec2\"\n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: auto\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.48xl-tp-2-mc-auto-ec2.yml",
      "scrapeId": "1ad22b40-5343-4ebe-a52d-47d9d75cdb7f",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.48xl-tp-2-mc-auto-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"config-llama3-8b-inf2-24xl-tp=8-bs=4-byoe\"\n  model_name: \"llama3-8b\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data #/Open-Orca\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  #- OpenOrca.jsonl\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n  # prompt_template_Open-Orca-OpenOrca-llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the\n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  #- system_prompt\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    #stop: '<|eot_id|>'\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: llama-3-8b-instruct-inf2-djl-inference:0.27.0-neuronx-sdk2.18.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version:\n    model_name: llama3-8b-inf2.24xl\n    ep_name: <your-endpoint-name>\n    instance_type: \"ml.inf2.24xlarge\"\n    image_uri:\n    deploy: no\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # currently the llama3-8b jumpstart model is compiled with a batch size of 1\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 15\n    - 20\n    - 30\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      model_id_or_path='/tmp/.djl.ai/download/ae03dd100c208acd82b5dbed563c971de864c408'\n      rolling_batch=<RollingBatchEnum.auto: 'auto'>\n      tensor_parallel_degree=8\n      trust_remote_code=False\n      enable_streaming=<StreamingEnum.false: 'false'>\n      batch_size=4\n      max_rolling_batch_size=4\n      dtype=<Dtype.f16: 'fp16'>\n      revision=None\n      output_formatter=None\n      waiting_steps=None\n      is_mpi=False\n      draft_model_id=None\n      spec_length=0\n      neuron_optimize_level=None\n      enable_mixed_precision_accumulation=False\n      enable_saturate_infinity=False\n      n_positions=4096\n      unroll=None\n      load_in_8bit=False\n      low_cpu_mem_usage=False\n      load_split_model=True\n      context_length_estimate=None\n      amp='f16'\n      quantize=None\n      compiled_graph_path=None\n      task=None\n      save_mp_checkpoint_path=None\n      group_query_attention=None\n      model_loader=<TnXModelLoaders.tnx: 'tnx'>\n      rolling_batch_strategy=<TnXGenerationStrategy.continuous_batching: 'continuous_batching'>\n      fuse_qkv=False\n      on_device_embedding=False\n      attention_layout=None\n      collectives_layout=None\n      cache_layout=None\n      partition_schema=None\n      all_reduce_dtype=None\n      cast_logits_dtype=None\n\n# parameters related to how the final report is generated\nreport:\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 2\n  cost_per_10k_txn_budget: 25\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-inf2-24xl-tp%3D8-bs%3D4-byoe.yml",
      "scrapeId": "220531f3-d214-44fd-a041-27738764d0db",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-inf2-24xl-tp%3D8-bs%3D4-byoe.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"fmbench-bedrock-nova-models\"\n  model_name: \"Nova models available in Amazon Bedrock\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket}\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n\n    configs_prefix: configs\n    config_files:\n    - pricing.yml ## mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_nova.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: no\n    3_run_inference.ipynb: yes\n    4_get_evaluations.ipynb: yes\n    5_model_metric_analysis.ipynb: yes\n    6_cleanup.ipynb: no\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-3840.jsonl\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n\npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n\ninference_parameters:\n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n    # Set use_boto3 to \"no\" if the model is supported by litellm and\n    # the litellm API will be used to invoke the model. If the model is on\n    # bedrock and is not supported by the latest version of litellm, then set\n    # the parameter below to \"yes\" and the bedrock converseAPI will be used.\n    use_boto3: yes\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  # This is the NOVA micro model\n  - name: us.amazon.nova-micro-v1:0\n    model_name: us.amazon.nova-micro-v1:0\n    ep_name: us.amazon.nova-micro-v1:0\n    instance_type: us.amazon.nova-micro-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    env:\n  - name: us.amazon.nova-lite-v1:0\n    model_name: us.amazon.nova-lite-v1:0\n    ep_name: us.amazon.nova-lite-v1:0\n    instance_type: us.amazon.nova-lite-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    env:\n  - name: us.amazon.nova-pro-v1:0\n    model_name: us.amazon.nova-pro-v1:0\n    ep_name: us.amazon.nova-pro-v1:0\n    instance_type: us.amazon.nova-pro-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-nova-all-models.yml",
      "scrapeId": "1dfa9672-f71d-4ca8-8006-75f2830086d1",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-nova-all-models.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"llama3-8b-g6-24xl-djl-lmi-dist\"\n  model_name: \"llama3-8b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\nmetrics:\n  dataset_of_interest: en_3000-3840\n\npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\nexperiments:\n\n  - name: llama-3-8b-instruct-g6-24xl-djl-deepspeed0.12.6-cu121\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct\n    ep_name: llama-3-8b-instruct-g6-24xl\n    instance_type: \"ml.g6.24xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.max_rolling_batch_size=256\n      option.rolling_batch=lmi-dist\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 15\n    - 20\n    - 30\n\n    accept_eula: true\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      job_queue_size: 1000\n      max_dynamic_batch_size: 1\n      max_batch_delay: 100\n      max_idle_time: 60\n      load_on_devices: *\n      engine: MPI\n      mpi_mode: true\n      option.entryPoint: null\n      option.tensor_parallel_degree: 4\n      option.max_rolling_batch_size: 256\n      option.mpi_mode: true\n      option.model_id: /opt/ml/model\n      option.rolling_batch: lmi-dist\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      #HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      # SM_NUM_GPUS: \"1\"\n      # SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-g6-24xl.yml",
      "scrapeId": "1e5895db-b80f-49ce-a138-efc2601e0bf9",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-g6-24xl.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"phi-3-v1\"\n  model_name: \"phi-3\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - rajpurkar/squad.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: phi_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_gemma.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the\n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 100\n    payload_file: payload_en_1-100.jsonl\n  - language: en\n    min_length_in_tokens: 100\n    max_length_in_tokens: 200\n    payload_file: payload_en_100-200.jsonl\n  - language: en\n    min_length_in_tokens: 150\n    max_length_in_tokens: 200\n    payload_file: payload_en_150-200.jsonl\n  - language: en\n    min_length_in_tokens: 200\n    max_length_in_tokens: 300\n    payload_file: payload_en_200-300.jsonl\n  - language: en\n    min_length_in_tokens: 300\n    max_length_in_tokens: 400\n    payload_file: payload_en_300-400.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_150-200\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 10\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: huggingface-llm-phi-3-mini-4k-instruct-huggingface-pytorch-tgi-inference:2.1.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: huggingface-llm-phi-3-mini-4k-instruct\n    model_version: \"1.*\"\n    model_name: phi-3\n    ep_name: phi-3\n    instance_type: \"ml.g5.2xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.3.0-tgi2.0.3-gpu-py310-cu121-ubuntu22.04'\n    deploy: yes\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script: jumpstart.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-100.jsonl\n    - payload_en_150-200.jsonl\n    - payload_en_200-300.jsonl\n    - payload_en_300-400.jsonl\n    #- payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    - 15\n    - 20\n    - 30\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"8191\"\n      MAX_TOTAL_TOKENS: \"8192\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\n# parameters related to how the final report is generated\nreport:\n  # markdown report title\n  title: \"Performance benchmarking results for Phi-3 on `g5.2xlarge` using the HuggingFace TGI container\"\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 0.3\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/phi/config-phi-3-g5.yml",
      "scrapeId": "22b4b527-8ace-4e6d-aefd-7cfe459ffe0c",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/phi/config-phi-3-g5.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# To run this configuration file, follow the steps here to deploy\n# the model first: https://github.com/madhurprash/EC2_tinyllama\ngeneral:\n  name: \"byo-REST-ep-tinyllama-ollama\"\n  model_name: \"tinyllama-ollama\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  # Format: hf:dataset-id/subset-name/split-name. Use   \"default\" if no subset name is provided.\n  - hf:THUDM/LongBench/2wikimqa_e/test\n  - hf:THUDM/LongBench/2wikimqa/test\n  - hf:THUDM/LongBench/hotpotqa_e/test\n  - hf:THUDM/LongBench/hotpotqa/test\n  - hf:THUDM/LongBench/narrativeqa/test\n  - hf:THUDM/LongBench/triviaqa_e/test\n  - hf:THUDM/LongBench/triviaqa/test\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1-500\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: custom_pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  custom_rest:\n    temperature: 0.3\n    top_p: 0.3\n    stream: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"custom-model-byoe\"\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: # model id, version and image uri not needed for byo endpoint\n    hf_tokenizer_model_id:\n    model_version:\n    model_name: \"custom-model\"\n    # the ep_name will contain the endpoint url that is used to invoke your model and get the response\n    # in this case, we use ray serve with `NousResearch/Llama-2-13b-chat-hf` model deployed on an EKS cluster.\n    # the endpoint url format used in this example is \"http://<NLB_DNS_NAME>/serve/infer?sentence=<PROMPT_PAYLOAD>\"\n    ep_name: 'http://localhost:8080/v2/completions' # public DNS/URL to send your request\n    instance_type: \"g6e.xlarge\"\n    image_uri:\n    deploy: no #setting to no since the endpoint has already been deployed on an EKS cluster\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example of a custom rest predictor\n    # that does a POST request on the endpoint URL (given on line 164) with custom headers,\n    # parameters and authentication information\n    inference_script: custom_rest_predictor.py\n    # This is the inference spec with custom information. You can add/remove variables\n    # and access those in the predictor file to use them as accepted by your container.\n    inference_spec:\n      # This parameter is specified above with custom configurations\n      parameter_set: custom_rest\n      # This model id is appended to the prompt as the payload is sent to the model id\n      model_id: \"tinyllama\"\n      # All key-value pairs added in this headers section are transparently\n      # passed via the POST request to the model server. You can add any custom\n      # header parameters needed for your specific implementation.\n      headers:\n        custom_authentication_token: \"your-secret-token-here\"  # Replace with actual token\n        content-type: \"application/json\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 5\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/byoe/config-byo-custom-rest-predictor-tinyllama.yml",
      "scrapeId": "28918887-493d-4e71-a862-24c2c0bb12e6",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/byoe/config-byo-custom-rest-predictor-tinyllama.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: {results_dir}\n  model_name: {model_id}\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_deepseek_longbench.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_ollama:\n    model: {model_id}\n    stream: false\n    options:\n      temperature: 0.1\n      top_p: 0.92\n      top_k: 120\n      num_predict: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: {model_id}\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    hf_tokenizer_model_id: deepseek-ai/DeepSeek-R1\n    model_id: {model_id} # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: {model_id}\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:11434/api/generate'\n    instance_type: {instance_type}\n    image_uri: ec2\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_ollama\n      container_type: ollama\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/deepseek/config-deepseek-r1-ollama.yml",
      "scrapeId": "240b8ccc-98c5-4775-8b3c-fe29a08e3ecf",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/deepseek/config-deepseek-r1-ollama.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g5.24xl-ec2\"\n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g5.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: auto\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.24xl-tp-2-mc-auto-ec2.yml",
      "scrapeId": "2472641a-8420-4784-9fa4-f0cb9cc88277",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.24xl-tp-2-mc-auto-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"fmbench-bedrock-llama3-2-all-models\"\n  model_name: \"llama3-2 Models on Amazon Bedrock\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-2-1b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: us.meta.llama3-2-1b-instruct-v1:0\n    model_version:\n    model_name: us.meta.llama3-2-1b-instruct-v1:0\n    ep_name: us.meta.llama3-2-1b-instruct-v1:0\n    instance_type: us.meta.llama3-2-1b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the\n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n\n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-2-3b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: us.meta.llama3-2-3b-instruct-v1:0\n    model_version:\n    model_name: us.meta.llama3-2-3b-instruct-v1:0\n    ep_name: us.meta.llama3-2-3b-instruct-v1:0\n    instance_type: us.meta.llama3-2-3b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the\n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n\n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-2-90b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: us.meta.llama3-2-90b-instruct-v1:0\n    model_version:\n    model_name: us.meta.llama3-2-90b-instruct-v1:0\n    ep_name: us.meta.llama3-2-90b-instruct-v1:0\n    instance_type: us.meta.llama3-2-90b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the\n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n\n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-2-11b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: us.meta.llama3-2-11b-instruct-v1:0\n    model_version:\n    model_name: us.meta.llama3-2-11b-instruct-v1:0\n    ep_name: us.meta.llama3-2-11b-instruct-v1:0\n    instance_type: us.meta.llama3-2-11b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the\n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n\n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 1\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-llama-3-2-all-models.yml",
      "scrapeId": "2a074f03-c574-42d0-9feb-1a5b30ff105c",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-llama-3-2-all-models.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-p5.2xl-ec2\"\n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: no\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 924\n    payload_file: payload_en_1-924.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"p5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    - 10\n    # - 12\n    # - 15\n    # - 20\n    # - 25\n    - 30\n    # - 40\n    - 50\n    - 60\n    - 65\n    - 70\n    - 75\n    #- 100\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-p5-tp-2-mc-max.yml",
      "scrapeId": "382afe11-5aea-4911-9b33-4c39e092c404",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-p5-tp-2-mc-max.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"Mistral-7B-Instruct-v2-p4d-lmi-dist\"\n  model_name: \"Mistral-7B\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: mistral_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_mistral.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n  - language: en\n    min_length_in_tokens: 1600\n    max_length_in_tokens: 5000\n    payload_file: payload_en_1600-5000.jsonl\n\nmetrics:\n  dataset_of_interest: en_1000-2000\n\npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Model configurations\nexperiments:\n  - name: Mistral-7B-Instruct-v0.2-p4d-lmi-dist\n    model_id: mistralai/Mistral-7B-Instruct-v0.2\n    model_version: \"*\"\n    model_name: Mistral7B\n    ep_name: Mistral7B\n    download_from_hf_place_in_s3: no\n    model_s3_path: s3://{write_bucket}/mistralai/Mistral-7B-Instruct-v0.2\n    instance_type: \"ml.p4d.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n        engine=MPI\n        option.tensor_parallel_degree=2\n        option.model_id=mistralai/Mistral-7B-Instruct-v0.2\n        option.max_rolling_batch_size=64\n        option.rolling_batch=lmi-dist\n    payload_files:\n    - payload_en_1000-2000.jsonl\n    concurrency_levels:\n    - 1\n    - 15\n    - 30\n    accept_eula: true\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n  latency_vs_token_len_chart:\n    y_ticks:\n    title: \"Effect of token length on inference latency for \\\"mistralai/Mistral-7B-Instruct-v0.2\\\"\"\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-instruct-v2-p4d-lmi-dist.yml",
      "scrapeId": "0ffacf92-444d-4929-9a21-62f80de81419",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-instruct-v2-p4d-lmi-dist.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.2xl-tp=1-mc=max-triton-ec2\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    temperature: 0.1\n    do_sample: yes\n    top_p: 0.92\n    top_k: 120\n    bad_words: \"\"\n    stop_words: \"<|eot_id|>\"\n    pad_id: 2\n    end_id: 2\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: http://localhost:8080/v2/models/ensemble/generate\n    instance_type: \"g6e.2xlarge\"\n    image_uri: nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 1\n      shm_size: 12g\n      batch_size: 64\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.2xl-tp-1-mc-max-triton-ec2.yml",
      "scrapeId": "3881a796-2f9a-4178-847f-1b80b29d244c",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.2xl-tp-1-mc-max-triton-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-70b-inf2.48xl-ec2\"\n  model_name: \"llama3-1-70b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-70B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    sagemaker_execution_role: {role_arn}\n    # S3 bucket to which metrics, plots and reports would be written to\n    bucket: {write_bucket} ## add the name of your desired bucket\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    prefix: \"lmi\"\n    model_id: meta-llama/Llama-3.1-70B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-70B-Instruct\"\n    model_id_wo_repo: \"Meta-Llama-3.1-70B-Instruct\"\n    model_id_wo_repo_split: \"Meta-Llama-3.1-70B-Instruct-split\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name:\n    instance_type: \"ml.inf2.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: neuron_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: sagemaker_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      batch_size: \"1\"\n      num_neuron_cores: \"24\"\n      neuron_version: \"2.19.1\"\n      model_loading_timeout: \"2400\"\n      instance_count: \"1\"\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=s3://{write_bucket}/lmi/Meta-Llama-3.1-70B-Instruct/Meta-Llama-3.1-70B-Instruct-split/\n      option.load_split_model=True\n      option.tensor_parallel_degree=24\n      option.n_positions=8192\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=8\n      option.dtype=fp16\n      option.model_loading_timeout=2400\n      option.neuron_optimize_level=2\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-ec2-llama3-1-70b-inf2-deploy-sm.yml",
      "scrapeId": "325aefd4-17c9-4e16-b989-7b8b460349a1",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-ec2-llama3-1-70b-inf2-deploy-sm.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"mistral-7b-tgi-g5-v1\"\n  model_name: \"mistral7b\"\n\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml ## mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: mistral_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_mistral.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n\nmetrics:\n  dataset_of_interest: en_2000-3000\n\npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n\ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Model configurations\nexperiments:\n  - name: mistral-7b--instruct-g5-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\n    model_id: huggingface-llm-mistral-7b-instruct\n    model_version: \"*\"\n    model_name: mistral-7b-instruct\n    ep_name: mistral7b-g5-2xlarge\n    instance_type: \"ml.g5.2xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi1.4.0-gpu-py310-cu121-ubuntu20.04\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    #- payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    # - 6\n    # - 8\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"8191\"\n      MAX_TOTAL_TOKENS: \"8192\"\n      MAX_BATCH_PREFILL_TOKENS: '8191'\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\nreport:\n  latency_budget: 5\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-7b-tgi-g5.yml",
      "scrapeId": "459bc70c-20ad-4410-a3c2-eee3c2d61daa",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-7b-tgi-g5.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.12xl-tp=2-mc=max-triton-ec2\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    temperature: 0.1\n    do_sample: yes\n    top_p: 0.92\n    top_k: 120\n    bad_words: \"\"\n    stop_words: \"<|eot_id|>\"\n    pad_id: 2\n    end_id: 2\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: http://localhost:8080/v2/models/ensemble/generate\n    instance_type: \"g6e.12xlarge\"\n    image_uri: nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      batch_size: 64\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.12xl-tp-2-mc-max-triton-ec2.yml",
      "scrapeId": "2ad2dbff-6840-4d67-928c-8824bb0223df",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.12xl-tp-2-mc-max-triton-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-70b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3.1-70b-g6.48xl-tp=8-mc=max-djl-ec2\"\n  model_name: \"Meta-Llama-3.1-70b-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-70b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-70B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-70b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g6.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged witsh multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-70b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-70b-Instruct\n      option.rolling_batch=lmi-dist\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 200\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-llama3-1-70b-g6.48xl-tp-8-mc-max-djl.yml",
      "scrapeId": "38e38ad3-9f05-493d-8f7a-703a1e3df2f0",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-llama3-1-70b-g6.48xl-tp-8-mc-max-djl.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"byo-sagemaker-ep\"\n  model_name: \"your-model\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  # Use S3 only, local file system only, or both (values are s3, local or both)\n  # If set to local or both, set the local_file_system_path parameter\n  s3_and_or_local_file_system: local\n  local_file_system_path: {write_tmpdir}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    # Use S3 only or local file system only (values are s3 or local)\n    # If set to local, set the local_file_system_path parameter\n    s3_or_local_file_system: local\n    local_file_system_path: {read_tmpdir}\n    # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama2.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: no\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 924\n    payload_file: payload_en_1-924.jsonl\n\nmetrics:\n  dataset_of_interest: en_1-924\n\npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n    return_full_text: False\n    #truncate: at-prompt-token-length\n\n# Model configurations for llama-2 7b for deploying on g5 x and 2x large instances\nexperiments:\n  - name: bring-your-own-sm-endpoint\n    model_id: # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: <your-model-name>\n    ep_name: \"<your-endpoint-name>\"\n    # If you are bringing your own endpoint and want to specify a custom variant name that is used to generate endpoint utilization metrics,\n    # mention that below. If not, the production variant name defaults to 'AllTraffic'\n    production_variant_name: AllTraffic\n    instance_type:  \"<your-instance-type>\"\n    image_uri:\n    deploy: no # setting to no since the endpoint has already been deployed\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    # - payload_en_1-500.jsonl\n    # - payload_en_500-1000.jsonl\n    # - payload_en_1000-2000.jsonl\n    # - payload_en_2000-3000.jsonl\n    - payload_en_1-924.jsonl\n\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\nreport:\n  latency_budget: 5\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/byoe/config-model-byo-sagemaker-endpoint.yml",
      "scrapeId": "382f8010-8e39-425d-b168-97aa78cc2fca",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/byoe/config-model-byo-sagemaker-endpoint.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"fmbench-bedrock-llama3-2-11b-vision-instruct-marqo-GS-10M\"\n  model_name: \"llama3-2 11b on Amazon Bedrock\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the marqo-GS-10M\n  # This dataset contains images without any questions. In this case, the same prompt\n  # template \"prompt_template_llama3_images_marqo-GS-10M\" will be used across all of the images in\n  # this hugging face dataset. It will ask to describe and mention the key components of each image.\n  source_data_files:\n  # If a split is specified in the dataset identifier and exists in the loaded dataset,\n  # it uses that split. If no split is specified (e.g., Marqo/marqo-GS-10M), it will default to\n  # the first available split. In the given example below, users can specify the 'in-domain' and 'novel_document' splits\n  # of the 'Marqo/marqo-GS-10M' dataset. If none are provided, then the first available split will be used.\n\n  # Follow this format below: hf:dataset-id/subset-name/split-name.\n  # If there is no specified subset name, use \"default\"\n  - hf:Marqo/marqo-GS-10M/default/in-domain\n  - hf:Marqo/marqo-GS-10M/default/novel_document\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3_images_marqo-GS-10M.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: no\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  # This represents the question that is asked as a part of the\n  # image messages API format. If this is not provided, a description\n  # of the image will be provided instead\n  image_col: image\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1-500\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-2-11b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: us.meta.llama3-2-11b-instruct-v1:0\n    model_version:\n    model_name: us.meta.llama3-2-11b-instruct-v1:0\n    ep_name: us.meta.llama3-2-11b-instruct-v1:0\n    instance_type: us.meta.llama3-2-11b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the\n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n\n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    - 14\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 1\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/multimodal/bedrock/config-llama-3-2-11b-vision-instruct-marqo-GS-10M.yml",
      "scrapeId": "4163cdcd-4945-4433-86c4-e999a83c69e5",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/multimodal/bedrock/config-llama-3-2-11b-vision-instruct-marqo-GS-10M.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"mistral-7b-instruct-AWQ\"\n  model_name: \"mistral7bInstruct-AWQ\"\n\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: tokenizer_mistral ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_mistral.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n  - language: en\n    min_length_in_tokens: 1600\n    max_length_in_tokens: 5000\n    payload_file: payload_en_1600-5000.jsonl\n\nmetrics:\n  dataset_of_interest: en_1000-2000\n\npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 200\n\n# Model configurations\nexperiments:\n  - name: mistral-Instruct7b-p4d-lmi-customer-drop\n    model_id: TheBloke/Mistral-7B-Instruct-v0.2-AWQ\n    model_version: \"*\"\n    model_name: mistral7bInstruct-AWQ\n    ep_name: mistral7bInstruct-p4dAWQ\n    download_from_hf_place_in_s3: yes\n    model_s3_path: s3://{write_bucket}/TheBloke/Mistral-7B-Instruct-v0.2-AWQ\n    instance_type: \"ml.p4d.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.26.0-deepspeed0.12.6-cu121\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n      engine=Python\n      option.tensor_parallel_degree=1\n      option.model_id=s3://{write_bucket}/TheBloke/Mistral-7B-Instruct-v0.2-AWQ\n      option.max_rolling_batch_size=64\n      option.rolling_batch=vllm\n    payload_files:\n    - payload_en_1000-2000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 5\n    - 10\n    - 20\n    - 30\n    env:\n\nreport:\n  latency_budget: 1\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n  latency_vs_token_len_chart:\n    y_ticks:\n    # - 1\n    # - 2\n    # - 3\n    # - 4\n    title: \"Effect of token length on inference latency for \\\"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\\\"\"\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-instruct-AWQ-p4d.yml",
      "scrapeId": "3111b65a-2cd2-46fb-bb05-fc2f91d1b2c2",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-instruct-AWQ-p4d.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"fmbench-bedrock-llama3-3-all-models\"\n  model_name: \"llama3-3 Models on Amazon Bedrock\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-3-70b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: us.meta.llama3-3-70b-instruct-v1:0\n    hf_tokenizer_model_id: meta-llama/Llama-3.3-70B-Instruct\n    model_version:\n    model_name: us.meta.llama3-3-70b-instruct-v1:0\n    ep_name: us.meta.llama3-3-70b-instruct-v1:0\n    instance_type: us.meta.llama3-3-70b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the\n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n\n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 1\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-llama-3-3-all-models.yml",
      "scrapeId": "395e8635-9c52-4301-b0fe-c92852bba412",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-llama-3-3-all-models.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"fmbench-bedrock-llama3-stream-eval-responses\"\n  model_name: \"Llama3 on Bedrock\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: no\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: no\n  5_model_metric_analysis.ipynb: no\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: meta.llama3-70b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: meta.llama3-70b-instruct-v1:0\n    model_version:\n    model_name: meta.llama3-70b-instruct-v1:0\n    ep_name: meta.llama3-70b-instruct-v1:0\n    instance_type: meta.llama3-70b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the\n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: True\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n\n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    - 2\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  - name: meta.llama3-8b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: meta.llama3-1-8b-instruct-v1:0\n    model_version:\n    model_name: meta.llama3-8b-instruct-v1:0\n    ep_name: meta.llama3-8b-instruct-v1:0\n    instance_type: meta.llama3-8b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the\n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: True\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n\n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    - 2\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-llama3-streaming.yml",
      "scrapeId": "4087c5a6-eea1-43a2-a3bb-af42b687be2f",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-llama3-streaming.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g6e.24xl-tp=2-mc=max-djl-ec2\"\n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g6e.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    - 14\n    - 15\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g6e.24xl-tp-2-mc-max-djl.yml",
      "scrapeId": "424ddbfd-28b5-46ab-b56d-25f6651da9b7",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g6e.24xl-tp-2-mc-max-djl.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.24xl-tp=2-mc=max-triton-ec2\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    temperature: 0.1\n    do_sample: yes\n    top_p: 0.92\n    top_k: 120\n    bad_words: \"\"\n    stop_words: \"<|eot_id|>\"\n    pad_id: 2\n    end_id: 2\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: http://localhost:8080/v2/models/ensemble/generate\n    instance_type: \"g6e.24xlarge\"\n    image_uri: nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      batch_size: 64\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.24xl-tp-2-mc-max-triton-ec2.yml",
      "scrapeId": "24b21137-0405-49f9-90de-224116ae6475",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.24xl-tp-2-mc-max-triton-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-2-13b-chat-hf deployed on an EKS cluster using ray serve\ngeneral:\n  name: \"byo-REST-ep\"\n  model_name: \"llama-2-13b-chat\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama2.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 924\n    payload_file: payload_en_1-924.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1-924\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  rest:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-2-13b-chat-EKS-ray-serve\"\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-2-13b-chat\"\n    # the ep_name will contain the endpoint url that is used to invoke your model and get the response\n    # in this case, we use ray serve with `NousResearch/Llama-2-13b-chat-hf` model deployed on an EKS cluster.\n    # the endpoint url format used in this example is \"http://<NLB_DNS_NAME>/serve/infer?sentence=<PROMPT_PAYLOAD>\"\n    ep_name: 'http://<NLB_DNS_NAME>/serve/infer?sentence=' # public DNS/URL to send your request\n    instance_type: \"ml.inf2.48xlarge\"\n    image_uri:\n    deploy: no #setting to no since the endpoint has already been deployed on an EKS cluster\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example of a rest predictor that handles\n    # inferences for a llama-2-13b-chat-hf model deployed on an EKS cluster using Ray\n    inference_script: rest_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: rest\n      timeout: 180\n      auth: # pass your authentication parameters here\n      # - auth_paramater_1:\n      # - auth_paramater_2:\n      # - auth_paramater_3:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-924.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    - 15\n    - 18\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 5\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama2/13b/config-byo-rest-ep-llama2-13b.yml",
      "scrapeId": "3a487e3b-a34c-43a6-a9ea-ca44450e3e27",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama2/13b/config-byo-rest-ep-llama2-13b.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"mistral-7b-trn1.32xl-ec2\"\n  model_name: \"mistral-7b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: mistral_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_mistral.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1000-2000\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n    return_full_text: no\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Mistral-7B-Instruct-v0.2\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: mistralai/Mistral-7B-Instruct-v0.2 #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Mistral-7B-Instruct-v0.2\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"trn1.32xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and Inferentia settings for the instance\n      # '-u djl' tells the container to use the Deep Java Library (DJL) runtime\n      # '--device /dev/neuron0 ... /dev/neuron5' specifies the Inferentia devices to be used\n      gpu_or_neuron_setting: -u djl --device /dev/neuron0 --device /dev/neuron1 --device /dev/neuron2 --device /dev/neuron3 --device /dev/neuron4 --device /dev/neuron5 --device /dev/neuron6 --device /dev/neuron7 --device /dev/neuron8 --device /dev/neuron9 --device /dev/neuron10 --device /dev/neuron11 --device /dev/neuron12 --device /dev/neuron13 --device /dev/neuron14 --device /dev/neuron15\n      #This setting specifies the timeout (in seconds) for loading the model.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 32\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=mistralai/Mistral-7B-Instruct-v0.2\n      option.tensor_parallel_degree=32\n      option.n_positions=4096\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=8\n      option.model_loading_timeout=2400\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 5\n    - 8\n    - 10\n    - 15\n    - 20\n    - 25\n    - 30\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O3\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-trn1-32xl-deploy-ec2-tp32.yml",
      "scrapeId": "2a9fd32d-d730-4ed0-9e99-f1f90048d455",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-trn1-32xl-deploy-ec2-tp32.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.48xl-tp=2-mc=max-triton-ec2\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    temperature: 0.1\n    do_sample: yes\n    top_p: 0.92\n    top_k: 120\n    bad_words: \"\"\n    stop_words: \"<|eot_id|>\"\n    pad_id: 2\n    end_id: 2\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: http://localhost:8080/v2/models/ensemble/generate\n    instance_type: \"g6e.48xlarge\"\n    image_uri: nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      batch_size: 64\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n    - 11\n    - 12\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.48xl-tp-2-mc-max-triton-ec2.yml",
      "scrapeId": "49a7439e-6dbe-4953-8028-2166c3792dd9",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.48xl-tp-2-mc-max-triton-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"bge-base-en-v1-5\"\n  model_name: \"bge-base-en-v1-5\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - just_text.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_bert.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - text\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 50\n    payload_file: payload_en_1-50.jsonl\n  - language: en\n    min_length_in_tokens: 50\n    max_length_in_tokens: 100\n    payload_file: payload_en_50-100.jsonl\n  - language: en\n    min_length_in_tokens: 100\n    max_length_in_tokens: 150\n    payload_file: payload_en_100-150.jsonl\n\n\nmetrics:\n  dataset_of_interest: en_50-100\n\npricing: pricing.yml\n\ninference_parameters:\n  embedding:\n    top_p: 0.92\n\nexperiments:\n  - name: bge-base-en-v1-5-g5.xl\n    model_id: huggingface-sentencesimilarity-bge-base-en-v1-5\n    model_version: \"*\"\n    model_name: bge-base-en-v1.5\n    ep_name: bge-base-en-v1-5\n    instance_type: \"ml.g5.xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-inference:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: embedding\n      container_type: huggingface\n    payload_files:\n    - payload_en_1-50.jsonl\n    - payload_en_50-100.jsonl\n    - payload_en_100-150.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n    accept_eula: false\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"1200\"\n\n  - name: bge-base-en-v1-5-g5.2xl\n    model_id: huggingface-sentencesimilarity-bge-base-en-v1-5\n    model_version: \"*\"\n    model_name: bge-base-en-v1.5\n    ep_name: bge-base-en-v1-5\n    instance_type: \"ml.g5.2xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-inference:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: embedding\n      container_type: huggingface\n    payload_files:\n    - payload_en_1-50.jsonl\n    - payload_en_50-100.jsonl\n    - payload_en_100-150.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n    accept_eula: false\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"1200\"\n      # SM_NUM_GPUS: \"1\"\n      # MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      # SAGEMAKER_ENV: \"1\"\n      # HF_MODEL_ID: \"BAAI/bge-base-en-v1.5\"\n      # MAX_INPUT_LENGTH: \"4095\"\n      # MAX_TOTAL_TOKENS: \"4096\"\n    #   # SAGEMAKER_MODEL_SERVER_WORKERS: \"2\"\n\nreport:\n  latency_budget: 1\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/embeddings/bge-base-en-v1-5-g5-embeddings.yml",
      "scrapeId": "445641ad-2634-4717-9cc8-db0fdbd79d16",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/embeddings/bge-base-en-v1-5-g5-embeddings.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.48xl-tp=4-mc=max-triton-ec2\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    temperature: 0.1\n    do_sample: yes\n    top_p: 0.92\n    top_k: 120\n    bad_words: \"\"\n    stop_words: \"<|eot_id|>\"\n    pad_id: 2\n    end_id: 2\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: http://localhost:8080/v2/models/ensemble/generate\n    instance_type: \"g6e.48xlarge\"\n    image_uri: nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n      batch_size: 256\n\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.48xl-tp-8-mc-max-triton-ec2.yml",
      "scrapeId": "65ea13a4-5879-4240-a5b7-d3572d545618",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.48xl-tp-8-mc-max-triton-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"fmbench-bedrock-anthropic-models\"\n  model_name: \"All anthropic models available in Amazon Bedrock\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n## S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - Open-Orca/OpenOrca.jsonl\n  tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n  prompt_template_dir: prompt_template\n  prompt_template_file: prompt_template_claude_OpenOrca.txt ## add your desired prompt template type\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - system_prompt\n  ground_truth_col_key: response\n  question_col_key: input\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n\nmetrics:\n  dataset_of_interest: en_1000-2000\n\n\npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n\ninference_parameters:\n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  # Experiment for claude 3 sonnet\n  - name: anthropic.claude-3-sonnet-20240229-v1:0\n    model_name: anthropic.claude-3-sonnet-20240229-v1:0\n    ep_name: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_type: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  # Experiment for claude 3 haiku\n  - name: anthropic.claude-3-haiku-20240307-v1:0\n    model_name: anthropic.claude-3-haiku-20240307-v1:0\n    ep_name: anthropic.claude-3-haiku-20240307-v1:0\n    instance_type: anthropic.claude-3-haiku-20240307-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n\nreport:\n  latency_budget: 5\n  cost_per_10k_txn_budget: 350\n  error_rate_budget: 0\n  # change this based on your dataset size and your needs\n  accuracy_budget: 0.90\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-anthropic-models-OpenOrca.yml",
      "scrapeId": "46a8f664-4869-44a9-bbe8-85f376c58f0d",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-anthropic-models-OpenOrca.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-p4de.24xl-ec2\"\n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: no\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"p4de.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 10\n    - 30\n    - 35\n    - 40\n    - 50\n    - 60\n    - 65\n    - 70\n    - 75\n    - 100\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-p4-tp-8-mc-max.yml",
      "scrapeId": "4927e2df-bf64-42b0-844b-9b56fafde982",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-p4-tp-8-mc-max.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"fmbench-bedrock\"\n  model_name: \"FMs available in Amazon Bedrock\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket}\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml ## mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_titan_text.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: no\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: no\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  # - text\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\nmetrics:\n  dataset_of_interest: en_3000-3840\n\npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n\ninference_parameters:\n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  - name: mistral.mistral-7b-instruct-v0:2\n    model_id: mistral.mistral-7b-instruct-v0:2\n    model_version: \"*\"\n    model_name: mistral.mistral-7b-instruct-v0:2\n    ep_name: mistral.mistral-7b-instruct-v0:2\n    instance_type: mistral.mistral-7b-instruct-v0:2\n    image_uri:\n    deploy: no\n    instance_count: 1\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  - name: mistral.mixtral-8x7b-instruct-v0:1\n    model_id: mistral.mixtral-8x7b-instruct-v0:1\n    model_version: \"*\"\n    model_name: mistral.mixtral-8x7b-instruct-v0:1\n    ep_name: mistral.mixtral-8x7b-instruct-v0:1\n    instance_type: mistral.mixtral-8x7b-instruct-v0:1\n    image_uri:\n    deploy: no\n    instance_count: 1\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n\n\n  - name: meta.llama2-13b-chat-v1\n    model_id: meta.llama2-13b-chat-v1\n    model_version: \"*\"\n    model_name: meta.llama2-13b-chat-v1\n    ep_name: meta.llama2-13b-chat-v1\n    instance_type: meta.llama2-13b-chat-v1\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  - name: meta.llama2-70b-chat-v1\n    model_id: meta.llama2-70b-chat-v1\n    model_version: \"*\"\n    model_name: meta.llama2-70b-chat-v1\n    ep_name: meta.llama2-70b-chat-v1\n    instance_type: meta.llama2-70b-chat-v1\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  - name: amazon.titan-text-lite-v1\n    model_id: amazon.titan-text-lite-v1\n    model_version: \"*\"\n    model_name: amazon.titan-text-lite-v1\n    ep_name: amazon.titan-text-lite-v1\n    instance_type: 'amazon.titan-text-lite-v1'\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  - name: amazon.titan-text-express-v1\n    model_id: amazon.titan-text-express-v1\n    model_version: \"*\"\n    model_name: amazon.titan-text-express-v1\n    ep_name: amazon.titan-text-express-v1\n    instance_type: amazon.titan-text-express-v1\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  - name: anthropic.claude-3-sonnet-20240229-v1:0\n    model_id: anthropic.claude-3-sonnet-20240229-v1:0\n    model_version: \"*\"\n    model_name: anthropic.claude-3-sonnet-20240229-v1:0\n    ep_name: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_type: anthropic.claude-3-sonnet-20240229-v1:0\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  - name: anthropic.claude-3-haiku-20240307-v1:0\n    model_id: anthropic.claude-3-haiku-20240307-v1:0\n    model_version: \"*\"\n    model_name: anthropic.claude-3-haiku-20240307-v1:0\n    ep_name: anthropic.claude-3-haiku-20240307-v1:0\n    instance_type: anthropic.claude-3-haiku-20240307-v1:0\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  - name: cohere.command-text-v14\n    model_id: cohere.command-text-v14\n    model_version: \"*\"\n    model_name: cohere.command-text-v14\n    ep_name: cohere.command-text-v14\n    instance_type: cohere.command-text-v14\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  - name: cohere.command-light-text-v14\n    model_id: cohere.command-light-text-v14\n    model_version: \"*\"\n    model_name: cohere.command-light-text-v14\n    ep_name: cohere.command-light-text-v14\n    instance_type: cohere.command-light-text-v14\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  - name: ai21.j2-mid-v1\n    model_id: ai21.j2-mid-v1\n    model_version: \"*\"\n    model_name: ai21.j2-mid-v1\n    ep_name: ai21.j2-mid-v1\n    instance_type: ai21.j2-mid-v1\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  - name: ai21.j2-ultra-v1\n    model_id: ai21.j2-ultra-v1\n    model_version: \"*\"\n    model_name: ai21.j2-ultra-v1\n    ep_name: ai21.j2-ultra-v1\n    instance_type: ai21.j2-ultra-v1\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-bedrock.yml",
      "scrapeId": "4fe11a8c-a8cd-4118-b090-9fa0fb608b60",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-bedrock.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"llama3-8b-g5-2xl\"\n  model_name: \"llama3-8b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\nmetrics:\n  dataset_of_interest: en_3000-3840\n\npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\nexperiments:\n  - name: llama-3-8b-instruct-g5-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct-g5.2xl\n    ep_name: llama-3-8b-instruct-g5-2xl\n    instance_type: \"ml.g5.2xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 5\n    - 8\n    - 10\n    - 16\n    - 20\n    - 30\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      job_queue_size: 1000\n      max_dynamic_batch_size: 1\n      max_batch_delay: 100\n      max_idle_time: 60\n      load_on_devices: *\n      engine: MPI\n      mpi_mode: true\n      option.entryPoint: null\n      option.tensor_parallel_degree: 1\n      option.max_rolling_batch_size: 256\n      option.mpi_mode: true\n      option.model_id: /opt/ml/model\n      option.rolling_batch: lmi-dist\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-g5-2xl.yml",
      "scrapeId": "4cbd6347-afe9-4548-97b1-d6bb8455dd39",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-g5-2xl.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"fmbench-bedrock-claude-sonnet\"\n  model_name: \"Claude Sonnet available in Amazon Bedrock\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket}\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n\n    configs_prefix: configs\n    config_files:\n    - pricing.yml ## mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_claude.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: no\n    3_run_inference.ipynb: yes\n    4_get_evaluations.ipynb: yes\n    5_model_metric_analysis.ipynb: yes\n    6_cleanup.ipynb: no\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\nmetrics:\n  dataset_of_interest: en_3000-4000\n\n\npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n\ninference_parameters:\n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  - name: anthropic.claude-v3-sonnet-pt\n    model_name: anthropic.claude-v3-sonnet-pt\n    ep_name: <your-provisioned-throughput-arn>\n    instance_type: anthropic.claude-v3-sonnet-pt-nc\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  - name: anthropic.claude-3-sonnet-20240229-v1:0\n    model_name: anthropic.claude-3-sonnet-20240229-v1:0\n    ep_name: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_type: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n\nreport:\n  latency_budget: 5\n  cost_per_10k_txn_budget: 350\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-claude.yml",
      "scrapeId": "56199cb8-35c0-4c34-8b12-8fe71d1d969e",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-claude.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-70b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3.1-70b-g6e.24xl-tp=4-mc=max-djl-ec2\"\n  model_name: \"Meta-Llama-3.1-70b-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-70b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-70B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-70b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g6e.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged witsh multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-70b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-70b-Instruct\n      option.rolling_batch=lmi-dist\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n    - 7\n    - 8\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 200\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-llama3-1-70b-g6e.24xl-tp-4-mc-max-djl.yml",
      "scrapeId": "473e0a28-647e-455b-8f74-12f1c4867f7a",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-llama3-1-70b-g6e.24xl-tp-4-mc-max-djl.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-8b-trn1.32xl-ec2-triton\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    top_k: 50\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct-triton\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8080/v2/models/Meta-Llama-3-8B-Instruct/generate'\n    instance_type: \"trn1.32xlarge\"\n    image_uri: tritonserver-neuronx:fmbench\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model.\n      model_loading_timeout: 8000\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # For deploying a model using the triton inference container:\n      # mention the backend type, if any. Options for triton are: [djl, vllm or tensorrt]\n      backend: djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured,\n      # max: FMBench figures out the max number of model containers to be run\n      #      based on TP degree configured and number of neuron cores/GPUs available.\n      #      For example, if TP=2, GPUs=8 then FMBench will start 4 containers and 1 load balancer,\n      # auto: only supported if the underlying inference container would automatically\n      #       start multiple copies of the model internally based on TP degree and neuron cores/GPUs\n      #       available. In this case only a single container is created, no load balancer is created.\n      #       The DJL serving containers supports auto.\n      model_copies: max\n      shm_size: 12g\n      # The model.json parameters are replaced within the model.json file\n      # for the triton on vllm/djl/tensorrt options. The model.json already contains\n      # the tp degree and model id from above in this config file. This is a dictionary\n      # that contains other variables, such as max_num_seqs, dtype, max_model_len, block_size and more\n      # For tensorrt, the tp degree, batch size and other relevant parameters are\n      # extracted directly from the inference spec.\n      container_params:\n        # tp degree is a mandatory parameter\n        tp_degree: 8\n        amp: \"f16\"\n        attention_layout: 'BSH'\n        collectives_layout: 'BSH'\n        context_length_estimate: 3072, 3584, 4096\n        max_rolling_batch_size: 8\n        model_loader: \"tnx\"\n        model_loading_timeout: 2400\n        n_positions: 4096\n        output_formatter: \"json\"\n        rolling_batch: \"auto\"\n        rolling_batch_strategy: \"continuous_batching\"\n        trust_remote_code: true\n        # modify the serving properties to match your model and requirements\n        serving.properties:\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 8\n    - 10\n    - 12\n    - 14\n    - 15\n    - 18\n    - 20\n    - 25\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-trn1-32xlarge-triton-djl.yml",
      "scrapeId": "51b7db10-1b43-4b5d-a373-c64d2b43e7e5",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-trn1-32xlarge-triton-djl.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.2-1b-g6e.2xl-tp=1-mc=max-djl-ec2\"\n  model_name: \"Meta-Llama-3.2-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 500_token_prompts_synthetic_data.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 1100\n    payload_file: payload_en_1-1100.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1-1100\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 15\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.2-1b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.2-1b-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3.2-1b-instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g6e.2xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      tp_degree: 1\n      shm_size: 12g\n      model_loading_timeout: 2400\n      model_copies: max\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=1\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Llama-3.2-1b-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-1100.jsonl\n\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 0.1\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.2/1b/config-llama3.2-1b-g6e.2xl-tp-1-mc-max-djl.yml",
      "scrapeId": "5c902283-571c-4096-bb78-0b83847ce163",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.2/1b/config-llama3.2-1b-g6e.2xl-tp-1-mc-max-djl.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"mistral-7b-p5-instruct-AWQ-byo-ep\"\n  model_name: \"mistral7bInstruct-AWQ\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: tokenizer_mistral ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_mistral.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n  - language: en\n    min_length_in_tokens: 1600\n    max_length_in_tokens: 5000\n    payload_file: payload_en_1600-5000.jsonl\n\nmetrics:\n  dataset_of_interest: en_1000-2000\n  weights:\n    price_per_tx_wt: 0.65\n    latenct_wt: 0.35\n\npricing:\n  ml.g5.2xlarge: 1.515\n  ml.g5.12xlarge: 7.09\n  ml.g5.24xlarge: 10.18\n  ml.g5.48xlarge: 20.36\n  ml.inf2.24xlarge: 7.79\n  ml.inf2.48xlarge: 15.58\n  ml.p4d.24xlarge: 37.688\n  ml.p5.48xlarge: 98.32\n\ninference_parameters:\n  do_sample: yes\n  temperature: 0.1\n  top_p: 0.92\n  top_k: 120\n  max_new_tokens: 100\n\n# Model configurations\nexperiments:\n  - name: mistral-Instruct7b-p5-byo-sagemaker-ep\n    model_id:\n    model_version:\n    model_name: mistral7bInstruct-AWQ\n    ep_name: <your-sagemaker-endpoint-name> # enter the name of the existing endpoint name already deployed on sagemaker\n    instance_type: \"ml.p5.48xlarge\"\n    image_uri:\n    deploy: no #setting to no since the endpoint has already been deployed\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n\n    concurrency_levels:\n    - 1\n    - 5\n    - 30\n    env:\n\nreport:\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n  latency_vs_token_len_chart:\n    y_ticks:\n    # - 1\n    # - 2\n    # - 3\n    # - 4\n    title: \"Effect of token length on inference latency for \\\"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\\\"\"\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-instruct-AWQ-p5-byo-ep.yml",
      "scrapeId": "4a62c121-8c0f-4ff8-9f39-e71159b27180",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-instruct-AWQ-p5-byo-ep.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"llama3-70b-p4d-g5-v1\"\n  model_name: \"llama3-70b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_70b_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\nmetrics:\n  dataset_of_interest: en_3000-3840\n\npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\nexperiments:\n  - name: llama-3-70b-instruct-g5-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-70b-instruct\n    model_version: \"*\"\n    model_name: llama-3-70b-instruct\n    ep_name: llama-3-70b-instruct-g5\n    instance_type: \"ml.g5.48xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"8\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\n  - name: llama-3-70b-instruct-p4d-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-70b-instruct\n    model_version: \"*\"\n    model_name: llama-3-70b-instruct\n    ep_name: llama-3-70b-instruct-p4d\n    instance_type: \"ml.p4d.24xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    - 15\n\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"8\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/70b/config-llama3-70b-instruct-g5-p4d.yml",
      "scrapeId": "4ae08dd9-11ba-4585-ad95-8c0f543f488a",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/70b/config-llama3-70b-instruct-g5-p4d.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-8b-trn1.32xl-ec2-triton\"\n  model_name: \"Meta-Llama-1-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    top_k: 50\n    max_tokens: 4096\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct-triton\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-8B\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8080/v2/models/Meta-Llama-3.1-8B/generate'\n    instance_type: \"trn1.32xlarge\"\n    image_uri: tritonserver-neuronx:fmbench\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and Inferentia settings for the instance\n      # '-u djl' tells the container to use the Deep Java Library (DJL) runtime\n      # '--device /dev/neuron0 ... /dev/neuron5' specifies the Inferentia devices to be used\n      gpu_or_neuron_setting: -u triton --device /dev/neuron0 --device /dev/neuron1 --device /dev/neuron2 --device /dev/neuron3 --device /dev/neuron4 --device /dev/neuron5 --device /dev/neuron6 --device /dev/neuron7 --device /dev/neuron8 --device /dev/neuron9 --device /dev/neuron10 --device /dev/neuron11 --device /dev/neuron12 --device /dev/neuron13 --device /dev/neuron14 --device /dev/neuron15\n      #This setting specifies the timeout (in seconds) for loading the model.\n      model_loading_timeout: 8000\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured,\n      # max: FMBench figures out the max number of model containers to be run\n      #      based on TP degree configured and number of neuron cores/GPUs available.\n      #      For example, if TP=2, GPUs=8 then FMBench will start 4 containers and 1 load balancer,\n      # auto: only supported if the underlying inference container would automatically\n      #       start multiple copies of the model internally based on TP degree and neuron cores/GPUs\n      #       available. In this case only a single container is created, no load balancer is created.\n      #       The DJL serving containers supports auto.\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      batch_size: 0\n      shm_size: 12g\n      model_loading_timeout: 2400\n      # The model.json parameters are replaced within the model.json file\n      # for the triton on vllm option. The model.json already contains\n      # the tp degree and model id from above in this config file. This is a dictionary\n      # that contains other variables, such as max_num_seqs, dtype, max_model_len, block_size and more\n      vllm_model_params:\n        max_num_seqs: 4\n        dtype: \"float16\"\n        # The max_model_len and block_size arguments are required to be same as\n        # max sequence length when targeting neuron device.\n        # Currently, this is a known limitation in continuous batching support\n        # in transformers-neuronx. View the latest vllm documentation: https://vllm.readthedocs.io/_/downloads/en/stable/pdf/\n        max_model_len: 8192\n        block_size: 8192\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n    - 7\n    - 8\n    - 9\n    - 10\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-trn32xl-triton-vllm.yml",
      "scrapeId": "5f7888fd-1503-47ad-842d-bc0af8d595bb",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-trn32xl-triton-vllm.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"llama3-8b-p4d-djl-lmi-dist\"\n  model_name: \"llama3-8b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\nmetrics:\n  dataset_of_interest: en_3000-3840\n\npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\nexperiments:\n\n  - name: llama-3-8b-instruct-p4d-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct\n    ep_name: llama-3-8b-instruct-p4d\n    instance_type: \"ml.p4d.24xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 15\n    - 20\n    - 30\n\n    accept_eula: true\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      job_queue_size: 1000\n      max_dynamic_batch_size: 1\n      max_batch_delay: 100\n      max_idle_time: 60\n      load_on_devices: *\n      engine: MPI\n      mpi_mode: true\n      option.entryPoint: null\n      option.tensor_parallel_degree: 8\n      option.max_rolling_batch_size: 256\n      option.mpi_mode: true\n      option.model_id: /opt/ml/model\n      option.rolling_batch: lmi-dist\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      # SM_NUM_GPUS: \"1\"\n      # SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-p4d-djl-lmi-dist.yml",
      "scrapeId": "4f04fe3c-2f39-49d0-b3f7-b678c87644dc",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-p4d-djl-lmi-dist.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-70b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-70b-inf2.48xl-ec2\"\n  model_name: \"llama3-1-70b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    top_k: 50\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-70b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-70B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-70b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8080/v2/models/Llama-3.1-70B-Instruct/generate'\n    instance_type: \"inf2.48xlarge\"\n    image_uri: tritonserver-neuronx:fmbench\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-70b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model.\n      model_loading_timeout: 10000\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # For deploying a model using the triton inference container:\n      # mention the backend type, if any. Options for triton are: [djl, vllm or tensorrt]\n      backend: djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured,\n      # max: FMBench figures out the max number of model containers to be run\n      #      based on TP degree configured and number of neuron cores/GPUs available.\n      #      For example, if TP=2, GPUs=8 then FMBench will start 4 containers and 1 load balancer,\n      # auto: only supported if the underlying inference container would automatically\n      #       start multiple copies of the model internally based on TP degree and neuron cores/GPUs\n      #       available. In this case only a single container is created, no load balancer is created.\n      #       The DJL serving containers supports auto.\n      model_copies: max\n      shm_size: 12g\n      # The model.json parameters are replaced within the model.json file\n      # for the triton on vllm/djl/tensorrt options. The model.json already contains\n      # the tp degree and model id from above in this config file. This is a dictionary\n      # that contains other variables, such as max_num_seqs, dtype, max_model_len, block_size and more\n      # For tensorrt, the tp degree, batch size and other relevant parameters are\n      # extracted directly from the inference spec.\n      container_params:\n        # tp degree is a mandatory parameter\n        tp_degree: 24\n        amp: \"f16\"\n        attention_layout: 'BSH'\n        collectives_layout: 'BSH'\n        context_length_estimate: 3072, 3584, 4096\n        max_rolling_batch_size: 8\n        model_loader: \"tnx\"\n        model_loading_timeout: 2400\n        n_positions: 4096\n        output_formatter: \"json\"\n        rolling_batch: \"auto\"\n        rolling_batch_strategy: \"continuous_batching\"\n        neuron_optimize_level: 2\n        trust_remote_code: true\n        # modify the serving properties to match your model and requirements\n        serving.properties:\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O2\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 200\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-llama3-1-70b-inf2.48xl-triton-tp24.yml",
      "scrapeId": "4cf0e08a-49cb-497d-b897-7be043321ec8",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-llama3-1-70b-inf2.48xl-triton-tp24.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"fmbench-bedrock-nova-models-openorca-dataset-new\"\n  model_name: \"Nova models available in Amazon Bedrock\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket}\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n\n    configs_prefix: configs\n    config_files:\n    - pricing.yml ## mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # This dataset is the custom open orca dataset which has the system prompt and the question columns\n    # merged. For a few rows, the system prompt is empty, so if there is a system prompt, it is appended to\n    # the question, else only the question is used. This is done to get the true accuracy measures of the model\n    # To get this custom jsonl file, run the `Preprocess the OpenOrca Dataset` section of the `bring_your_dataset`\n    # notebook within the FMBench repository.\n    - Open-Orca/OpenOrca.jsonl\n    tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_nova_open_orca.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: no\n    3_run_inference.ipynb: yes\n    4_get_evaluations.ipynb: yes\n    5_model_metric_analysis.ipynb: yes\n    6_cleanup.ipynb: no\n\ndatasets:\n  prompt_template_keys:\n  - question\n  ground_truth_col_key: response\n  question_col_key: question\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 4000\n    max_length_in_tokens: 5000\n    payload_file: payload_en_4000-5000.jsonl\n  - language: en\n    min_length_in_tokens: 5000\n    max_length_in_tokens: 6000\n    payload_file: payload_en_5000-6000.jsonl\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\nmetrics:\n  dataset_of_interest: en_1000-2000\n\n\npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n\ninference_parameters:\n  bedrock:\n    temperature: 0.1\n    max_tokens: 512\n    top_p: 0.92\n    caching: False\n    # Set use_boto3 to \"no\" if the model is supported by litellm and\n    # the litellm API will be used to invoke the model. If the model is on\n    # bedrock and is not supported by the latest version of litellm, then set\n    # the parameter below to \"yes\" and the bedrock converseAPI will be used.\n    use_boto3: no\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  # This is the NOVA micro model\n  - name: amazon.nova-micro-v1:0\n    model_name: amazon.nova-micro-v1:0\n    ep_name: amazon.nova-micro-v1:0\n    instance_type: amazon.nova-micro-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    concurrency_levels:\n    - 1\n    env:\n  - name: amazon.nova-lite-v1:0\n    model_name: amazon.nova-lite-v1:0\n    ep_name: amazon.nova-lite-v1:0\n    instance_type: amazon.nova-lite-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    concurrency_levels:\n    - 1\n    env:\n  - name: amazon.nova-pro-v1:0\n    model_name: amazon.nova-pro-v1:0\n    ep_name: amazon.nova-pro-v1:0\n    instance_type: amazon.nova-pro-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    - payload_en_4000-5000.jsonl\n    - payload_en_5000-6000.jsonl\n    concurrency_levels:\n    - 1\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-nova-all-models-openarca.yml",
      "scrapeId": "4c4700cf-915f-4e8b-a090-41254d5e7ab0",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-nova-all-models-openarca.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-p4de.24xl-ec2\"\n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"p4de.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 10\n    - 30\n    - 35\n    - 40\n    - 50\n    - 60\n    - 65\n    - 70\n    - 75\n    - 100\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-p4-tp-4-mc-max.yml",
      "scrapeId": "5c31e377-0b0c-439b-8186-47b0370a2eb3",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-p4-tp-4-mc-max.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\npricing:\n  instance_based:\n    # Instance Based Pricing: SageMaker, EKS, Bedrock Provisioned Throughput, Bring your own endpoints that are priced hourly\n    # SageMaker Hourly Instance Pricing\n    ml.c5.xlarge: 0.204\n    ml.c5.2xlarge: 0.408\n    ml.c5.4xlarge: 0.816\n    ml.c7i.xlarge: 0.214\n    ml.m5.xlarge: 0.23\n    ml.g5.xlarge: 1.4084\n    ml.g5.4xlarge: 2.03\n    ml.g5.8xlarge: 3.06\n    ml.g5.2xlarge: 1.515\n    ml.g5.12xlarge: 7.09\n    ml.g5.24xlarge: 10.18\n    ml.g5.48xlarge: 20.36\n    ml.inf2.xlarge: 0.99\n    ml.inf2.8xlarge: 2.36\n    ml.inf2.24xlarge: 7.79\n    ml.inf2.48xlarge: 15.58\n    ml.trn1.32xlarge: 28.497\n    ml.p4d.24xlarge: 37.688\n    ml.p5.48xlarge: 113.068\n    ml.p3.2xlarge: 3.825\n    ml.g4dn.xlarge: 0.7364\n    ml.g4dn.2xlarge: 0.94\n    ml.g4dn.4xlarge: 1.505\n    ml.g4dn.8xlarge: 2.72\n    ml.g4dn.12xlarge: 4.89\n    ml.g4dn.16xlarge: 5.44\n    ml.g6.2xlarge: 1.222\n    ml.g6.16xlarge: 4.246\n    ml.g6.12xlarge: 5.752\n    ml.g6.24xlarge: 8.344\n    ml.g6.48xlarge: 16.688\n    anthropic.claude-v3-sonnet-pt-nc: 88\n    # corresponding hourly pricing for EC2 instances if your model is hosted on EC2\n    # all EC2 pricing is based on public on-demand pricing information that can be\n    # viewed in this link: https://aws.amazon.com/ec2/pricing/on-demand/\n    m5.16xlarge: 3.072\n    c5.18xlarge: 3.06\n    m7i.12xlarge: 2.419\n    m7i.24xlarge: 4.8384\n    m7a.4xlarge: 0.9274\n    m7a.16xlarge: 3.709\n    m7a.24xlarge: 5.564\n    m5.xlarge: 0.192\n    g5.xlarge: 1.006\n    g5.4xlarge: 1.624\n    g5.2xlarge: 1.212\n    g5.12xlarge: 5.672\n    g5.24xlarge: 8.144\n    g5.48xlarge: 16.288\n    inf2.xlarge: 0.7582\n    inf2.8xlarge: 1.96786\n    inf2.24xlarge: 6.49063\n    inf2.48xlarge: 12.98127\n    trn1.32xlarge: 21.50\n    p4d.24xlarge: 32.7726\n    p4de.24xlarge: 40.965\n    p5.48xlarge: 98.32\n    p5e.48xlarge: 110.92\n    p3.2xlarge: 3.06\n    g4dn.12xlarge: 3.912\n    g6.2xlarge: 0.9776\n    g6.4xlarge: 1.3512\n    g6.16xlarge: 3.3968\n    g6.12xlarge: 4.6016\n    g6.24xlarge: 6.6752\n    g6.48xlarge: 13.3504\n    g6e.2xlarge: 2.242\n    g6e.4xlarge: 3.1294\n    g6e.12xlarge: 10.493\n    g6e.16xlarge: 7.577\n    g6e.24xlarge: 15.066\n    g6e.48xlarge: 30.131\n    c8g.24xlarge: 3.828\n\n  token_based:\n    amazon.nova-micro-v1:0:\n      input-per-1k-tokens: 0.000035\n      output-per-1k-tokens: 0.00014\n    amazon.nova-lite-v1:0:\n      input-per-1k-tokens: 0.00006\n      output-per-1k-tokens: 0.00024\n    amazon.nova-pro-v1:0:\n      input-per-1k-tokens: 0.0008\n      output-per-1k-tokens: 0.0032\n    ai21.j2-mid-v1:\n      input-per-1k-tokens: 0.0125\n      output-per-1k-tokens: 0.0125\n    ai21.j2-ultra-v1:\n      input-per-1k-tokens: 0.0188\n      output-per-1k-tokens: 0.0188\n    amazon.titan-text-express-v1:\n      input-per-1k-tokens: 0.0002\n      output-per-1k-tokens: 0.0006\n    amazon.titan-text-lite-v1:\n      input-per-1k-tokens: 0.00015\n      output-per-1k-tokens: 0.0002\n    us.anthropic.claude-3-5-sonnet-20241022-v2:0:\n      input-per-1k-tokens: 0.003\n      output-per-1k-tokens: 0.015\n    anthropic.claude-3-5-sonnet-20241022-v2:0:\n      input-per-1k-tokens: 0.003\n      output-per-1k-tokens: 0.015\n    anthropic.claude-3-5-sonnet-20240620-v1:0:\n      input-per-1k-tokens: 0.003\n      output-per-1k-tokens: 0.015\n    anthropic.claude-3-haiku-20240307-v1:0:\n      input-per-1k-tokens: 0.00025\n      output-per-1k-tokens: 0.00125\n    anthropic.claude-3-opus-20240229-v1:0:\n      input-per-1k-tokens: 0.015\n      output-per-1k-tokens: 0.075\n    anthropic.claude-3-sonnet-20240229-v1:0:\n      input-per-1k-tokens: 0.003\n      output-per-1k-tokens: 0.015\n    anthropic.claude-instant-v1:\n      input-per-1k-tokens: 0.0008\n      output-per-1k-tokens: 0.0024\n    anthropic.claude-v2:\n      input-per-1k-tokens: 0.008\n      output-per-1k-tokens: 0.024\n    anthropic.claude-v2:1:\n      input-per-1k-tokens: 0.008\n      output-per-1k-tokens: 0.024\n    cohere.command-light-text-v14:\n      input-per-1k-tokens: 0.0003\n      output-per-1k-tokens: 0.0006\n    cohere.command-r-plus-v1:0:\n      input-per-1k-tokens: 0.003\n      output-per-1k-tokens: 0.015\n    cohere.command-text-v14:\n      input-per-1k-tokens: 0.0015\n      output-per-1k-tokens: 0.002\n    meta.llama3-3-70b-instruct-v1:0:\n      input-per-1k-tokens: 0.00072\n      output-per-1k-tokens: 0.00072\n    us.meta.llama3-3-70b-instruct-v1:0:\n      input-per-1k-tokens: 0.00072\n      output-per-1k-tokens: 0.00072\n    meta.llama2-13b-chat-v1:\n      input-per-1k-tokens: 0.00075\n      output-per-1k-tokens: 0.001\n    meta.llama2-70b-chat-v1:\n      input-per-1k-tokens: 0.00195\n      output-per-1k-tokens: 0.00256\n    meta.llama3-1-405b-instruct-v1:0:\n      input-per-1k-tokens: 0.00532\n      output-per-1k-tokens: 0.016\n    meta.llama3-1-70b-instruct-v1:0:\n      input-per-1k-tokens: 0.00072\n      output-per-1k-tokens: 0.00072\n    meta.llama3-1-8b-instruct-v1:0:\n      input-per-1k-tokens: 0.00022\n      output-per-1k-tokens: 0.00022\n    meta.llama3-70b-instruct-v1:0:\n      input-per-1k-tokens: 0.00265\n      output-per-1k-tokens: 0.0035\n    meta.llama3-8b-instruct-v1:0:\n      input-per-1k-tokens: 0.0003\n      output-per-1k-tokens: 0.0006\n    mistral.mistral-7b-instruct-v0:2:\n      input-per-1k-tokens: 0.00015\n      output-per-1k-tokens: 0.0002\n    mistral.mixtral-8x7b-instruct-v0:1:\n      input-per-1k-tokens: 0.00045\n      output-per-1k-tokens: 0.0007\n    us.meta.llama3-2-11b-instruct-v1:0:\n      input-per-1k-tokens: 0.00016\n      output-per-1k-tokens: 0.00016\n    us.meta.llama3-2-1b-instruct-v1:0:\n      input-per-1k-tokens: 0.0001\n      output-per-1k-tokens: 0.0001\n    us.meta.llama3-2-3b-instruct-v1:0:\n      input-per-1k-tokens: 0.00015\n      output-per-1k-tokens: 0.00015\n    us.meta.llama3-2-90b-instruct-v1:0:\n      input-per-1k-tokens: 0.00072\n      output-per-1k-tokens: 0.00072\n    us.amazon.nova-micro-v1:0:\n      input-per-1k-tokens: 0.000035\n      output-per-1k-tokens: 0.00014\n    us.amazon.nova-lite-v1:0:\n      input-per-1k-tokens: 0.00006\n      output-per-1k-tokens: 0.00024\n    us.amazon.nova-pro-v1:0:\n      input-per-1k-tokens: 0.0008\n      output-per-1k-tokens: 0.0032\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/pricing_fallback.yml",
      "scrapeId": "5bb9cfce-8046-49d2-9a47-f4f7fda2c658",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/pricing_fallback.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-p4d.24xl-ec2\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"p4d.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 10\n    - 30\n    - 35\n    - 40\n    - 50\n    - 60\n    - 65\n    - 70\n    - 75\n    - 100\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-p4d-tp-4-mc-max.yml",
      "scrapeId": "5ec0a874-edc6-4089-a6da-1bbf19839b96",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-p4d-tp-4-mc-max.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"config-llama3-8b-trn1-32xl-tp=16-bs=4\"\n  model_name: \"llama3-8b\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data #/Open-Orca\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  #- OpenOrca.jsonl\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n  # prompt_template_Open-Orca-OpenOrca-llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the\n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  #- system_prompt\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: llama-3-8b-instruct-trn1-djl-inference:0.27.0-neuronx-sdk2.18.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    region: {region}\n    sagemaker_execution_role: {role_arn}\n    # S3 bucket to which metrics, plots and reports would be written to\n    bucket: {write_bucket} ## add the name of your desired bucket\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    prefix: \"lmi\"\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3-8b-instruct\"\n    model_id_wo_repo: \"Meta-Llama-3-8B-Instruct\"\n    model_id_wo_repo_split: \"Meta-Llama-3-8B-Instruct-split\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name:\n    instance_type: \"ml.trn1.32xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: neuron_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: sagemaker_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      batch_size: \"1\"\n      num_neuron_cores: \"16\"\n      neuron_version: \"2.19.1\"\n      model_loading_timeout: \"2400\"\n      instance_count: \"1\"\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=s3://{write_bucket}/lmi/Meta-Llama-3-8B-Instruct/Meta-Llama-3-8B-Instruct-split/\n      option.load_split_model=True\n      option.tensor_parallel_degree=16\n      option.n_positions=4096\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=4\n      option.dtype=fp16\n      option.model_loading_timeout=2400\n\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # currently the llama3-8b jumpstart model is compiled with a batch size of 1\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 11\n    - 13\n    - 15\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      model_id_or_path='/tmp/.djl.ai/download/ae03dd100c208acd82b5dbed563c971de864c408'\n      rolling_batch=<RollingBatchEnum.auto: 'auto'>\n      tensor_parallel_degree=16\n      trust_remote_code=False\n      enable_streaming=<StreamingEnum.false: 'false'>\n      batch_size=4\n      max_rolling_batch_size=4\n      dtype=<Dtype.f16: 'fp16'>\n      revision=None\n      output_formatter=None\n      waiting_steps=None\n      is_mpi=False\n      draft_model_id=None\n      spec_length=0\n      neuron_optimize_level=None\n      enable_mixed_precision_accumulation=False\n      enable_saturate_infinity=False\n      n_positions=4096\n      unroll=None\n      load_in_8bit=False\n      low_cpu_mem_usage=False\n      load_split_model=True\n      context_length_estimate=None\n      amp='f16'\n      quantize=None\n      compiled_graph_path=None\n      task=None\n      save_mp_checkpoint_path=None\n      group_query_attention=None\n      model_loader=<TnXModelLoaders.tnx: 'tnx'>\n      rolling_batch_strategy=<TnXGenerationStrategy.continuous_batching: 'continuous_batching'>\n      fuse_qkv=False\n      on_device_embedding=False\n      attention_layout=None\n      collectives_layout=None\n      cache_layout=None\n      partition_schema=None\n      all_reduce_dtype=None\n      cast_logits_dtype=None\n\n# parameters related to how the final report is generated\nreport:\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-neuron-trn1-32xl-tp16-sm.yml",
      "scrapeId": "63281a9e-3b80-47fb-a327-222ff4b68a88",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-neuron-trn1-32xl-tp16-sm.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-70b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-70b-inf2.48xl-ec2-djl\"\n  model_name: \"llama3-1-70b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-70b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-70B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-70b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"inf2.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-70b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model.\n      model_loading_timeout: 3600\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 24\n      shm_size: 12g\n      model_loading_timeout: 3600\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=meta-llama/Llama-3.1-70b-Instruct\n      option.tensor_parallel_degree=24\n      option.n_positions=4096\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=8\n      option.dtype=fp16\n      option.model_loading_timeout=2400\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O2\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-ec2-llama3-1-70b-inf2-48xl-deploy-ec2-djl.yml",
      "scrapeId": "5cd312d5-6d30-4262-ae52-786415f28899",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-ec2-llama3-1-70b-inf2-48xl-deploy-ec2-djl.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.12xl-tp=4-mc=max-triton-ec2\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    temperature: 0.1\n    do_sample: yes\n    top_p: 0.92\n    top_k: 120\n    bad_words: \"\"\n    stop_words: \"<|eot_id|>\"\n    pad_id: 2\n    end_id: 2\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: http://localhost:8080/v2/models/ensemble/generate\n    instance_type: \"g6e.12xlarge\"\n    image_uri: nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n      batch_size: 256\n\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.12xl-tp-4-mc-max-triton-ec2.yml",
      "scrapeId": "51f886db-ad07-4d38-aaa7-45dd06ee76e3",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.12xl-tp-4-mc-max-triton-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\ngeneral:\n  name: \"Mixtral-8x7B-Instruct-v0.1-AWQ-ec2\"\n  model_name: \"Mixtral-8x7B-Instruct-v0.1-AWQ\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: mistral_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_mistral.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 4500\n    max_length_in_tokens: 5500\n    payload_file: payload_en_4500-5500.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_4500-5500\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Mixtral-8x7B-Instruct-v0.1-AWQ\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: ybelkada/Mixtral-8x7B-Instruct-v0.1-AWQ # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Mixtral-8x7B-Instruct-v0.1-AWQ\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g6e.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=64\n      option.model_id=ybelkada/Mixtral-8x7B-Instruct-v0.1-AWQ\n      option.rolling_batch=vllm\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    - payload_en_4500-5500.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2.5\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mixtral/config-mixtral-8x7b-g6e.48xl-ec2.yml",
      "scrapeId": "58c22fde-4515-41ad-b1aa-d314c1fa92e5",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mixtral/config-mixtral-8x7b-g6e.48xl-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g6e.48xl-tp=4-mc=max-djl-ec2\"\n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g6e.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    - 14\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g6e.48xl-tp-4-mc-max-djl.yml",
      "scrapeId": "6e37e65a-59e0-4465-81cc-7a9be0ffcff2",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g6e.48xl-tp-4-mc-max-djl.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"llama3-8b-inf2-g5-v1\"\n  model_name: \"llama3-8b\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data/Open-Orca\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - OpenOrca.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3_OpenOrca.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the\n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  - system_prompt\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_2000-3000\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: llama-3-8b-instruct-inf2-djl-inference:0.27.0-neuronx-sdk2.18.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: llama-3-8b-instruct\n    model_version:\n    model_name: llama3-8b-inf2.24xl\n    ep_name: \"your-llama3-8b-inf2-24x-large-endpoint-name\"\n    instance_type: \"ml.inf2.24xlarge\"\n    image_uri:\n    deploy: no\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # Make sure your Llama3 model is compiled for a batch size of 4\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  - name: llama-3-8b-instruct-g5-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct-g5.12xl\n    ep_name: llama-3-8b-instruct-g5-12xl\n    instance_type: \"ml.g5.12xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\n# parameters related to how the final report is generated\nreport:\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-inf2-g5-byoe-w-openorca.yml",
      "scrapeId": "5865ebf3-34b7-4def-84cc-8ea04a5aa827",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-inf2-g5-byoe-w-openorca.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"llama3-8b-g5-24xl\"\n  model_name: \"llama3-8b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\nmetrics:\n  dataset_of_interest: en_3000-3840\n\npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\nexperiments:\n  - name: llama-3-8b-instruct-g5-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct-g5.24xl\n    ep_name: llama-3-8b-instruct-g5-24xl\n    instance_type: \"ml.g5.24xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 5\n    - 8\n    - 10\n    - 16\n    - 20\n    - 30\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      # SM_NUM_GPUS: \"8\"\n      # SAGEMAKER_MODEL_SERVER_WORKERS: \"2\"\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-g5-24xl.yml",
      "scrapeId": "52265dbb-e111-4dc9-8c9a-2d2944aa9b0d",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-g5-24xl.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"fmbench-bedrock-anthropic-models\"\n  model_name: \"Sonnet and Haiku on Amazon Bedrock\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n## S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n  prompt_template_dir: prompt_template\n  prompt_template_file: prompt_template_claude.txt ## add your desired prompt template type\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\nmetrics:\n  dataset_of_interest: en_3000-4000\n\n\npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n\ninference_parameters:\n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  # Experiment for claude 3 sonnet\n  - name: anthropic.claude-3-sonnet-20240229-v1:0\n    model_name: anthropic.claude-3-sonnet-20240229-v1:0\n    ep_name: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_type: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  # Experiment for claude 3 haiku\n  - name: anthropic.claude-3-haiku-20240307-v1:0\n    model_name: anthropic.claude-3-haiku-20240307-v1:0\n    ep_name: anthropic.claude-3-haiku-20240307-v1:0\n    instance_type: anthropic.claude-3-haiku-20240307-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n\nreport:\n  latency_budget: 5\n  cost_per_10k_txn_budget: 350\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-haiku-sonnet-majority-voting.yml",
      "scrapeId": "542b2668-7c31-4453-bfca-d3a11a3ccb95",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-haiku-sonnet-majority-voting.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"config-llama3-8b-trn1-32xl-tp=16-bs=4-byoe\"\n  model_name: \"llama3-8b\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data #/Open-Orca\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  #- OpenOrca.jsonl\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n  # prompt_template_Open-Orca-OpenOrca-llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the\n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  #- system_prompt\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    #stop: '<|eot_id|>'\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: llama-3-8b-instruct-inf2-djl-inference:0.27.0-neuronx-sdk2.18.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version:\n    model_name: llama3-8b-trn1.32xl\n    ep_name: <your-endpoint-name>\n    instance_type: \"ml.trn1.32xlarge\"\n    image_uri:\n    deploy: no\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # currently the llama3-8b jumpstart model is compiled with a batch size of 1\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 15\n    - 20\n    - 30\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      model_id_or_path='/tmp/.djl.ai/download/ae03dd100c208acd82b5dbed563c971de864c408'\n      rolling_batch=<RollingBatchEnum.auto: 'auto'>\n      tensor_parallel_degree=16\n      trust_remote_code=False\n      enable_streaming=<StreamingEnum.false: 'false'>\n      batch_size=4\n      max_rolling_batch_size=4\n      dtype=<Dtype.f16: 'fp16'>\n      revision=None\n      output_formatter=None\n      waiting_steps=None\n      is_mpi=False\n      draft_model_id=None\n      spec_length=0\n      neuron_optimize_level=None\n      enable_mixed_precision_accumulation=False\n      enable_saturate_infinity=False\n      n_positions=4096\n      unroll=None\n      load_in_8bit=False\n      low_cpu_mem_usage=False\n      load_split_model=True\n      context_length_estimate=None\n      amp='f16'\n      quantize=None\n      compiled_graph_path=None\n      task=None\n      save_mp_checkpoint_path=None\n      group_query_attention=None\n      model_loader=<TnXModelLoaders.tnx: 'tnx'>\n      rolling_batch_strategy=<TnXGenerationStrategy.continuous_batching: 'continuous_batching'>\n      fuse_qkv=False\n      on_device_embedding=False\n      attention_layout=None\n      collectives_layout=None\n      cache_layout=None\n      partition_schema=None\n      all_reduce_dtype=None\n      cast_logits_dtype=None\n\n# parameters related to how the final report is generated\nreport:\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 2\n  cost_per_10k_txn_budget: 25\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-trn1-32xl-tp-16-bs-4-byoe.yml",
      "scrapeId": "57692694-0808-42aa-a016-eb8eee95d024",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-trn1-32xl-tp-16-bs-4-byoe.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g6e.2xl-ec2_ollama\"\n  model_name: \"llama3.1:8b\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_ollama:\n    model: llama3.1:8b\n    stream: false\n    options:\n      temperature: 0.1\n      top_p: 0.92\n      top_k: 120\n      num_predict: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3.1:8b\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    hf_tokenizer_model_id: meta-llama/Llama-3.1-8B-Instruct\n    model_id: llama3.1:8b # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: llama3.1:8b\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:11434/api/generate'\n    instance_type: \"g6e.2xlarge\"\n    image_uri: ec2\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_ollama\n      container_type: ollama\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g6e.2xl-ollama.yml",
      "scrapeId": "6053b5c6-68b8-401b-9178-9aed0de0ee00",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g6e.2xl-ollama.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g5.12xl-ec2\"\n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 924\n    payload_file: payload_en_1-924.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g5.12xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: auto\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.12xl-tp-2-mc-auto-ec2.yml",
      "scrapeId": "55330447-1bc9-4eca-95bb-e30674162a64",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.12xl-tp-2-mc-auto-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.2-1b-m7a.24xl-ec2-summarization\"\n  model_name: \"llama3.2-1b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - synthetic_data_large_prompts.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3_summarization.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 2500\n    max_length_in_tokens: 3500\n    payload_file: payload_en_2500-3500.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_2500-3500\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_vllm:\n    model: meta-llama/Llama-3.2-1b-Instruct\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.2-1b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.2-1b-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.2-1b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8000/v1/completions'\n    instance_type: \"m7a.24xlarge\"\n    image_uri: vllm-cpu-env\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # Privileged Mode makes the docker container run with root.\n      # This basically means that if you are root in a container you have the privileges of root on the host system\n      # Only need this if you need to set VLLM_CPU_OMP_THREADS_BIND env variable.\n      privileged_mode: yes\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting:\n      # This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_vllm\n      # if not set assume djl\n      container_type: vllm\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_2500-3500.jsonl\n    - payload_en_3000-4000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      MODEL_LOADING_TIMEOUT: 2400\n      VLLM_CPU_KVCACHE_SPACE: 192\n      # This instance is equipped with 96 CPUs, and we are allocating 93 of them to run this container.\n      # For additional details, refer to the following URL:\n      # https://docs.vllm.ai/en/latest/getting_started/cpu-installation.html#related-runtime-environment-variables\n      VLLM_CPU_OMP_THREADS_BIND: 0-92\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 60\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.2/1b/config-llama3.2-1b-m7a-24xlarge-ec2-summarization.yml",
      "scrapeId": "5ea23253-ff64-4f85-809f-99c0890719c8",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.2/1b/config-llama3.2-1b-m7a-24xlarge-ec2-summarization.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-p5.2xl-ec2\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 924\n    payload_file: payload_en_1-924.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"p5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 10\n    - 30\n    - 50\n    - 60\n    - 65\n    - 70\n    - 75\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-p5-tp-2-mc-max.yml",
      "scrapeId": "77372b14-4f12-480d-bdb8-1233c0523241",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-p5-tp-2-mc-max.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-8b-inf2.48xl-ec2\"\n  model_name: \"llama3-1-8b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: no\n  1_generate_data.ipynb: no\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: no\n  4_model_metric_analysis.ipynb: no\n  5_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-8b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"inf2.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and Inferentia settings for the instance\n      # '-u djl' tells the container to use the Deep Java Library (DJL) runtime\n      # '--device /dev/neuron0 ... /dev/neuron5' specifies the Inferentia devices to be used\n      gpu_or_neuron_setting: -u djl --device /dev/neuron0  --device /dev/neuron1  --device /dev/neuron2  --device /dev/neuron3  --device /dev/neuron4 --device /dev/neuron5 --device /dev/neuron6 --device /dev/neuron7 --device /dev/neuron8 --device /dev/neuron9 --device /dev/neuron10 --device /dev/neuron11\n      #This setting specifies the timeout (in seconds) for loading the model.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=meta-llama/Meta-Llama-3.1-8b-Instruct\n      option.load_split_model=True\n      option.tensor_parallel_degree=8\n      option.n_positions=4096\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=8\n      option.dtype=fp16\n      option.model_loading_timeout=2400\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O3\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/server-config-ec2-llama3-1-8b-inf2-48xl-deploy-ec2.yml",
      "scrapeId": "6515a520-a9a2-46a9-adf0-61de706f6d18",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/server-config-ec2-llama3-1-8b-inf2-48xl-deploy-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.24xl-tp=4-mc=max-djl-ec2\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g6e.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.24xl-tp-4-mc-max-djl-ec2.yml",
      "scrapeId": "71ad5687-3bb7-4da0-9763-bc485a7cdad2",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.24xl-tp-4-mc-max-djl-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.48xl-tp=4-mc=max-djl-ec2\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g6e.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.48xl-tp-8-mc-max-djl-ec2.yml",
      "scrapeId": "6bbeaa31-aa20-429d-ba79-cd918e12c216",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.48xl-tp-8-mc-max-djl-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-8b-inf2.48xl-ec2\"\n  model_name: \"llama3-8b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 924\n    payload_file: payload_en_1-924.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-8b-instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3-8b-instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"inf2.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.28.0-neuronx-sdk2.18.2\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and Inferentia settings for the instance\n      # '-u djl' tells the container to use the Deep Java Library (DJL) runtime\n      # '--device /dev/neuron0 ... /dev/neuron5' specifies the Inferentia devices to be used\n      gpu_or_neuron_setting: -u djl --device /dev/neuron0 --device /dev/neuron1 --device /dev/neuron2  --device /dev/neuron3  --device /dev/neuron4  --device /dev/neuron5 --device /dev/neuron6 --device /dev/neuron7 --device /dev/neuron8 --device /dev/neuron9 --device /dev/neuron10 --device /dev/neuron11\n      #This setting specifies the timeout (in seconds) for loading the model.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.load_split_model=True\n      option.tensor_parallel_degree=8\n      option.n_positions=4096\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=8\n      option.dtype=fp16\n      option.model_loading_timeout=2400\n      option.neuron_optimize_level=3\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-inf2-48xl.yml",
      "scrapeId": "60f09d53-3991-4241-ad4b-21f28107777e",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-inf2-48xl.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-8b-inf2.24xl-ec2\"\n  model_name: \"llama3-8b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    #stop: '<|eot_id|>'\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-8b-instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    sagemaker_execution_role: {role_arn}\n    # S3 bucket to which metrics, plots and reports would be written to\n    bucket: {write_bucket} ## add the name of your desired bucket\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    prefix: \"lmi\"\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3-8b-instruct\"\n    model_id_wo_repo: \"Meta-Llama-3-8B-Instruct\"\n    model_id_wo_repo_split: \"Meta-Llama-3-8B-Instruct-split\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name:\n    instance_type: \"ml.inf2.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.28.0-neuronx-sdk2.18.2\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: neuron_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: sagemaker_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      batch_size: \"4\"\n      num_neuron_cores: \"8\"\n      neuron_version: \"2.18\"\n      model_loading_timeout: \"2400\"\n      instance_count: \"1\"\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=s3://{write_bucket}/lmi/Meta-Llama-3-8B-Instruct/Meta-Llama-3-8B-Instruct-split/\n      option.load_split_model=True\n      option.tensor_parallel_degree=8\n      option.n_positions=4096\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=8\n      option.dtype=fp16\n      option.model_loading_timeout=2400\n      option.neuron_optimize_level=3\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-neuron-llama3-8b-inf2-24xl-deploy-sm.yml",
      "scrapeId": "660bac46-4b4b-432f-ab99-f4bbac293af8",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-neuron-llama3-8b-inf2-24xl-deploy-sm.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-p4de.24xl-ec2\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"p4de.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 10\n    - 30\n    - 35\n    - 40\n    - 50\n    - 60\n    - 65\n    - 70\n    - 75\n    - 100\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-p4de-tp-4-mc-max.yml",
      "scrapeId": "681367bf-3909-4b64-9956-8ad244319a40",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-p4de-tp-4-mc-max.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.2-1b-m7i.12xl-ec2\"\n  model_name: \"llama3.2-1b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_vllm:\n    model: meta-llama/Llama-3.2-1b-Instruct\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.2-1b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.2-1b-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.2-1b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8000/v1/completions'\n    instance_type: \"m7i.12xlarge\"\n    image_uri: vllm-cpu-env\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # Privileged Mode makes the docker container run with root.\n      # This basically means that if you are root in a container you have the privileges of root on the host system\n      # Only need this if you need to set VLLM_CPU_OMP_THREADS_BIND env variable.\n      privileged_mode: yes\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting:\n      # This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_vllm\n      # if not set assume djl\n      container_type: vllm\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      VLLM_CPU_KVCACHE_SPACE: 40\n      # This instance has 48 CPUs, and we are allocating 40 of them to run this container.\n      # For more information, visit the following URL:\n      # https://docs.vllm.ai/en/latest/getting_started/cpu-installation.html#related-runtime-environment-variables\n      VLLM_CPU_OMP_THREADS_BIND: 0-40\n\nreport:\n  latency_budget: 10\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.2/1b/config-llama3.2-1b-m7i-12xlarge-ec2.yml",
      "scrapeId": "67d31594-f4e1-448a-bf35-04cd62ee9a38",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.2/1b/config-llama3.2-1b-m7i-12xlarge-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"llama3-8b-trn1-byoe\"\n  model_name: \"llama3-8b\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data #/Open-Orca\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  #- OpenOrca.jsonl\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n  # prompt_template_Open-Orca-OpenOrca-llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the\n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  #- system_prompt\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    #stop: '<|eot_id|>'\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: llama-3-8b-instruct-inf2-djl-inference:0.27.0-neuronx-sdk2.18.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version:\n    model_name: llama3-8b-trn1.32xl\n    ep_name: <your-endpoint-name>\n    instance_type: \"ml.trn1.32xlarge\"\n    image_uri:\n    deploy: no\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # currently the llama3-8b jumpstart model is compiled with a batch size of 1\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 8\n    - 10\n    - 15\n    - 20\n    - 30\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      model_id_or_path='/tmp/.djl.ai/download/ae03dd100c208acd82b5dbed563c971de864c408'\n      rolling_batch=<RollingBatchEnum.auto: 'auto'>\n      tensor_parallel_degree=8\n      trust_remote_code=False\n      enable_streaming=<StreamingEnum.false: 'false'>\n      batch_size=4\n      max_rolling_batch_size=4\n      dtype=<Dtype.f16: 'fp16'>\n      revision=None\n      output_formatter=None\n      waiting_steps=None\n      is_mpi=False\n      draft_model_id=None\n      spec_length=0\n      neuron_optimize_level=None\n      enable_mixed_precision_accumulation=False\n      enable_saturate_infinity=False\n      n_positions=4096\n      unroll=None\n      load_in_8bit=False\n      low_cpu_mem_usage=False\n      load_split_model=True\n      context_length_estimate=None\n      amp='f16'\n      quantize=None\n      compiled_graph_path=None\n      task=None\n      save_mp_checkpoint_path=None\n      group_query_attention=None\n      model_loader=<TnXModelLoaders.tnx: 'tnx'>\n      rolling_batch_strategy=<TnXGenerationStrategy.continuous_batching: 'continuous_batching'>\n      fuse_qkv=False\n      on_device_embedding=False\n      attention_layout=None\n      collectives_layout=None\n      cache_layout=None\n      partition_schema=None\n      all_reduce_dtype=None\n      cast_logits_dtype=None\n\n# parameters related to how the final report is generated\nreport:\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-trn1-32xl-tp-8-bs-4-byoe.yml",
      "scrapeId": "7534f4b2-e839-484f-a8df-df1992611d26",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-trn1-32xl-tp-8-bs-4-byoe.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"fmbench-bedrock-claude-realworldqa\"\n  model_name: \"Anthropic vision models on Amazon Bedrock\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the https://huggingface.co/datasets/derek-thomas/ScienceQA\n  # This dataset contains images containing science questions and associated ground truth responses to the questions.\n  # when this notebook looks for this dataset - if it is prefixed with hf, and if the token is already there\n  # if the token is there and the hf: is there, then download the dataset from hf first and then process it -\n  source_data_files:\n  # If a split is specified in the dataset identifier and exists in the loaded dataset,\n  # it uses that split. If no split is specified (e.g., hf:derek-thomas/ScienceQA), it will default to\n  # the first available split. In the given example below, users can specify the 'train' and 'validation' splits\n  # of the 'derek-thomas/ScienceQA' dataset. If none are provided, then the first available split will be used.\n\n  # Follow this format below: hf:dataset-id/subset-name/split-name.\n  # If there is no specified subset name, use \"default\"\n  - hf:derek-thomas/ScienceQA/default/train\n  - hf:derek-thomas/ScienceQA/default/validation\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_claude_images_ScienceQA.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: no\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  # This represents the question that is asked as a part of the\n  # image messages API format. If this is not provided, a description\n  # of the image will be provided instead\n  - question\n  # If you want to benchmark a multimodal model on an image dataset,\n  # then it an image_col parameter is required. This parameter refers to the\n  # name of the column in the dataset that contains the images to be used\n  # during the benchmarking process. If this column is not provided, the\n  # standard text generation benchmark process is be used in the FMBench run.\n  image_col: image\n  ground_truth_col_key: solution\n  question_col_key: question\n  # This is the number of rows of the dataset that you want to load\n  # if this parameter is not given, it is defaulted to 100 rows\n  ds_N: 150\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1-500\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: anthropic.claude-3-sonnet-20240229-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: anthropic.claude-3-sonnet-20240229-v1:0\n    model_version:\n    model_name: anthropic.claude-3-sonnet-20240229-v1:0\n    ep_name: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_type: anthropic.claude-3-sonnet-20240229-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the\n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n\n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  - name: anthropic.claude-3-5-sonnet-20240620-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: anthropic.claude-3-5-sonnet-20240620-v1:0\n    model_version:\n    model_name: anthropic.claude-3-5-sonnet-20240620-v1:0\n    ep_name: anthropic.claude-3-5-sonnet-20240620-v1:0\n    instance_type: anthropic.claude-3-5-sonnet-20240620-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the\n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n\n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\nreport:\n  latency_budget: 1\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/multimodal/bedrock/config-claude-scienceqa.yml",
      "scrapeId": "6f3610cd-ba1f-430f-8b8f-ae691528627f",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/multimodal/bedrock/config-claude-scienceqa.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g5.4xl-tp=1-mc=max-triton-ec2\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    temperature: 0.1\n    do_sample: yes\n    top_p: 0.92\n    top_k: 120\n    bad_words: \"\"\n    stop_words: \"<|eot_id|>\"\n    pad_id: 2\n    end_id: 2\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: http://localhost:8080/v2/models/ensemble/generate\n    instance_type: \"g5.4xlarge\"\n    image_uri: nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 1\n      shm_size: 12g\n      batch_size: 64\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g5.4xl-tp-1-mc-max-triton-ec2.yml",
      "scrapeId": "7569e635-589a-4891-b583-01fb5abbcc15",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g5.4xl-tp-1-mc-max-triton-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama3.1-8b deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g6e.2xl-ec2\"\n  model_name: \"llama3.1-8b\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: no\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_ollama:\n    model: llama3.1:8b\n    stream: false\n    options:\n      temperature: 0.1\n      top_p: 0.92\n      top_k: 120\n      num_predict: 100\n\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3.1-8b\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"meta-llama/Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:11434/api/generate'\n    instance_type: \"g6e.2xlarge\"\n    image_uri: None\n    deploy: no #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # Privileged Mode makes the docker container run with root.\n      # This basically means that if you are root in a container you have the privileges of root on the host system\n      # Only need this if you need to set VLLM_CPU_OMP_THREADS_BIND env variable.\n      privileged_mode: yes\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting:\n      # This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_ollama\n      # if not set assume djl\n      container_type: ollama\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-g6e-2xlarge-byoe-ollama.yml",
      "scrapeId": "7a077514-a254-4d8b-8484-5884222bcf5e",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-g6e-2xlarge-byoe-ollama.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n#----------------------------------------------------------------------\n# NOTE: For deployment instructions, refer to the DoEKS website.\n#----------------------------------------------------------------------\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: llama3\n\n---\napiVersion: ray.io/v1\nkind: RayService\nmetadata:\n  name: llama3-service\n  namespace: llama3\nspec:\n  serviceUnhealthySecondThreshold: 900\n  deploymentUnhealthySecondThreshold: 900\n  serveConfigV2: |\n    applications:\n      - name: llama3\n        import_path: ray_serve_llama3:entrypoint\n        runtime_env:\n          env_vars:\n            MODEL_ID: \"meta-llama/Meta-Llama-3-8B-Instruct\"\n            HUGGING_FACE_HUB_TOKEN: $HUGGING_FACE_HUB_TOKEN\n            LD_LIBRARY_PATH: \"/home/ray/anaconda3/lib\"\n  rayClusterConfig:\n    rayVersion: '2.11.0'\n    headGroupSpec:\n      headService:\n        metadata:\n          name: llama3-service\n          namespace: llama3\n      rayStartParams:\n        dashboard-host: '0.0.0.0'\n      template:\n        spec:\n          containers:\n          - name: ray-head\n            image: public.ecr.aws/data-on-eks/ray-serve-inf2-llama3:latest\n            imagePullPolicy: Always\n            lifecycle:\n              preStop:\n                exec:\n                  command: [\"/bin/sh\", \"-c\", \"ray stop\"]\n            ports:\n            - containerPort: 6379\n              name: gcs-server\n            - containerPort: 8265\n              name: dashboard\n            - containerPort: 10001\n              name: client\n            - containerPort: 8000\n              name: serve\n            volumeMounts:\n            - mountPath: /tmp/ray\n              name: ray-logs\n            resources:\n              limits:\n                cpu: \"4\"\n                memory: \"20G\"\n              requests:\n                cpu: \"4\"\n                memory: \"20G\"\n            env:\n              - name: LD_LIBRARY_PATH\n                value: \"/home/ray/anaconda3/lib\"\n          nodeSelector:\n            instanceType: mixed-x86\n            provisionerType: Karpenter\n            workload: rayhead\n          volumes:\n          - name: ray-logs\n            emptyDir: {}\n\n    workerGroupSpecs:\n    - groupName: inf2-worker-group\n      replicas: 1\n      minReplicas: 1\n      maxReplicas: 1\n      rayStartParams: {}\n      template:\n        spec:\n          containers:\n          - name: ray-worker\n            image: public.ecr.aws/data-on-eks/ray-serve-inf2-llama3:latest\n            imagePullPolicy: Always\n            lifecycle:\n              preStop:\n                exec:\n                  command: [\"/bin/sh\", \"-c\", \"ray stop\"]\n            resources:\n              limits:\n                cpu: \"180\"\n                memory: \"700G\"\n                aws.amazon.com/neuron: \"12\"\n              requests:\n                cpu: \"180\"\n                memory: \"700G\"\n                aws.amazon.com/neuron: \"12\"\n            env:\n              - name: LD_LIBRARY_PATH\n                value: /home/ray/anaconda3/lib\n          nodeSelector:\n            instanceType: inferentia-inf2\n            provisionerType: Karpenter\n          tolerations:\n          - key: \"aws.amazon.com/neuron\"\n            operator: \"Exists\"\n            effect: \"NoSchedule\"\n          - key: \"hub.jupyter.org/dedicated\"\n            operator: \"Equal\"\n            value: \"user\"\n            effect: \"NoSchedule\"\n\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: llama3-ingress\n  namespace: llama3\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: \"/$1\"\nspec:\n  ingressClassName: nginx\n  rules:\n  - http:\n      paths:\n      - path: /dashboard/(.*)\n        pathType: ImplementationSpecific\n        backend:\n          service:\n            name: llama3-service\n            port:\n              number: 8265\n      - path: /serve/(.*)\n        pathType: ImplementationSpecific\n        backend:\n          service:\n            name: llama3-service\n            port:\n              number: 8000\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/eks_manifests/llama3-ray-service.yaml",
      "scrapeId": "686f3d83-13e3-4554-bd26-f648cc3e7b8f",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/eks_manifests/llama3-ray-service.yaml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"llama3-70b-p4d-v1\"\n  model_name: \"llama3-70b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_70b_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\nmetrics:\n  dataset_of_interest: en_3000-3840\n\npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\nexperiments:\n\n  - name: llama-3-70b-instruct-p4d-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-70b-instruct\n    model_version: \"*\"\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-70B-Instruct\n    model_name: llama-3-70b-instruct\n    ep_name: llama-3-70b-instruct-p4d\n    instance_type: \"ml.p4d.24xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 12\n    - 15\n\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"8\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/70b/config-llama3-70b-instruct-p4d.yml",
      "scrapeId": "6a84e827-85fe-4d23-aa44-2230ea6a48d6",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/70b/config-llama3-70b-instruct-p4d.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"llama3-1-8b-g5-12xlarge-djl-lmi\"\n  model_name: \"llama3-1-8b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\nmetrics:\n  dataset_of_interest: en_3000-3840\n\npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n    return_full_text: False\n\nexperiments:\n\n  - name: llama-3-1-8b-instruct-g6-48xl\n    model_id: meta-llama/Llama-3.1-8B-Instruct\n    model_version: \"*\"\n    model_name: Llama-3-1-8B-Instruct\n    ep_name: Llama-3-1-8B-Instruct\n    instance_type: \"ml.g5.12xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124'\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.p\n      y\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      #HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      # SM_NUM_GPUS: \"1\"\n      # SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3-1-8b-g5.12xlarge-djl-lmi-sm.yml",
      "scrapeId": "695fa0cb-6301-4c8c-bef4-c2577d7df3f9",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3-1-8b-g5.12xlarge-djl-lmi-sm.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-70bdeployed on ec2\ngeneral:\n  name: \"llama3-70b-instruct-g5.48xl-ec2\"\n  model_name: \"Meta-Llama-3-70B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 924\n    payload_file: payload_en_1-924.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_tokens: 4096\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-70B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-70B-Instruct\n    model_version:\n    model_name: Meta-Llama-3-70B-Instruct\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 24 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 24g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 7200\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3-70B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/70b/config-ec2-llama3-70b-instruct.yml",
      "scrapeId": "70970d21-99a6-43af-86b8-6db05018e882",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/70b/config-ec2-llama3-70b-instruct.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.2-1b-m7a.24xl-ec2\"\n  model_name: \"llama3.2-1b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_vllm:\n    model: meta-llama/Llama-3.2-1b-Instruct\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.2-1b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.2-1b-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.2-1b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8000/v1/completions'\n    instance_type: \"m7a.24xlarge\"\n    image_uri: vllm-cpu-env\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # Privileged Mode makes the docker container run with root.\n      # This basically means that if you are root in a container you have the privileges of root on the host system\n      # Only need this if you need to set VLLM_CPU_OMP_THREADS_BIND env variable.\n      privileged_mode: yes\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting:\n      # This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_vllm\n      # if not set assume djl\n      container_type: vllm\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      MODEL_LOADING_TIMEOUT: 2400\n      VLLM_CPU_KVCACHE_SPACE: 192\n      # This instance is equipped with 96 CPUs, and we are allocating 93 of them to run this container.\n      # For additional details, refer to the following URL:\n      # https://docs.vllm.ai/en/latest/getting_started/cpu-installation.html#related-runtime-environment-variables\n      VLLM_CPU_OMP_THREADS_BIND: 0-92\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 60\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.2/1b/config-llama3.2-1b-m7a-24xlarge-ec2.yml",
      "scrapeId": "6adc2a0e-bd27-41df-a7e0-db214d5015bc",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.2/1b/config-llama3.2-1b-m7a-24xlarge-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"llama3-8b-inf2-48xl-byoe\"\n  model_name: \"llama3-8b\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data #/Open-Orca\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  #- OpenOrca.jsonl\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n  # prompt_template_Open-Orca-OpenOrca-llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the\n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  #- system_prompt\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: llama-3-8b-instruct-inf2-djl-inference:0.27.0-neuronx-sdk2.18.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version:\n    model_name: llama3-8b-inf2.24xl\n    ep_name: <your-endpoint-name>\n    instance_type: \"ml.inf2.48xlarge\"\n    image_uri:\n    deploy: no\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # currently the llama3-8b jumpstart model is compiled with a batch size of 1\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 8\n    - 10\n    - 15\n    - 20\n    - 30\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      model_id_or_path='/tmp/.djl.ai/download/ae03dd100c208acd82b5dbed563c971de864c408'\n      rolling_batch=<RollingBatchEnum.auto: 'auto'>\n      tensor_parallel_degree=8\n      trust_remote_code=False\n      enable_streaming=<StreamingEnum.false: 'false'>\n      batch_size=4\n      max_rolling_batch_size=4\n      dtype=<Dtype.f16: 'fp16'>\n      revision=None\n      output_formatter=None\n      waiting_steps=None\n      is_mpi=False\n      draft_model_id=None\n      spec_length=0\n      neuron_optimize_level=None\n      enable_mixed_precision_accumulation=False\n      enable_saturate_infinity=False\n      n_positions=4096\n      unroll=None\n      load_in_8bit=False\n      low_cpu_mem_usage=False\n      load_split_model=True\n      context_length_estimate=None\n      amp='f16'\n      quantize=None\n      compiled_graph_path=None\n      task=None\n      save_mp_checkpoint_path=None\n      group_query_attention=None\n      model_loader=<TnXModelLoaders.tnx: 'tnx'>\n      rolling_batch_strategy=<TnXGenerationStrategy.continuous_batching: 'continuous_batching'>\n      fuse_qkv=False\n      on_device_embedding=False\n      attention_layout=None\n      collectives_layout=None\n      cache_layout=None\n      partition_schema=None\n      all_reduce_dtype=None\n      cast_logits_dtype=None\n\n# parameters related to how the final report is generated\nreport:\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-inf2-48xl-tp%3D8-bs%3D4-byoe.yml",
      "scrapeId": "757081ae-3104-4f47-8535-61b03f486e62",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-inf2-48xl-tp%3D8-bs%3D4-byoe.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-8b-inf2.48xl-ec2\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n    return_full_text: no\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"inf2.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and Inferentia settings for the instance\n      # '-u djl' tells the container to use the Deep Java Library (DJL) runtime\n      # '--device /dev/neuron0 ... /dev/neuron5' specifies the Inferentia devices to be used\n      gpu_or_neuron_setting: -u djl --device /dev/neuron0 --device /dev/neuron1 --device /dev/neuron2 --device /dev/neuron3 --device /dev/neuron4 --device /dev/neuron5 --device /dev/neuron6 --device /dev/neuron7 --device /dev/neuron8 --device /dev/neuron9 --device /dev/neuron10 --device /dev/neuron11\n      #This setting specifies the timeout (in seconds) for loading the model.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 24\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.tensor_parallel_degree=24\n      option.n_positions=4096\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=4\n      option.model_loading_timeout=2400\n      option.group_query_attention=replicated-heads\n      option.attention_layout=BSH\n      option.fuse_qkv=True\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n    # - 7\n    # - 8\n    # - 9\n    # - 10\n    # - 11\n    # - 12\n    # - 15\n    # - 20\n    # - 25\n    # - 30\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O3\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-inf2-48xl-deploy-tp-24-ec2.yml",
      "scrapeId": "7731933c-3b57-4928-b5cb-cc77e9108d74",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-inf2-48xl-deploy-tp-24-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-8b-trn1.32xl-ec2-triton\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    top_k: 50\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct-triton\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8080/v2/models/Meta-Llama-3-8B-Instruct/generate'\n    instance_type: \"trn1.32xlarge\"\n    image_uri: tritonserver-neuronx:fmbench\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and Inferentia settings for the instance\n      # '-u djl' tells the container to use the Deep Java Library (DJL) runtime\n      # '--device /dev/neuron0 ... /dev/neuron5' specifies the Inferentia devices to be used\n      gpu_or_neuron_setting: -u triton --device /dev/neuron0 --device /dev/neuron1 --device /dev/neuron2 --device /dev/neuron3 --device /dev/neuron4 --device /dev/neuron5 --device /dev/neuron6 --device /dev/neuron7 --device /dev/neuron8 --device /dev/neuron9 --device /dev/neuron10 --device /dev/neuron11 --device /dev/neuron12 --device /dev/neuron13 --device /dev/neuron14 --device /dev/neuron15\n      #This setting specifies the timeout (in seconds) for loading the model.\n      model_loading_timeout: 8000\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # mention the backend type, if any\n      backend: vllm\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured,\n      # max: FMBench figures out the max number of model containers to be run\n      #      based on TP degree configured and number of neuron cores/GPUs available.\n      #      For example, if TP=2, GPUs=8 then FMBench will start 4 containers and 1 load balancer,\n      # auto: only supported if the underlying inference container would automatically\n      #       start multiple copies of the model internally based on TP degree and neuron cores/GPUs\n      #       available. In this case only a single container is created, no load balancer is created.\n      #       The DJL serving containers supports auto.\n      model_copies: max\n      shm_size: 12g\n      model_loading_timeout: 2400\n      # The model.json parameters are replaced within the model.json file\n      # for the triton on vllm/djl/tensorrt options. The model.json already contains\n      # the tp degree and model id from above in this config file. This is a dictionary\n      # that contains other variables, such as max_num_seqs, dtype, max_model_len, block_size and more\n      # For tensorrt, the tp degree, batch size and other relevant parameters are\n      # extracted directly from the inference spec.\n      container_params:\n        # if you set the model_copies parameter then it is mandatory to set the\n        # tp_degree, shm_size, model_loading_timeout parameters\n        tp_degree: 8\n        # The model.json parameters are replaced within the model.json file\n        # for the triton on vllm option. The model.json already contains\n        # the tp degree and model id from above in this config file. This is a dictionary\n        # that contains other variables, such as max_num_seqs, dtype, max_model_len, block_size and more\n        max_num_seqs: 4\n        dtype: \"float16\"\n        # The max_model_len and block_size arguments are required to be same as\n        # max sequence length when targeting neuron device.\n        # Currently, this is a known limitation in continuous batching support\n        # in transformers-neuronx. View the latest vllm documentation: https://vllm.readthedocs.io/_/downloads/en/stable/pdf/\n        max_model_len: 4096\n        block_size: 4096\n        # modify the serving properties to match your model and requirements\n        serving.properties:\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n    - 7\n    - 8\n    - 9\n    - 10\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-trn1-32xlarge-triton-vllm.yml",
      "scrapeId": "7ce31ad7-721b-42db-b84a-56282bac6fdc",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-trn1-32xlarge-triton-vllm.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g5.xl-tp=1-mc=max-ec2\"\n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_2000-3000\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g5.xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 1\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=1\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.xl-tp-1-mc-max-ec2-conc-1-2.yml",
      "scrapeId": "7cbd5613-272a-48b6-92d6-54dadd622bbc",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.xl-tp-1-mc-max-ec2-conc-1-2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-p4de.24xl-ec2\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"p4de.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 10\n    - 30\n    - 35\n    - 40\n    - 50\n    - 60\n    - 65\n    - 70\n    - 75\n    - 100\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-p4de-tp-8-mc-max.yml",
      "scrapeId": "759cb6bd-9096-4c00-9137-16d6e0196fd0",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-p4de-tp-8-mc-max.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.2xl-ec2\"\n  model_name: \"llama3-8b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-8b-instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3-8b-instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g6e.2xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      tp_degree: 1\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=1\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-g6e-2xlarge.yml",
      "scrapeId": "83d85cde-babc-40fb-9a7f-59b8850f2d82",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-g6e-2xlarge.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"llama3-8b-g6-48xl-djl-lmi-dist\"\n  model_name: \"llama3-8b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\nmetrics:\n  dataset_of_interest: en_3000-3840\n\npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\nexperiments:\n\n  - name: llama-3-8b-instruct-g6-48xl-djl-deepspeed0.12.6-cu121\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct\n    ep_name: llama-3-8b-instruct-g6-48xl\n    instance_type: \"ml.g6.48xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.max_rolling_batch_size=256\n      option.rolling_batch=lmi-dist\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 15\n    - 20\n    - 30\n\n    accept_eula: true\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      job_queue_size: 1000\n      max_dynamic_batch_size: 1\n      max_batch_delay: 100\n      max_idle_time: 60\n      load_on_devices: *\n      engine: MPI\n      mpi_mode: true\n      option.entryPoint: null\n      option.tensor_parallel_degree: 8\n      option.max_rolling_batch_size: 256\n      option.mpi_mode: true\n      option.model_id: /opt/ml/model\n      option.rolling_batch: lmi-dist\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      #HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      # SM_NUM_GPUS: \"1\"\n      # SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-g6-48xl.yml",
      "scrapeId": "79f75db6-43d8-4632-be78-18f92fa8354e",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-g6-48xl.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.12xl-tp=4-mc=max-djl-ec2\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g6e.12xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.12xl-tp-4-mc-max-djl-ec2.yml",
      "scrapeId": "7a918fdf-3be9-454b-9c4f-9c17dca0d33d",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.12xl-tp-4-mc-max-djl-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-c5.18xl-ec2\"\n  model_name: \"llama3-8b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_vllm:\n    model: meta-llama/Meta-Llama-3-8B-Instruct\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-8b-instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3-8b-instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8000/v1/completions'\n    instance_type: \"c5.18xlarge\"\n    image_uri: vllm-cpu-env\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # Privileged Mode makes the docker container run with root.\n      # This basically means that if you are root in a container you have the privileges of root on the host system\n      # Only need this if you need to set VLLM_CPU_OMP_THREADS_BIND env variable.\n      privileged_mode: yes\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting:\n      # This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_vllm\n      # if not set assume djl\n      container_type: vllm\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      MODEL_LOADING_TIMEOUT: 2400\n      VLLM_CPU_KVCACHE_SPACE: 40\n      # This instance has 72 CPUs, and we are allocating 70 of them to run this container.\n      # For more information, visit the following URL:\n      # https://docs.vllm.ai/en/latest/getting_started/cpu-installation.html#related-runtime-environment-variables\n      VLLM_CPU_OMP_THREADS_BIND: 0-69\n\nreport:\n  latency_budget: 25\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-c5-18xlarge.yml",
      "scrapeId": "82f94c35-2fd4-4be9-9ca6-7f6f5d471240",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-c5-18xlarge.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-8b-inf2.48xl-ec2-triton-djl\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    top_k: 50\n    max_tokens: 100\n    max_model_len: 4096\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct-triton\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8080/v2/models/Meta-Llama-3-8B-Instruct/generate'\n    instance_type: \"inf2.48xlarge\"\n    image_uri: tritonserver-neuronx:fmbench\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model.\n      model_loading_timeout: 8000\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # For deploying a model using the triton inference container:\n      # mention the backend type, if any. Options for triton are: [djl, vllm or tensorrt]\n      backend: djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured,\n      # max: FMBench figures out the max number of model containers to be run\n      #      based on TP degree configured and number of neuron cores/GPUs available.\n      #      For example, if TP=2, GPUs=8 then FMBench will start 4 containers and 1 load balancer,\n      # auto: only supported if the underlying inference container would automatically\n      #       start multiple copies of the model internally based on TP degree and neuron cores/GPUs\n      #       available. In this case only a single container is created, no load balancer is created.\n      #       The DJL serving containers supports auto.\n      model_copies: max\n      shm_size: 12g\n      # The model.json parameters are replaced within the model.json file\n      # for the triton on vllm/djl/tensorrt options. The model.json already contains\n      # the tp degree and model id from above in this config file. This is a dictionary\n      # that contains other variables, such as max_num_seqs, dtype, max_model_len, block_size and more\n      # For tensorrt, the tp degree, batch size and other relevant parameters are\n      # extracted directly from the inference spec.\n      container_params:\n        # tp degree is a mandatory parameter\n        tp_degree: 8\n        amp: \"f16\"\n        attention_layout: 'BSH'\n        collectives_layout: 'BSH'\n        context_length_estimate: 3072, 3584, 4096\n        max_rolling_batch_size: 8\n        model_loader: \"tnx\"\n        model_loading_timeout: 2400\n        n_positions: 4096\n        output_formatter: \"json\"\n        rolling_batch: \"auto\"\n        rolling_batch_strategy: \"continuous_batching\"\n        trust_remote_code: true\n        # modify the serving properties to match your model and requirements\n        serving.properties:\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 8\n    - 10\n    - 12\n    - 14\n    - 15\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-inf2-48xlarge-triton-djl.yml",
      "scrapeId": "7e42bee6-1e81-4142-8008-99677825e397",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-inf2-48xlarge-triton-djl.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-70b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-70b-p5.48xl-ec2-longbench\"\n  model_name: \"llama3-1-70b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-70b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-70B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-70b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"p5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged witsh multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-70b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-70b-Instruct\n      option.rolling_batch=lmi-dist\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 10\n    - 15\n    - 20\n    - 25\n    - 30\n    - 33\n    - 35\n    - 40\n    - 45\n    - 50\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 200\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-llama3-1-70b-p5-djl-lmi.yml",
      "scrapeId": "7a384ae8-6231-4084-ad01-76a52d07749e",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-llama3-1-70b-p5-djl-lmi.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"mistral-7b-p5-instruct-AWQ\"\n  model_name: \"mistral7bInstruct-AWQ\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: mistral_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_mistral.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n  - language: en\n    min_length_in_tokens: 1600\n    max_length_in_tokens: 5000\n    payload_file: payload_en_1600-5000.jsonl\n\nmetrics:\n  dataset_of_interest: en_1000-2000\n\npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Model configurations\nexperiments:\n  - name: mistral-Instruct7b-p5\n    model_id: TheBloke/Mistral-7B-Instruct-v0.2-AWQ\n    model_version: \"*\"\n    model_name: mistral7bInstruct-AWQ\n    ep_name: mistral7bInstruct-P5AWQ\n    download_from_hf_place_in_s3: yes\n    model_s3_path: s3://{write_bucket}/TheBloke/Mistral-7B-Instruct-v0.2-AWQ\n    instance_type: \"ml.p5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.26.0-deepspeed0.12.6-cu121\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n      engine=Python\n      option.tensor_parallel_degree=1\n      option.model_id=s3://{write_bucket}/TheBloke/Mistral-7B-Instruct-v0.2-AWQ\n      option.max_rolling_batch_size=64\n      option.rolling_batch=vllm\n    payload_files:\n    - payload_en_1000-2000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 5\n    - 10\n    - 20\n    - 30\n    env:\n\nreport:\n  latency_budget: 1\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n  latency_vs_token_len_chart:\n    y_ticks:\n    # - 1\n    # - 2\n    # - 3\n    # - 4\n    title: \"Effect of token length on inference latency for \\\"TheBloke/Mistral-7B-Instruct-v0.2-AWQ\\\"\"\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-instruct-AWQ-p5.yml",
      "scrapeId": "83a01570-1623-4abd-99ec-ac3010913170",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-instruct-AWQ-p5.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-70b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3.1-70b-g5.48xl-tp=8-mc=max-djl-ec2\"\n  model_name: \"llama3-1-70b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-70b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-70B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-70b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged witsh multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-70b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-70b-Instruct\n      option.rolling_batch=lmi-dist\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 200\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-llama3-1-70b-g5.48xl-tp-8-mc-max-djl.yml",
      "scrapeId": "8880be19-4a23-4300-b861-16cdfc7081e1",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-llama3-1-70b-g5.48xl-tp-8-mc-max-djl.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g5.24xl-ec2\"\n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g5.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.24xl-tp-2-mc-max-ec2.yml",
      "scrapeId": "7e523ee7-f6ae-485a-b578-f712508a16e2",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.24xl-tp-2-mc-max-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# This config file uses custom payload functionalities, and points to\n# a \"custom_rest_predictor.py\" that does a request.post to an endpoint\n# url, with the model id, inference parameters and a few headers configured\n# within this config file\ngeneral:\n  name: \"<add-your-rest-ep-model-name-here>\"\n  model_name: \"<your-custom-model-name>\"\n\n# AWS and SageMaker settings\n# These are placeholder values that will be substituted\n# dynamically once FMBench runs the test\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\n# These directory paths point to the data, prompts, metrics and other\n# model and metadata information that will be stored in the FMBench write\n# bucket once the benchmarking test starts\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  # Add your custom pricing in this yml file which is located at \"configs/pricing.yml\" that contains\n  # instance hourly and token-based pricing. NOTE: The instance_type parameter in this config file should match\n  # the instance_type parameter in the pricing.yml file for FMBench to correctly map the values before\n  # calculating the cost to run and evaluate the models to be benchmarked.\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n\n  # FOR HUGGING FACE DATASETS:\n  # ---------------------------\n  # FMBench now supports benchmarking models using datasets from Hugging Face with a simplified prefixing method.\n  # To specify a Hugging Face dataset and its split, use the hf: prefix followed by the dataset identifier, subset name, and split name.\n  # If a subset name is not provided, it defaults to default. If a split name is not provided, FMBench automatically selects the next available split at runtime.\n  # FOR CUSTOM DATASETS:\n  # ---------------------------\n  # To use custom data, convert it into JSONL format. We provide a sample notebook to help convert Hugging Face or custom datasets into JSONL and upload them to an S3 bucket used by FMBench.\n  # Follow the steps in the https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/fmbench/bring_your_own_dataset.ipynb notebook to integrate your own dataset into FMBench.\n  # Place this JSONL file in the local fmbench-read/scripts directory in your FMBench EC2 instance or in the fmbench-read S3 bucket in the scripts directory.\n  source_data_files:\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  # Format: hf:dataset-id/subset-name/split-name. Use   \"default\" if no subset name is provided.\n  - hf:THUDM/LongBench/2wikimqa_e/test\n  - hf:THUDM/LongBench/2wikimqa/test\n  - hf:THUDM/LongBench/hotpotqa_e/test\n  - hf:THUDM/LongBench/hotpotqa/test\n  - hf:THUDM/LongBench/narrativeqa/test\n  - hf:THUDM/LongBench/triviaqa_e/test\n  - hf:THUDM/LongBench/triviaqa/test\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: tokenizer\n\n  # S3 prefix for prompt templates - this does not have to be changed\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  # If you have a custom prompt template, place the file in the local fmbench-read/scripts directory and point to it here\n  prompt_template_file: <your-prompt-template.txt>\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  # <replace with the keys in your dataset. If your dataset contains another key you want inputted into the prompt, then specify that here and it will\n  # be plugged into the prompt\n  - input\n  - context\n  # If you want to measure model accuracy, mention the column keys in the dataset that point to the question/task and the ground truth.\n  # This will be used by FMBench's LLM evaluators to evaluate the correctness of models to be benchmarked.\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1-500\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: custom_pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  custom_rest:\n    generation_config:\n      temperature: 0.3\n      top_p: 0.3\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"<your-model-name>\"\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: # model id, version and image uri not needed for byo endpoint\n    hf_tokenizer_model_id:\n    model_version:\n    model_name: \"<your-model-name>\"\n    # the ep_name will contain the endpoint url that is used to invoke your model and get the response\n    # in this case, we use ray serve with `NousResearch/Llama-2-13b-chat-hf` model deployed on an EKS cluster.\n    # the endpoint url format used in this example is \"http://<NLB_DNS_NAME>/serve/infer?sentence=<PROMPT_PAYLOAD>\"\n    ep_name: '<your-ep-url>' # public DNS/URL to send your request\n    instance_type: \"<your-instance-type>\"\n    image_uri:\n    deploy: no #setting to no since the endpoint has already been deployed on an EKS cluster\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script: # No deployment script is needed since this is a byoe configuration file\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example of a custom rest predictor\n    # that does a POST request on the endpoint URL (given on line 164) with custom headers,\n    # parameters and authentication information\n    inference_script: custom_rest_predictor.py\n    # This is the inference spec with custom information. You can add/remove variables\n    # and access those in the predictor file to use them as accepted by your container.\n    inference_spec:\n      # This parameter is specified above with custom configurations\n      parameter_set: custom_rest\n      # This model id is appended to the prompt as the payload is sent to the model id\n      model_id: \"<custom-model-id>\"\n      # All key-value pairs added in this headers section are transparently\n      # passed via the POST request to the model server. You can add any custom\n      # header parameters needed for your specific implementation.\n      headers:\n        custom_client_id: <your-custom-client-id>\n        custom_authentication_token: \"<your-auth-token>\"  # Replace with actual token\n        content-type: \"application/json\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 5\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/byoe/config-byo-custom-rest-predictor.yml",
      "scrapeId": "7e84badd-fb48-4d1f-bbef-6a1cff594165",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/byoe/config-byo-custom-rest-predictor.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"mistral-7b-p5-instructv2-tp4-trt\"\n  model_name: \"mistral7bInstruct\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: mistral_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_mistral.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n  - language: en\n    min_length_in_tokens: 1600\n    max_length_in_tokens: 5000\n    payload_file: payload_en_1600-5000.jsonl\n\nmetrics:\n  dataset_of_interest: en_1000-2000\n\npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Model configurations\nexperiments:\n  - name: mistral-Instruct7b-p5-tp4-trt\n    model_id: mistralai/Mistral-7B-Instruct-v0.2\n    model_version: \"*\"\n    model_name: Mistral7BInstruct\n    ep_name: mistral7bInstruct-P5\n    download_from_hf_place_in_s3: no\n    model_s3_path: s3://{write_bucket}/mistralai/Mistral-7B-Instruct-v0.2\n    instance_type: \"ml.p5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-tensorrtllm0.8.0-cu122\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n        engine=MPI\n        option.tensor_parallel_degree=4\n        option.model_id=mistralai/Mistral-7B-Instruct-v0.2\n        option.max_rolling_batch=64\n        option.max_input_len=8192\n        option.max_output_len=8192\n        option.max_num_tokens=100000\n        option.use_custom_all_reduce=true\n        option.num_engine_workers=1\n        option.num_checkpoint_workers=1\n\n    payload_files:\n    - payload_en_1000-2000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 5\n    - 10\n    - 20\n    - 30\n    accept_eula: true\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 200\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n  latency_vs_token_len_chart:\n    y_ticks:\n    title: \"Effect of token length on inference latency for \\\"mistralai/Mistral-7B-Instruct-v0.2\\\"\"\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-instruct-v2-p5-trtllm.yml",
      "scrapeId": "8027c2ee-3ff6-4b89-9543-15f896a03009",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-instruct-v2-p5-trtllm.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: bedrock-nova-models-convfinqa\n  model_name: \"Nova models available in Amazon Bedrock\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket}\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n\n    configs_prefix: configs\n    config_files:\n    - pricing.yml ## mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - hf:AdaptLLM/finance-tasks/ConvFinQA/test\n    tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_nova_convfinqa.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: no\n    3_run_inference.ipynb: yes\n    4_get_evaluations.ipynb: yes\n    5_model_metric_analysis.ipynb: yes\n    6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  ground_truth_col_key: label\n  question_col_key: input\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\nmetrics:\n  dataset_of_interest: en_1000-2000\n\n\npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n\ninference_parameters:\n  bedrock:\n    temperature: 0.6\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n    # Set use_boto3 to \"no\" if the model is supported by litellm and\n    # the litellm API will be used to invoke the model. If the model is on\n    # bedrock and is not supported by the latest version of litellm, then set\n    # the parameter below to \"yes\" and the bedrock converseAPI will be used.\n    use_boto3: no\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  # This is the NOVA micro model\n  - name: amazon.nova-micro-v1:0\n    model_name: amazon.nova-micro-v1:0\n    ep_name: amazon.nova-micro-v1:0\n    instance_type: amazon.nova-micro-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    concurrency_levels:\n    - 1\n    env:\n  - name: amazon.nova-lite-v1:0\n    model_name: amazon.nova-lite-v1:0\n    ep_name: amazon.nova-lite-v1:0\n    instance_type: amazon.nova-lite-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    concurrency_levels:\n    - 1\n    env:\n  - name: amazon.nova-pro-v1:0\n    model_name: amazon.nova-pro-v1:0\n    ep_name: amazon.nova-pro-v1:0\n    instance_type: amazon.nova-pro-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    - payload_en_4000-5000.jsonl\n    - payload_en_5000-6000.jsonl\n    concurrency_levels:\n    - 1\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-nova-all-models-convfinqa.yml",
      "scrapeId": "83f7c8bf-959e-4f2f-a076-1ca00eb1a6bc",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-nova-all-models-convfinqa.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"llama3-8b-g6-12xl-djl-lmi-dist\"\n  model_name: \"llama3-8b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\nmetrics:\n  dataset_of_interest: en_3000-3840\n\npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\nexperiments:\n\n  - name: llama-3-8b-instruct-g6-12xl-djl-deepspeed0.12.6-cu121\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct\n    ep_name: llama-3-8b-instruct-g6-12xl\n    instance_type: \"ml.g6.12xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.max_rolling_batch_size=256\n      option.rolling_batch=lmi-dist\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 15\n    - 20\n    - 30\n\n    accept_eula: true\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      job_queue_size: 1000\n      max_dynamic_batch_size: 1\n      max_batch_delay: 100\n      max_idle_time: 60\n      load_on_devices: *\n      engine: MPI\n      mpi_mode: true\n      option.entryPoint: null\n      option.tensor_parallel_degree: 4\n      option.max_rolling_batch_size: 256\n      option.mpi_mode: true\n      option.model_id: /opt/ml/model\n      option.rolling_batch: lmi-dist\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      #HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      # SM_NUM_GPUS: \"1\"\n      # SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-g6-12xl.yml",
      "scrapeId": "88c04ba0-4cef-4365-b39d-567b8c8f67a3",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-g6-12xl.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-m7i.12xl-ec2\"\n  model_name: \"llama3-8b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_vllm:\n    model: meta-llama/Meta-Llama-3-8B-Instruct\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-8b-instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3-8b-instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8000/v1/completions'\n    instance_type: \"m7i.12xlarge\"\n    image_uri: vllm-cpu-env\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # Privileged Mode makes the docker container run with root.\n      # This basically means that if you are root in a container you have the privileges of root on the host system\n      # Only need this if you need to set VLLM_CPU_OMP_THREADS_BIND env variable.\n      privileged_mode: yes\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting:\n      # This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_vllm\n      # if not set assume djl\n      container_type: vllm\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      VLLM_CPU_KVCACHE_SPACE: 40\n      # This instance has 48 CPUs, and we are allocating 40 of them to run this container.\n      # For more information, visit the following URL:\n      # https://docs.vllm.ai/en/latest/getting_started/cpu-installation.html#related-runtime-environment-variables\n      VLLM_CPU_OMP_THREADS_BIND: 0-40\n\nreport:\n  latency_budget: 25\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-m7i-12xlarge.yml",
      "scrapeId": "86c9f009-3933-41d7-a666-cd7d2bd71ec5",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-m7i-12xlarge.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g6e.4xl-tp=1-mc=max-djl-ec2\"\n  model_name: \"llama3.1-8b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-8b-instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3.1-8b-instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g6e.4xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      tp_degree: 1\n      shm_size: 12g\n      model_loading_timeout: 2400\n      model_copies: max\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=1\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g6e.4xl-tp-1-mc-max-djl.yml",
      "scrapeId": "872a8ef2-4bee-45b1-8204-eba8235d0bf7",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g6e.4xl-tp-1-mc-max-djl.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# This file contains the evaluation information for majority voting. Here, we initialize\n# the embeddings model used to calculate quantitative metrics such as\n# cosine similarity. The other part of this evaluation is using subjective\n# evaluation methods: majority voting. In the case of when a ground truth\n# is provided, FMBench can use majority voting with the help of a 'panel of judges' to get a verdict [correct, incorrect].\n# For more information, view this paper: https://arxiv.org/pdf/2404.18796. Majority voting using a panel of LLM evaluators\n# helps in getting a 'close to human evaluation', reduces cost of evaluations, and eliminates intra model bias.\nmodel_evaluations:\n  ground_truth_col: {ground_truth}\n  question_col: {question}\n\n  PoLL_Composition_and_Voting:\n    method: majority_vote\n    # Set this variable to yes if you want to make partial correct/incorrect decisions based\n    # on quantitative metrics like cosine similarity, levenshtein score and token set ratio. Set\n    # this to yes only if you have a very direct QnA use case\n    use_quantitative_metrics: yes\n\n  model_eval_dir:\n    # This is the directory in S3 and locally where all the evaluation instructions are stored for\n    # evaluating the candidate model responses using majority voting\n    eval_prompts_dir: eval_criteria\n    # the directory contains a folder that contains all the files with rules for evaluations\n    # and another directory that stores the standard prompt template that is used for evlauation\n    # of different answers at runtime. For example, `claude_eval_prompt_templates` contains the\n    # prompt template that claude will use for majority voting, etc.\n    eval_prompt_template_dir_list:\n    - claude_eval_prompt_templates\n    - llama3_eval_prompt_templates\n    - cohere_eval_prompt_templates\n    - mistral_eval_prompt_templates\n\n    # These are the rules that are prefilled within the\n    # prompt templates evaluating for majority voting\n    eval_instructions_dir: eval_instructions\n    eval_instructions_files:\n    - evaluation_instructions_majority_vote.txt\n\n  # This represents the information that is used to get the quantitative metrics\n  # from the evaluation step. This includes calculating the cosine similarity.\n  # If a ground truth is provided, measure the cosine similarity against the ground truth,\n  # else measure it against the context provided. We use the `sentence-transformers/all-mpnet-base-v2`\n  # dataset. There is also an option to use the Titan embeddings model (WIP)\n  quantitative_eval_info:\n    embeddings_model_id:\n      model_id: sentence-transformers/all-mpnet-base-v2\n    # This contains information about quantitative metrics thresholds that need to be set while\n    # evaluating whether a candidate model response is correct or incorrect without parsing it through\n    # the panel of LLM evaluation procedure\n\n    # There are two cosine similarity verdict scores that are used, one to determine whether a candidate model\n    # response is incorrect and another to determine whether it is correct. If the incorrect threshold is met, for\n    # example if the LLM evaluator provides an incorrect verdict, the actual incorrectness will be defined once\n    # it also is below the incorrect cosine similarity threshold of for example 0.40.\n    # If the LLM evaluator provides a correct verdict and it exceeds the correctness cosine similarity score of\n    # 0.05 for example, then the answer is defined as correctly evaluated as \"correct\"\n    incorrect_verdict_cosine_similarity_threshold: 0.40\n    correct_verdict_cosine_similarity_threshold: 0.01\n  # This represents the information that is used to get subjective evaluations on the\n  # content that is generated. It uses an LLM as a judge (that is configurable) and evaluates\n  # each content from the inference step on different evaluation criteria. The information about\n  # the LLM as a judge panel is given below that is used in the majority voting\n  subjective_eval_info:\n    # this is the judge panel list that is used in the evaluation process\n    judge_panel_list:\n      # Information on judge 1 on the evaluation judge panel\n      - model_id: us.meta.llama3-3-70b-instruct-v1:0\n        # this is the prompt template that is used in the evaluation process\n        # based on the method: majority voting\n        eval_prompt_template_dir: \"llama3_eval_prompt_templates\"\n        eval_prompt_template_name: \"llama3_eval_{method_name}\"\n      # Information on judge 2 on the evaluation judge panel\n      - model_id: us.anthropic.claude-3-5-sonnet-20241022-v2:0\n        # this is the prompt template that is used in the evaluation process\n        # based on the method: majority voting\n        eval_prompt_template_dir: \"claude_eval_prompt_templates\"\n        eval_prompt_template_name: \"claude_eval_{method_name}\"\n      # Information on judge 3 on the evaluation judge panel\n      # We use the most powerful cohere model - cohere command R +\n      - model_id: cohere.command-r-plus-v1:0\n        # this is the prompt template that is used in the evaluation process\n        # based on the method: majority voting\n        eval_prompt_template_dir: \"cohere_eval_prompt_templates\"\n        eval_prompt_template_name: \"cohere_eval_{method_name}\"\n    # number of parallel calls made asyncronously to bedrock using Ray\n    run_parallel_inference_count: 5\n    # Common inference parameters used in the evaluation process\n    # We use LiteLLM for interfacing with Bedrock\n    inference_parameters:\n      temperature: 0.1\n      max_tokens: 300\n      top_p: 0.92\n      caching: False\n\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/model_eval_all_info.yml",
      "scrapeId": "8698c06b-9453-496c-917b-d1cd4b9413b3",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/model_eval_all_info.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "[Skip to content](https://aws-samples.github.io/foundation-model-benchmarking-tool/misc/eks_cluster-creation_steps.html#eks-cluster-creation-steps)\n\n# EKS cluster creation steps [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/misc/eks_cluster-creation_steps.html\\#eks-cluster-creation-steps \"Permanent link\")\n\nThe steps below create an EKS cluster called `trainium-inferentia`.\n\n1. Before we begin, ensure you have all the prerequisites in place to make the deployment process smooth and hassle-free. Ensure that you have installed the following tools on your machine: [aws-cli](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html), [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/) and [terraform](https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli). We use the [`DoEKS`](https://github.com/awslabs/data-on-eks/tree/main) repository as a guide to deploy the cluster infrastructure in an AWS account.\n\n2. Ensue that your account has enough `Inf2` on-demand VCPUs as most of the DoEKS blueprints utilize this specific instance. To increase service quota navigate to the service quota page for the region you are in [service quota](https://us-east-1.console.aws.amazon.com/servicequotas/home?region=us-east-1). Then select **services** under the left side menu and search for **Amazon Elastic Compute Cloud (Amazon EC2)**. This will bring up the service quota page, here search for `inf` and there should be an option for **Running On-Demand Inf instances**. Increase this quota to 300.\n\n3. Clone the [`DoEKS`](https://github.com/awslabs/data-on-eks) repository\n\n\n\n```md-code__content\ngit clone https://github.com/awslabs/data-on-eks.git\n\n```\n\n4. Ensure that the region names are correct in [`variables.tf`](https://github.com/awslabs/data-on-eks/blob/d532720d0746959daa6d3a3f5925fc8be114ccc4/ai-ml/trainium-inferentia/variables.tf#L12) file before running the cluster creation script.\n\n5. Ensure that the ELB to be created would be external facing. Change the helm value from `internal` to `internet-facing` [here](https://github.com/awslabs/data-on-eks/blob/3ef55e21cf30b54341bb771a2bb2dbd1280c3edd/ai-ml/trainium-inferentia/helm-values/ingress-nginx-values.yaml#L8).\n\n6. Ensure that the IAM role you are using has the permissions needed to create the cluster. **While we expect the following set of permissions to work but the current recommendation is to also add the `AdminstratorAccess` permission to the IAM role. At a later date you could remove the `AdminstratorAccess` and experiment with cluster creation without it.**\n1. Attach the following managed policies: `AmazonEKSClusterPolicy`, `AmazonEKS_CNI_Policy`, and `AmazonEKSWorkerNodePolicy`.\n2. In addition to the managed policies add the following as inline policy. Replace _your-account-id_ with the actual value of the AWS account id you are using.\n\n\n\n\n\n      ```md-code__content\n      {\n      \"Version\": \"2012-10-17\",\n      \"Statement\": [\\\n          {\\\n              \"Sid\": \"VisualEditor0\",\\\n              \"Effect\": \"Allow\",\\\n              \"Action\": [\\\n                  \"ec2:CreateVpc\",\\\n                  \"ec2:DeleteVpc\"\\\n              ],\\\n              \"Resource\": [\\\n                  \"arn:aws:ec2:*:your-account-id:ipv6pool-ec2/*\",\\\n                  \"arn:aws:ec2::your-account-id:ipam-pool/*\",\\\n                  \"arn:aws:ec2:*:your-account-id:vpc/*\"\\\n              ]\\\n          },\\\n          {\\\n              \"Sid\": \"VisualEditor1\",\\\n              \"Effect\": \"Allow\",\\\n              \"Action\": [\\\n                  \"ec2:ModifyVpcAttribute\",\\\n                  \"ec2:DescribeVpcAttribute\"\\\n              ],\\\n              \"Resource\": \"arn:aws:ec2:*:<your-account-id>:vpc/*\"\\\n          },\\\n          {\\\n              \"Sid\": \"VisualEditor2\",\\\n              \"Effect\": \"Allow\",\\\n              \"Action\": \"ec2:AssociateVpcCidrBlock\",\\\n              \"Resource\": [\\\n                  \"arn:aws:ec2:*:your-account-id:ipv6pool-ec2/*\",\\\n                  \"arn:aws:ec2::your-account-id:ipam-pool/*\",\\\n                  \"arn:aws:ec2:*:your-account-id:vpc/*\"\\\n              ]\\\n          },\\\n          {\\\n              \"Sid\": \"VisualEditor3\",\\\n              \"Effect\": \"Allow\",\\\n              \"Action\": [\\\n                  \"ec2:DescribeSecurityGroupRules\",\\\n                  \"ec2:DescribeNatGateways\",\\\n                  \"ec2:DescribeAddressesAttribute\"\\\n              ],\\\n              \"Resource\": \"*\"\\\n          },\\\n          {\\\n              \"Sid\": \"VisualEditor4\",\\\n              \"Effect\": \"Allow\",\\\n              \"Action\": [\\\n                  \"ec2:CreateInternetGateway\",\\\n                  \"ec2:RevokeSecurityGroupEgress\",\\\n                  \"ec2:CreateRouteTable\",\\\n                  \"ec2:CreateSubnet\"\\\n              ],\\\n              \"Resource\": [\\\n                  \"arn:aws:ec2:*:your-account-id:security-group/*\",\\\n                  \"arn:aws:ec2:*:your-account-id:internet-gateway/*\",\\\n                  \"arn:aws:ec2:*:your-account-id:subnet/*\",\\\n                  \"arn:aws:ec2:*:your-account-id:route-table/*\",\\\n                  \"arn:aws:ec2::your-account-id:ipam-pool/*\",\\\n                  \"arn:aws:ec2:*:your-account-id:vpc/*\"\\\n              ]\\\n          },\\\n          {\\\n              \"Sid\": \"VisualEditor5\",\\\n              \"Effect\": \"Allow\",\\\n              \"Action\": [\\\n                  \"ec2:AttachInternetGateway\",\\\n                  \"ec2:AssociateRouteTable\"\\\n              ],\\\n              \"Resource\": [\\\n                  \"arn:aws:ec2:*:your-account-id:vpn-gateway/*\",\\\n                  \"arn:aws:ec2:*:your-account-id:internet-gateway/*\",\\\n                  \"arn:aws:ec2:*:your-account-id:subnet/*\",\\\n                  \"arn:aws:ec2:*:your-account-id:route-table/*\",\\\n                  \"arn:aws:ec2:*:your-account-id:vpc/*\"\\\n              ]\\\n          },\\\n          {\\\n              \"Sid\": \"VisualEditor6\",\\\n              \"Effect\": \"Allow\",\\\n              \"Action\": \"ec2:AllocateAddress\",\\\n              \"Resource\": [\\\n                  \"arn:aws:ec2:*:your-account-id:ipv4pool-ec2/*\",\\\n                  \"arn:aws:ec2:*:your-account-id:elastic-ip/*\"\\\n              ]\\\n          },\\\n          {\\\n              \"Sid\": \"VisualEditor7\",\\\n              \"Effect\": \"Allow\",\\\n              \"Action\": \"ec2:ReleaseAddress\",\\\n              \"Resource\": \"arn:aws:ec2:*:your-account-id:elastic-ip/*\"\\\n          },\\\n          {\\\n              \"Sid\": \"VisualEditor8\",\\\n              \"Effect\": \"Allow\",\\\n              \"Action\": \"ec2:CreateNatGateway\",\\\n              \"Resource\": [\\\n                  \"arn:aws:ec2:*:your-account-id:subnet/*\",\\\n                  \"arn:aws:ec2:*:your-account-id:natgateway/*\",\\\n                  \"arn:aws:ec2:*:your-account-id:elastic-ip/*\"\\\n              ]\\\n          }\\\n      ]\n      }\n\n      ```\n\n\n\n      1\\. Add the Role ARN and name here in the `variables.tf` file by updating [these lines](https://github.com/awslabs/data-on-eks/blob/d532720d0746959daa6d3a3f5925fc8be114ccc4/ai-ml/trainium-inferentia/variables.tf#L126). Move the structure inside the `defaut` list and replace the role ARN and name values with the values for the role you are using.\n7. Navigate into the `ai-ml/trainium-inferentia/` directory and run install.sh script.\n\n\n\n```md-code__content\ncd data-on-eks/ai-ml/trainium-inferentia/\n./install.sh\n\n```\n\n\n\nNote: This step takes about 12-15 minutes to deploy the EKS infrastructure and cluster in the AWS account. To view more details on cluster creation, view an example here: [Deploy Llama3 on EKS](https://awslabs.github.io/data-on-eks/docs/gen-ai/inference/llama3-inf2) in the _prerequisites_ section.\n\n8. After the cluster is created, navigate to the **Karpenter EC2 node IAM role** called `karpenter-trainium-inferentia-XXXXXXXXXXXXXXXXXXXXXXXXX`. Attach the following inline policy to the role:\n\n\n\n```md-code__content\n{\n       \"Version\": \"2012-10-17\",\n       \"Statement\": [\\\n           {\\\n               \"Sid\": \"Statement1\",\\\n               \"Effect\": \"Allow\",\\\n               \"Action\": [\\\n                   \"iam:CreateServiceLinkedRole\"\\\n               ],\\\n               \"Resource\": \"*\"\\\n           }\\\n       ]\n}\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/misc/eks_cluster-creation_steps.html",
      "title": "EKS cluster creation steps - Foundation Model Benchmarking Tool (FMBench)",
      "favicon": {},
      "language": "en",
      "scrapeId": "8f39e54d-5986-4f36-b6f2-a85c2a791640",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/misc/eks_cluster-creation_steps.html",
      "statusCode": 200,
      "theme-color": "#00000000",
      "color-scheme": "normal"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"llama2-7b-v1\"\n  model_name: \"Llama2-7b\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: llama2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama2.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the\n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1000-2000\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: llama2-7b-g5.xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: meta-textgeneration-llama-2-7b-f\n    # Add the hugging face model id to load the specific tokenizer at runtime.\n    # To load this tokenizer, make sure to provide the hf_token.txt file.\n    hf_tokenizer_model_id: meta-llama/Llama-2-7b\n    model_version: \"3.*\"\n    model_name: llama2-7b-f\n    ep_name: llama-2-7b-g5-xlarge\n    instance_type: \"ml.g5.xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04'\n    deploy: yes\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script: jumpstart.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n      stream: False\n      stop_token: \".</s>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    # - payload_en_2000-3000.jsonl\n    # - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\n  - name: llama2-7b-g5.2xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: meta-textgeneration-llama-2-7b-f\n    model_version: \"3.*\"\n    model_name: llama2-7b-f\n    ep_name: llama-2-7b-g5-2xlarge\n    instance_type: \"ml.g5.2xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04'\n    deploy: yes\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count: 1\n    deployment_script: jumpstart.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n      stream: False\n      stop_token: \".\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    # - payload_en_2000-3000.jsonl\n    # - payload_en_3000-3840.jsonl\n\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n\n    # Added for models that require accepting a EULA\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\n# parameters related to how the final report is generated\nreport:\n  # markdown report title\n  title: \"Performance benchmarking results for Llama2-7b on `g5.xlarge` and `g5.2xlarge` using the HuggingFace TGI container\"\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama2/7b/config-llama2-7b-g5-quick.yml",
      "scrapeId": "8848a9c0-4117-4ae9-bee4-3c3d7bf26b61",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama2/7b/config-llama2-7b-g5-quick.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"mistral-7b-p5-instructV1-tp4-trt\"\n  model_name: \"mistral7bInstruct\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: mistral_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_mistral.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n  - language: en\n    min_length_in_tokens: 1600\n    max_length_in_tokens: 5000\n    payload_file: payload_en_1600-5000.jsonl\n\nmetrics:\n  dataset_of_interest: en_1000-2000\n\npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Model configurations\nexperiments:\n  - name: mistral-Instruct7b-p5-trt-version-1\n    model_id: mistralai/Mistral-7B-Instruct-v0.1\n    model_version: \"*\"\n    model_name: Mistral7BInstructv1\n    ep_name: mistral7bInstruct-P5\n    download_from_hf_place_in_s3: no\n    model_s3_path: s3://{write_bucket}/mistralai/Mistral-7B-Instruct-v0.1\n    instance_type: \"ml.p5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-tensorrtllm0.8.0-cu122\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n        engine=MPI\n        option.tensor_parallel_degree=4\n        option.model_id=mistralai/Mistral-7B-Instruct-v0.1\n        option.max_rolling_batch=64\n        option.max_input_len=8192\n        option.max_output_len=8192\n        option.max_num_tokens=100000\n        option.use_custom_all_reduce=true\n        option.num_engine_workers=1\n        option.num_checkpoint_workers=1\n\n    payload_files:\n    - payload_en_1000-2000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 5\n    - 10\n    - 20\n    - 30\n    accept_eula: true\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n  latency_vs_token_len_chart:\n    y_ticks:\n    title: \"Effect of token length on inference latency for \\\"mistralai/Mistral-7B-Instruct-v0.1\\\"\"\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-instruct-v1-p5-trtllm.yml",
      "scrapeId": "a00ab40d-67cb-4f2e-83bd-ad811e4faa26",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-instruct-v1-p5-trtllm.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"Llama3-1-8b-g5\"\n  model_name: \"Llama3-1-8b\"\n\naws:\n  region: {region}\n  sagemaker_execution_role: {role_arn}\n  bucket: {write_bucket}\n\ndir_paths:\n    data_prefix: data\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata\n\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts\n    script_files:\n    - hf_token.txt\n    configs_prefix: configs\n    config_files:\n    - pricing.yml\n    source_data_prefix: source_data\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_1_tokenizer\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_get_evaluations.ipynb: no\n    5_model_metric_analysis.ipynb: yes\n    6_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\nmetrics:\n  dataset_of_interest: en_2000-3000\n\npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\nexperiments:\n  - name: Llama3-1-8b-g5.2xl-djl-inference:0.29.0-lmi11.0.0-cu124\n    model_id: meta-llama/Llama-3.1-8B-Instruct\n    model_version: \"*\"\n    model_name: Meta-Llama-3-1-8B-Instruct\n    ep_name: Meta-Llama-3-1-8B-Instruct-g5-2xl\n    download_from_hf_place_in_s3: yes\n    model_s3_path: s3://{write_bucket}/meta-llama/Llama-3.1-8B-Instruct\n    instance_type: \"ml.g5.xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n      engine=Python\n      option.model_id=s3://{write_bucket}/meta-llama/Llama-3.1-8B-Instruct\n      option.dtype=fp16\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    # - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n\n    accept_eula: true\n    env:\n  - name: Llama3-1-8b-g5.4xl-djl-inference:0.29.0-lmi11.0.0-cu124\n    model_id: meta-llama/Llama-3.1-8B-Instruct\n    model_version: \"*\"\n    model_name: Meta-Llama-3-1-8B-Instruct\n    ep_name: Meta-Llama-3-1-8B-Instruct-g5-4xl\n    download_from_hf_place_in_s3: yes\n    model_s3_path: s3://{write_bucket}/meta-llama/Llama-3.1-8B-Instruct\n    instance_type: \"ml.g5.2xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n      engine=Python\n      option.model_id=s3://{write_bucket}/meta-llama/Llama-3.1-8B-Instruct\n      option.dtype=fp16\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    # - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    # - 4\n    # - 10\n\n    accept_eula: true\n    env:\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n  latency_vs_token_len_chart:\n    y_ticks:\n    title: \"Effect of token length on inference latency for \\\"meta-llama/Meta-Llama-3.1-8B-Instruct\\\"\"\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.2xl-g5.4xl-sm.yml",
      "scrapeId": "8c8259b8-e0da-4edf-93c7-39351fc24819",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.2xl-g5.4xl-sm.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"llama2-7b-inf2-g5-v1\"\n  model_name: \"Llama2-7b\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama2.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\nmetrics:\n  dataset_of_interest: en_3000-3840\n\npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n    return_full_text: False\n\n# Model configurations for llama-2 7b for deploying on g5 and inf2 instances\nexperiments:\n  # llama2-7b neuron on inf2\n  - name: llama2-7b-inf2.8xlarge-djl-inference:0.27.0-neuronx\n    model_id: meta-textgenerationneuron-llama-2-7b-f\n    model_version: \"*\"\n    model_name: llama2-7b\n    ep_name: llama2-7b-inf2-8xl\n    instance_type: \"ml.inf2.8xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-neuronx-sdk2.18.1'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n      OPTION_TENSOR_PARALLEL_DEGREE: \"2\"\n      OPTION_N_POSITIONS: \"4096\"\n      OPTION_DTYPE: \"fp16\"\n      OPTION_ROLLING_BATCH: \"auto\"\n      OPTION_MAX_ROLLING_BATCH_SIZE: \"1\"\n      OPTION_NEURON_OPTIMIZE_LEVEL: \"2\"\n  - name: llama2-7b-inf2.24xlarge-djl-inference:0.27.0-neuronx\n    model_id: meta-textgenerationneuron-llama-2-7b-f\n    model_version: \"*\"\n    model_name: llama2-7b\n    ep_name: llama2-7b-inf2-24xl\n    instance_type: \"ml.inf2.24xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-neuronx-sdk2.18.1'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n      OPTION_TENSOR_PARALLEL_DEGREE: \"2\"\n      OPTION_N_POSITIONS: \"4096\"\n      OPTION_DTYPE: \"fp16\"\n      OPTION_ROLLING_BATCH: \"auto\"\n      OPTION_MAX_ROLLING_BATCH_SIZE: \"1\"\n      OPTION_NEURON_OPTIMIZE_LEVEL: \"2\"\n\n  # llama2-7b on hf tgi, on g5 instances\n  - name: llama2-7b-g5.2xlarge-huggingface-pytorch-tgi\n    model_id: meta-textgeneration-llama-2-7b-f\n    model_version: \"3.*\"\n    model_name: llama2-7b-f\n    ep_name: llama2-7b-g5-2x\n    instance_type: \"ml.g5.2xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi1.4.5-gpu-py310-cu121-ubuntu22.04-v1.0'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n  - name: llama2-7b-g5.12xlarge-huggingface-pytorch-tgi\n    model_id: meta-textgeneration-llama-2-7b-f\n    model_version: \"3.*\"\n    model_name: llama2-7b-f\n    ep_name: llama2-7b-g5-12x\n    instance_type: \"ml.g5.12xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.1.1-tgi1.4.5-gpu-py310-cu121-ubuntu22.04-v1.0'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 30\n  error_rate_budget: 0.0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama2/7b/config-llama2-7b-inf2-g5.yml",
      "scrapeId": "9788aa81-70c7-45db-9acd-f7b66098f8de",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama2/7b/config-llama2-7b-inf2-g5.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-1-8b-g5.2xl-ec2\"\n  model_name: \"llama3-1-8b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 924\n    payload_file: payload_en_1-924.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-1-8b-instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3-1-8b-instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g5.2xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 1\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=1\n      option.model_id=meta-llama/Meta-Llama-3.1-8b-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5-ec2.yml",
      "scrapeId": "8aaebbd0-79fd-42c2-bb4d-6d700bc8b9e4",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-8b-inf2.48xl-ec2-tp24-bs12\"\n  model_name: \"llama3-1-8b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1000-2000\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-8b-Instruct-tp24-bs12\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"inf2.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and Inferentia settings for the instance\n      # '-u djl' tells the container to use the Deep Java Library (DJL) runtime\n      # '--device /dev/neuron0 ... /dev/neuron5' specifies the Inferentia devices to be used\n      gpu_or_neuron_setting: -u djl --device /dev/neuron0 --device /dev/neuron1 --device /dev/neuron2 --device /dev/neuron3 --device /dev/neuron4 --device /dev/neuron5 --device /dev/neuron6 --device /dev/neuron7 --device /dev/neuron8 --device /dev/neuron9 --device /dev/neuron10 --device /dev/neuron11\n      #This setting specifies the timeout (in seconds) for loading the model.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 24\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=meta-llama/Meta-Llama-3.1-8b-Instruct\n      option.tensor_parallel_degree=24\n      option.n_positions=4096\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=12\n      option.model_loading_timeout=2400\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O3\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-inf2-48xl-deploy-ec2-tp24-bs12.yml",
      "scrapeId": "9fe96bda-3cee-4686-9a36-7db1993f571c",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-inf2-48xl-deploy-ec2-tp24-bs12.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"mistral-7b-trn1.32xl-ec2-triton\"\n  model_name: \"Mistral-7B-Instruct-V0.2\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: mistral_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_mistral.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Mistral-7B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: mistralai/Mistral-7B-Instruct-v0.2 #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Mistral-7B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8080/v2/models/Mistral-7B-Instruct-v0.2/generate'\n    instance_type: \"trn1.32xlarge\"\n    image_uri: tritonserver-neuronx:24.06-2.x\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and Inferentia settings for the instance\n      # '-u djl' tells the container to use the Deep Java Library (DJL) runtime\n      # '--device /dev/neuron0 ... /dev/neuron5' specifies the Inferentia devices to be used\n      gpu_or_neuron_setting: -u triton --device /dev/neuron0 --device /dev/neuron1 --device /dev/neuron2 --device /dev/neuron3 --device /dev/neuron4 --device /dev/neuron5 --device /dev/neuron6 --device /dev/neuron7 --device /dev/neuron8 --device /dev/neuron9 --device /dev/neuron10 --device /dev/neuron11 --device /dev/neuron12 --device /dev/neuron13 --device /dev/neuron14 --device /dev/neuron15\n      #This setting specifies the timeout (in seconds) for loading the model.\n      model_loading_timeout: 8000\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured,\n      # max: FMBench figures out the max number of model containers to be run\n      #      based on TP degree configured and number of neuron cores/GPUs available.\n      #      For example, if TP=2, GPUs=8 then FMBench will start 4 containers and 1 load balancer,\n      # auto: only supported if the underlying inference container would automatically\n      #       start multiple copies of the model internally based on TP degree and neuron cores/GPUs\n      #       available. In this case only a single container is created, no load balancer is created.\n      #       The DJL serving containers supports auto.\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      batch_size: 4\n      n_positions: 8192\n      shm_size: 12g\n      model_loading_timeout: 2400\n      # The max new tokens, context length of the model, and the neuron\n      # config are used within the model respository of the triton container\n      # during model deployment\n      max_new_tokens: 100\n      # context length for mistral is 8192\n      context_len: 8192\n      neuron_config:\n        max_length: 8192\n        top_k: 50\n        do_sample: true\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O3\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-7b-trn1-32xl-triton.yml",
      "scrapeId": "8a457b25-c222-4399-80c7-35d790c2338d",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-7b-trn1-32xl-triton.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"llama2-7b-byo-sagemaker-ep\"\n  model_name: \"Llama2-7b\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama2.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n\nmetrics:\n  dataset_of_interest: en_1-500\n  weights:\n    price_per_tx_wt: 0.65\n    latenct_wt: 0.35\n\npricing:\n  ml.g5.xlarge: 1.006\n  ml.g5.2xlarge: 1.212\n  ml.g5.12xlarge: 7.09\n  ml.g5.24xlarge: 10.18\n  ml.g5.48xlarge: 20.36\n  ml.inf2.24xlarge: 7.79\n  ml.inf2.48xlarge: 15.58\n  ml.p4d.24xlarge: 37.688\n\ninference_parameters:\n  do_sample: yes\n  temperature: 0.1\n  top_p: 0.92\n  top_k: 120\n  max_new_tokens: 100\n  truncate: at-prompt-token-length\n\n# Model configurations for llama-2 7b for deploying on g5 x and 2x large instances\nexperiments:\n  - name: llama2-7b-g5.2xlarge-bring-your-own-sm-endpoint\n    model_id: # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: llama2-7b-f\n    ep_name: <your-sagemaker-endpoint-name>\n    instance_type: \"ml.g5.2xlarge\"\n    image_uri:\n    deploy: no #setting to no since the endpoint has already been deployed\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\n  - name: llama2-7b-g5.2xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\n    model_id: meta-textgeneration-llama-2-7b-f\n    model_version: \"3.*\"\n    model_name: llama2-7b-f\n    ep_name: llama-2-7b-g5-2xlarge\n    instance_type: \"ml.g5.2xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\nreport:\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama2/7b/config-llama2-7b-byo-sagemaker-endpoint.yml",
      "scrapeId": "979a2198-18fa-4465-a54c-a29e788170e7",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama2/7b/config-llama2-7b-byo-sagemaker-endpoint.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g5.2xl-tp=1-mc=max-ec2\"\n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_2000-3000\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g5.2xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 1\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=1\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.2xl-tp-1-mc-max-ec2-conc-1-2.yml",
      "scrapeId": "9062be06-cb13-4cc4-9218-425d38ed1dbc",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.2xl-tp-1-mc-max-ec2-conc-1-2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-p5.48xl-ec2\"\n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 924\n    payload_file: payload_en_1-924.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"p5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: auto\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # - 6\n    # - 8\n    - 10\n    # - 12\n    # - 15\n    # - 20\n    # - 25\n    - 30\n    # - 40\n    # - 50\n    # - 60\n    # - 65\n    # - 70\n    # - 75\n    #- 100\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-tp-8-mc-auto-p5.yml",
      "scrapeId": "936a2071-f9d2-409a-99bc-ce20eba096c8",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-tp-8-mc-auto-p5.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"fmbench-bedrock\"\n  model_name: \"FMs available in Amazon Bedrock\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket}\n\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n\n    configs_prefix: configs\n    config_files:\n    - pricing.yml ## mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_titan_text.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: no\n    3_run_inference.ipynb: yes\n    4_get_evaluations.ipynb: yes\n    5_model_metric_analysis.ipynb: yes\n    6_cleanup.ipynb: no\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # - text\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# name of the file that contains the model evaluation information\n# change to model_eval_avg pooling for average pooling use cases\nmodel_evaluations: model_eval_all_info.yml\n\npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n\ninference_parameters:\n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  - name: amazon.titan-text-express-v1\n    model_id: amazon.titan-text-express-v1\n    model_version: \"*\"\n    model_name: amazon.titan-text-express-v1\n    ep_name: amazon.titan-text-express-v1\n    instance_type: amazon.titan-text-express-v1\n    image_uri:\n    deploy: no\n    instance_count:\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 50\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-titan-text-express.yml",
      "scrapeId": "995c7758-1f3f-49cc-bf8d-d55c223ed321",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-titan-text-express.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\npricing:\n  instance_based:\n    anthropic.claude-v3-sonnet-pt-nc: 88\n    ml.c5.2xlarge: 0.408\n    ml.c5.4xlarge: 0.816\n    ml.c5.xlarge: 0.204\n    ml.c7i.xlarge: 0.214\n    ml.g4dn.12xlarge: 4.89\n    ml.g4dn.16xlarge: 5.44\n    ml.g4dn.2xlarge: 0.94\n    ml.g4dn.4xlarge: 1.505\n    ml.g4dn.8xlarge: 2.72\n    ml.g4dn.xlarge: 0.7364\n    ml.g5.12xlarge: 7.09\n    ml.g5.24xlarge: 10.18\n    ml.g5.2xlarge: 1.515\n    ml.g5.48xlarge: 20.36\n    ml.g5.4xlarge: 2.03\n    ml.g5.8xlarge: 3.06\n    ml.g5.xlarge: 1.4084\n    ml.g6.12xlarge: 5.752\n    ml.g6.16xlarge: 4.246\n    ml.g6.24xlarge: 8.344\n    ml.g6.2xlarge: 1.222\n    ml.g6.48xlarge: 16.688\n    ml.inf2.24xlarge: 7.79\n    ml.inf2.48xlarge: 15.58\n    ml.inf2.8xlarge: 2.36\n    ml.inf2.xlarge: 0.99\n    ml.m5.xlarge: 0.23\n    ml.p3.2xlarge: 3.825\n    ml.p4d.24xlarge: 37.688\n    ml.p5.48xlarge: 113.068\n    ml.trn1.32xlarge: 28.497\n    p5e.48xlarge: 110.92\n  token_based:\n    ai21.j2-mid-v1:\n      input-per-1k-tokens: 0.0125\n      output-per-1k-tokens: 0.0125\n    ai21.j2-ultra-v1:\n      input-per-1k-tokens: 0.0188\n      output-per-1k-tokens: 0.0188\n    amazon.titan-text-express-v1:\n      input-per-1k-tokens: 0.0002\n      output-per-1k-tokens: 0.0006\n    amazon.titan-text-lite-v1:\n      input-per-1k-tokens: 0.00015\n      output-per-1k-tokens: 0.0002\n    anthropic.claude-3-5-sonnet-20240620-v1:0:\n      input-per-1k-tokens: 0.003\n      output-per-1k-tokens: 0.015\n    anthropic.claude-3-haiku-20240307-v1:0:\n      input-per-1k-tokens: 0.00025\n      output-per-1k-tokens: 0.00125\n    anthropic.claude-3-opus-20240229-v1:0:\n      input-per-1k-tokens: 0.015\n      output-per-1k-tokens: 0.075\n    anthropic.claude-3-sonnet-20240229-v1:0:\n      input-per-1k-tokens: 0.003\n      output-per-1k-tokens: 0.015\n    anthropic.claude-instant-v1:\n      input-per-1k-tokens: 0.0008\n      output-per-1k-tokens: 0.0024\n    anthropic.claude-v2:\n      input-per-1k-tokens: 0.008\n      output-per-1k-tokens: 0.024\n    anthropic.claude-v2:1:\n      input-per-1k-tokens: 0.008\n      output-per-1k-tokens: 0.024\n    cohere.command-light-text-v14:\n      input-per-1k-tokens: 0.0003\n      output-per-1k-tokens: 0.0006\n    cohere.command-r-plus-v1:0:\n      input-per-1k-tokens: 0.003\n      output-per-1k-tokens: 0.015\n    cohere.command-text-v14:\n      input-per-1k-tokens: 0.0015\n      output-per-1k-tokens: 0.002\n    meta.llama2-13b-chat-v1:\n      input-per-1k-tokens: 0.00075\n      output-per-1k-tokens: 0.001\n    meta.llama2-70b-chat-v1:\n      input-per-1k-tokens: 0.00195\n      output-per-1k-tokens: 0.00256\n    meta.llama3-1-405b-instruct-v1:0:\n      input-per-1k-tokens: 0.00532\n      output-per-1k-tokens: 0.016\n    us.meta.llama3-1-70b-instruct-v1:0:\n      input-per-1k-tokens: 0.00072\n      output-per-1k-tokens: 0.00072\n    us.meta.llama3-1-8b-instruct-v1:0:\n      input-per-1k-tokens: 0.00022\n      output-per-1k-tokens: 0.00022\n    meta.llama3-70b-instruct-v1:0:\n      input-per-1k-tokens: 0.00265\n      output-per-1k-tokens: 0.0035\n    meta.llama3-8b-instruct-v1:0:\n      input-per-1k-tokens: 0.0003\n      output-per-1k-tokens: 0.0006\n    mistral.mistral-7b-instruct-v0:2:\n      input-per-1k-tokens: 0.00015\n      output-per-1k-tokens: 0.0002\n    mistral.mixtral-8x7b-instruct-v0:1:\n      input-per-1k-tokens: 0.00045\n      output-per-1k-tokens: 0.0007\n    us.meta.llama3-2-11b-instruct-v1:0:\n      input-per-1k-tokens: 0.00016\n      output-per-1k-tokens: 0.00016\n    us.meta.llama3-2-1b-instruct-v1:0:\n      input-per-1k-tokens: 0.0001\n      output-per-1k-tokens: 0.0001\n    us.meta.llama3-2-3b-instruct-v1:0:\n      input-per-1k-tokens: 0.00015\n      output-per-1k-tokens: 0.00015\n    us.meta.llama3-2-90b-instruct-v1:0:\n      input-per-1k-tokens: 0.00072\n      output-per-1k-tokens: 0.00072\n    amazon.nova-micro-v1:0:\n      input-per-1k-tokens: 0.000035\n      output-per-1k-tokens: 0.00014\n    amazon.nova-lite-v1:0:\n      input-per-1k-tokens: 0.00006\n      output-per-1k-tokens: 0.00024\n    amazon.nova-pro-v1:0:\n      input-per-1k-tokens: 0.0008\n      output-per-1k-tokens: 0.0032\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/pricing.yml",
      "scrapeId": "8b8f257a-12e5-496e-a479-ed0ee1e5e55d",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/pricing.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-p4de.24xl-ec2\"\n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"p4de.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 10\n    - 30\n    - 35\n    - 40\n    - 50\n    - 60\n    - 65\n    - 70\n    - 75\n    - 100\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-p4-tp-2-mc-max.yml",
      "scrapeId": "a12f1561-1447-4312-baa8-5c7de33e107c",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-p4-tp-2-mc-max.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"llama3-8b-instruct-all\"\n  model_name: \"llama3-8b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\nmetrics:\n  dataset_of_interest: en_3000-3840\n\npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\nexperiments:\n  - name: llama-3-8b-instruct-g5-2xl-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct-g5.2xl\n    ep_name: llama-3-8b-instruct-g5-2xl\n    instance_type: \"ml.g5.2xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 5\n    - 8\n    - 10\n    - 16\n    - 20\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      job_queue_size: 1000\n      max_dynamic_batch_size: 1\n      max_batch_delay: 100\n      max_idle_time: 60\n      load_on_devices: *\n      engine: MPI\n      mpi_mode: true\n      option.entryPoint: null\n      option.tensor_parallel_degree: 1\n      option.max_rolling_batch_size: 256\n      option.mpi_mode: true\n      option.model_id: /opt/ml/model\n      option.rolling_batch: lmi-dist\n\n  - name: llama-3-8b-instruct-g5-48xl-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct-g5.48xl\n    ep_name: llama-3-8b-instruct-g5-48xl\n    instance_type: \"ml.g5.48xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 5\n    - 8\n    - 10\n    - 16\n    - 20\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"8\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"2\"\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      job_queue_size: 1000\n      max_dynamic_batch_size: 1\n      max_batch_delay: 100\n      max_idle_time: 60\n      load_on_devices: *\n      engine: MPI\n      mpi_mode: true\n      option.entryPoint: null\n      option.tensor_parallel_degree: 8\n      option.max_rolling_batch_size: 256\n      option.mpi_mode: true\n      option.model_id: /opt/ml/model\n      option.rolling_batch: lmi-dist\n\n  - name: llama-3-8b-instruct-g5-24xl-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct-g5.24xl\n    ep_name: llama-3-8b-instruct-g5-24xl\n    instance_type: \"ml.g5.24xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 5\n    - 8\n    - 10\n    - 16\n    - 20\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      job_queue_size: 1000\n      max_dynamic_batch_size: 1\n      max_batch_delay: 100\n      max_idle_time: 60\n      load_on_devices: *\n      engine: MPI\n      mpi_mode: true\n      option.entryPoint: null\n      option.tensor_parallel_degree: 4\n      option.max_rolling_batch_size: 256\n      option.mpi_mode: true\n      option.model_id: /opt/ml/model\n      option.rolling_batch: lmi-dist\n\n  - name: llama-3-8b-instruct-g5-12xl-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct-g5.12xl\n    ep_name: llama-3-8b-instruct-g5-12xl\n    instance_type: \"ml.g5.12xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 5\n    - 8\n    - 10\n    - 15\n    - 20\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      job_queue_size: 1000\n      max_dynamic_batch_size: 1\n      max_batch_delay: 100\n      max_idle_time: 60\n      load_on_devices: *\n      engine: MPI\n      mpi_mode: true\n      option.entryPoint: null\n      option.tensor_parallel_degree: 4\n      option.max_rolling_batch_size: 256\n      option.mpi_mode: true\n      option.model_id: /opt/ml/model\n      option.rolling_batch: lmi-dist\n  - name: llama-3-8b-instruct-p4d-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct\n    ep_name: llama-3-8b-instruct-p4d\n    instance_type: \"ml.p4d.24xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 15\n    - 20\n\n    accept_eula: true\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      job_queue_size: 1000\n      max_dynamic_batch_size: 1\n      max_batch_delay: 100\n      max_idle_time: 60\n      load_on_devices: *\n      engine: MPI\n      mpi_mode: true\n      option.entryPoint: null\n      option.tensor_parallel_degree: 8\n      option.max_rolling_batch_size: 256\n      option.mpi_mode: true\n      option.model_id: /opt/ml/model\n      option.rolling_batch: lmi-dist\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      # SM_NUM_GPUS: \"1\"\n      # SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n  - name: llama-3-8b-instruct-trn1-32xl-djl-inference:0.27.0-neuronx-sdk2.18.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version:\n    model_name: llama3-8b-trn1.32xl\n    ep_name: \"<your-endpoint-name>\"\n    instance_type: \"ml.trn1.32xlarge\"\n    image_uri:\n    deploy: no\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # currently the llama3-8b jumpstart model is compiled with a batch size of 1\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 8\n    - 10\n    - 15\n    - 20\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      model_id_or_path='/tmp/.djl.ai/download/ae03dd100c208acd82b5dbed563c971de864c408'\n      rolling_batch=<RollingBatchEnum.auto: 'auto'>\n      tensor_parallel_degree=8\n      trust_remote_code=False\n      enable_streaming=<StreamingEnum.false: 'false'>\n      batch_size=4\n      max_rolling_batch_size=4\n      dtype=<Dtype.f16: 'fp16'>\n      revision=None\n      output_formatter=None\n      waiting_steps=None\n      is_mpi=False\n      draft_model_id=None\n      spec_length=0\n      neuron_optimize_level=None\n      enable_mixed_precision_accumulation=False\n      enable_saturate_infinity=False\n      n_positions=4096\n      unroll=None\n      load_in_8bit=False\n      low_cpu_mem_usage=False\n      load_split_model=True\n      context_length_estimate=None\n      amp='f16'\n      quantize=None\n      compiled_graph_path=None\n      task=None\n      save_mp_checkpoint_path=None\n      group_query_attention=None\n      model_loader=<TnXModelLoaders.tnx: 'tnx'>\n      rolling_batch_strategy=<TnXGenerationStrategy.continuous_batching: 'continuous_batching'>\n      fuse_qkv=False\n      on_device_embedding=False\n      attention_layout=None\n      collectives_layout=None\n      cache_layout=None\n      partition_schema=None\n      all_reduce_dtype=None\n      cast_logits_dtype=None\n  - name: llama-3-8b-instruct-inf2-48xl-djl-inference:0.27.0-neuronx-sdk2.18.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version:\n    model_name: llama3-8b-inf2.24xl\n    ep_name: \"<your-endpoint-name>\"\n    instance_type: \"ml.inf2.48xlarge\"\n    image_uri:\n    deploy: no\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # currently the llama3-8b jumpstart model is compiled with a batch size of 1\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 8\n    - 10\n    - 15\n    - 20\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      model_id_or_path='/tmp/.djl.ai/download/ae03dd100c208acd82b5dbed563c971de864c408'\n      rolling_batch=<RollingBatchEnum.auto: 'auto'>\n      tensor_parallel_degree=8\n      trust_remote_code=False\n      enable_streaming=<StreamingEnum.false: 'false'>\n      batch_size=4\n      max_rolling_batch_size=4\n      dtype=<Dtype.f16: 'fp16'>\n      revision=None\n      output_formatter=None\n      waiting_steps=None\n      is_mpi=False\n      draft_model_id=None\n      spec_length=0\n      neuron_optimize_level=None\n      enable_mixed_precision_accumulation=False\n      enable_saturate_infinity=False\n      n_positions=4096\n      unroll=None\n      load_in_8bit=False\n      low_cpu_mem_usage=False\n      load_split_model=True\n      context_length_estimate=None\n      amp='f16'\n      quantize=None\n      compiled_graph_path=None\n      task=None\n      save_mp_checkpoint_path=None\n      group_query_attention=None\n      model_loader=<TnXModelLoaders.tnx: 'tnx'>\n      rolling_batch_strategy=<TnXGenerationStrategy.continuous_batching: 'continuous_batching'>\n      fuse_qkv=False\n      on_device_embedding=False\n      attention_layout=None\n      collectives_layout=None\n      cache_layout=None\n      partition_schema=None\n      all_reduce_dtype=None\n      cast_logits_dtype=None\n\n  - name: llama-3-8b-instruct-inf2-24xl-djl-inference:0.27.0-neuronx-sdk2.18.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version:\n    model_name: llama3-8b-inf2.24xl\n    ep_name: \"<your-endpoint-name>\"\n    instance_type: \"ml.inf2.24xlarge\"\n    image_uri:\n    deploy: no\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # currently the llama3-8b jumpstart model is compiled with a batch size of 1\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 15\n    - 20\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      model_id_or_path='/tmp/.djl.ai/download/ae03dd100c208acd82b5dbed563c971de864c408'\n      rolling_batch=<RollingBatchEnum.auto: 'auto'>\n      tensor_parallel_degree=8\n      trust_remote_code=False\n      enable_streaming=<StreamingEnum.false: 'false'>\n      batch_size=4\n      max_rolling_batch_size=4\n      dtype=<Dtype.f16: 'fp16'>\n      revision=None\n      output_formatter=None\n      waiting_steps=None\n      is_mpi=False\n      draft_model_id=None\n      spec_length=0\n      neuron_optimize_level=None\n      enable_mixed_precision_accumulation=False\n      enable_saturate_infinity=False\n      n_positions=4096\n      unroll=None\n      load_in_8bit=False\n      low_cpu_mem_usage=False\n      load_split_model=True\n      context_length_estimate=None\n      amp='f16'\n      quantize=None\n      compiled_graph_path=None\n      task=None\n      save_mp_checkpoint_path=None\n      group_query_attention=None\n      model_loader=<TnXModelLoaders.tnx: 'tnx'>\n      rolling_batch_strategy=<TnXGenerationStrategy.continuous_batching: 'continuous_batching'>\n      fuse_qkv=False\n      on_device_embedding=False\n      attention_layout=None\n      collectives_layout=None\n      cache_layout=None\n      partition_schema=None\n      all_reduce_dtype=None\n      cast_logits_dtype=None\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-all.yml",
      "scrapeId": "8e3c0c2d-954e-4546-9ce6-c6a2d7fe71df",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-all.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g5.12xl-ec2\"\n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 924\n    payload_file: payload_en_1-924.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g5.12xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.12xl-tp-4-mc-max-ec2.yml",
      "scrapeId": "917cf9ff-2dc3-42e2-8c26-e205ae94299a",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.12xl-tp-4-mc-max-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.2-1b-g5.2xl-tp=1-mc=max-djl-ec2\"\n  model_name: \"Meta-Llama-3.2-1b-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - hf:databricks/databricks-dolly-15k\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_claude_dolly_dataset.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: no\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - instruction\n  - context\n  ground_truth_col_key: response\n  question_col_key: instruction\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 1000\n    payload_file: payload_en_1-1000.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1-1000\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 20\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.2-1b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.2-1b-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.2-1b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g5.4xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.2-1b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 1\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=1\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Llama-3.2-1b-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-1000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 0.1\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.2/1b/config-llama3.2-1b-g5.2xl-tp-1-mc-max-djl-ec2.yml",
      "scrapeId": "9dc00310-3b61-4176-8a87-13d1e6d5a1de",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.2/1b/config-llama3.2-1b-g5.2xl-tp-1-mc-max-djl-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.2-3b-g5.4xl-tp=1-mc=max-djl-ec2\"\n  model_name: \"Meta-Llama-3.2-3B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.2_vision.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: no\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.2-11B-Vision-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.2-11B-Vision-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g6e.2xlarge\"\n    image_uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.30.0-lmi12.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.2-3b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      container_type: djl\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 1\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      option.tensor_parallel_degree=1\n      option.max_rolling_batch_size=8\n      option.model_id=meta-llama/Llama-3.2-11B-Vision-Instruct\n      option.rolling_batch=vllm\n      option.enforce_eager=true\n      option.max_rolling_batch_prefill_tokens=4096\n      option.max_model_len=4096\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n    - 8\n    - 10\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.2/11b/config-llama3.2-11b-g6e-2xl-tp-1-mc-max-djl-vllm-ec2.yml",
      "scrapeId": "8c98671a-0dad-4aa0-9c8a-a4a69be69f27",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.2/11b/config-llama3.2-11b-g6e-2xl-tp-1-mc-max-djl-vllm-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g5.12xl-ec2\"\n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 924\n    payload_file: payload_en_1-924.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g5.12xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.12xl-tp-2-mc-max-ec2.yml",
      "scrapeId": "8de07e2e-8664-443e-afd3-3dee6b9a4e72",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.12xl-tp-2-mc-max-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"llama3-8b-trn1-32xl-byoe-g5-24xl\"\n  model_name: \"llama3-8b\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the\n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: llama-3-8b-instruct-trn1-32xl-djl-inference:0.27.0-neuronx-sdk2.18.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: llama-3-8b-instruct\n    model_version:\n    model_name: llama3-8b-trn1.32xl\n    ep_name: meta-llama-Meta-Llama-3-8B-Instruct-inf-2024-06-07-16-59-37-956\n    instance_type: \"ml.trn1.32xlarge\"\n    image_uri:\n    deploy: no\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # currently the llama3-8b jumpstart model is compiled with a batch size of 1\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      model_id_or_path='/tmp/.djl.ai/download/ae03dd100c208acd82b5dbed563c971de864c408'\n      rolling_batch=<RollingBatchEnum.auto: 'auto'>\n      tensor_parallel_degree=8\n      trust_remote_code=False\n      enable_streaming=<StreamingEnum.false: 'false'>\n      batch_size=4\n      max_rolling_batch_size=4\n      dtype=<Dtype.f16: 'fp16'>\n      revision=None\n      output_formatter=None\n      waiting_steps=None\n      is_mpi=False\n      draft_model_id=None\n      spec_length=0\n      neuron_optimize_level=None\n      enable_mixed_precision_accumulation=False\n      enable_saturate_infinity=False\n      n_positions=4096\n      unroll=None\n      load_in_8bit=False\n      low_cpu_mem_usage=False\n      load_split_model=True\n      context_length_estimate=None\n      amp='f16'\n      quantize=None\n      compiled_graph_path=None\n      task=None\n      save_mp_checkpoint_path=None\n      group_query_attention=None\n      model_loader=<TnXModelLoaders.tnx: 'tnx'>\n      rolling_batch_strategy=<TnXGenerationStrategy.continuous_batching: 'continuous_batching'>\n      fuse_qkv=False\n      on_device_embedding=False\n      attention_layout=None\n      collectives_layout=None\n      cache_layout=None\n      partition_schema=None\n      all_reduce_dtype=None\n      cast_logits_dtype=None\n\n  - name: llama-3-8b-instruct-g5-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct-g5.24xl\n    ep_name: llama-3-8b-instruct-g5-24xl\n    instance_type: \"ml.g5.24xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      # SM_NUM_GPUS: \"1\"\n      # SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\n# parameters related to how the final report is generated\nreport:\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/llama3-8b-trn1-32xl-byoe-g5-24xl.yml",
      "scrapeId": "8dd97bbc-7daa-4f36-a566-85036b8d441d",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/llama3-8b-trn1-32xl-byoe-g5-24xl.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-70b-trn1.32xl-ec2-triton\"\n  model_name: \"Meta-Llama-3-70B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    top_k: 50\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-70B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-70B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-70B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8080/v2/models/Llama-3.1-70B-Instruct/generate'\n    instance_type: \"trn1.32xlarge\"\n    image_uri: tritonserver-neuronx:fmbench\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model.\n      model_loading_timeout: 10000\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # For deploying a model using the triton inference container:\n      # mention the backend type, if any. Options for triton are: [djl, vllm or tensorrt]\n      backend: djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured,\n      # max: FMBench figures out the max number of model containers to be run\n      #      based on TP degree configured and number of neuron cores/GPUs available.\n      #      For example, if TP=2, GPUs=8 then FMBench will start 4 containers and 1 load balancer,\n      # auto: only supported if the underlying inference container would automatically\n      #       start multiple copies of the model internally based on TP degree and neuron cores/GPUs\n      #       available. In this case only a single container is created, no load balancer is created.\n      #       The DJL serving containers supports auto.\n      model_copies: max\n      shm_size: 12g\n      # The model.json parameters are replaced within the model.json file\n      # for the triton on vllm/djl/tensorrt options. The model.json already contains\n      # the tp degree and model id from above in this config file. This is a dictionary\n      # that contains other variables, such as max_num_seqs, dtype, max_model_len, block_size and more\n      # For tensorrt, the tp degree, batch size and other relevant parameters are\n      # extracted directly from the inference spec.\n      container_params:\n        # tp degree is a mandatory parameter\n        tp_degree: 32\n        amp: \"f16\"\n        attention_layout: 'BSH'\n        collectives_layout: 'BSH'\n        context_length_estimate: 3072, 3584, 4096\n        max_rolling_batch_size: 8\n        model_loader: \"tnx\"\n        model_loading_timeout: 2400\n        n_positions: 4096\n        output_formatter: \"json\"\n        rolling_batch: \"auto\"\n        rolling_batch_strategy: \"continuous_batching\"\n        trust_remote_code: true\n        # modify the serving properties to match your model and requirements\n        serving.properties:\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n    - 8\n    - 10\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O3\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 200\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-llama3-1-70b-trn1.32xl-deploy-ec2-triton.yml",
      "scrapeId": "9a01e9eb-5ac2-4280-be90-24e0831907f8",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-llama3-1-70b-trn1.32xl-deploy-ec2-triton.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-8b-inf2.48xl-ec2\"\n  model_name: \"llama3-1-8b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-8b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    sagemaker_execution_role: {role_arn}\n    # S3 bucket to which metrics, plots and reports would be written to\n    bucket: {write_bucket} ## add the name of your desired bucket\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    prefix: \"lmi\"\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-8b-Instruct\"\n    model_id_wo_repo: \"Meta-Llama-3.1-8b-Instruct\"\n    model_id_wo_repo_split: \"Meta-Llama-3.1-8b-Instruct-split\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name:\n    instance_type: \"ml.inf2.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: neuron_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: sagemaker_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      batch_size: \"1\"\n      num_neuron_cores: \"24\"\n      neuron_version: \"2.19.1\"\n      model_loading_timeout: \"2400\"\n      instance_count: \"1\"\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=s3://{write_bucket}/lmi/Meta-Llama-3.1-8b-Instruct/Meta-Llama-3.1-8b-Instruct-split/\n      option.load_split_model=True\n      option.tensor_parallel_degree=24\n      option.n_positions=8192\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=8\n      option.dtype=fp16\n      option.model_loading_timeout=2400\n      option.neuron_optimize_level=2\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-inf2.yml",
      "scrapeId": "9a2d9755-cf77-454a-aa5e-c96f25ba310b",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-inf2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-70b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3.1-70b-g6e.48xl-tp=8-mc=max-djl-ec2\"\n  model_name: \"Meta-Llama-3.1-70b-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-70b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-70B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-70b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g6e.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged witsh multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-70b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-70b-Instruct\n      option.rolling_batch=lmi-dist\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n    - 7\n    - 8\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 200\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-llama3-1-70b-g6e.48xl-tp-8-mc-max-djl.yml",
      "scrapeId": "8e6a5bb7-89bb-4528-ace7-fdddaacf9476",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-llama3-1-70b-g6e.48xl-tp-8-mc-max-djl.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"llama3-8b-inf2-24xl-byoe-g5-12xl\"\n  model_name: \"llama3-8b\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the\n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: llama-3-8b-instruct-inf2-24xl-djl-inference:0.27.0-neuronx-sdk2.18.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: llama-3-8b-instruct\n    model_version:\n    model_name: llama3-8b-inf2.24xl\n    ep_name: meta-llama-Meta-Llama-3-8B-Instruct-inf-2024-06-07-12-29-27-768\n    instance_type: \"ml.inf2.24xlarge\"\n    image_uri:\n    deploy: no\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # currently the llama3-8b jumpstart model is compiled with a batch size of 1\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      model_id_or_path='/tmp/.djl.ai/download/ae03dd100c208acd82b5dbed563c971de864c408'\n      rolling_batch=<RollingBatchEnum.auto: 'auto'>\n      tensor_parallel_degree=8\n      trust_remote_code=False\n      enable_streaming=<StreamingEnum.false: 'false'>\n      batch_size=4\n      max_rolling_batch_size=4\n      dtype=<Dtype.f16: 'fp16'>\n      revision=None\n      output_formatter=None\n      waiting_steps=None\n      is_mpi=False\n      draft_model_id=None\n      spec_length=0\n      neuron_optimize_level=None\n      enable_mixed_precision_accumulation=False\n      enable_saturate_infinity=False\n      n_positions=4096\n      unroll=None\n      load_in_8bit=False\n      low_cpu_mem_usage=False\n      load_split_model=True\n      context_length_estimate=None\n      amp='f16'\n      quantize=None\n      compiled_graph_path=None\n      task=None\n      save_mp_checkpoint_path=None\n      group_query_attention=None\n      model_loader=<TnXModelLoaders.tnx: 'tnx'>\n      rolling_batch_strategy=<TnXGenerationStrategy.continuous_batching: 'continuous_batching'>\n      fuse_qkv=False\n      on_device_embedding=False\n      attention_layout=None\n      collectives_layout=None\n      cache_layout=None\n      partition_schema=None\n      all_reduce_dtype=None\n      cast_logits_dtype=None\n\n  - name: llama-3-8b-instruct-g5-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct-g5.12xl\n    ep_name: llama-3-8b-instruct-g5-12xl\n    instance_type: \"ml.g5.12xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\n# parameters related to how the final report is generated\nreport:\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/llama3-8b-inf2-24xl-byoe-g5-12xl.yml",
      "scrapeId": "9c3d6660-ddab-4602-95b9-5c88ef999d5d",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/llama3-8b-inf2-24xl-byoe-g5-12xl.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a Qwen2.5-72b deployed on ec2\ngeneral:\n  # if it is auto, then retrieve the model id and the instance type/container -\n  # derive the name based on that (?)\n  # if name: name, then provide the name as -A <model-id-instance>\n  name: {results_dir}\n  model_name: {model_id}\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_qwen.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: {model_id}\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: {model_id} # model id, version and image uri not needed for byo endpoint\n    hf_model_id: {model_id}\n    model_version:\n    model_name: {model_id}\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: {instance_type}\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a Qwen deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: {tp_degree}\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree={tp_degree}\n      option.max_rolling_batch_size={batch_size}\n      option.model_id={model_id}\n      # Setting this to 68832 because the default max model len that djl sets is 131702.\n      # For Qwen, the default max model len that djl sets (131072) cannot be larger than the\n      # maximum number of tokens that can be stored in the KV cache. For Qwen2.5-72b, the\n      # max tokens that can be stored in the KV cache is 68832\n      option.max_model_len={max_model_len}\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/generic/ec2/Qwen2.5_djl.yml",
      "scrapeId": "97c575e8-4cbf-4ee2-9540-9e2178d5018f",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/generic/ec2/Qwen2.5_djl.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"llama3-8b-inf2-g5-v1\"\n  model_name: \"llama3-8b\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the\n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: llama-3-8b-instruct-inf2-djl-inference:0.27.0-neuronx-sdk2.18.1\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: meta-textgenerationneuron-llama-3-8b-instruct\n    # If you want to use an hf model id tokenizer for the model that is being benchmarked,\n    # add the model id below to the \"hf_tokenizer_model_id\" parameter. If this is not provided\n    # OR if the config.json and tokenizer.json are not loaded in the read directory, then the\n    # default 750-1000 tokens tokenizer will be used.\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-inf2.24xl\n    ep_name: llama-3-8b-instruct-inf2-24xl\n    instance_type: \"ml.inf2.24xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.28.0-neuronx-sdk2.18.2'\n    deploy: yes\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script: jumpstart.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # currently the llama3-8b jumpstart model is compiled with a batch size of 1\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 8\n    - 10\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: '1'\n      SERVING_LOAD_MODELS: 'test::Python=/opt/ml/model'\n      OPTION_ENTRYPOINT: 'inference.py'\n      OPTION_TENSOR_PARALLEL_DEGREEL: '12'\n      OPTION_N_POSITIONS: '8192'\n      OPTION_ROLLING_BATCH: 'auto'\n      OPTION_LOAD_SPLIT_MODEL: 'False'\n      OPTION_NEURON_OPTIMIZE_LEVEL: '2'\n      OPTION_DTYPE: 'bf16'\n      OPTION_MAX_ROLLING_BATCH_SIZE: '1'\n      OPTION_MODEL_LOADING_TIMEOUT: '6000'\n      OPTION_TRUST_REMOTE_CODE: 'true'\n      OPTION_FUSE_QKV: 'true'\n      OPTION_ATTENTION_LAYOUT: 'BSH'\n      OPTION_GROUP_QUERY_ATTENTION: 'replicated-heads'\n\n  - name: llama-3-8b-instruct-g5-djl-deepspeed0.12.6-cu121\n    model_id: meta-textgeneration-llama-3-8b-instruct\n    hf_tokenizer_model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct-g5.12xl\n    ep_name: llama-3-8b-instruct-g5-12xl\n    instance_type: \"ml.g5.12xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.28.0-lmi10.0.0-cu124'\n    deploy: yes\n    instance_count: 1\n    deployment_script: jumpstart.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 8\n    - 10\n\n    accept_eula: true\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\n# parameters related to how the final report is generated\nreport:\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 2\n  cost_per_10k_txn_budget: 30\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-inf2-g5.yml",
      "scrapeId": "9fc13d6d-c373-4af2-995a-41876b3b8610",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-inf2-g5.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "[Skip to content](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_eks.html#benchmark-models-on-eks)\n\n# Benchmark models on EKS [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_eks.html\\#benchmark-models-on-eks \"Permanent link\")\n\nYou can use `FMBench` to benchmark models on hosted on EKS. This can be done in one of two ways:\n\n- Deploy the model on your EKS cluster independantly of `FMBench` and then benchmark it through the [Bring your own endpoint](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_eks.html#bring-your-own-endpoint-aka-support-for-external-endpoints) mode.\n- Deploy the model on your EKS cluster through `FMBench` and then benchmark it.\n\nThe steps for deploying the model on your EKS cluster are described below.\n\n👉 **_EKS cluster creation itself is not a part of the `FMBench` functionality, the cluster needs to exist before you run the following steps_**. Steps for cluster creation are provided in [this](https://aws-samples.github.io/foundation-model-benchmarking-tool/misc/eks_cluster-creation_steps.html) file but it would be best to consult the [DoEKS](https://github.com/awslabs/data-on-eks) repo on GitHub for comprehensive instructions.\n\n1. Add the following IAM policies to your existing `FMBench` Role:\n1. [AmazonEKSClusterPolicy](https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonEKSClusterPolicy.html): This policy provides Kubernetes the permissions it requires to manage resources on your behalf.\n\n2. [AmazonEKS\\_CNI\\_Policy](https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonEKS_CNI_Policy.html): This policy provides the Amazon VPC CNI Plugin (amazon-vpc-cni-k8s) the permissions it requires to modify the IP address configuration on your EKS worker nodes. This permission set allows the CNI to list, describe, and modify Elastic Network Interfaces on your behalf.\n\n3. [AmazonEKSWorkerNodePolicy](https://docs.aws.amazon.com/aws-managed-policy/latest/reference/AmazonEKSWorkerNodePolicy.html): This policy allows Amazon EKS worker nodes to connect to Amazon EKS Clusters.\n2. Once the EKS cluster is available you can use either the following two files or create your own config files using these files as examples for running benchmarking for these models. **_These config files require that the EKS cluster has been created as per the steps in these [instructions](https://awslabs.github.io/data-on-eks/docs/gen-ai/inference/llama3-inf2)_**.\n\n\n1. [config-llama3-8b-eks-inf2.yml](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/src/fmbench/configs/llama3/8b/config-llama3-8b-eks-inf2.yml): Deploy Llama3 on Trn1/Inf2 instances.\n\n2. [config-mistral-7b-eks-inf2.yml](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/main/src/fmbench/configs/mistral/config-mistral-7b-eks-inf2.yml): Deploy Mistral 7b on Trn1/Inf2 instances.\n\n\nFor more information about the [blueprints](https://github.com/aws-samples/foundation-model-benchmarking-tool/tree/main/src/fmbench/configs/eks_manifests) used by FMBench to deploy these models, view: [DoEKS docs gen-ai](https://awslabs.github.io/data-on-eks/docs/gen-ai).\n\n3. Run the `Llama3-8b` benchmarking using the command below (replace the config file as needed for a different model). This will first deploy the model on your EKS cluster and then run benchmarking on the deployed model.\n\n\n\n```md-code__content\nfmbench --config-file https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/fmbench/configs/llama3/8b/config-llama3-8b-eks-inf2.yml > fmbench.log 2>&1\n\n```\n\n4. As the model is getting deployed you might want to run the following `kubectl` commands to monitor the deployment progress. Set the _model\\_namespace_ to `llama3` or `mistral` or a different model as appropriate.\n1. `kubectl get pods -n <model_namespace> -w`: Watch the pods in the model specific namespace.\n2. `kubectl -n karpenter get pods`: Get the pods in the karpenter namespace.\n3. `kubectl describe pod -n <model_namespace> <pod-name>`: Describe a specific pod in the mistral namespace to view the live logs.",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_eks.html",
      "title": "EKS - Foundation Model Benchmarking Tool (FMBench)",
      "favicon": {},
      "language": "en",
      "scrapeId": "959c40b5-d3bc-40ee-9cd2-d9c26235883b",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_eks.html",
      "statusCode": 200,
      "theme-color": "#00000000",
      "color-scheme": "normal"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g6e.24xl-tp=4-mc=max-djl-ec2\"\n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g6e.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g6e.24xl-tp-4-mc-max-djl.yml",
      "scrapeId": "9a132235-45e9-4df8-a9f0-a12426063a42",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g6e.24xl-tp-4-mc-max-djl.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"llama3-8b-p5-djl-lmi-dist\"\n  model_name: \"llama3-8b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n    configs_prefix: configs\n    config_files:\n    - pricing.yml # mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    # - rajpurkar/squad_v2.jsonl\n    - 2wikimqa_e.jsonl\n    - 2wikimqa.jsonl\n    - hotpotqa_e.jsonl\n    - hotpotqa.jsonl\n    - narrativeqa.jsonl\n    - triviaqa_e.jsonl\n    - triviaqa.jsonl\n    tokenizer_prefix: llama3_tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_llama3.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  prompt_template_keys:\n  - input\n  - context\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\nmetrics:\n  dataset_of_interest: en_3000-3840\n\npricing: pricing.yml\n\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    stop: '<|eot_id|>'\n\nexperiments:\n\n  - name: llama-3-8b-instruct-p5-djl-deepspeed0.12.6-cu121\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct\n    model_version: \"*\"\n    model_name: llama3-8b-instruct\n    ep_name: llama-3-8b-instruct-p5\n    instance_type: \"ml.p5.48xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121'\n    deploy: yes\n    instance_count: 1\n    deployment_script: deploy_w_djl_serving.py\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      parameter_set: sagemaker\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.max_rolling_batch_size=256\n      option.rolling_batch=lmi-dist\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    - 15\n    - 20\n    - 30\n\n    accept_eula: true\n    # optional metadata about the model, this is not used for anything other than\n    # just being logged in the report as is as part of the config file dump\n    additional_metadata: |\n      job_queue_size: 1000\n      max_dynamic_batch_size: 1\n      max_batch_delay: 100\n      max_idle_time: 60\n      load_on_devices: *\n      engine: MPI\n      mpi_mode: true\n      option.entryPoint: null\n      option.tensor_parallel_degree: 8\n      option.max_rolling_batch_size: 256\n      option.mpi_mode: true\n      option.model_id: /opt/ml/model\n      option.rolling_batch: lmi-dist\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      # SM_NUM_GPUS: \"1\"\n      # SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-p5-djl-lmi-dist.yml",
      "scrapeId": "9b0a0f86-2723-4d7c-a306-edda80d784d0",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-instruct-p5-djl-lmi-dist.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"fmbench-bedrock-claude-sonnet-dolly-dataset\"\n  model_name: \"Claude Sonnet available in Amazon Bedrock\"\n\n# AWS and SageMaker settings\naws:\n  region: {region}\n  # uncomment and set the Role ARN if not running on sagemaker\n  sagemaker_execution_role: {role_arn}\n  ## these are the buckets/resources you will create in your account below:\n  bucket: {write_bucket}\n\n## WRITE BUCKET -- Write the results, data, metrics, endpoint.json and payloads to this bucket directory\ndir_paths:\n    data_prefix: data ## add the prefix for all your data management/storage\n    prompts_prefix: prompts\n    all_prompts_file: all_prompts.csv\n    metrics_dir: metrics\n    models_dir: models\n    metadata_dir: metadata ## add a file here to dynamically track the metrics dir\n\n## READ BUCKET -- Represents the section to read from scripts, source data and tokenizer for a separate s3 bucket for read/write segregation\ns3_read_data:\n    read_bucket: {read_bucket}\n    scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n    script_files:\n    - hf_token.txt  ## add your scripts files you have in s3 (including inference files, serving stacks, if any)\n\n    configs_prefix: configs\n    config_files:\n    - pricing.yml ## mention the name of the config files that you want to be downloaded from s3 into the configs directory locally\n    source_data_prefix: source_data  ## Add a source_data folder to store your raw data in an s3 path configured by you\n    source_data_files:\n    - hf:databricks/databricks-dolly-15k\n    tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n    prompt_template_dir: prompt_template\n    prompt_template_file: prompt_template_claude_dolly_dataset.txt ## add your desired prompt template type\n\n## section that enables container to run notebooks and python scripts automatically\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: no\n    3_run_inference.ipynb: yes\n    4_get_evaluations.ipynb: yes\n    5_model_metric_analysis.ipynb: yes\n    6_cleanup.ipynb: no\n\ndatasets:\n  prompt_template_keys:\n  - instruction\n  - context\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\nmetrics:\n  dataset_of_interest: en_1-500\n\n\npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n\ninference_parameters:\n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  - name: anthropic.claude-3-sonnet-20240229-v1:0\n    model_name: anthropic.claude-3-sonnet-20240229-v1:0\n    ep_name: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_type: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n\nreport:\n  latency_budget: 5\n  cost_per_10k_txn_budget: 350\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-claude-dolly-dataset.yml",
      "scrapeId": "a9caed0f-d831-44cd-9da0-49d15023cc16",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-claude-dolly-dataset.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g5.24xl-ec2\"\n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g5.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.24xl-tp-4-mc-max-ec2.yml",
      "scrapeId": "a4f60f1b-b504-449d-9a29-a522fe09aaec",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.24xl-tp-4-mc-max-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g6e.2xl-tp=1-mc=max-djl-ec2\"\n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3.1-8b-instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g6e.2xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      tp_degree: 1\n      shm_size: 12g\n      model_loading_timeout: 2400\n      model_copies: max\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=1\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g6e.2xl-tp-1-mc-max-djl.yml",
      "scrapeId": "a22979e1-6143-4ebc-bd58-bd80e5e53529",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g6e.2xl-tp-1-mc-max-djl.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "[Skip to content](https://aws-samples.github.io/foundation-model-benchmarking-tool/deepseek.html#deepseek-r1)\n\n# DeepSeek-R1 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/deepseek.html\\#deepseek-r1 \"Permanent link\")\n\nThe distilled version of Deepseek-R1 models are now supported for both performance benchmarking and model evaluations 🎉. You can use built in support for 4 different datasets: [`LongBench`](https://huggingface.co/datasets/THUDM/LongBench), [`Dolly`](https://huggingface.co/datasets/databricks/databricks-dolly-15k), [`OpenOrca`](https://huggingface.co/datasets/Open-Orca/OpenOrca) and [`ConvFinQA`](https://huggingface.co/datasets/AdaptLLM/finance-tasks/tree/refs%2Fconvert%2Fparquet/ConvFinQA). You can deploy the Deepseek-R1 distilled models on Amazon EC2, Amazon Bedrock or Amazon SageMaker.\n\nThe easiest way to benchmark the DeepSeek models is through the [`FMBench-orchestrator`](https://github.com/awslabs/fmbench-orchestrator) on Amazon EC2 VMs.\n\n## Benchmark Deepseek-R1 distilled models on Amazon EC2 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/deepseek.html\\#benchmark-deepseek-r1-distilled-models-on-amazon-ec2 \"Permanent link\")\n\n👉 Make sure your account has enough service quota for vCPUs to run this benchmark. We would be using `g6e.xlarge`, `g6e.2xlarge`, `g6e.12xlarge` and `g6e.48xlarge` instances, if you do not have sufficient service quota then you can set `deploy: no` in the `configs/deepseek/deepseek-convfinqa.yml` (or other) file to disable some tests as needed.\n\nFollow instructions [here](https://github.com/awslabs/fmbench-orchestrator?tab=readme-ov-file#install-fmbench-orchestrator-on-ec2) to install the orchestrator. Once installed you can run Deepseek-r1 benchmarking with the [`ConvFinQA`](https://huggingface.co/datasets/AdaptLLM/finance-tasks/tree/refs%2Fconvert%2Fparquet/ConvFinQA) dataset the following command:\n\n```md-code__content\npython main.py --config-file configs/deepseek/deepseek-convfinqa.yml\n\n```\n\nChange the `--config-file` parameter to [`configs/deepseek/deepseek-longbench.yml`](https://github.com/aws-samples/fmbench-orchestrator/blob/main/configs/deepseek/deepseek-longbench.yml) or [`configs/deepseek/deepseek-openorca.yml`](https://github.com/aws-samples/fmbench-orchestrator/blob/main/configs/deepseek/deepseek-openorca.yml) to use other datasets for benchmarking. These orchestrator files test various Deepseek-R1 distilled models on `g6e` instances, edit this file as per your requirements.\n\n## Benchmark Deepseek-R1 quantized models on Amazon EC2 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/deepseek.html\\#benchmark-deepseek-r1-quantized-models-on-amazon-ec2 \"Permanent link\")\n\n👉 Make sure your account has enough service quota for vCPUs to run this benchmark. We would be using `g6e.12xlarge` instance for this test.\n\n1. Create a `g6e.12xlarge` instance and run the `DeepSeek-R1 1.58b quantized` model on this instance by following the steps 1 through 8 described [here](https://github.com/aarora79/deepseek-r1-ec2?tab=readme-ov-file#quantized-models).\n\n2. Follow steps 1 through 5 [here](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#benchmarking-on-an-instance-type-with-nvidia-gpus-or-aws-chips) to setup `FMBench` on this instance.\n\n3. Next run the following command to benchmark LongBench\n\n\n\n```md-code__content\nTMP_DIR=/tmp\nfmbench --config-file $TMP_DIR/fmbench-read/configs/deepseek/config-deepseek-r1-quant1.58-longbench-byoe.yml --local-mode yes --write-bucket placeholder --tmp-dir $TMP_DIR > fmbench.log 2>&1\n\n```\n\n4. Once the run completes you should see the benchmarking results in a folder called `results-DeepSeek-R1-quant-1.58bit-g6e.12xl` present in your current directory.",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/deepseek.html",
      "title": "Deepseek - Foundation Model Benchmarking Tool (FMBench)",
      "favicon": {},
      "language": "en",
      "scrapeId": "a0a4b982-2579-44f2-8d7d-5987e0ec2bbf",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/deepseek.html",
      "statusCode": 200,
      "theme-color": "#00000000",
      "color-scheme": "normal"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"llama2-7b-v1\"\n  model_name: \"Llama2-7b\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # Use S3 only, local file system only, or both (values are s3, local or both)\n  # If set to local or both, set the local_file_system_path parameter\n  s3_and_or_local_file_system: local\n  local_file_system_path: {write_tmpdir}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # Use S3 only or local file system only (values are s3 or local)\n  # If set to local, set the local_file_system_path parameter\n  s3_or_local_file_system: local\n  local_file_system_path: {read_tmpdir}\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama2.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the\n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_2000-3000\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: llama2-7b-g5.xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: meta-textgeneration-llama-2-7b-f\n    model_version: \"3.*\"\n    model_name: llama2-7b-f\n    ep_name: llama-2-7b-g5xlarge\n    instance_type: \"ml.g5.xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04'\n    deploy: yes\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script: jumpstart.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    #- payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\n  - name: llama2-7b-g5.2xlarge-huggingface-pytorch-tgi-inference-2.0.1-tgi1.1.0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: meta-textgeneration-llama-2-7b-f\n    model_version: \"3.*\"\n    model_name: llama2-7b-f\n    ep_name: llama-2-7b-g5-2xlarge\n    instance_type: \"ml.g5.2xlarge\"\n    image_uri: '763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi1.1.0-gpu-py39-cu118-ubuntu20.04'\n    deploy: yes\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count: 1\n    deployment_script: jumpstart.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: sagemaker_predictor.py\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    #- payload_en_3000-3840.jsonl\n\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    # Added for models that require accepting a EULA\n    accept_eula: true\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      SAGEMAKER_PROGRAM: \"inference.py\"\n      ENDPOINT_SERVER_TIMEOUT: \"3600\"\n      MODEL_CACHE_ROOT: \"/opt/ml/model\"\n      SAGEMAKER_ENV: \"1\"\n      HF_MODEL_ID: \"/opt/ml/model\"\n      MAX_INPUT_LENGTH: \"4095\"\n      MAX_TOTAL_TOKENS: \"4096\"\n      SM_NUM_GPUS: \"1\"\n      SAGEMAKER_MODEL_SERVER_WORKERS: \"1\"\n\n# parameters related to how the final report is generated\nreport:\n  # markdown report title\n  title: \"Performance benchmarking results for Llama2-7b on `g5.xlarge` and `g5.2xlarge` using the HuggingFace TGI container\"\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama2/7b/config-llama2-7b-g5-no-s3-quick.yml",
      "scrapeId": "a659a3a8-7ce9-4d06-a0d8-e77dc8cd5fa3",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama2/7b/config-llama2-7b-g5-no-s3-quick.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"mistral-7b-v3-inf2.48xl-ec2\"\n  model_name: \"mistral-7b-instruct-v3\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: mistral_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_mistral.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n    return_full_text: no\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Mistral-7B-Instruct-v0.3\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: mistralai/Mistral-7B-Instruct-v0.3 #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Mistral-7B-Instruct-v0.3\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"inf2.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and Inferentia settings for the instance\n      # '-u djl' tells the container to use the Deep Java Library (DJL) runtime\n      # '--device /dev/neuron0 ... /dev/neuron5' specifies the Inferentia devices to be used\n      gpu_or_neuron_setting: -u djl --device /dev/neuron0 --device /dev/neuron1 --device /dev/neuron2 --device /dev/neuron3 --device /dev/neuron4 --device /dev/neuron5 --device /dev/neuron6 --device /dev/neuron7 --device /dev/neuron8 --device /dev/neuron9 --device /dev/neuron10 --device /dev/neuron11\n      #This setting specifies the timeout (in seconds) for loading the model.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 6\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=mistralai/Mistral-7B-Instruct-v0.3\n      option.tensor_parallel_degree=6\n      option.n_positions=4096\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=4\n      option.model_loading_timeout=2400\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n    - 7\n    - 8\n    - 9\n    - 10\n    - 15\n    - 20\n    # - 25\n    # - 30\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O3\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-v3-inf2-48xl-deploy-ec2-tp24.yml",
      "scrapeId": "ac68a618-4d99-4ac9-b844-dc9b51501c7c",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-v3-inf2-48xl-deploy-ec2-tp24.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"mistral-7b-eks-inf2\"\n  model_name: \"mistral7b\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket}\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in llama2_tokenizer and so on and so forth.\n  tokenizer_prefix: mistral_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_mistral.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n    0_setup.ipynb: yes\n    1_generate_data.ipynb: yes\n    2_deploy_model.ipynb: yes\n    3_run_inference.ipynb: yes\n    4_model_metric_analysis.ipynb: yes\n    5_cleanup.ipynb: yes\n\ndatasets:\n  # Refer to the 1_generate_data.ipynb notebook\n  # the dataset you use is expected to have the\n  # columns you put in prompt_template_keys list\n  # and your prompt template also needs to have\n  # the same placeholders (refer to the prompt template folder)\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1-500\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  eks_parameter_set:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: Mistral-7B-Instruct-eks\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: Mistral-7B-Instruct-v0.2\n    model_version: \"*\"\n    model_name: mistral-7b-instruct\n    # the endpoint name is not needed, since it is created dynamically and used\n    # the deploy script for eks stores the endpoint url that is saved and used in the\n    # rest of the test\n    ep_name:\n    instance_type: \"ml.inf2.24xlarge\"\n    image_uri: 'public.ecr.aws/data-on-eks/ray2.11.0-py310-mistral7b-neuron:latest'\n    deploy: yes\n    instance_count: 1\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    deployment_script: eks_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: eks_predictor.py\n    # refers to the eks custom parameters for the model\n    eks:\n      # This is the cluster created on Kubernetes\n      # using do-eks: https://github.com/awslabs/data-on-eks/tree/main\n      # The cluster is created using the github repository above. If there\n      # is a cluster that you create, that is out of this repository or has a different\n      # name, replace the cluster name below with your custom cluster name\n      eks_cluster_name: trainium-inferentia\n      # Represents the logical grouping of EKS resources\n      # for the mistral model. All kubernetes resources related\n      # to mistral will be in this namespace. Change this to\n      # llama3/llama2 for your use case\n      eks_model_namespace: mistral\n      # name of the manifest directory where all of the EKS manifest files\n      # reside\n      manifest_dir: configs/eks_manifests\n      # this is the yaml file to deploy the mistral model\n      manifest_file: mistral-ray-service.yaml\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: eks_parameter_set\n      # this is the url format that gets appended to the\n      # model endpoint URL to run inferences from\n      inference_url_format: /serve/infer?sentence=\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    # currently the llama3-8b jumpstart model is compiled with a batch size of 1\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\n# parameters related to how the final report is generated\nreport:\n  # constraints for latency, cost and error rate\n  # an experiment is considered successful or eligible for\n  # selection for a use-case if it satisfies all of the following\n  # constraints. Experiments are scored as per this criteria\n  # higher score is better (see 4_model_metric_analysis.ipynb score_run function)\n  latency_budget: 2\n  cost_per_10k_txn_budget: 30\n  error_rate_budget: 0\n  # other misc reporting parameters, see 4_model_metric_analysis.ipynb\n  # for more information\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n  latency_vs_token_len_chart:\n    y_ticks:\n    # - 1\n    # - 2\n    # - 3\n    # - 4\n    title: \"Effect of token length on inference latency for \\\"Mistral-7B-Instruct-eks\\\"\"\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-7b-eks-inf2.yml",
      "scrapeId": "ac17a550-cfbd-4653-b0fd-9b634118447b",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/mistral/config-mistral-7b-eks-inf2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g6e.12xl-tp=4-mc=max-djl-ec2\"\n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g6e.12xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g6e.12xl-tp-4-mc-max-djl.yml",
      "scrapeId": "a6f23045-e9c7-405f-bce3-c34ae7cef848",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g6e.12xl-tp-4-mc-max-djl.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "[Skip to content](https://aws-samples.github.io/foundation-model-benchmarking-tool/build.html#building-the-fmbench-python-package)\n\n# Building the `FMBench` Python package [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/build.html\\#building-the-fmbench-python-package \"Permanent link\")\n\nIf you would like to build a dev version of `FMBench` for your own development and testing purposes, the following steps describe how to do that.\n\n1. Clone the `FMBench` repo from GitHub, change directory to `foundation-model-benchmarking-tool`.\n\n\n\n```md-code__content\ncd foundation-model-benchmarking-tool\n\n```\n\n2. Install [`uv`](https://docs.astral.sh/uv/getting-started/) and create a virtual environment with all the dependencies that `FMBench` needs.\n\n\n\n```md-code__content\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nuv venv .fmbench_python312 && source .fmbench_python312/bin/activate && uv pip install --requirement pyproject.toml\nexport UV_PROJECT_ENVIRONMENT=.fmbench_python312\nuv add zmq\npython -m ipykernel install --user --name=.venv --display-name=\"Python (uv fmbench env)\"\nsudo apt-get install tree\n\n```\n\n3. Make any code changes as needed. If you want edit any notebooks in the `FMBench` then select `Python (uv fmbench env)` conda kernel for your notebook.\n\n4. Build the `FMBench` Python package.\n\n\n\n```md-code__content\nuv build\n\n```\n\n5. The `.whl` file is generated in the `dist` folder. Install the `.whl` in your current Python environment.\n\n\n\n```md-code__content\nuv pip install -U dist/*.whl\n\n```\n\n6. Run `FMBench` as usual through the `FMBench` CLI command.\n\n7. You may have added new config files as part of your work, to make sure these files are called out in the `manifest.txt` run the following command. This command will overwrite the existing `manifest.txt` and `manifest.md` files. Both these files need to be committed to the repo. Reach out to the maintainers of this repo so that they can add new or modified config files to the blogs bucket (the CloudFormation stack would fail if a new file is added to the manifest but is not available for download through the S3 bucket).\n\n\n\n```md-code__content\npython create_manifest.py\n\n```\n\n8. To create updated documentation run the following command. You need to be added as a contributor to the `FMBench` repo to be able to publish to the website, so this command would not work for you if you are not added as a contributor to the repo.\n\n\n\n```md-code__content\nmkdocs gh-deploy\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/build.html",
      "title": "Build FMBench - Foundation Model Benchmarking Tool (FMBench)",
      "favicon": {},
      "language": "en",
      "scrapeId": "a72f9476-7777-474a-b0a2-a90a890b7f7e",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/build.html",
      "statusCode": 200,
      "theme-color": "#00000000",
      "color-scheme": "normal"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-70b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3.1-70b-g5.48xl-tp=8-mc=max-djl-ec2\"\n  model_name: \"Hermes-3-Llama-3.1-70B\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Hermes-3-Llama-3.1-70B\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: NousResearch/Hermes-3-Llama-3.1-70B #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Hermes-3-Llama-3.1-70B\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged witsh multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-70b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model.\n      model_loading_timeout: 24000\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 24000\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=4\n      option.model_id=NousResearch/Hermes-3-Llama-3.1-70B\n      option.rolling_batch=lmi-dist\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 200\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/NousResearchHermes70B/config-Nous-Hermes3-1-70b-g5.48xl-tp-8-mc-max-djl.yml",
      "scrapeId": "a24d8345-e070-45f1-adbf-4c85809b4c2a",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/NousResearchHermes70B/config-Nous-Hermes3-1-70b-g5.48xl-tp-8-mc-max-djl.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"fmbench-bedrock-llama3-3-all-models\"\n  model_name: \"llama3-3 Models on Amazon Bedrock\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  # This is a preprocessed open orca file for accuracy benchmarking. To create this file, run the\n  # \"pre process dolly dataset\" section in the bring your own dataset notebook\n  - Open-Orca/OpenOrca.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3_OpenOrca_accuracy.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - question\n  ground_truth_col_key: response\n  question_col_key: question\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1000-2000\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-3-70b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: us.meta.llama3-3-70b-instruct-v1:0\n    hf_tokenizer_model_id: meta-llama/Llama-3.3-70B-Instruct\n    model_version:\n    model_name: us.meta.llama3-3-70b-instruct-v1:0\n    ep_name: us.meta.llama3-3-70b-instruct-v1:0\n    instance_type: us.meta.llama3-3-70b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the\n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n\n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-llama-3-3-all-models-open-orca.yml",
      "scrapeId": "acc789cc-fd68-4a61-8073-2b90b866b4a7",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-llama-3-3-all-models-open-orca.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-8b-inf2.48xl-ec2\"\n  model_name: \"llama3-8b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    #stop: '<|eot_id|>'\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-8b-instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    sagemaker_execution_role: {role_arn}\n    # S3 bucket to which metrics, plots and reports would be written to\n    bucket: {write_bucket} ## add the name of your desired bucket\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    prefix: \"lmi\"\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3-8b-instruct\"\n    model_id_wo_repo: \"Meta-Llama-3-8B-Instruct\"\n    model_id_wo_repo_split: \"Meta-Llama-3-8B-Instruct-split\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name:\n    instance_type: \"ml.inf2.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: neuron_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: sagemaker_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      batch_size: \"1\"\n      num_neuron_cores: \"24\"\n      neuron_version: \"2.19.1\"\n      model_loading_timeout: \"2400\"\n      instance_count: \"1\"\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=s3://{write_bucket}/lmi/Meta-Llama-3-8B-Instruct/Meta-Llama-3-8B-Instruct-split/\n      option.load_split_model=True\n      option.tensor_parallel_degree=24\n      option.n_positions=4096\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=8\n      option.dtype=fp16\n      option.model_loading_timeout=2400\n      option.neuron_optimize_level=2\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-neuron-llama3-8b-inf2-48xl-deploy-sm.yml",
      "scrapeId": "b6a5a028-0e95-462c-9627-80d425f93742",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-neuron-llama3-8b-inf2-48xl-deploy-sm.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.24xl-tp=2-mc=max-djl-ec2\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g6e.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.24xl-tp-2-mc-max-djl-ec2.yml",
      "scrapeId": "a6c8b989-2e3c-49b6-b089-b81c552c5a2e",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.24xl-tp-2-mc-max-djl-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"Meta-Llama-3.2-1b-Instruct-g5.2xl-tp=1-mc=max-triton-ec2\"\n  model_name: \"Meta-Llama-3.2-1b-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_1-500\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 50\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.2-1b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.2-1b-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.2-1b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g5.2xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.2-1b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 1\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=1\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Llama-3.2-1b-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 0.1\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.2/1b/config-llama3.2-1b-g5.2xl-summarization-500-50.yml",
      "scrapeId": "afedffc0-c73c-4d29-9f9f-adb384501c75",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.2/1b/config-llama3.2-1b-g5.2xl-summarization-500-50.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-70b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-70b-p4de.24xl-ec2-longbench\"\n  model_name: \"llama3-1-70b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-70b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-70B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-70b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"p4de.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged witsh multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-70b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=8\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-70b-Instruct\n      option.rolling_batch=lmi-dist\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 5\n    - 10\n    - 15\n    - 20\n    - 25\n    - 30\n    - 35\n    - 37\n    - 40\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 200\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-ec2-llama3-1-70b-p4de.24xl-deploy-ec2-longbench.yml",
      "scrapeId": "a8538ba6-d380-475d-8ba9-fe8ea95456e1",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-ec2-llama3-1-70b-p4de.24xl-deploy-ec2-longbench.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.48xl-tp=4-mc=max-triton-ec2\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    temperature: 0.1\n    do_sample: yes\n    top_p: 0.92\n    top_k: 120\n    bad_words: \"\"\n    stop_words: \"<|eot_id|>\"\n    pad_id: 2\n    end_id: 2\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: http://localhost:8080/v2/models/ensemble/generate\n    instance_type: \"g6e.48xlarge\"\n    image_uri: nvcr.io/nvidia/tritonserver:24.08-trtllm-python-py3\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 8\n      shm_size: 12g\n      model_loading_timeout: 2400\n      batch_size: 256\n\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n    - 10\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.48xl-tp-4-mc-max-triton-ec2.yml",
      "scrapeId": "b266bbbb-f41f-4f01-bd74-2dc5d169c937",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.48xl-tp-4-mc-max-triton-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-70b-inf2.48xl-ec2\"\n  model_name: \"llama3-70b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  sagemaker:\n    max_new_tokens: 100\n    top_p: 0.92\n    temperature: 0.1\n    details: True\n    #stop: '<|eot_id|>'\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-70b-instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    sagemaker_execution_role: {role_arn}\n    # S3 bucket to which metrics, plots and reports would be written to\n    bucket: {write_bucket} ## add the name of your desired bucket\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    prefix: \"lmi\"\n    model_id: meta-llama/Meta-Llama-3-70b-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3-70b-instruct\"\n    model_id_wo_repo: \"Meta-Llama-3-70b-Instruct\"\n    model_id_wo_repo_split: \"Meta-Llama-3-70b-Instruct-split\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name:\n    instance_type: \"ml.inf2.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-neuronx-sdk2.19.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: neuron_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: sagemaker_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      batch_size: \"1\"\n      num_neuron_cores: \"24\"\n      neuron_version: \"2.19.1\"\n      model_loading_timeout: \"2400\"\n      instance_count: \"1\"\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: sagemaker\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=Python\n      option.entryPoint=djl_python.transformers_neuronx\n      option.model_id=s3://{write_bucket}/lmi/Meta-Llama-3-70b-Instruct/Meta-Llama-3-70b-Instruct-split/\n      option.load_split_model=True\n      option.tensor_parallel_degree=24\n      option.n_positions=4096\n      option.rolling_batch=auto\n      option.max_rolling_batch_size=8\n      option.dtype=fp16\n      option.model_loading_timeout=2400\n      option.neuron_optimize_level=2\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/70b/config-ec2-neuron-llama3-70b-inf2-48xl-deploy-sm.yml",
      "scrapeId": "b33f06f0-b2c1-4638-b6c3-d4f38e06559e",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/70b/config-ec2-neuron-llama3-70b-inf2-48xl-deploy-sm.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"fmbench-bedrock-llama3.2-1b-3b\"\n  model_name: \"llama3-2 Models on Amazon Bedrock\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_2_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: no\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-2-1b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: us.meta.llama3-2-1b-instruct-v1:0\n    model_version:\n    model_name: us.meta.llama3-2-1b-instruct-v1:0\n    ep_name: us.meta.llama3-2-1b-instruct-v1:0\n    instance_type: us.meta.llama3-2-1b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the\n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n\n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    # - 6\n    # - 8\n    # - 10\n    # - 12\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  # for llama3-2, use 'us' as a prefix, else an error will be encountered.\n  - name: us.meta.llama3-2-3b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: us.meta.llama3-2-3b-instruct-v1:0\n    model_version:\n    model_name: us.meta.llama3-2-3b-instruct-v1:0\n    ep_name: us.meta.llama3-2-3b-instruct-v1:0\n    instance_type: us.meta.llama3-2-3b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the\n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n\n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    # - 6\n    # - 8\n    # - 10\n    # - 12\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 1\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-llama-3-2-1b-3b-no-evals.yml",
      "scrapeId": "bc79498a-8eb6-47f6-8dd8-2b2b604355de",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-llama-3-2-1b-3b-no-evals.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g5.12xl-tp=4-mc=max-djl-ec2\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g5.12xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g5.12xl-tp-4-mc-max-djl-ec2.yml",
      "scrapeId": "b5ddc6d0-7c1c-4872-b59a-f10ed5e9d44f",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g5.12xl-tp-4-mc-max-djl-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-70b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-70b-inf2.48xl-ec2-triton\"\n  model_name: \"llama3-1-70b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    top_k: 50\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3.1-70b-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-70B-Instruct #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3.1-70b-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8080/v2/models/Llama-3.1-70B-Instruct/generate'\n    instance_type: \"inf2.48xlarge\"\n    image_uri: tritonserver-neuronx:fmbench\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-70b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model.\n      model_loading_timeout: 10000\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # For deploying a model using the triton inference container:\n      # mention the backend type, if any. Options for triton are: [djl, vllm or tensorrt]\n      backend: djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured,\n      # max: FMBench figures out the max number of model containers to be run\n      #      based on TP degree configured and number of neuron cores/GPUs available.\n      #      For example, if TP=2, GPUs=8 then FMBench will start 4 containers and 1 load balancer,\n      # auto: only supported if the underlying inference container would automatically\n      #       start multiple copies of the model internally based on TP degree and neuron cores/GPUs\n      #       available. In this case only a single container is created, no load balancer is created.\n      #       The DJL serving containers supports auto.\n      model_copies: max\n      shm_size: 12g\n      # The model.json parameters are replaced within the model.json file\n      # for the triton on vllm/djl/tensorrt options. The model.json already contains\n      # the tp degree and model id from above in this config file. This is a dictionary\n      # that contains other variables, such as max_num_seqs, dtype, max_model_len, block_size and more\n      # For tensorrt, the tp degree, batch size and other relevant parameters are\n      # extracted directly from the inference spec.\n      container_params:\n        # tp degree is a mandatory parameter\n        tp_degree: 24\n        amp: \"f16\"\n        attention_layout: 'BSH'\n        collectives_layout: 'BSH'\n        context_length_estimate: 3072, 3584, 4096\n        max_rolling_batch_size: 8\n        model_loader: \"tnx\"\n        model_loading_timeout: 2400\n        n_positions: 4096\n        output_formatter: \"json\"\n        rolling_batch: \"auto\"\n        rolling_batch_strategy: \"continuous_batching\"\n        trust_remote_code: true\n        # modify the serving properties to match your model and requirements\n        serving.properties:\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O3\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 200\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-llama3-1-70b-inf2.48xl-deploy-ec2-triton.yml",
      "scrapeId": "b340ed3f-1bba-4dd6-84bc-7bfebf47bc64",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/70b/config-llama3-1-70b-inf2.48xl-deploy-ec2-triton.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-c8g.24xl-ec2\"\n  model_name: \"llama3-8b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_vllm:\n    model: meta-llama/Meta-Llama-3-8B-Instruct\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-8b-instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3-8b-instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8000/v1/completions'\n    instance_type: \"c8g.24xlarge\"\n    image_uri: vllm-cpu-env\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # Privileged Mode makes the docker container run with root.\n      # This basically means that if you are root in a container you have the privileges of root on the host system\n      # Only need this if you need to set VLLM_CPU_OMP_THREADS_BIND env variable.\n      privileged_mode: yes\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting:\n      # This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_vllm\n      # if not set assume djl\n      container_type: vllm\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      MODEL_LOADING_TIMEOUT: 2400\n      # This instance is equipped with 96 CPUs, and we are allocating 93 of them to run this container.\n      # For additional details, refer to the following URL:\n      # https://docs.vllm.ai/en/latest/getting_started/cpu-installation.html#related-runtime-environment-variables\n      VLLM_CPU_OMP_THREADS_BIND: 0-92\n\nreport:\n  latency_budget: 35\n  cost_per_10k_txn_budget: 60\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-c8g-24xlarge.yml",
      "scrapeId": "b445add7-9183-4d5e-9dd8-32e295dacc35",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-c8g-24xlarge.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"fmbench-bedrock-latest-models\"\n  model_name: \"All latest models available in Amazon Bedrock\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n## S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-<region>-<account_id>\n  read_bucket: {read_bucket}\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - Open-Orca/OpenOrca.jsonl\n  tokenizer_prefix: tokenizer ## add the tokenizer.json and config.json from your specific tokenizer type\n  prompt_template_dir: prompt_template\n  prompt_template_file: prompt_template_claude_OpenOrca.txt ## add your desired prompt template type\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: no\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: no\n  4_get_evaluations.ipynb: no\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - system_prompt\n  ground_truth_col_key: response\n  question_col_key: input\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 4000\n    payload_file: payload_en_3000-4000.jsonl\n  - language: en\n    min_length_in_tokens: 305\n    max_length_in_tokens: 3997\n    payload_file: payload_en_305-3997.jsonl\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\nmetrics:\n  dataset_of_interest: en_1000-2000\n\npricing: pricing.yml ## mention the name of the file that contains pricing information, within s3 or locally in the configs folder\n\ninference_parameters:\n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Model configurations for mistral 7b instruct on Amazon Bedrock\nexperiments:\n  # Experiment for claude 3 sonnet\n  - name: anthropic.claude-3-sonnet-20240229-v1:0\n    model_name: anthropic.claude-3-sonnet-20240229-v1:0\n    ep_name: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_type: anthropic.claude-3-sonnet-20240229-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  # Experiment for claude 3 haiku\n  - name: anthropic.claude-3-haiku-20240307-v1:0\n    model_name: anthropic.claude-3-haiku-20240307-v1:0\n    ep_name: anthropic.claude-3-haiku-20240307-v1:0\n    instance_type: anthropic.claude-3-haiku-20240307-v1:0\n    instance_count:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-4000.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  - name: mistral.mistral-7b-instruct-v0:2\n    model_id: mistral.mistral-7b-instruct-v0:2\n    model_version: \"*\"\n    model_name: mistral.mistral-7b-instruct-v0:2\n    ep_name: mistral.mistral-7b-instruct-v0:2\n    instance_type: mistral.mistral-7b-instruct-v0:2\n    image_uri:\n    deploy: no\n    instance_count: 1\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n      stream: True\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  - name: mistral.mixtral-8x7b-instruct-v0:1\n    model_id: mistral.mixtral-8x7b-instruct-v0:1\n    model_version: \"*\"\n    model_name: mistral.mixtral-8x7b-instruct-v0:1\n    ep_name: mistral.mixtral-8x7b-instruct-v0:1\n    instance_type: mistral.mixtral-8x7b-instruct-v0:1\n    image_uri:\n    deploy: no\n    instance_count: 1\n    deployment_script:\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      parameter_set: bedrock\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    env:\n  - name: meta.llama3-70b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: meta.llama3-70b-instruct-v1:0\n    model_version:\n    model_name: meta.llama3-70b-instruct-v1:0\n    ep_name: meta.llama3-70b-instruct-v1:0\n    instance_type: meta.llama3-70b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      stream: True\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  - name: meta.llama3-8b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: meta.llama3-8b-instruct-v1:0\n    model_version:\n    model_name: meta.llama3-8b-instruct-v1:0\n    ep_name: meta.llama3-8b-instruct-v1:0\n    instance_type: meta.llama3-8b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      stream: True\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 5\n  cost_per_10k_txn_budget: 350\n  error_rate_budget: 0\n  # change this based on your dataset size and your needs\n  accuracy_budget: 0.90\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-models-OpenOrca.yml",
      "scrapeId": "b5afde1e-0c89-47e4-8aa3-10e88d9eb3ac",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-models-OpenOrca.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g5.2xl-djl-ec2\"\n  model_name: \"llama3-8b-instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n    Content-type: application/json\n    return_full_text: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"llama3-8b-instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"llama3-8b-instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g5.2xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 1\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=1\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n    - 10\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b.yml",
      "scrapeId": "b9c443bb-3272-444c-b02a-c881fe4d4e42",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-instruct deployed on ec2 on an inf2 instance\ngeneral:\n  name: \"llama3-1-70b-trn1.32xl-ec2-triton\"\n  model_name: \"Hermes-3-Llama-3.1-70B\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    top_k: 50\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Hermes-3-Llama-3.1-70B\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: NousResearch/Hermes-3-Llama-3.1-70B #model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Hermes-3-Llama-3.1-70B\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8080/v2/models/Hermes-3-Llama-3.1-70B/generate'\n    instance_type: \"trn1.32xlarge\"\n    image_uri: tritonserver-neuronx:fmbench\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model.\n      model_loading_timeout: 10000\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      container_type: triton\n      # For deploying a model using the triton inference container:\n      # mention the backend type, if any. Options for triton are: [djl, vllm or tensorrt]\n      backend: djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured,\n      # max: FMBench figures out the max number of model containers to be run\n      #      based on TP degree configured and number of neuron cores/GPUs available.\n      #      For example, if TP=2, GPUs=8 then FMBench will start 4 containers and 1 load balancer,\n      # auto: only supported if the underlying inference container would automatically\n      #       start multiple copies of the model internally based on TP degree and neuron cores/GPUs\n      #       available. In this case only a single container is created, no load balancer is created.\n      #       The DJL serving containers supports auto.\n      model_copies: max\n      shm_size: 12g\n      # The model.json parameters are replaced within the model.json file\n      # for the triton on vllm/djl/tensorrt options. The model.json already contains\n      # the tp degree and model id from above in this config file. This is a dictionary\n      # that contains other variables, such as max_num_seqs, dtype, max_model_len, block_size and more\n      # For tensorrt, the tp degree, batch size and other relevant parameters are\n      # extracted directly from the inference spec.\n      container_params:\n        # tp degree is a mandatory parameter\n        tp_degree: 32\n        amp: \"f16\"\n        attention_layout: 'BSH'\n        collectives_layout: 'BSH'\n        context_length_estimate: 3072, 3584, 4096\n        max_rolling_batch_size: 8\n        model_loader: \"tnx\"\n        model_loading_timeout: 2400\n        n_positions: 4096\n        output_formatter: \"json\"\n        rolling_batch: \"auto\"\n        rolling_batch_strategy: \"continuous_batching\"\n        trust_remote_code: true\n        # modify the serving properties to match your model and requirements\n        serving.properties:\n\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 5\n    - 6\n    - 8\n    - 10\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n      NEURON_CC_FLAGS: -O3\n\nreport:\n  latency_budget: 3\n  cost_per_10k_txn_budget: 200\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/NousResearchHermes70B/config-Nous-Hermes3-1-70b-trn1.32xl-deploy-ec2-triton.yml",
      "scrapeId": "b12c5e8e-fa61-4e35-97c3-968752a0f94f",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/NousResearchHermes70B/config-Nous-Hermes3-1-70b-trn1.32xl-deploy-ec2-triton.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "[Skip to content](https://aws-samples.github.io/foundation-model-benchmarking-tool/mm_copies.html#running-multiple-model-copies-on-amazon-ec2)\n\n# Running multiple model copies on Amazon EC2 [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/mm_copies.html\\#running-multiple-model-copies-on-amazon-ec2 \"Permanent link\")\n\nIt is possible to run multiple copies of a model if the tensor parallelism degree and the number of GPUs/Neuron cores on the instance allow it. For example if a model can fit into 2 GPU devices and there are 8 devices available then we could run 4 copies of the model on that instance. Some inference containers, such as the [DJL Serving LMI](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/index.html) automatically start multiple copies of the model within the same inference container for the scenario described in the example above. However, it is also possible to do this ourselves by running multiple containers and a load balancer through a Docker compose file. `FMBench` now supports this functionality by adding a single parameter called `model_copies` in the configuration file.\n\nFor example, here is a snippet from the [config-ec2-llama3-1-8b-p4-tp-2-mc-max](https://github.com/aws-samples/foundation-model-benchmarking-tool/blob/1db3cdd09ba4dafc095f3c5313fcd5dd1a48313c/src/fmbench/configs/llama3.1/8b/config-llama3.1-8b-trn1-32xl-deploy-tp-8-ec2.yml#L199) config file. The new parameters are `model_copies`, `tp_degree` and `shm_size` in the `inference_spec` section. **_Note that the `tp_degree` in the `inference_spec` and `option.tensor_parallel_degree` in the `serving.properties` section should be set to the same value_**.\n\n```md-code__content\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, \"1\", \"2\",..max\n      # set to 1 in the code if not configured,\n      # max: FMBench figures out the max number of model containers to be run\n      #      based on TP degree configured and number of neuron cores/GPUs available.\n      #      For example, if TP=2, GPUs=8 then FMBench will start 4 containers and 1 load balancer,\n      # auto: only supported if the underlying inference container would automatically\n      #       start multiple copies of the model internally based on TP degree and neuron cores/GPUs\n      #       available. In this case only a single container is created, no load balancer is created.\n      #       The DJL serving containers supports auto.\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n\n```\n\n## Considerations while setting the `model_copies` parameter [¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/mm_copies.html\\#considerations-while-setting-the-model_copies-parameter \"Permanent link\")\n\n1. The `model_copies` parameter is an EC2 only parameter, which means that you cannot use it when deploying models on SageMaker for example.\n\n2. If you are looking for the best (lowest) inference latency then you might get better results with setting the `tp_degree` and `option.tensor_parallel_degree` to the total number of GPUs/Neuron cores available on your EC2 instance and `model_copies` to `max` or `auto` or `1`, in other words, the model is being shared across all accelerators and there can be only 1 copy of the model that can run on that instance (therefore setting `model_copies` to `max` or `auto` or `1` all result in the same thing i.e. a single copy of the model running on that EC2 instance).\n\n3. If you are looking for the best (highest) transaction throughput while keeping the inference latency within a given latency budget then you might want to configure `tp_degree` and `option.tensor_parallel_degree` to the least number of GPUs/Neuron cores on which the model can run (for example for `Llama3.1-8b` that would be 2 GPUs or 4 Neuron cores) and set the `model_copies` to `max`. Let us understand this with an example, say you want to run `Llama3.1-8b` on a `p4de.24xlarge` instance type, you set `tp_degree` and `option.tensor_parallel_degree` to 2 and `model_copies` to `max`, `FMBench` will start 4 containers (as the `p4de.24xlarge` has 8 GPUs) and an Nginx load balancer that will round-robin the incoming requests to these 4 containers. In case of the DJL serving LMI you can achieve similar results by setting the `model_copies` to `auto` in which case `FMBench` will start a single container (and no load balancer since there is only one container) and then the DJL serving container will internally start 4 copies of the model within the same container and route the requests to these 4 copies internally. Theoretically you should expect the same performance but in our testing we have seen better performance with `model_copies` set to `max` and having an external (Nginx) container doing the load balancing.",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/mm_copies.html",
      "title": "Running multiple model copies - Foundation Model Benchmarking Tool (FMBench)",
      "favicon": {},
      "language": "en",
      "scrapeId": "c1550759-3ca3-4bbb-84f0-0f683abf3156",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/mm_copies.html",
      "statusCode": 200,
      "theme-color": "#00000000",
      "color-scheme": "normal"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\ngeneral:\n  name: \"fmbench-bedrock-llama3-1\"\n  model_name: \"Llama3-1 Models on Amazon Bedrock\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: no\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: no\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: no\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  bedrock:\n    temperature: 0.1\n    max_tokens: 100\n    top_p: 0.92\n    caching: False\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options (such as one experiment is SageMaker and the other is Bedrock).\nexperiments:\n  - name: us.meta.llama3-1-70b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: us.meta.llama3-1-70b-instruct-v1:0\n    hf_tokenizer_model_id: meta-llama/Llama-3.1-70B-Instruct\n    model_version:\n    model_name: us.meta.llama3-1-70b-instruct-v1:0\n    ep_name: us.meta.llama3-1-70b-instruct-v1:0\n    instance_type: us.meta.llama3-1-70b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the\n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n\n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n  - name: us.meta.llama3-1-8b-instruct-v1:0\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    model_id: us.meta.llama3-1-8b-instruct-v1:0\n    hf_tokenizer_model_id: meta-llama/Llama-3.1-8B-Instruct\n    model_version:\n    model_name: us.meta.llama3-1-8b-instruct-v1:0\n    ep_name: us.meta.llama3-1-8b-instruct-v1:0\n    instance_type: us.meta.llama3-1-8b-instruct-v1:0\n    image_uri:\n    deploy: no\n    # FMBench comes packaged with multiple deployment scripts, such as scripts for JumpStart\n    # scripts for deploying using DJL DeepSpeed, tensorRT etc. You can also add your own.\n    # See repo for details\n    instance_count:\n    deployment_script:\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. See repo for details\n    inference_script: bedrock_predictor.py\n    inference_spec:\n      split_input_and_parameters: no\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: bedrock\n      # to stream responses, set stream to true. Enter the start and stop token for the\n      # Time To First Token, Time To Last Token, and Time Per Output Token (TTFT, TTLT, TPOT)\n      # metrics to be calculated. The responses from bedrock stream is received in chunks, so mention\n      # the stop token only.\n      stream: False\n      start_token:\n      stop_token: \"<|eot_id|>\"\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n\n    # for streaming responses on bedrock, only a concurrency of 1 is supported on FMBench\n    concurrency_levels:\n    - 1\n    # - 2\n    # - 4\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-llama3-1.yml",
      "scrapeId": "c561abde-a0f7-4643-a4f9-a5dc2cf202f5",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/bedrock/config-bedrock-llama3-1.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-g6e.48xl-tp=4-mc=max-djl-ec2\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n  ground_truth_col_key: answers\n  question_col_key: input\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g6e.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 3\n    - 4\n    - 6\n    - 8\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.48xl-tp-4-mc-max-djl-ec2.yml",
      "scrapeId": "b10c55b0-83f8-49e7-8a8f-433f6830fd58",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-llama3-8b-g6e.48xl-tp-4-mc-max-djl-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: {results_dir}\n  model_name: {model_id}\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - OpenOrca.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_open_orca.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_get_evaluations.ipynb: yes\n  5_model_metric_analysis.ipynb: yes\n  6_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - question\n  ground_truth_col_key: response\n  question_col_key: question\n  # maximum number of iterations per concurrency and payload size combination\n  max_iters_per_combination: 105\n  # minimum number of iterations per concurrency and payload size combination\n  min_iters_per_combination: 5\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# name of the file that contains the model evaluation information\n# for example, the prompt template names, the ground truth column name (if any),\n# LLM panelist information, inference parameters, etc.\nmodel_evaluations: model_eval_all_info.yml\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_500-1000\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_vllm:\n    model: {model_id}\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_tokens: 2048\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: {model_id}\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    hf_tokenizer_model_id: {model_id}\n    model_id: {model_id} # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: {model_id}\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://localhost:8000/v1/completions'\n    instance_type: {instance_type}\n    image_uri: vllm/vllm-openai:v0.7.1\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_vllm\n      container_type: vllm_gpu\n      cli_params: {cli_params}\n    # modify the serving properties to match your model and requirements\n    serving.properties:\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    #- 2\n    # - 3\n    #- 4\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 20\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/deepseek/config-deepseek-r1-vllm-openorca.yml",
      "scrapeId": "c87895b5-5cfc-49f9-8a07-4f2a41d187a2",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/deepseek/config-deepseek-r1-vllm-openorca.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "# 404 - Not found",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/src/configs/llama3/8b/config-llama3-8b-g5-streaming.yml",
      "error": "Not Found",
      "title": "Foundation Model Benchmarking Tool (FMBench)",
      "favicon": {},
      "language": "en",
      "scrapeId": "bce2271b-237e-415b-b5d3-2bb4fc02b3e5",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.11",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/src/configs/llama3/8b/config-llama3-8b-g5-streaming.yml",
      "statusCode": 404
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3-8b-p4d.24xl-ec2\"\n  model_name: \"Meta-Llama-3-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Llama-3-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Meta-Llama-3-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Llama-3-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"p4d.24xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 2\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=2\n      option.max_rolling_batch_size=256\n      option.model_id=meta-llama/Meta-Llama-3-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 10\n    - 30\n    - 35\n    - 40\n    - 50\n    - 60\n    - 65\n    - 70\n    - 75\n    - 100\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-p4d-tp-2-mc-max.yml",
      "scrapeId": "c146ec42-4c1e-4bae-b32c-5afa875b8df4",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3/8b/config-ec2-llama3-8b-p4d-tp-2-mc-max.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  },
  {
    "warning": "This scrape job was throttled at your current concurrency limit. If you'd like to scrape faster, you can upgrade your plan.",
    "markdown": "```\n# config file for a rest endpoint supported on fmbench -\n# this file uses a llama-3-8b-chat-hf deployed on ec2\ngeneral:\n  name: \"llama3.1-8b-g5.48xl-ec2\"\n  model_name: \"Meta-Llama-3.1-8B-Instruct\"\n\n# AWS and SageMaker settings\naws:\n  # AWS region, this parameter is templatized, no need to change\n  region: {region}\n  # SageMaker execution role used to run FMBench, this parameter is templatized, no need to change\n  sagemaker_execution_role: {role_arn}\n  # S3 bucket to which metrics, plots and reports would be written to\n  bucket: {write_bucket} ## add the name of your desired bucket\n\n# directory paths in the write bucket, no need to change these\ndir_paths:\n  data_prefix: data\n  prompts_prefix: prompts\n  all_prompts_file: all_prompts.csv\n  metrics_dir: metrics\n  models_dir: models\n  metadata_dir: metadata\n\n# S3 information for reading datasets, scripts and tokenizer\ns3_read_data:\n  # read bucket name, templatized, if left unchanged will default to sagemaker-fmbench-read-region-account_id\n  read_bucket: {read_bucket}\n  scripts_prefix: scripts ## add your own scripts in case you are using anything that is not on jumpstart\n\n  # S3 prefix in the read bucket where deployment and inference scripts should be placed\n  scripts_prefix: scripts\n\n  # deployment and inference script files to be downloaded are placed in this list\n  # only needed if you are creating a new deployment script or inference script\n  # your HuggingFace token does need to be in this list and should be called \"hf_token.txt\"\n  script_files:\n  - hf_token.txt\n\n  # configuration files (like this one) are placed in this prefix\n  configs_prefix: configs\n\n  # list of configuration files to download, for now only pricing.yml needs to be downloaded\n  config_files:\n  - pricing.yml\n\n  # S3 prefix for the dataset files\n  source_data_prefix: source_data\n  # list of dataset files, the list below is from the LongBench dataset https://huggingface.co/datasets/THUDM/LongBench\n  source_data_files:\n  - 2wikimqa_e.jsonl\n  - 2wikimqa.jsonl\n  - hotpotqa_e.jsonl\n  - hotpotqa.jsonl\n  - narrativeqa.jsonl\n  - triviaqa_e.jsonl\n  - triviaqa.jsonl\n\n  # S3 prefix for the tokenizer to be used with the models\n  # NOTE 1: the same tokenizer is used with all the models being tested through a config file\n  # NOTE 2: place your model specific tokenizers in a prefix named as <model_name>_tokenizer\n  #         so the mistral tokenizer goes in mistral_tokenizer, Llama2 tokenizer goes in  llama2_tokenizer\n  tokenizer_prefix: llama3_1_tokenizer\n\n  # S3 prefix for prompt templates\n  prompt_template_dir: prompt_template\n\n  # prompt template to use, NOTE: same prompt template gets used for all models being tested through a config file\n  # the FMBench repo already contains a bunch of prompt templates so review those first before creating a new one\n  prompt_template_file: prompt_template_llama3.txt\n\n# steps to run, usually all of these would be\n# set to yes so nothing needs to change here\n# you could, however, bypass some steps for example\n# set the 2_deploy_model.ipynb to no if you are re-running\n# the same config file and the model is already deployed\nrun_steps:\n  0_setup.ipynb: yes\n  1_generate_data.ipynb: yes\n  2_deploy_model.ipynb: yes\n  3_run_inference.ipynb: yes\n  4_model_metric_analysis.ipynb: yes\n  5_cleanup.ipynb: yes\n\ndatasets:\n  # dataset related configuration\n  prompt_template_keys:\n  - input\n  - context\n\n  # if your dataset has multiple languages and it has a language\n  # field then you could filter it for a language. Similarly,\n  # you can filter your dataset to only keep prompts between\n  # a certain token length limit (the token length is determined\n  # using the tokenizer you provide in the tokenizer_prefix prefix in the\n  # read S3 bucket). Each of the array entries below create a payload file\n  # containing prompts matching the language and token length criteria.\n  filters:\n  - language: en\n    min_length_in_tokens: 1\n    max_length_in_tokens: 500\n    payload_file: payload_en_1-500.jsonl\n  - language: en\n    min_length_in_tokens: 500\n    max_length_in_tokens: 1000\n    payload_file: payload_en_500-1000.jsonl\n  - language: en\n    min_length_in_tokens: 1000\n    max_length_in_tokens: 2000\n    payload_file: payload_en_1000-2000.jsonl\n  - language: en\n    min_length_in_tokens: 2000\n    max_length_in_tokens: 3000\n    payload_file: payload_en_2000-3000.jsonl\n  - language: en\n    min_length_in_tokens: 3000\n    max_length_in_tokens: 3840\n    payload_file: payload_en_3000-3840.jsonl\n\n# While the tests would run on all the datasets\n# configured in the experiment entries below but\n# the price:performance analysis is only done for 1\n# dataset which is listed below as the dataset_of_interest\nmetrics:\n  dataset_of_interest: en_3000-3840\n\n# all pricing information is in the pricing.yml file\n# this file is provided in the repo. You can add entries\n# to this file for new instance types and new Bedrock models\npricing: pricing.yml\n\n# inference parameters, these are added to the payload\n# for each inference request. The list here is not static\n# any parameter supported by the inference container can be\n# added to the list. Put the sagemaker parameters in the sagemaker\n# section, bedrock parameters in the bedrock section (not shown here).\n# Use the section name (sagemaker in this example) in the inference_spec.parameter_set\n# section under experiments.\ninference_parameters:\n  ec2_djl:\n    do_sample: yes\n    temperature: 0.1\n    top_p: 0.92\n    top_k: 120\n    max_new_tokens: 100\n\n# Configuration for experiments to be run. The experiments section is an array\n# so more than one experiments can be added, these could belong to the same model\n# but different instance types, or different models, or even different hosting\n# options.\nexperiments:\n  - name: \"Meta-Llama-3.1-8B-Instruct\"\n    # AWS region, this parameter is templatized, no need to change\n    region: {region}\n    # model_id is interpreted in conjunction with the deployment_script, so if you\n    # use a JumpStart model id then set the deployment_script to jumpstart.py.\n    # if deploying directly from HuggingFace this would be a HuggingFace model id\n    # see the DJL serving deployment script in the code repo for reference.\n    #from huggingface to grab\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model id, version and image uri not needed for byo endpoint\n    model_version:\n    model_name: \"Meta-Llama-3.1-8B-Instruct\"\n    # this can be changed to the IP address of your specific EC2 instance where the model is hosted\n    ep_name: 'http://127.0.0.1:8080/invocations'\n    instance_type: \"g5.48xlarge\"\n    image_uri: 763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124\n    deploy: yes #setting to yes to run deployment script for ec2\n    instance_count:\n    deployment_script: ec2_deploy.py\n    # FMBench comes packaged with multiple inference scripts, such as scripts for SageMaker\n    # and Bedrock. You can also add your own. This is an example for a rest DJL predictor\n    # for a llama3.1-8b-instruct deployed on ec2\n    inference_script: ec2_predictor.py\n    # This section defines the settings for Amazon EC2 instances\n    ec2:\n      # The following line specifies the runtime and GPU settings for the instance\n      # '--runtime=nvidia' tells the container runtime to use the NVIDIA runtime\n      # '--gpus all' makes all GPUs available to the container\n      # '--shm-size 12g' sets the size of the shared memory to 12 gigabytes\n      gpu_or_neuron_setting: --gpus all --shm-size 12g\n      #This setting specifies the timeout (in seconds) for loading the model. In this case, the timeout is set to 2400 seconds, which is 40 minutes.\n      # If the model takes longer than 40 minutes to load, the process will time out and fail.\n      model_loading_timeout: 2400\n    inference_spec:\n      # this should match one of the sections in the inference_parameters section above\n      parameter_set: ec2_djl\n      # how many copies of the model, 1, 2,..max\n      # set to 1 in the code if not configured, setting to max means that\n      # the code will determine how many copies can be loaded based on TP and\n      # number of GPU/Neuron devices available\n      model_copies: max\n      # if you set the model_copies parameter then it is mandatory to set the\n      # tp_degree, shm_size, model_loading_timeout parameters\n      tp_degree: 4\n      shm_size: 12g\n      model_loading_timeout: 2400\n    # modify the serving properties to match your model and requirements\n    serving.properties: |\n      engine=MPI\n      option.tensor_parallel_degree=4\n      option.max_rolling_batch_size=4\n      option.model_id=meta-llama/Meta-Llama-3.1-8B-Instruct\n      option.rolling_batch=lmi-dist\n    # runs are done for each combination of payload file and concurrency level\n    payload_files:\n    - payload_en_1-500.jsonl\n    - payload_en_500-1000.jsonl\n    - payload_en_1000-2000.jsonl\n    - payload_en_2000-3000.jsonl\n    - payload_en_3000-3840.jsonl\n    # concurrency level refers to number of requests sent in parallel to an endpoint\n    # the next set of requests is sent once responses for all concurrent requests have\n    # been received.\n    concurrency_levels:\n    - 1\n    - 2\n    - 4\n    - 6\n    - 8\n\n    # Environment variables to be passed to the container\n    # this is not a fixed list, you can add more parameters as applicable.\n    env:\n\nreport:\n  latency_budget: 2\n  cost_per_10k_txn_budget: 100\n  error_rate_budget: 0\n  per_inference_request_file: per_inference_request_results.csv\n  all_metrics_file: all_metrics.csv\n  txn_count_for_showing_cost: 10000\n  v_shift_w_single_instance: 0.025\n  v_shift_w_gt_one_instance: 0.025\n\n```",
    "metadata": {
      "url": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.48xl-tp-4-mc-max-ec2.yml",
      "scrapeId": "c756ee65-b769-46bf-b405-4178efbdb2b7",
      "sourceURL": "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-llama3.1-8b-g5.48xl-tp-4-mc-max-ec2.yml",
      "statusCode": 200,
      "color-scheme": "light dark"
    }
  }
]