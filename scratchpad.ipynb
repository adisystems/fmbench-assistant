{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from colorama import init, Fore\n",
    "from botocore.config import Config\n",
    "from langchain_core.tools import tool\n",
    "from utils import create_bedrock_client\n",
    "from fmbench_rag_setup import FMBenchRagSetup\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from typing import List, Optional\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "from fastapi.responses import RedirectResponse\n",
    "from guardrails import BedrockGuardrailManager\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 21:07:07.262,INFO,p1547739,fmbench_rag_setup.py,149,Initializing Bedrock client for region: us-east-1\n",
      "2025-04-06 21:07:07.344,INFO,p1547739,fmbench_rag_setup.py,107,Bedrock client initialized\n",
      "2025-04-06 21:07:07.346,INFO,p1547739,fmbench_rag_setup.py,188,Loading vector store from indexes/fmbench_index\n",
      "2025-04-06 21:07:07.537,INFO,p1547739,fmbench_rag_setup.py,190,Successfully loaded vector store from indexes/fmbench_index\n",
      "2025-04-06 21:07:07.544,INFO,p1547739,fmbench_rag_setup.py,311,RAG setup complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bedrock_role_arn = os.environ.get(\"BEDROCK_ROLE_ARN\")\n",
    "_rag_system = FMBenchRagSetup(bedrock_role_arn=bedrock_role_arn).setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 21:07:07.563,INFO,p1547739,fmbench_rag_setup.py,408,Processing query: can we benchmark on nvidia gpu on g6e instances\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n'\n",
      " '\\n'\n",
      " \"result={'input': 'can we benchmark on nvidia gpu on g6e instances', \"\n",
      " \"'context': [Document(id='d583ff09-66a7-4578-9d59-6b0e572d6e41', \"\n",
      " \"metadata={'url': \"\n",
      " \"'https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html', \"\n",
      " \"'title': 'EC2 - Foundation Model Benchmarking Tool (FMBench)', 'favicon': \"\n",
      " \"{}, 'language': 'en', 'scrapeId': '5c876876-7753-46a4-a81a-53ce6e9e2438', \"\n",
      " \"'viewport': 'width=device-width,initial-scale=1', 'generator': \"\n",
      " \"'mkdocs-1.6.1, mkdocs-material-9.6.11', 'sourceURL': \"\n",
      " \"'https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html', \"\n",
      " \"'statusCode': 200, 'theme-color': '#00000000', 'color-scheme': 'normal'}, \"\n",
      " \"page_content='[Skip to \"\n",
      " 'content](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#benchmark-models-on-ec2)\\\\n\\\\n# '\n",
      " 'Benchmark models on EC2 '\n",
      " '[Â¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html\\\\\\\\#benchmark-models-on-ec2 '\n",
      " '\"Permanent link\")\\\\n\\\\nYou can use `FMBench` to benchmark models on hosted '\n",
      " 'on EC2. This can be done in one of two ways:\\\\n\\\\n- Deploy the model on your '\n",
      " 'EC2 instance independently of `FMBench` and then benchmark it through the '\n",
      " '[Bring your own '\n",
      " 'endpoint](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#bring-your-own-endpoint-aka-support-for-external-endpoints) '\n",
      " 'mode.\\\\n- Deploy the model on your EC2 instance through `FMBench` and then '\n",
      " 'benchmark it.\\\\n\\\\nThe steps for deploying the model on your EC2 instance '\n",
      " 'are described below.\\\\n\\\\nðŸ‘‰ In this configuration both the model being '\n",
      " 'benchmarked and `FMBench` are deployed on the same EC2 instance.\\\\n\\\\nCreate '\n",
      " 'a new EC2 instance suitable for hosting an LMI as per the steps described '\n",
      " '[here](https://aws-samples.github.io/foundation-model-benchmarking-tool/misc/ec2_instance_creation_steps.html). '\n",
      " '_Note that you will need to select the correct AMI based on your instance '\n",
      " 'type, this is called out in the instructions_.\\\\n\\\\nThe steps for '\n",
      " 'benchmarking on different types of EC2 instances (GPU/CPU/Neuron) and '\n",
      " 'different inference containers differ slightly. These are all described '\n",
      " 'below.\\\\n\\\\n## Benchmarking options on EC2 '\n",
      " '[Â¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html\\\\\\\\#benchmarking-options-on-ec2 '\n",
      " '\"Permanent link\")\\\\n\\\\n- [Benchmarking on an instance type with NVIDIA GPUs '\n",
      " 'or AWS '\n",
      " 'Chips](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#benchmarking-on-an-instance-type-with-nvidia-gpus-or-aws-chips)\\\\n- '\n",
      " '[Benchmarking on an instance type with NVIDIA GPU and the Triton inference '\n",
      " 'server](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#benchmarking-on-an-instance-type-with-nvidia-gpu-and-the-triton-inference-server)\\\\n- '\n",
      " '[Benchmarking on an instance type with AWS Chips and the Triton inference '\n",
      " 'server](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#benchmarking-on-an-instance-type-with-aws-chips-and-the-triton-inference-server)\\\\n- '\n",
      " '[Benchmarking on an CPU instance type with AMD '\n",
      " 'processors](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#benchmarking-on-an-cpu-instance-type-with-amd-processors)\\\\n- '\n",
      " '[Benchmarking on an CPU instance type with Intel '\n",
      " 'processors](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#benchmarking-on-an-cpu-instance-type-with-intel-processors)\\\\n- '\n",
      " '[Benchmarking on an CPU instance type with ARM processors (Graviton '\n",
      " '4)](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#benchmarking-on-an-cpu-instance-type-with-arm-processors)\\\\n\\\\n- '\n",
      " '[Benchmarking the Triton inference '\n",
      " 'server](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#benchmarking-the-triton-inference-server)\\\\n\\\\n- '\n",
      " '[Benchmarking models on '\n",
      " 'Ollama](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#benchmarking-models-on-ollama)\\\\n\\\\n## '\n",
      " 'Benchmarking on an instance type with NVIDIA GPUs or AWS Chips '\n",
      " '[Â¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html\\\\\\\\#benchmarking-on-an-instance-type-with-nvidia-gpus-or-aws-chips '\n",
      " '\"Permanent link\")\\\\n\\\\n1. Connect to your instance using any of the options '\n",
      " 'in EC2 (SSH/EC2 Connect), run the following in the EC2 terminal. This '\n",
      " 'command installs `uv` on the instance which is then used to create a new '\n",
      " 'virtual environment for `FMBench`.\\\\n\\\\n\\\\n\\\\n```md-code__content\\\\ncurl '\n",
      " '-LsSf https://astral.sh/uv/install.sh | sh\\\\nexport '\n",
      " 'PATH=\"$HOME/.local/bin:$PATH\"\\\\n\\\\n```\\\\n\\\\n2. Install `docker-compose`.\\'), '\n",
      " \"Document(id='02ceb37c-a4f6-4197-b323-f1bfabf06b5f', metadata={'url': \"\n",
      " \"'https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html', \"\n",
      " \"'title': 'EC2 - Foundation Model Benchmarking Tool (FMBench)', 'favicon': \"\n",
      " \"{}, 'language': 'en', 'scrapeId': '5c876876-7753-46a4-a81a-53ce6e9e2438', \"\n",
      " \"'viewport': 'width=device-width,initial-scale=1', 'generator': \"\n",
      " \"'mkdocs-1.6.1, mkdocs-material-9.6.11', 'sourceURL': \"\n",
      " \"'https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html', \"\n",
      " \"'statusCode': 200, 'theme-color': '#00000000', 'color-scheme': 'normal'}, \"\n",
      " \"page_content='```\\\\n\\\\n03. Build the `vllm` container for serving the \"\n",
      " 'model.\\\\n    1. ðŸ‘‰ The `vllm` container we are building locally is going to '\n",
      " 'be references in the `FMBench` config file.\\\\n\\\\n    2. The container being '\n",
      " 'build is for CPU only (GPU support might be added in '\n",
      " 'future).\\\\n\\\\n\\\\n\\\\n       ```md-code__content\\\\n       # Clone the vLLM '\n",
      " 'project repository from GitHub\\\\n       git clone '\n",
      " 'https://github.com/vllm-project/vllm.git\\\\n\\\\n       # Change the directory '\n",
      " 'to the cloned vLLM project\\\\n       cd vllm\\\\n\\\\n       # Build a Docker '\n",
      " 'image using the provided Dockerfile for CPU, with a shared memory size of '\n",
      " '4GB\\\\n       sudo docker build -f Dockerfile.cpu -t vllm-cpu-env '\n",
      " '--shm-size=4g .\\\\n\\\\n       ```\\\\n04. Create local directory structure '\n",
      " 'needed for `FMBench` and copy all publicly available dependencies from the '\n",
      " 'AWS S3 bucket for `FMBench`. This is done by running the '\n",
      " '`copy_s3_content.sh` script available as part of the `FMBench` repo. '\n",
      " '**Replace `/tmp` in the command below with a different path if you want to '\n",
      " 'store the config files and the `FMBench` generated data in a different '\n",
      " 'directory**.\\\\n\\\\n\\\\n\\\\n    ```md-code__content\\\\n    # Replace \"/tmp\" with '\n",
      " '\"/path/to/your/custom/tmp\" if you want to use a custom tmp directory\\\\n    '\n",
      " 'TMP_DIR=\"/tmp\"\\\\n    curl -s '\n",
      " 'https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/copy_s3_content.sh '\n",
      " '| sh -s -- \"$TMP_DIR\"\\\\n\\\\n    ```\\\\n\\\\n05. To download the model files from '\n",
      " 'HuggingFace, create a `hf_token.txt` file in the '\n",
      " '`/tmp/fmbench-read/scripts/` directory containing the Hugging Face token you '\n",
      " 'would like to use. In the command below replace the `hf_yourtokenstring` '\n",
      " 'with your Hugging Face token. **Replace `/tmp` in the command below if you '\n",
      " 'are using `/path/to/your/custom/tmp` to store the config files and the '\n",
      " '`FMBench` generated data**.\\\\n\\\\n\\\\n\\\\n    ```md-code__content\\\\n    echo '\n",
      " 'hf_yourtokenstring > $TMP_DIR/fmbench-read/scripts/hf_token.txt\\\\n\\\\n    '\n",
      " '```\\\\n\\\\n06. Before running FMBench, add the current user to the docker '\n",
      " 'group. Run the following commands to run Docker without needing to use '\n",
      " '`sudo` each time.\\\\n\\\\n\\\\n\\\\n    ```md-code__content\\\\n    sudo usermod -a '\n",
      " '-G docker $USER\\\\n    newgrp docker\\\\n\\\\n    ```\\\\n\\\\n07. Install '\n",
      " '`docker-compose`.\\\\n\\\\n\\\\n\\\\n    ```md-code__content\\\\n    '\n",
      " 'DOCKER_CONFIG=${DOCKER_CONFIG:-$HOME/.docker}\\\\n    mkdir -p '\n",
      " '$DOCKER_CONFIG/cli-plugins\\\\n    sudo curl -L '\n",
      " 'https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname '\n",
      " '-s)-$(uname -m) -o $DOCKER_CONFIG/cli-plugins/docker-compose\\\\n    sudo '\n",
      " 'chmod +x $DOCKER_CONFIG/cli-plugins/docker-compose\\\\n    docker compose '\n",
      " 'version\\\\n\\\\n    ```\\\\n\\\\n08. Run `FMBench` with a packaged or a custom '\n",
      " 'config file. **_This step will also deploy the model on the EC2 instance_**. '\n",
      " 'The `--write-bucket` parameter value is just a placeholder and an actual S3 '\n",
      " 'bucket is not required. You could set the `--tmp-dir` flag to an EFA path '\n",
      " 'instead of `/tmp` if using a shared path for storing config files and '\n",
      " 'reports.\\\\n\\\\n\\\\n\\\\n    ```md-code__content\\\\n    fmbench --config-file '\n",
      " '$TMP_DIR/fmbench-read/configs/llama3/8b/config-ec2-llama3-8b-m7a-16xlarge.yml '\n",
      " '--local-mode yes --write-bucket placeholder --tmp-dir $TMP_DIR > fmbench.log '\n",
      " '2>&1\\\\n\\\\n    ```\\\\n\\\\n09. Open a new Terminal and and do a `tail` on '\n",
      " '`fmbench.log` to see a live log of the run.\\\\n\\\\n\\\\n\\\\n    '\n",
      " '```md-code__content\\\\n    tail -f fmbench.log\\\\n\\\\n    ```\\\\n\\\\n10. All '\n",
      " 'metrics are stored in the `/tmp/fmbench-write` directory created '\n",
      " 'automatically by the `fmbench` package. Once the run completes all files are '\n",
      " 'copied locally in a `results-*` folder as usual.\\\\n\\\\n\\\\n## Benchmarking on '\n",
      " 'an CPU instance type with Intel processors '\n",
      " '[Â¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html\\\\\\\\#benchmarking-on-an-cpu-instance-type-with-intel-processors '\n",
      " '\"Permanent link\")\\\\n\\\\n**_As of 2024-08-27 this has been tested on '\n",
      " \"`c5.18xlarge` and `m5.16xlarge` instances_**'), \"\n",
      " \"Document(id='8d2efce3-9782-4177-b231-90d29b5d7a38', metadata={'url': \"\n",
      " \"'https://aws-samples.github.io/foundation-model-benchmarking-tool/quickstart.html', \"\n",
      " \"'title': 'SageMaker - Foundation Model Benchmarking Tool (FMBench)', \"\n",
      " \"'favicon': {}, 'language': 'en', 'scrapeId': \"\n",
      " \"'5db06fd3-85d9-452f-a2b6-004dc8410ef3', 'viewport': \"\n",
      " \"'width=device-width,initial-scale=1', 'generator': 'mkdocs-1.6.1, \"\n",
      " \"mkdocs-material-9.6.11', 'sourceURL': \"\n",
      " \"'https://aws-samples.github.io/foundation-model-benchmarking-tool/quickstart.html', \"\n",
      " \"'statusCode': 200, 'theme-color': '#00000000', 'color-scheme': 'normal'}, \"\n",
      " \"page_content='2. It uses a simple relationship of 750 words equals 1000 \"\n",
      " 'tokens, to get a more accurate representation of token counts use the '\n",
      " '`Llama2 tokenizer` (instructions are provided in the next section). **_It is '\n",
      " 'strongly recommended that for more accurate results on token throughput you '\n",
      " 'use a tokenizer specific to the model you are testing rather than the '\n",
      " 'default tokenizer. See instructions provided later in this document on how '\n",
      " 'to use a custom tokenizer_**.\\\\n\\\\n\\\\n\\\\n      ```md-code__content\\\\n      '\n",
      " 'account=`aws sts get-caller-identity | jq .Account | tr -d '\n",
      " '\\\\\\'\"\\\\\\'`\\\\n      region=`aws configure get region`\\\\n      fmbench '\n",
      " '--config-file '\n",
      " 's3://sagemaker-fmbench-read-${region}-${account}/configs/llama2/7b/config-llama2-7b-g5-quick.yml '\n",
      " '> fmbench.log 2>&1\\\\n\\\\n      ```\\\\n\\\\n3. Open another terminal window and '\n",
      " 'do a `tail -f` on the `fmbench.log` file to see all the traces being '\n",
      " 'generated at runtime.\\\\n\\\\n\\\\n\\\\n      ```md-code__content\\\\n      tail -f '\n",
      " 'fmbench.log\\\\n\\\\n      ```\\\\n\\\\n4. ðŸ‘‰ For streaming support on SageMaker and '\n",
      " 'Bedrock checkout these config files:\\\\n      1. '\n",
      " '[config-llama3-8b-g5-streaming.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/src/configs/llama3/8b/config-llama3-8b-g5-streaming.yml)\\\\n      '\n",
      " '2. '\n",
      " '[config-bedrock-llama3-streaming.yml](https://aws-samples.github.io/foundation-model-benchmarking-tool/src/configs/bedrock/config-bedrock-llama3-streaming.yml)\\\\n4. '\n",
      " 'The generated reports and metrics are available in the '\n",
      " '`sagemaker-fmbench-write-<replace_w_your_aws_region>-<replace_w_your_aws_account_id>` '\n",
      " 'bucket. The metrics and report files are also downloaded locally and in the '\n",
      " '`results` directory (created by `FMBench`) and the benchmarking report is '\n",
      " 'available as a markdown file called `report.md` in the `results` directory. '\n",
      " 'You can view the rendered Markdown report in the SageMaker notebook itself '\n",
      " 'or download the metrics and report files to your machine for offline '\n",
      " 'analysis.\\\\n\\\\n\\\\n## `FMBench` on GovCloud '\n",
      " '[Â¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/quickstart.html\\\\\\\\#fmbench-on-govcloud '\n",
      " '\"Permanent link\")\\\\n\\\\nNo special steps are required for running `FMBench` '\n",
      " 'on GovCloud. The CloudFormation link for `us-gov-west-1` has been provided '\n",
      " 'in the section above.\\\\n\\\\n1. Not all models available via Bedrock or other '\n",
      " 'services may be available in GovCloud. The following commands show how to '\n",
      " 'run `FMBench` to benchmark the [Amazon Titan Text '\n",
      " 'Express](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-text-models.html#titantx-express) '\n",
      " 'model in the GovCloud. See the [Amazon Bedrock '\n",
      " 'GovCloud](https://docs.aws.amazon.com/govcloud-us/latest/UserGuide/govcloud-bedrock.html) '\n",
      " 'page for more details.\\\\n\\\\n\\\\n\\\\n```md-code__content\\\\naccount=`aws sts '\n",
      " 'get-caller-identity | jq .Account | tr -d \\\\\\'\"\\\\\\'`\\\\nregion=`aws configure '\n",
      " 'get region`\\\\nfmbench --config-file '\n",
      " 's3://sagemaker-fmbench-read-${region}-${account}/configs/bedrock/config-bedrock-titan-text-express.yml '\n",
      " \"> fmbench.log 2>&1\\\\n\\\\n```'), \"\n",
      " \"Document(id='4625751d-da8d-450f-83ee-ad520727491c', metadata={'url': \"\n",
      " \"'https://aws-samples.github.io/foundation-model-benchmarking-tool/deepseek.html', \"\n",
      " \"'title': 'Deepseek - Foundation Model Benchmarking Tool (FMBench)', \"\n",
      " \"'favicon': {}, 'language': 'en', 'scrapeId': \"\n",
      " \"'a0a4b982-2579-44f2-8d7d-5987e0ec2bbf', 'viewport': \"\n",
      " \"'width=device-width,initial-scale=1', 'generator': 'mkdocs-1.6.1, \"\n",
      " \"mkdocs-material-9.6.11', 'sourceURL': \"\n",
      " \"'https://aws-samples.github.io/foundation-model-benchmarking-tool/deepseek.html', \"\n",
      " \"'statusCode': 200, 'theme-color': '#00000000', 'color-scheme': 'normal'}, \"\n",
      " \"page_content='[Skip to \"\n",
      " 'content](https://aws-samples.github.io/foundation-model-benchmarking-tool/deepseek.html#deepseek-r1)\\\\n\\\\n# '\n",
      " 'DeepSeek-R1 '\n",
      " '[Â¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/deepseek.html\\\\\\\\#deepseek-r1 '\n",
      " '\"Permanent link\")\\\\n\\\\nThe distilled version of Deepseek-R1 models are now '\n",
      " 'supported for both performance benchmarking and model evaluations ðŸŽ‰. You can '\n",
      " 'use built in support for 4 different datasets: '\n",
      " '[`LongBench`](https://huggingface.co/datasets/THUDM/LongBench), '\n",
      " '[`Dolly`](https://huggingface.co/datasets/databricks/databricks-dolly-15k), '\n",
      " '[`OpenOrca`](https://huggingface.co/datasets/Open-Orca/OpenOrca) and '\n",
      " '[`ConvFinQA`](https://huggingface.co/datasets/AdaptLLM/finance-tasks/tree/refs%2Fconvert%2Fparquet/ConvFinQA). '\n",
      " 'You can deploy the Deepseek-R1 distilled models on Amazon EC2, Amazon '\n",
      " 'Bedrock or Amazon SageMaker.\\\\n\\\\nThe easiest way to benchmark the DeepSeek '\n",
      " 'models is through the '\n",
      " '[`FMBench-orchestrator`](https://github.com/awslabs/fmbench-orchestrator) on '\n",
      " 'Amazon EC2 VMs.\\\\n\\\\n## Benchmark Deepseek-R1 distilled models on Amazon EC2 '\n",
      " '[Â¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/deepseek.html\\\\\\\\#benchmark-deepseek-r1-distilled-models-on-amazon-ec2 '\n",
      " '\"Permanent link\")\\\\n\\\\nðŸ‘‰ Make sure your account has enough service quota for '\n",
      " 'vCPUs to run this benchmark. We would be using `g6e.xlarge`, `g6e.2xlarge`, '\n",
      " '`g6e.12xlarge` and `g6e.48xlarge` instances, if you do not have sufficient '\n",
      " 'service quota then you can set `deploy: no` in the '\n",
      " '`configs/deepseek/deepseek-convfinqa.yml` (or other) file to disable some '\n",
      " 'tests as needed.\\\\n\\\\nFollow instructions '\n",
      " '[here](https://github.com/awslabs/fmbench-orchestrator?tab=readme-ov-file#install-fmbench-orchestrator-on-ec2) '\n",
      " 'to install the orchestrator. Once installed you can run Deepseek-r1 '\n",
      " 'benchmarking with the '\n",
      " '[`ConvFinQA`](https://huggingface.co/datasets/AdaptLLM/finance-tasks/tree/refs%2Fconvert%2Fparquet/ConvFinQA) '\n",
      " 'dataset the following command:\\\\n\\\\n```md-code__content\\\\npython main.py '\n",
      " '--config-file configs/deepseek/deepseek-convfinqa.yml\\\\n\\\\n```\\\\n\\\\nChange '\n",
      " 'the `--config-file` parameter to '\n",
      " '[`configs/deepseek/deepseek-longbench.yml`](https://github.com/aws-samples/fmbench-orchestrator/blob/main/configs/deepseek/deepseek-longbench.yml) '\n",
      " 'or '\n",
      " '[`configs/deepseek/deepseek-openorca.yml`](https://github.com/aws-samples/fmbench-orchestrator/blob/main/configs/deepseek/deepseek-openorca.yml) '\n",
      " 'to use other datasets for benchmarking. These orchestrator files test '\n",
      " 'various Deepseek-R1 distilled models on `g6e` instances, edit this file as '\n",
      " 'per your requirements.\\\\n\\\\n## Benchmark Deepseek-R1 quantized models on '\n",
      " 'Amazon EC2 '\n",
      " '[Â¶](https://aws-samples.github.io/foundation-model-benchmarking-tool/deepseek.html\\\\\\\\#benchmark-deepseek-r1-quantized-models-on-amazon-ec2 '\n",
      " '\"Permanent link\")\\\\n\\\\nðŸ‘‰ Make sure your account has enough service quota for '\n",
      " 'vCPUs to run this benchmark. We would be using `g6e.12xlarge` instance for '\n",
      " 'this test.\\\\n\\\\n1. Create a `g6e.12xlarge` instance and run the `DeepSeek-R1 '\n",
      " '1.58b quantized` model on this instance by following the steps 1 through 8 '\n",
      " 'described '\n",
      " '[here](https://github.com/aarora79/deepseek-r1-ec2?tab=readme-ov-file#quantized-models).\\\\n\\\\n2. '\n",
      " 'Follow steps 1 through 5 '\n",
      " '[here](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#benchmarking-on-an-instance-type-with-nvidia-gpus-or-aws-chips) '\n",
      " 'to setup `FMBench` on this instance.\\\\n\\\\n3. Next run the following command '\n",
      " 'to benchmark '\n",
      " 'LongBench\\\\n\\\\n\\\\n\\\\n```md-code__content\\\\nTMP_DIR=/tmp\\\\nfmbench '\n",
      " '--config-file '\n",
      " '$TMP_DIR/fmbench-read/configs/deepseek/config-deepseek-r1-quant1.58-longbench-byoe.yml '\n",
      " '--local-mode yes --write-bucket placeholder --tmp-dir $TMP_DIR > fmbench.log '\n",
      " '2>&1\\\\n\\\\n```\\\\n\\\\n4. Once the run completes you should see the benchmarking '\n",
      " 'results in a folder called `results-DeepSeek-R1-quant-1.58bit-g6e.12xl` '\n",
      " \"present in your current directory.'), \"\n",
      " \"Document(id='faaaae8e-d2b7-4264-96d6-1b72ca1d45e7', metadata={'url': \"\n",
      " \"'https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-g6e-2xlarge-byoe-ollama.yml', \"\n",
      " \"'scrapeId': '7a077514-a254-4d8b-8484-5884222bcf5e', 'sourceURL': \"\n",
      " \"'https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-g6e-2xlarge-byoe-ollama.yml', \"\n",
      " \"'statusCode': 200, 'color-scheme': 'light dark'}, page_content='# \"\n",
      " 'Configuration for experiments to be run. The experiments section is an '\n",
      " 'array\\\\n# so more than one experiments can be added, these could belong to '\n",
      " 'the same model\\\\n# but different instance types, or different models, or '\n",
      " 'even different hosting\\\\n# options.\\\\nexperiments:\\\\n  - name: '\n",
      " '\"llama3.1-8b\"\\\\n    # AWS region, this parameter is templatized, no need to '\n",
      " 'change\\\\n    region: {region}\\\\n    # model_id is interpreted in conjunction '\n",
      " 'with the deployment_script, so if you\\\\n    # use a JumpStart model id then '\n",
      " 'set the deployment_script to jumpstart.py.\\\\n    # if deploying directly '\n",
      " 'from HuggingFace this would be a HuggingFace model id\\\\n    # see the DJL '\n",
      " 'serving deployment script in the code repo for reference.\\\\n    #from '\n",
      " 'huggingface to grab\\\\n    model_id: meta-llama/Llama-3.1-8B-Instruct # model '\n",
      " 'id, version and image uri not needed for byo endpoint\\\\n    '\n",
      " 'model_version:\\\\n    model_name: \"meta-llama/Llama-3.1-8B-Instruct\"\\\\n    # '\n",
      " 'this can be changed to the IP address of your specific EC2 instance where '\n",
      " 'the model is hosted\\\\n    ep_name: '\n",
      " \"\\\\'http://localhost:11434/api/generate\\\\'\\\\n    instance_type: \"\n",
      " '\"g6e.2xlarge\"\\\\n    image_uri: None\\\\n    deploy: no #setting to yes to run '\n",
      " 'deployment script for ec2\\\\n    instance_count:\\\\n    deployment_script: '\n",
      " 'ec2_deploy.py\\\\n    # FMBench comes packaged with multiple inference '\n",
      " 'scripts, such as scripts for SageMaker\\\\n    # and Bedrock. You can also add '\n",
      " 'your own. This is an example for a rest DJL predictor\\\\n    # for a '\n",
      " 'llama3-8b-instruct deployed on ec2\\\\n    inference_script: '\n",
      " 'ec2_predictor.py\\\\n    # This section defines the settings for Amazon EC2 '\n",
      " 'instances\\\\n    ec2:\\\\n      # Privileged Mode makes the docker container '\n",
      " 'run with root.\\\\n      # This basically means that if you are root in a '\n",
      " 'container you have the privileges of root on the host system\\\\n      # Only '\n",
      " 'need this if you need to set VLLM_CPU_OMP_THREADS_BIND env variable.\\\\n      '\n",
      " 'privileged_mode: yes\\\\n      # The following line specifies the runtime and '\n",
      " \"GPU settings for the instance\\\\n      # \\\\'--runtime=nvidia\\\\' tells the \"\n",
      " \"container runtime to use the NVIDIA runtime\\\\n      # \\\\'--gpus all\\\\' makes \"\n",
      " \"all GPUs available to the container\\\\n      # \\\\'--shm-size 12g\\\\' sets the \"\n",
      " 'size of the shared memory to 12 gigabytes\\\\n      '\n",
      " 'gpu_or_neuron_setting:\\\\n      # This setting specifies the timeout (in '\n",
      " 'seconds) for loading the model. In this case, the timeout is set to 2400 '\n",
      " 'seconds, which is 40 minutes.\\\\n      # If the model takes longer than 40 '\n",
      " 'minutes to load, the process will time out and fail.\\\\n      '\n",
      " 'model_loading_timeout: 2400\\\\n    inference_spec:\\\\n      # this should '\n",
      " 'match one of the sections in the inference_parameters section above\\\\n      '\n",
      " 'parameter_set: ec2_ollama\\\\n      # if not set assume djl\\\\n      '\n",
      " 'container_type: ollama\\\\n    # modify the serving properties to match your '\n",
      " 'model and requirements\\\\n    serving.properties:\\\\n    # runs are done for '\n",
      " 'each combination of payload file and concurrency level\\\\n    '\n",
      " 'payload_files:\\\\n    - payload_en_1-500.jsonl\\\\n    - '\n",
      " 'payload_en_500-1000.jsonl\\\\n    - payload_en_1000-2000.jsonl\\\\n    - '\n",
      " 'payload_en_2000-3000.jsonl\\\\n    - payload_en_3000-3840.jsonl\\\\n    # '\n",
      " 'concurrency level refers to number of requests sent in parallel to an '\n",
      " 'endpoint\\\\n    # the next set of requests is sent once responses for all '\n",
      " 'concurrent requests have\\\\n    # been received.\\\\n    '\n",
      " 'concurrency_levels:\\\\n    - 1\\\\n    - 2\\\\n    - 3\\\\n    - 4\\\\n\\\\nreport:\\\\n  '\n",
      " 'latency_budget: 3\\\\n  cost_per_10k_txn_budget: 50\\\\n  error_rate_budget: '\n",
      " '0\\\\n  per_inference_request_file: per_inference_request_results.csv\\\\n  '\n",
      " 'all_metrics_file: all_metrics.csv\\\\n  txn_count_for_showing_cost: 10000\\\\n  '\n",
      " 'v_shift_w_single_instance: 0.025\\\\n  v_shift_w_gt_one_instance: '\n",
      " \"0.025\\\\n\\\\n```')], 'answer': 'Yes, you can benchmark on an instance type \"\n",
      " 'with an NVIDIA GPU on g6e instances using the Foundation Model Benchmarking '\n",
      " 'Tool (FMBench). Here are the steps to follow:\\\\n\\\\n1. **Connect to your EC2 '\n",
      " 'instance**: Use SSH or EC2 Connect to access your instance.\\\\n\\\\n2. '\n",
      " '**Install `uv`**: Run the following command to install `uv` on your '\n",
      " 'instance, which is used to create a new virtual environment for '\n",
      " '`FMBench`.\\\\n\\\\n    ```sh\\\\n    curl -LsSf https://astral.sh/uv/install.sh | '\n",
      " 'sh\\\\n    export PATH=\"$HOME/.local/bin:$PATH\"\\\\n    ```\\\\n\\\\n3. **Install '\n",
      " '`docker-compose`**:\\\\n\\\\n    ```sh\\\\n    '\n",
      " 'DOCKER_CONFIG=${DOCKER_CONFIG:-$HOME/.docker}\\\\n    mkdir -p '\n",
      " '$DOCKER_CONFIG/cli-plugins\\\\n    sudo curl -L '\n",
      " 'https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname '\n",
      " '-s)-$(uname -m) -o $DOCKER_CONFIG/cli-plugins/docker-compose\\\\n    sudo '\n",
      " 'chmod +x $DOCKER_CONFIG/cli-plugins/docker-compose\\\\n    docker compose '\n",
      " 'version\\\\n    ```\\\\n\\\\n4. **Build the `vllm` container**:\\\\n\\\\n    '\n",
      " '```sh\\\\n    # Clone the vLLM project repository from GitHub\\\\n    git clone '\n",
      " 'https://github.com/vllm-project/vllm.git\\\\n\\\\n    # Change the directory to '\n",
      " 'the cloned vLLM project\\\\n    cd vllm\\\\n\\\\n    # Build a Docker image using '\n",
      " 'the provided Dockerfile for CPU with a shared memory size of 4GB\\\\n    sudo '\n",
      " 'docker build -f Dockerfile.cpu -t vllm-cpu-env --shm-size=4g .\\\\n    '\n",
      " '```\\\\n\\\\n5. **Create local directory structure and copy '\n",
      " 'dependencies**:\\\\n\\\\n    ```sh\\\\n    # Replace \"/tmp\" with '\n",
      " '\"/path/to/your/custom/tmp\" if you want to use a custom tmp directory\\\\n    '\n",
      " 'TMP_DIR=\"/tmp\"\\\\n    curl -s '\n",
      " 'https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/copy_s3_content.sh '\n",
      " '| sh -s -- \"$TMP_DIR\"\\\\n    ```\\\\n\\\\n6. **Create Hugging Face token '\n",
      " 'file**:\\\\n\\\\n    ```sh\\\\n    echo hf_yourtokenstring > '\n",
      " '$TMP_DIR/fmbench-read/scripts/hf_token.txt\\\\n    ```\\\\n\\\\n7. **Add current '\n",
      " 'user to the docker group**:\\\\n\\\\n    ```sh\\\\n    sudo usermod -a -G docker '\n",
      " '$USER\\\\n    newgrp docker\\\\n    ```\\\\n\\\\n8. **Run `FMBench`**:\\\\n\\\\n    '\n",
      " '```sh\\\\n    fmbench --config-file '\n",
      " '$TMP_DIR/fmbench-read/configs/llama3/8b/config-ec2-llama3-8b-m7a-16xlarge.yml '\n",
      " '--local-mode yes --write-bucket placeholder --tmp-dir $TMP_DIR > fmbench.log '\n",
      " '2>&1\\\\n    ```\\\\n\\\\n9. **Monitor the run**:\\\\n\\\\n    ```sh\\\\n    tail -f '\n",
      " 'fmbench.log\\\\n    ```\\\\n\\\\n10. **Check results**:\\\\n\\\\n    All metrics are '\n",
      " 'stored in the `/tmp/fmbench-write` directory created automatically by the '\n",
      " '`fmbench` package. Once the run completes, all files are copied locally in a '\n",
      " '`results-*` folder as usual.\\\\n\\\\nFor more details, refer to the [official '\n",
      " \"documentation](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#benchmarking-on-an-instance-type-with-nvidia-gpus-or-aws-chips).'}\\n\"\n",
      " '\\n')\n",
      "citations=Source(s): https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html\n",
      "https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html\n",
      "https://aws-samples.github.io/foundation-model-benchmarking-tool/quickstart.html\n",
      "https://aws-samples.github.io/foundation-model-benchmarking-tool/deepseek.html\n",
      "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-g6e-2xlarge-byoe-ollama.yml\n",
      "answer=Yes, you can benchmark on an instance type with an NVIDIA GPU on g6e instances using the Foundation Model Benchmarking Tool (FMBench). Here are the steps to follow:\n",
      "\n",
      "1. **Connect to your EC2 instance**: Use SSH or EC2 Connect to access your instance.\n",
      "\n",
      "2. **Install `uv`**: Run the following command to install `uv` on your instance, which is used to create a new virtual environment for `FMBench`.\n",
      "\n",
      "    ```sh\n",
      "    curl -LsSf https://astral.sh/uv/install.sh | sh\n",
      "    export PATH=\"$HOME/.local/bin:$PATH\"\n",
      "    ```\n",
      "\n",
      "3. **Install `docker-compose`**:\n",
      "\n",
      "    ```sh\n",
      "    DOCKER_CONFIG=${DOCKER_CONFIG:-$HOME/.docker}\n",
      "    mkdir -p $DOCKER_CONFIG/cli-plugins\n",
      "    sudo curl -L https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m) -o $DOCKER_CONFIG/cli-plugins/docker-compose\n",
      "    sudo chmod +x $DOCKER_CONFIG/cli-plugins/docker-compose\n",
      "    docker compose version\n",
      "    ```\n",
      "\n",
      "4. **Build the `vllm` container**:\n",
      "\n",
      "    ```sh\n",
      "    # Clone the vLLM project repository from GitHub\n",
      "    git clone https://github.com/vllm-project/vllm.git\n",
      "\n",
      "    # Change the directory to the cloned vLLM project\n",
      "    cd vllm\n",
      "\n",
      "    # Build a Docker image using the provided Dockerfile for CPU with a shared memory size of 4GB\n",
      "    sudo docker build -f Dockerfile.cpu -t vllm-cpu-env --shm-size=4g .\n",
      "    ```\n",
      "\n",
      "5. **Create local directory structure and copy dependencies**:\n",
      "\n",
      "    ```sh\n",
      "    # Replace \"/tmp\" with \"/path/to/your/custom/tmp\" if you want to use a custom tmp directory\n",
      "    TMP_DIR=\"/tmp\"\n",
      "    curl -s https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/copy_s3_content.sh | sh -s -- \"$TMP_DIR\"\n",
      "    ```\n",
      "\n",
      "6. **Create Hugging Face token file**:\n",
      "\n",
      "    ```sh\n",
      "    echo hf_yourtokenstring > $TMP_DIR/fmbench-read/scripts/hf_token.txt\n",
      "    ```\n",
      "\n",
      "7. **Add current user to the docker group**:\n",
      "\n",
      "    ```sh\n",
      "    sudo usermod -a -G docker $USER\n",
      "    newgrp docker\n",
      "    ```\n",
      "\n",
      "8. **Run `FMBench`**:\n",
      "\n",
      "    ```sh\n",
      "    fmbench --config-file $TMP_DIR/fmbench-read/configs/llama3/8b/config-ec2-llama3-8b-m7a-16xlarge.yml --local-mode yes --write-bucket placeholder --tmp-dir $TMP_DIR > fmbench.log 2>&1\n",
      "    ```\n",
      "\n",
      "9. **Monitor the run**:\n",
      "\n",
      "    ```sh\n",
      "    tail -f fmbench.log\n",
      "    ```\n",
      "\n",
      "10. **Check results**:\n",
      "\n",
      "    All metrics are stored in the `/tmp/fmbench-write` directory created automatically by the `fmbench` package. Once the run completes, all files are copied locally in a `results-*` folder as usual.\n",
      "\n",
      "For more details, refer to the [official documentation](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#benchmarking-on-an-instance-type-with-nvidia-gpus-or-aws-chips).\n",
      "\n",
      "Source(s): https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html\n",
      "https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html\n",
      "https://aws-samples.github.io/foundation-model-benchmarking-tool/quickstart.html\n",
      "https://aws-samples.github.io/foundation-model-benchmarking-tool/deepseek.html\n",
      "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-g6e-2xlarge-byoe-ollama.yml\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "# Use the RAG system to answer the question\n",
    "question = \"can we benchmark on nvidia gpu on g6e instances\"\n",
    "result = _rag_system.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you can benchmark on an instance type with an NVIDIA GPU on g6e instances using the Foundation Model Benchmarking Tool (FMBench). Here are the steps to follow:\n",
      "\n",
      "1. **Connect to your EC2 instance**: Use SSH or EC2 Connect to access your instance.\n",
      "\n",
      "2. **Install `uv`**: Run the following command to install `uv` on your instance, which is used to create a new virtual environment for `FMBench`.\n",
      "\n",
      "    ```sh\n",
      "    curl -LsSf https://astral.sh/uv/install.sh | sh\n",
      "    export PATH=\"$HOME/.local/bin:$PATH\"\n",
      "    ```\n",
      "\n",
      "3. **Install `docker-compose`**:\n",
      "\n",
      "    ```sh\n",
      "    DOCKER_CONFIG=${DOCKER_CONFIG:-$HOME/.docker}\n",
      "    mkdir -p $DOCKER_CONFIG/cli-plugins\n",
      "    sudo curl -L https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m) -o $DOCKER_CONFIG/cli-plugins/docker-compose\n",
      "    sudo chmod +x $DOCKER_CONFIG/cli-plugins/docker-compose\n",
      "    docker compose version\n",
      "    ```\n",
      "\n",
      "4. **Build the `vllm` container**:\n",
      "\n",
      "    ```sh\n",
      "    # Clone the vLLM project repository from GitHub\n",
      "    git clone https://github.com/vllm-project/vllm.git\n",
      "\n",
      "    # Change the directory to the cloned vLLM project\n",
      "    cd vllm\n",
      "\n",
      "    # Build a Docker image using the provided Dockerfile for CPU with a shared memory size of 4GB\n",
      "    sudo docker build -f Dockerfile.cpu -t vllm-cpu-env --shm-size=4g .\n",
      "    ```\n",
      "\n",
      "5. **Create local directory structure and copy dependencies**:\n",
      "\n",
      "    ```sh\n",
      "    # Replace \"/tmp\" with \"/path/to/your/custom/tmp\" if you want to use a custom tmp directory\n",
      "    TMP_DIR=\"/tmp\"\n",
      "    curl -s https://raw.githubusercontent.com/aws-samples/foundation-model-benchmarking-tool/main/copy_s3_content.sh | sh -s -- \"$TMP_DIR\"\n",
      "    ```\n",
      "\n",
      "6. **Create Hugging Face token file**:\n",
      "\n",
      "    ```sh\n",
      "    echo hf_yourtokenstring > $TMP_DIR/fmbench-read/scripts/hf_token.txt\n",
      "    ```\n",
      "\n",
      "7. **Add current user to the docker group**:\n",
      "\n",
      "    ```sh\n",
      "    sudo usermod -a -G docker $USER\n",
      "    newgrp docker\n",
      "    ```\n",
      "\n",
      "8. **Run `FMBench`**:\n",
      "\n",
      "    ```sh\n",
      "    fmbench --config-file $TMP_DIR/fmbench-read/configs/llama3/8b/config-ec2-llama3-8b-m7a-16xlarge.yml --local-mode yes --write-bucket placeholder --tmp-dir $TMP_DIR > fmbench.log 2>&1\n",
      "    ```\n",
      "\n",
      "9. **Monitor the run**:\n",
      "\n",
      "    ```sh\n",
      "    tail -f fmbench.log\n",
      "    ```\n",
      "\n",
      "10. **Check results**:\n",
      "\n",
      "    All metrics are stored in the `/tmp/fmbench-write` directory created automatically by the `fmbench` package. Once the run completes, all files are copied locally in a `results-*` folder as usual.\n",
      "\n",
      "For more details, refer to the [official documentation](https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html#benchmarking-on-an-instance-type-with-nvidia-gpus-or-aws-chips).\n",
      "\n",
      "Source(s): https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html\n",
      "https://aws-samples.github.io/foundation-model-benchmarking-tool/benchmarking_on_ec2.html\n",
      "https://aws-samples.github.io/foundation-model-benchmarking-tool/quickstart.html\n",
      "https://aws-samples.github.io/foundation-model-benchmarking-tool/deepseek.html\n",
      "https://aws-samples.github.io/foundation-model-benchmarking-tool/configs/llama3.1/8b/config-ec2-llama3-1-8b-g6e-2xlarge-byoe-ollama.yml\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
